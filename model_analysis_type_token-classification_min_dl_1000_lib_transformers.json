[
  {
    "model_id": "dslim/bert-base-NER",
    "model_name": "dslim/bert-base-NER",
    "author": "dslim",
    "downloads": 6069961,
    "likes": 585,
    "tags": [
      "transformers",
      "pytorch",
      "tf",
      "jax",
      "onnx",
      "safetensors",
      "bert",
      "token-classification",
      "en",
      "dataset:conll2003",
      "arxiv:1810.04805",
      "license:mit",
      "model-index",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/dslim/bert-base-NER",
    "dependencies": [
      [
        "transformers",
        null
      ]
    ],
    "analysis_date": "2025-03-26T00:50:47.460418",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "bert",
    "pipeline_tag": "token-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": "en",
      "license": "mit",
      "datasets": [
        "conll2003"
      ],
      "model-index": [
        {
          "name": "dslim/bert-base-NER",
          "results": [
            {
              "task": {
                "type": "token-classification",
                "name": "Token Classification"
              },
              "dataset": {
                "name": "conll2003",
                "type": "conll2003",
                "config": "conll2003",
                "split": "test"
              },
              "metrics": [
                {
                  "type": "accuracy",
                  "value": 0.9118041001560013,
                  "name": "Accuracy",
                  "verified": true
                },
                {
                  "type": "precision",
                  "value": 0.9211550382257732,
                  "name": "Precision",
                  "verified": true
                },
                {
                  "type": "recall",
                  "value": 0.9306415698281261,
                  "name": "Recall",
                  "verified": true
                },
                {
                  "type": "f1",
                  "value": 0.9258740048459675,
                  "name": "F1",
                  "verified": true
                },
                {
                  "type": "loss",
                  "value": 0.48325642943382263,
                  "name": "loss",
                  "verified": true
                }
              ]
            }
          ]
        }
      ]
    },
    "card_text": "# bert-base-NER\n\nIf my open source models have been useful to you, please consider supporting me in building small, useful AI models for everyone (and help me afford med school / help out my parents financially). Thanks!\n\n<a href=\"https://www.buymeacoffee.com/dslim\" target=\"_blank\"><img src=\"https://cdn.buymeacoffee.com/buttons/v2/arial-yellow.png\" alt=\"Buy Me A Coffee\" style=\"height: 60px !important;width: 217px !important;\" ></a>\n\n## Model description\n\n**bert-base-NER** is a fine-tuned BERT model that is ready to use for **Named Entity Recognition** and achieves **state-of-the-art performance** for the NER task. It has been trained to recognize four types of entities: location (LOC), organizations (ORG), person (PER) and Miscellaneous (MISC). \n\nSpecifically, this model is a *bert-base-cased* model that was fine-tuned on the English version of the standard [CoNLL-2003 Named Entity Recognition](https://www.aclweb.org/anthology/W03-0419.pdf) dataset. \n\nIf you'd like to use a larger BERT-large model fine-tuned on the same dataset, a [**bert-large-NER**](https://huggingface.co/dslim/bert-large-NER/) version is also available. \n\n### Available NER models \n| Model Name | Description | Parameters |\n|-------------------|-------------|------------------|\n| [distilbert-NER](https://huggingface.co/dslim/distilbert-NER) **(NEW!)** | Fine-tuned DistilBERT - a smaller, faster, lighter version of BERT | 66M |\n| [bert-large-NER](https://huggingface.co/dslim/bert-large-NER/) | Fine-tuned bert-large-cased - larger model with slightly better performance | 340M |\n| [bert-base-NER](https://huggingface.co/dslim/bert-base-NER)-([uncased](https://huggingface.co/dslim/bert-base-NER-uncased)) | Fine-tuned bert-base, available in both cased and uncased versions | 110M |\n\n\n## Intended uses & limitations\n\n#### How to use\n\nYou can use this model with Transformers *pipeline* for NER.\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\nfrom transformers import pipeline\n\ntokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\nmodel = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n\nnlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\nexample = \"My name is Wolfgang and I live in Berlin\"\n\nner_results = nlp(example)\nprint(ner_results)\n```\n\n#### Limitations and bias\n\nThis model is limited by its training dataset of entity-annotated news articles from a specific span of time. This may not generalize well for all use cases in different domains. Furthermore, the model occassionally tags subword tokens as entities and post-processing of results may be necessary to handle those cases. \n\n## Training data\n\nThis model was fine-tuned on English version of the standard [CoNLL-2003 Named Entity Recognition](https://www.aclweb.org/anthology/W03-0419.pdf) dataset. \n\nThe training dataset distinguishes between the beginning and continuation of an entity so that if there are back-to-back entities of the same type, the model can output where the second entity begins. As in the dataset, each token will be classified as one of the following classes:\n\nAbbreviation|Description\n-|-\nO|Outside of a named entity\nB-MISC |Beginning of a miscellaneous entity right after another miscellaneous entity\nI-MISC | Miscellaneous entity\nB-PER |Beginning of a person’s name right after another person’s name\nI-PER |Person’s name\nB-ORG |Beginning of an organization right after another organization\nI-ORG |organization\nB-LOC |Beginning of a location right after another location\nI-LOC |Location\n\n\n### CoNLL-2003 English Dataset Statistics\nThis dataset was derived from the Reuters corpus which consists of Reuters news stories. You can read more about how this dataset was created in the CoNLL-2003 paper. \n#### # of training examples per entity type\nDataset|LOC|MISC|ORG|PER\n-|-|-|-|-\nTrain|7140|3438|6321|6600\nDev|1837|922|1341|1842\nTest|1668|702|1661|1617\n#### # of articles/sentences/tokens per dataset\nDataset |Articles |Sentences |Tokens\n-|-|-|-\nTrain |946 |14,987 |203,621\nDev |216 |3,466 |51,362\nTest |231 |3,684 |46,435\n\n## Training procedure\n\nThis model was trained on a single NVIDIA V100 GPU with recommended hyperparameters from the [original BERT paper](https://arxiv.org/pdf/1810.04805) which trained & evaluated the model on CoNLL-2003 NER task. \n\n## Eval results\nmetric|dev|test\n-|-|-\nf1 |95.1 |91.3\nprecision |95.0 |90.7\nrecall |95.3 |91.9\n\nThe test metrics are a little lower than the official Google BERT results which encoded document context & experimented with CRF. More on replicating the original results [here](https://github.com/google-research/bert/issues/223).\n\n### BibTeX entry and citation info\n\n```\n@article{DBLP:journals/corr/abs-1810-04805,\n  author    = {Jacob Devlin and\n               Ming{-}Wei Chang and\n               Kenton Lee and\n               Kristina Toutanova},\n  title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language\n               Understanding},\n  journal   = {CoRR},\n  volume    = {abs/1810.04805},\n  year      = {2018},\n  url       = {http://arxiv.org/abs/1810.04805},\n  archivePrefix = {arXiv},\n  eprint    = {1810.04805},\n  timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n```\n```\n@inproceedings{tjong-kim-sang-de-meulder-2003-introduction,\n    title = \"Introduction to the {C}o{NLL}-2003 Shared Task: Language-Independent Named Entity Recognition\",\n    author = \"Tjong Kim Sang, Erik F.  and\n      De Meulder, Fien\",\n    booktitle = \"Proceedings of the Seventh Conference on Natural Language Learning at {HLT}-{NAACL} 2003\",\n    year = \"2003\",\n    url = \"https://www.aclweb.org/anthology/W03-0419\",\n    pages = \"142--147\",\n}\n```\n",
    "card_content": "---\nlanguage: en\nlicense: mit\ndatasets:\n- conll2003\nmodel-index:\n- name: dslim/bert-base-NER\n  results:\n  - task:\n      type: token-classification\n      name: Token Classification\n    dataset:\n      name: conll2003\n      type: conll2003\n      config: conll2003\n      split: test\n    metrics:\n    - type: accuracy\n      value: 0.9118041001560013\n      name: Accuracy\n      verified: true\n    - type: precision\n      value: 0.9211550382257732\n      name: Precision\n      verified: true\n    - type: recall\n      value: 0.9306415698281261\n      name: Recall\n      verified: true\n    - type: f1\n      value: 0.9258740048459675\n      name: F1\n      verified: true\n    - type: loss\n      value: 0.48325642943382263\n      name: loss\n      verified: true\n---\n# bert-base-NER\n\nIf my open source models have been useful to you, please consider supporting me in building small, useful AI models for everyone (and help me afford med school / help out my parents financially). Thanks!\n\n<a href=\"https://www.buymeacoffee.com/dslim\" target=\"_blank\"><img src=\"https://cdn.buymeacoffee.com/buttons/v2/arial-yellow.png\" alt=\"Buy Me A Coffee\" style=\"height: 60px !important;width: 217px !important;\" ></a>\n\n## Model description\n\n**bert-base-NER** is a fine-tuned BERT model that is ready to use for **Named Entity Recognition** and achieves **state-of-the-art performance** for the NER task. It has been trained to recognize four types of entities: location (LOC), organizations (ORG), person (PER) and Miscellaneous (MISC). \n\nSpecifically, this model is a *bert-base-cased* model that was fine-tuned on the English version of the standard [CoNLL-2003 Named Entity Recognition](https://www.aclweb.org/anthology/W03-0419.pdf) dataset. \n\nIf you'd like to use a larger BERT-large model fine-tuned on the same dataset, a [**bert-large-NER**](https://huggingface.co/dslim/bert-large-NER/) version is also available. \n\n### Available NER models \n| Model Name | Description | Parameters |\n|-------------------|-------------|------------------|\n| [distilbert-NER](https://huggingface.co/dslim/distilbert-NER) **(NEW!)** | Fine-tuned DistilBERT - a smaller, faster, lighter version of BERT | 66M |\n| [bert-large-NER](https://huggingface.co/dslim/bert-large-NER/) | Fine-tuned bert-large-cased - larger model with slightly better performance | 340M |\n| [bert-base-NER](https://huggingface.co/dslim/bert-base-NER)-([uncased](https://huggingface.co/dslim/bert-base-NER-uncased)) | Fine-tuned bert-base, available in both cased and uncased versions | 110M |\n\n\n## Intended uses & limitations\n\n#### How to use\n\nYou can use this model with Transformers *pipeline* for NER.\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\nfrom transformers import pipeline\n\ntokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\nmodel = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n\nnlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\nexample = \"My name is Wolfgang and I live in Berlin\"\n\nner_results = nlp(example)\nprint(ner_results)\n```\n\n#### Limitations and bias\n\nThis model is limited by its training dataset of entity-annotated news articles from a specific span of time. This may not generalize well for all use cases in different domains. Furthermore, the model occassionally tags subword tokens as entities and post-processing of results may be necessary to handle those cases. \n\n## Training data\n\nThis model was fine-tuned on English version of the standard [CoNLL-2003 Named Entity Recognition](https://www.aclweb.org/anthology/W03-0419.pdf) dataset. \n\nThe training dataset distinguishes between the beginning and continuation of an entity so that if there are back-to-back entities of the same type, the model can output where the second entity begins. As in the dataset, each token will be classified as one of the following classes:\n\nAbbreviation|Description\n-|-\nO|Outside of a named entity\nB-MISC |Beginning of a miscellaneous entity right after another miscellaneous entity\nI-MISC | Miscellaneous entity\nB-PER |Beginning of a person’s name right after another person’s name\nI-PER |Person’s name\nB-ORG |Beginning of an organization right after another organization\nI-ORG |organization\nB-LOC |Beginning of a location right after another location\nI-LOC |Location\n\n\n### CoNLL-2003 English Dataset Statistics\nThis dataset was derived from the Reuters corpus which consists of Reuters news stories. You can read more about how this dataset was created in the CoNLL-2003 paper. \n#### # of training examples per entity type\nDataset|LOC|MISC|ORG|PER\n-|-|-|-|-\nTrain|7140|3438|6321|6600\nDev|1837|922|1341|1842\nTest|1668|702|1661|1617\n#### # of articles/sentences/tokens per dataset\nDataset |Articles |Sentences |Tokens\n-|-|-|-\nTrain |946 |14,987 |203,621\nDev |216 |3,466 |51,362\nTest |231 |3,684 |46,435\n\n## Training procedure\n\nThis model was trained on a single NVIDIA V100 GPU with recommended hyperparameters from the [original BERT paper](https://arxiv.org/pdf/1810.04805) which trained & evaluated the model on CoNLL-2003 NER task. \n\n## Eval results\nmetric|dev|test\n-|-|-\nf1 |95.1 |91.3\nprecision |95.0 |90.7\nrecall |95.3 |91.9\n\nThe test metrics are a little lower than the official Google BERT results which encoded document context & experimented with CRF. More on replicating the original results [here](https://github.com/google-research/bert/issues/223).\n\n### BibTeX entry and citation info\n\n```\n@article{DBLP:journals/corr/abs-1810-04805,\n  author    = {Jacob Devlin and\n               Ming{-}Wei Chang and\n               Kenton Lee and\n               Kristina Toutanova},\n  title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language\n               Understanding},\n  journal   = {CoRR},\n  volume    = {abs/1810.04805},\n  year      = {2018},\n  url       = {http://arxiv.org/abs/1810.04805},\n  archivePrefix = {arXiv},\n  eprint    = {1810.04805},\n  timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n```\n```\n@inproceedings{tjong-kim-sang-de-meulder-2003-introduction,\n    title = \"Introduction to the {C}o{NLL}-2003 Shared Task: Language-Independent Named Entity Recognition\",\n    author = \"Tjong Kim Sang, Erik F.  and\n      De Meulder, Fien\",\n    booktitle = \"Proceedings of the Seventh Conference on Natural Language Learning at {HLT}-{NAACL} 2003\",\n    year = \"2003\",\n    url = \"https://www.aclweb.org/anthology/W03-0419\",\n    pages = \"142--147\",\n}\n```\n",
    "library_name": "transformers"
  },
  {
    "model_id": "w11wo/indonesian-roberta-base-posp-tagger",
    "model_name": "w11wo/indonesian-roberta-base-posp-tagger",
    "author": "w11wo",
    "downloads": 3393302,
    "likes": 7,
    "tags": [
      "transformers",
      "pytorch",
      "tf",
      "tensorboard",
      "safetensors",
      "roberta",
      "token-classification",
      "generated_from_trainer",
      "ind",
      "dataset:indonlu",
      "base_model:flax-community/indonesian-roberta-base",
      "base_model:finetune:flax-community/indonesian-roberta-base",
      "license:mit",
      "model-index",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/w11wo/indonesian-roberta-base-posp-tagger",
    "dependencies": [
      [
        "transformers",
        "4.37.2"
      ],
      [
        "pytorch",
        "2.2.0+cu118"
      ],
      [
        "datasets",
        "2.16.1"
      ],
      [
        "tokenizers",
        "0.15.1"
      ]
    ],
    "analysis_date": "2025-03-26T00:50:49.296810",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "roberta",
    "pipeline_tag": "token-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": [
        "ind"
      ],
      "license": "mit",
      "tags": [
        "generated_from_trainer"
      ],
      "datasets": [
        "indonlu"
      ],
      "metrics": [
        "precision",
        "recall",
        "f1",
        "accuracy"
      ],
      "base_model": "flax-community/indonesian-roberta-base",
      "model-index": [
        {
          "name": "indonesian-roberta-base-posp-tagger",
          "results": [
            {
              "task": {
                "type": "token-classification",
                "name": "Token Classification"
              },
              "dataset": {
                "name": "indonlu",
                "type": "indonlu",
                "config": "posp",
                "split": "test",
                "args": "posp"
              },
              "metrics": [
                {
                  "type": "precision",
                  "value": 0.9625100240577386,
                  "name": "Precision"
                },
                {
                  "type": "recall",
                  "value": 0.9625100240577386,
                  "name": "Recall"
                },
                {
                  "type": "f1",
                  "value": 0.9625100240577386,
                  "name": "F1"
                },
                {
                  "type": "accuracy",
                  "value": 0.9625100240577386,
                  "name": "Accuracy"
                }
              ]
            }
          ]
        }
      ]
    },
    "card_text": "\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# indonesian-roberta-base-posp-tagger\n\nThis model is a fine-tuned version of [flax-community/indonesian-roberta-base](https://huggingface.co/flax-community/indonesian-roberta-base) on the indonlu dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.1395\n- Precision: 0.9625\n- Recall: 0.9625\n- F1: 0.9625\n- Accuracy: 0.9625\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 16\n- eval_batch_size: 16\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 10\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss | Precision | Recall | F1     | Accuracy |\n|:-------------:|:-----:|:----:|:---------------:|:---------:|:------:|:------:|:--------:|\n| No log        | 1.0   | 420  | 0.2254          | 0.9313    | 0.9313 | 0.9313 | 0.9313   |\n| 0.4398        | 2.0   | 840  | 0.1617          | 0.9499    | 0.9499 | 0.9499 | 0.9499   |\n| 0.1566        | 3.0   | 1260 | 0.1431          | 0.9569    | 0.9569 | 0.9569 | 0.9569   |\n| 0.103         | 4.0   | 1680 | 0.1412          | 0.9605    | 0.9605 | 0.9605 | 0.9605   |\n| 0.0723        | 5.0   | 2100 | 0.1408          | 0.9635    | 0.9635 | 0.9635 | 0.9635   |\n| 0.051         | 6.0   | 2520 | 0.1408          | 0.9642    | 0.9642 | 0.9642 | 0.9642   |\n| 0.051         | 7.0   | 2940 | 0.1510          | 0.9635    | 0.9635 | 0.9635 | 0.9635   |\n| 0.0368        | 8.0   | 3360 | 0.1653          | 0.9645    | 0.9645 | 0.9645 | 0.9645   |\n| 0.0277        | 9.0   | 3780 | 0.1664          | 0.9644    | 0.9644 | 0.9644 | 0.9644   |\n| 0.0231        | 10.0  | 4200 | 0.1668          | 0.9646    | 0.9646 | 0.9646 | 0.9646   |\n\n\n### Framework versions\n\n- Transformers 4.37.2\n- Pytorch 2.2.0+cu118\n- Datasets 2.16.1\n- Tokenizers 0.15.1\n",
    "card_content": "---\nlanguage:\n- ind\nlicense: mit\ntags:\n- generated_from_trainer\ndatasets:\n- indonlu\nmetrics:\n- precision\n- recall\n- f1\n- accuracy\nbase_model: flax-community/indonesian-roberta-base\nmodel-index:\n- name: indonesian-roberta-base-posp-tagger\n  results:\n  - task:\n      type: token-classification\n      name: Token Classification\n    dataset:\n      name: indonlu\n      type: indonlu\n      config: posp\n      split: test\n      args: posp\n    metrics:\n    - type: precision\n      value: 0.9625100240577386\n      name: Precision\n    - type: recall\n      value: 0.9625100240577386\n      name: Recall\n    - type: f1\n      value: 0.9625100240577386\n      name: F1\n    - type: accuracy\n      value: 0.9625100240577386\n      name: Accuracy\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# indonesian-roberta-base-posp-tagger\n\nThis model is a fine-tuned version of [flax-community/indonesian-roberta-base](https://huggingface.co/flax-community/indonesian-roberta-base) on the indonlu dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.1395\n- Precision: 0.9625\n- Recall: 0.9625\n- F1: 0.9625\n- Accuracy: 0.9625\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 16\n- eval_batch_size: 16\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 10\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss | Precision | Recall | F1     | Accuracy |\n|:-------------:|:-----:|:----:|:---------------:|:---------:|:------:|:------:|:--------:|\n| No log        | 1.0   | 420  | 0.2254          | 0.9313    | 0.9313 | 0.9313 | 0.9313   |\n| 0.4398        | 2.0   | 840  | 0.1617          | 0.9499    | 0.9499 | 0.9499 | 0.9499   |\n| 0.1566        | 3.0   | 1260 | 0.1431          | 0.9569    | 0.9569 | 0.9569 | 0.9569   |\n| 0.103         | 4.0   | 1680 | 0.1412          | 0.9605    | 0.9605 | 0.9605 | 0.9605   |\n| 0.0723        | 5.0   | 2100 | 0.1408          | 0.9635    | 0.9635 | 0.9635 | 0.9635   |\n| 0.051         | 6.0   | 2520 | 0.1408          | 0.9642    | 0.9642 | 0.9642 | 0.9642   |\n| 0.051         | 7.0   | 2940 | 0.1510          | 0.9635    | 0.9635 | 0.9635 | 0.9635   |\n| 0.0368        | 8.0   | 3360 | 0.1653          | 0.9645    | 0.9645 | 0.9645 | 0.9645   |\n| 0.0277        | 9.0   | 3780 | 0.1664          | 0.9644    | 0.9644 | 0.9644 | 0.9644   |\n| 0.0231        | 10.0  | 4200 | 0.1668          | 0.9646    | 0.9646 | 0.9646 | 0.9646   |\n\n\n### Framework versions\n\n- Transformers 4.37.2\n- Pytorch 2.2.0+cu118\n- Datasets 2.16.1\n- Tokenizers 0.15.1\n",
    "library_name": "transformers"
  },
  {
    "model_id": "flair/ner-english-fast",
    "model_name": "flair/ner-english-fast",
    "author": "flair",
    "downloads": 1198665,
    "likes": 22,
    "tags": [
      "flair",
      "pytorch",
      "token-classification",
      "sequence-tagger-model",
      "en",
      "dataset:conll2003",
      "region:us"
    ],
    "card_url": "https://huggingface.co/flair/ner-english-fast",
    "dependencies": [
      [
        "flair",
        null
      ]
    ],
    "analysis_date": "2025-03-26T00:50:50.645748",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "unknown",
    "pipeline_tag": "token-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": "en",
      "tags": [
        "flair",
        "token-classification",
        "sequence-tagger-model"
      ],
      "datasets": [
        "conll2003"
      ],
      "widget": [
        {
          "text": "George Washington went to Washington"
        }
      ]
    },
    "card_text": "\n## English NER in Flair (fast model)\n\nThis is the fast 4-class NER model for English that ships with [Flair](https://github.com/flairNLP/flair/).\n\nF1-Score: **92,92** (corrected CoNLL-03)\n\nPredicts 4 tags:\n\n| **tag**                        | **meaning** |\n|---------------------------------|-----------|\n| PER         | person name | \n| LOC         | location name | \n| ORG         | organization name | \n| MISC         | other name | \n\nBased on [Flair embeddings](https://www.aclweb.org/anthology/C18-1139/) and LSTM-CRF.\n\n---\n\n### Demo: How to use in Flair\n\nRequires: **[Flair](https://github.com/flairNLP/flair/)** (`pip install flair`)\n\n```python\nfrom flair.data import Sentence\nfrom flair.models import SequenceTagger\n\n# load tagger\ntagger = SequenceTagger.load(\"flair/ner-english-fast\")\n\n# make example sentence\nsentence = Sentence(\"George Washington went to Washington\")\n\n# predict NER tags\ntagger.predict(sentence)\n\n# print sentence\nprint(sentence)\n\n# print predicted NER spans\nprint('The following NER tags are found:')\n# iterate over entities and print\nfor entity in sentence.get_spans('ner'):\n    print(entity)\n\n```\n\nThis yields the following output:\n```\nSpan [1,2]: \"George Washington\"   [− Labels: PER (0.9515)]\nSpan [5]: \"Washington\"   [− Labels: LOC (0.992)]\n```\n\nSo, the entities \"*George Washington*\" (labeled as a **person**) and \"*Washington*\" (labeled as a **location**) are found in the sentence \"*George Washington went to Washington*\". \n\n\n---\n\n### Training: Script to train this model\n\nThe following Flair script was used to train this model: \n\n```python\nfrom flair.data import Corpus\nfrom flair.datasets import CONLL_03\nfrom flair.embeddings import WordEmbeddings, StackedEmbeddings, FlairEmbeddings\n\n# 1. get the corpus\ncorpus: Corpus = CONLL_03()\n\n# 2. what tag do we want to predict?\ntag_type = 'ner'\n\n# 3. make the tag dictionary from the corpus\ntag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)\n\n# 4. initialize each embedding we use\nembedding_types = [\n\n    # GloVe embeddings\n    WordEmbeddings('glove'),\n\n    # contextual string embeddings, forward\n    FlairEmbeddings('news-forward-fast'),\n\n    # contextual string embeddings, backward\n    FlairEmbeddings('news-backward-fast'),\n]\n\n# embedding stack consists of Flair and GloVe embeddings\nembeddings = StackedEmbeddings(embeddings=embedding_types)\n\n# 5. initialize sequence tagger\nfrom flair.models import SequenceTagger\n\ntagger = SequenceTagger(hidden_size=256,\n                        embeddings=embeddings,\n                        tag_dictionary=tag_dictionary,\n                        tag_type=tag_type)\n\n# 6. initialize trainer\nfrom flair.trainers import ModelTrainer\n\ntrainer = ModelTrainer(tagger, corpus)\n\n# 7. run training\ntrainer.train('resources/taggers/ner-english',\n              train_with_dev=True,\n              max_epochs=150)\n```\n\n\n\n---\n\n### Cite\n\nPlease cite the following paper when using this model.\n\n```\n@inproceedings{akbik2018coling,\n  title={Contextual String Embeddings for Sequence Labeling},\n  author={Akbik, Alan and Blythe, Duncan and Vollgraf, Roland},\n  booktitle = {{COLING} 2018, 27th International Conference on Computational Linguistics},\n  pages     = {1638--1649},\n  year      = {2018}\n}\n```\n\n---\n\n### Issues?\n\nThe Flair issue tracker is available [here](https://github.com/flairNLP/flair/issues/).\n",
    "card_content": "---\nlanguage: en\ntags:\n- flair\n- token-classification\n- sequence-tagger-model\ndatasets:\n- conll2003\nwidget:\n- text: George Washington went to Washington\n---\n\n## English NER in Flair (fast model)\n\nThis is the fast 4-class NER model for English that ships with [Flair](https://github.com/flairNLP/flair/).\n\nF1-Score: **92,92** (corrected CoNLL-03)\n\nPredicts 4 tags:\n\n| **tag**                        | **meaning** |\n|---------------------------------|-----------|\n| PER         | person name | \n| LOC         | location name | \n| ORG         | organization name | \n| MISC         | other name | \n\nBased on [Flair embeddings](https://www.aclweb.org/anthology/C18-1139/) and LSTM-CRF.\n\n---\n\n### Demo: How to use in Flair\n\nRequires: **[Flair](https://github.com/flairNLP/flair/)** (`pip install flair`)\n\n```python\nfrom flair.data import Sentence\nfrom flair.models import SequenceTagger\n\n# load tagger\ntagger = SequenceTagger.load(\"flair/ner-english-fast\")\n\n# make example sentence\nsentence = Sentence(\"George Washington went to Washington\")\n\n# predict NER tags\ntagger.predict(sentence)\n\n# print sentence\nprint(sentence)\n\n# print predicted NER spans\nprint('The following NER tags are found:')\n# iterate over entities and print\nfor entity in sentence.get_spans('ner'):\n    print(entity)\n\n```\n\nThis yields the following output:\n```\nSpan [1,2]: \"George Washington\"   [− Labels: PER (0.9515)]\nSpan [5]: \"Washington\"   [− Labels: LOC (0.992)]\n```\n\nSo, the entities \"*George Washington*\" (labeled as a **person**) and \"*Washington*\" (labeled as a **location**) are found in the sentence \"*George Washington went to Washington*\". \n\n\n---\n\n### Training: Script to train this model\n\nThe following Flair script was used to train this model: \n\n```python\nfrom flair.data import Corpus\nfrom flair.datasets import CONLL_03\nfrom flair.embeddings import WordEmbeddings, StackedEmbeddings, FlairEmbeddings\n\n# 1. get the corpus\ncorpus: Corpus = CONLL_03()\n\n# 2. what tag do we want to predict?\ntag_type = 'ner'\n\n# 3. make the tag dictionary from the corpus\ntag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)\n\n# 4. initialize each embedding we use\nembedding_types = [\n\n    # GloVe embeddings\n    WordEmbeddings('glove'),\n\n    # contextual string embeddings, forward\n    FlairEmbeddings('news-forward-fast'),\n\n    # contextual string embeddings, backward\n    FlairEmbeddings('news-backward-fast'),\n]\n\n# embedding stack consists of Flair and GloVe embeddings\nembeddings = StackedEmbeddings(embeddings=embedding_types)\n\n# 5. initialize sequence tagger\nfrom flair.models import SequenceTagger\n\ntagger = SequenceTagger(hidden_size=256,\n                        embeddings=embeddings,\n                        tag_dictionary=tag_dictionary,\n                        tag_type=tag_type)\n\n# 6. initialize trainer\nfrom flair.trainers import ModelTrainer\n\ntrainer = ModelTrainer(tagger, corpus)\n\n# 7. run training\ntrainer.train('resources/taggers/ner-english',\n              train_with_dev=True,\n              max_epochs=150)\n```\n\n\n\n---\n\n### Cite\n\nPlease cite the following paper when using this model.\n\n```\n@inproceedings{akbik2018coling,\n  title={Contextual String Embeddings for Sequence Labeling},\n  author={Akbik, Alan and Blythe, Duncan and Vollgraf, Roland},\n  booktitle = {{COLING} 2018, 27th International Conference on Computational Linguistics},\n  pages     = {1638--1649},\n  year      = {2018}\n}\n```\n\n---\n\n### Issues?\n\nThe Flair issue tracker is available [here](https://github.com/flairNLP/flair/issues/).\n",
    "library_name": "flair"
  },
  {
    "model_id": "Babelscape/wikineural-multilingual-ner",
    "model_name": "Babelscape/wikineural-multilingual-ner",
    "author": "Babelscape",
    "downloads": 1146396,
    "likes": 137,
    "tags": [
      "transformers",
      "pytorch",
      "tensorboard",
      "safetensors",
      "bert",
      "token-classification",
      "named-entity-recognition",
      "sequence-tagger-model",
      "de",
      "en",
      "es",
      "fr",
      "it",
      "nl",
      "pl",
      "pt",
      "ru",
      "multilingual",
      "dataset:Babelscape/wikineural",
      "license:cc-by-nc-sa-4.0",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/Babelscape/wikineural-multilingual-ner",
    "dependencies": [
      [
        "transformers",
        null
      ]
    ],
    "analysis_date": "2025-03-26T00:50:51.745013",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "bert",
    "pipeline_tag": "token-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": [
        "de",
        "en",
        "es",
        "fr",
        "it",
        "nl",
        "pl",
        "pt",
        "ru",
        "multilingual"
      ],
      "license": [
        "cc-by-nc-sa-4.0"
      ],
      "tags": [
        "named-entity-recognition",
        "sequence-tagger-model"
      ],
      "datasets": [
        "Babelscape/wikineural"
      ],
      "annotations_creators": [
        "machine-generated"
      ],
      "language_creators": [
        "machine-generated"
      ],
      "widget": [
        {
          "text": "My name is Wolfgang and I live in Berlin."
        },
        {
          "text": "George Washington went to Washington."
        },
        {
          "text": "Mi nombre es Sarah y vivo en Londres."
        },
        {
          "text": "Меня зовут Симона, и я живу в Риме."
        }
      ],
      "pretty_name": "wikineural-dataset",
      "source_datasets": [
        "original"
      ],
      "task_categories": [
        "structure-prediction"
      ],
      "task_ids": [
        "named-entity-recognition"
      ]
    },
    "card_text": "\n# WikiNEuRal: Combined Neural and Knowledge-based Silver Data Creation for Multilingual NER\nThis is the model card for the EMNLP 2021 paper [WikiNEuRal: Combined Neural and Knowledge-based Silver Data Creation for Multilingual NER](https://aclanthology.org/2021.findings-emnlp.215/). We fine-tuned a multilingual language model (mBERT) for 3 epochs on our [WikiNEuRal dataset](https://huggingface.co/datasets/Babelscape/wikineural) for Named Entity Recognition (NER). The resulting multilingual NER model supports the 9 languages covered by WikiNEuRal (de, en, es, fr, it, nl, pl, pt, ru), and it was trained on all 9 languages jointly.\n\n**If you use the model, please reference this work in your paper**:\n\n```bibtex\n@inproceedings{tedeschi-etal-2021-wikineural-combined,\n    title = \"{W}iki{NE}u{R}al: {C}ombined Neural and Knowledge-based Silver Data Creation for Multilingual {NER}\",\n    author = \"Tedeschi, Simone  and\n      Maiorca, Valentino  and\n      Campolungo, Niccol{\\`o}  and\n      Cecconi, Francesco  and\n      Navigli, Roberto\",\n    booktitle = \"Findings of the Association for Computational Linguistics: EMNLP 2021\",\n    month = nov,\n    year = \"2021\",\n    address = \"Punta Cana, Dominican Republic\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.findings-emnlp.215\",\n    pages = \"2521--2533\",\n    abstract = \"Multilingual Named Entity Recognition (NER) is a key intermediate task which is needed in many areas of NLP. In this paper, we address the well-known issue of data scarcity in NER, especially relevant when moving to a multilingual scenario, and go beyond current approaches to the creation of multilingual silver data for the task. We exploit the texts of Wikipedia and introduce a new methodology based on the effective combination of knowledge-based approaches and neural models, together with a novel domain adaptation technique, to produce high-quality training corpora for NER. We evaluate our datasets extensively on standard benchmarks for NER, yielding substantial improvements up to 6 span-based F1-score points over previous state-of-the-art systems for data creation.\",\n}\n```\n    \nThe original repository for the paper can be found at [https://github.com/Babelscape/wikineural](https://github.com/Babelscape/wikineural).\n\n## How to use\n\nYou can use this model with Transformers *pipeline* for NER. \n\n```python\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\nfrom transformers import pipeline\n\ntokenizer = AutoTokenizer.from_pretrained(\"Babelscape/wikineural-multilingual-ner\")\nmodel = AutoModelForTokenClassification.from_pretrained(\"Babelscape/wikineural-multilingual-ner\")\n\nnlp = pipeline(\"ner\", model=model, tokenizer=tokenizer, grouped_entities=True)\nexample = \"My name is Wolfgang and I live in Berlin\"\n\nner_results = nlp(example)\nprint(ner_results)\n```\n\n## Limitations and bias\n\nThis model is trained on WikiNEuRal, a state-of-the-art dataset for Multilingual NER automatically derived from Wikipedia. Therefore, it might not generalize well to all textual genres (e.g. news). On the other hand, models trained only on news articles (e.g. only on CoNLL03) have been proven to obtain much lower scores on encyclopedic articles. To obtain more robust systems, we encourage you to train a system on the combination of WikiNEuRal with other datasets (e.g. WikiNEuRal + CoNLL).\n\n## Licensing Information\n\nContents of this repository are restricted to only non-commercial research purposes under the [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License (CC BY-NC-SA 4.0)](https://creativecommons.org/licenses/by-nc-sa/4.0/). Copyright of the dataset contents and models belongs to the original copyright holders.",
    "card_content": "---\nlanguage:\n- de\n- en\n- es\n- fr\n- it\n- nl\n- pl\n- pt\n- ru\n- multilingual\nlicense:\n- cc-by-nc-sa-4.0\ntags:\n- named-entity-recognition\n- sequence-tagger-model\ndatasets:\n- Babelscape/wikineural\nannotations_creators:\n- machine-generated\nlanguage_creators:\n- machine-generated\nwidget:\n- text: My name is Wolfgang and I live in Berlin.\n- text: George Washington went to Washington.\n- text: Mi nombre es Sarah y vivo en Londres.\n- text: Меня зовут Симона, и я живу в Риме.\npretty_name: wikineural-dataset\nsource_datasets:\n- original\ntask_categories:\n- structure-prediction\ntask_ids:\n- named-entity-recognition\n---\n\n# WikiNEuRal: Combined Neural and Knowledge-based Silver Data Creation for Multilingual NER\nThis is the model card for the EMNLP 2021 paper [WikiNEuRal: Combined Neural and Knowledge-based Silver Data Creation for Multilingual NER](https://aclanthology.org/2021.findings-emnlp.215/). We fine-tuned a multilingual language model (mBERT) for 3 epochs on our [WikiNEuRal dataset](https://huggingface.co/datasets/Babelscape/wikineural) for Named Entity Recognition (NER). The resulting multilingual NER model supports the 9 languages covered by WikiNEuRal (de, en, es, fr, it, nl, pl, pt, ru), and it was trained on all 9 languages jointly.\n\n**If you use the model, please reference this work in your paper**:\n\n```bibtex\n@inproceedings{tedeschi-etal-2021-wikineural-combined,\n    title = \"{W}iki{NE}u{R}al: {C}ombined Neural and Knowledge-based Silver Data Creation for Multilingual {NER}\",\n    author = \"Tedeschi, Simone  and\n      Maiorca, Valentino  and\n      Campolungo, Niccol{\\`o}  and\n      Cecconi, Francesco  and\n      Navigli, Roberto\",\n    booktitle = \"Findings of the Association for Computational Linguistics: EMNLP 2021\",\n    month = nov,\n    year = \"2021\",\n    address = \"Punta Cana, Dominican Republic\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.findings-emnlp.215\",\n    pages = \"2521--2533\",\n    abstract = \"Multilingual Named Entity Recognition (NER) is a key intermediate task which is needed in many areas of NLP. In this paper, we address the well-known issue of data scarcity in NER, especially relevant when moving to a multilingual scenario, and go beyond current approaches to the creation of multilingual silver data for the task. We exploit the texts of Wikipedia and introduce a new methodology based on the effective combination of knowledge-based approaches and neural models, together with a novel domain adaptation technique, to produce high-quality training corpora for NER. We evaluate our datasets extensively on standard benchmarks for NER, yielding substantial improvements up to 6 span-based F1-score points over previous state-of-the-art systems for data creation.\",\n}\n```\n    \nThe original repository for the paper can be found at [https://github.com/Babelscape/wikineural](https://github.com/Babelscape/wikineural).\n\n## How to use\n\nYou can use this model with Transformers *pipeline* for NER. \n\n```python\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\nfrom transformers import pipeline\n\ntokenizer = AutoTokenizer.from_pretrained(\"Babelscape/wikineural-multilingual-ner\")\nmodel = AutoModelForTokenClassification.from_pretrained(\"Babelscape/wikineural-multilingual-ner\")\n\nnlp = pipeline(\"ner\", model=model, tokenizer=tokenizer, grouped_entities=True)\nexample = \"My name is Wolfgang and I live in Berlin\"\n\nner_results = nlp(example)\nprint(ner_results)\n```\n\n## Limitations and bias\n\nThis model is trained on WikiNEuRal, a state-of-the-art dataset for Multilingual NER automatically derived from Wikipedia. Therefore, it might not generalize well to all textual genres (e.g. news). On the other hand, models trained only on news articles (e.g. only on CoNLL03) have been proven to obtain much lower scores on encyclopedic articles. To obtain more robust systems, we encourage you to train a system on the combination of WikiNEuRal with other datasets (e.g. WikiNEuRal + CoNLL).\n\n## Licensing Information\n\nContents of this repository are restricted to only non-commercial research purposes under the [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License (CC BY-NC-SA 4.0)](https://creativecommons.org/licenses/by-nc-sa/4.0/). Copyright of the dataset contents and models belongs to the original copyright holders.",
    "library_name": "transformers"
  },
  {
    "model_id": "kredor/punctuate-all",
    "model_name": "kredor/punctuate-all",
    "author": "kredor",
    "downloads": 1023479,
    "likes": 19,
    "tags": [
      "transformers",
      "pytorch",
      "xlm-roberta",
      "token-classification",
      "dataset:wmt/europarl",
      "license:mit",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/kredor/punctuate-all",
    "dependencies": [
      [
        "transformers",
        null
      ],
      [
        "torch",
        null
      ]
    ],
    "analysis_date": "2025-03-26T00:50:52.667797",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "xlm-roberta",
    "pipeline_tag": "token-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "license": "mit",
      "datasets": [
        "wmt/europarl"
      ],
      "metrics": [
        "f1",
        "recall",
        "precision"
      ]
    },
    "card_text": "This is based on [Oliver Guhr's work](https://huggingface.co/oliverguhr/fullstop-punctuation-multilang-large). The difference is that it is a finetuned xlm-roberta-base instead of an xlm-roberta-large and on twelve languages instead of four. The languages are: English, German, French, Spanish, Bulgarian, Italian, Polish, Dutch, Czech, Portugese, Slovak, Slovenian.\n\n----- report -----\n\n              precision    recall  f1-score   support\n\n           0       0.99      0.99      0.99  73317475\n           .       0.94      0.95      0.95   4484845\n           ,       0.86      0.86      0.86   6100650\n           ?       0.88      0.85      0.86    136479\n           -       0.60      0.29      0.39    233630\n           :       0.71      0.49      0.58    152424\n\n    accuracy                           0.98  84425503\n   macro avg       0.83      0.74      0.77  84425503\nweighted avg       0.98      0.98      0.98  84425503\n\n\n----- confusion matrix -----\n\n     t/p      0     .     ,     ?     -     : \n        0   1.0   0.0   0.0   0.0   0.0   0.0 \n        .   0.0   1.0   0.0   0.0   0.0   0.0 \n        ,   0.1   0.0   0.9   0.0   0.0   0.0 \n        ?   0.0   0.1   0.0   0.8   0.0   0.0 \n        -   0.1   0.1   0.5   0.0   0.3   0.0 \n        :   0.0   0.3   0.1   0.0   0.0   0.5",
    "card_content": "---\nlicense: mit\ndatasets:\n- wmt/europarl\nmetrics:\n- f1\n- recall\n- precision\n---\nThis is based on [Oliver Guhr's work](https://huggingface.co/oliverguhr/fullstop-punctuation-multilang-large). The difference is that it is a finetuned xlm-roberta-base instead of an xlm-roberta-large and on twelve languages instead of four. The languages are: English, German, French, Spanish, Bulgarian, Italian, Polish, Dutch, Czech, Portugese, Slovak, Slovenian.\n\n----- report -----\n\n              precision    recall  f1-score   support\n\n           0       0.99      0.99      0.99  73317475\n           .       0.94      0.95      0.95   4484845\n           ,       0.86      0.86      0.86   6100650\n           ?       0.88      0.85      0.86    136479\n           -       0.60      0.29      0.39    233630\n           :       0.71      0.49      0.58    152424\n\n    accuracy                           0.98  84425503\n   macro avg       0.83      0.74      0.77  84425503\nweighted avg       0.98      0.98      0.98  84425503\n\n\n----- confusion matrix -----\n\n     t/p      0     .     ,     ?     -     : \n        0   1.0   0.0   0.0   0.0   0.0   0.0 \n        .   0.0   1.0   0.0   0.0   0.0   0.0 \n        ,   0.1   0.0   0.9   0.0   0.0   0.0 \n        ?   0.0   0.1   0.0   0.8   0.0   0.0 \n        -   0.1   0.1   0.5   0.0   0.3   0.0 \n        :   0.0   0.3   0.1   0.0   0.0   0.5",
    "library_name": "transformers"
  },
  {
    "model_id": "gilf/french-camembert-postag-model",
    "model_name": "gilf/french-camembert-postag-model",
    "author": "gilf",
    "downloads": 936292,
    "likes": 9,
    "tags": [
      "transformers",
      "pytorch",
      "tf",
      "safetensors",
      "camembert",
      "token-classification",
      "fr",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/gilf/french-camembert-postag-model",
    "dependencies": [
      [
        "transformers",
        null
      ]
    ],
    "analysis_date": "2025-03-26T00:50:54.066943",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "camembert",
    "pipeline_tag": "token-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": "fr",
      "widget": [
        {
          "text": "Face à un choc inédit, les mesures mises en place par le gouvernement ont permis une protection forte et efficace des ménages"
        }
      ]
    },
    "card_text": "\n## About\n\nThe  *french-camembert-postag-model* is a part of speech tagging model for French that was trained on the *free-french-treebank* dataset available on \n[github](https://github.com/nicolashernandez/free-french-treebank). The base tokenizer and model used for training is *'camembert-base'*.\n\n## Supported Tags\n\nIt uses the following tags:\n\n| Tag      |          Category              |  Extra Info |\n|----------|:------------------------------:|------------:|\n| ADJ      |           adjectif             |             |\n| ADJWH    |           adjectif             |             |\n| ADV      |           adverbe              |             |\n| ADVWH    |           adverbe              |             |\n| CC       |  conjonction de coordination   |             |\n| CLO      |            pronom              |     obj     |\n| CLR      |            pronom              |     refl    |\n| CLS      |            pronom              |     suj     |\n| CS       |  conjonction de subordination  |             |\n| DET      |          déterminant           |             |\n| DETWH    |          déterminant           |             |\n| ET       |          mot étranger          |             |\n| I        |          interjection          |             |\n| NC       |          nom commun            |             |\n| NPP      |          nom propre            |             |\n| P        |          préposition           |             |\n| P+D      |   préposition + déterminant    |             |\n| PONCT    |      signe de ponctuation      |             |\n| PREF     |            préfixe             |             |\n| PRO      |        autres pronoms          |             |\n| PROREL   |        autres pronoms          |     rel     |\n| PROWH    |        autres pronoms          |     int     |\n| U        |               ?                |             |\n| V        |             verbe              |             |\n| VIMP     |        verbe imperatif         |             |\n| VINF     |        verbe infinitif         |             |\n| VPP      |        participe passé         |             |\n| VPR      |        participe présent       |             |\n| VS       |        subjonctif              |             |\n\nMore information on the tags can be found here:\n\nhttp://alpage.inria.fr/statgram/frdep/Publications/crabbecandi-taln2008-final.pdf\n\n## Usage\n\nThe usage of this model follows the common transformers patterns. Here is a short example of its usage:\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\n\ntokenizer = AutoTokenizer.from_pretrained(\"gilf/french-camembert-postag-model\")\nmodel = AutoModelForTokenClassification.from_pretrained(\"gilf/french-camembert-postag-model\")\n\nfrom transformers import pipeline\n\nnlp_token_class = pipeline('ner', model=model, tokenizer=tokenizer, grouped_entities=True)\n\nnlp_token_class('Face à un choc inédit, les mesures mises en place par le gouvernement ont permis une protection forte et efficace des ménages')\n```\n\nThe lines above would display something like this on a Jupyter notebook:\n\n```\n[{'entity_group': 'NC', 'score': 0.5760144591331482, 'word': '<s>'},\n {'entity_group': 'U', 'score': 0.9946700930595398, 'word': 'Face'},\n {'entity_group': 'P', 'score': 0.999615490436554, 'word': 'à'},\n {'entity_group': 'DET', 'score': 0.9995906352996826, 'word': 'un'},\n {'entity_group': 'NC', 'score': 0.9995531439781189, 'word': 'choc'},\n {'entity_group': 'ADJ', 'score': 0.999183714389801, 'word': 'inédit'},\n {'entity_group': 'P', 'score': 0.3710663616657257, 'word': ','},\n {'entity_group': 'DET', 'score': 0.9995903968811035, 'word': 'les'},\n {'entity_group': 'NC', 'score': 0.9995649456977844, 'word': 'mesures'},\n {'entity_group': 'VPP', 'score': 0.9988670349121094, 'word': 'mises'},\n {'entity_group': 'P', 'score': 0.9996246099472046, 'word': 'en'},\n {'entity_group': 'NC', 'score': 0.9995329976081848, 'word': 'place'},\n {'entity_group': 'P', 'score': 0.9996233582496643, 'word': 'par'},\n {'entity_group': 'DET', 'score': 0.9995935559272766, 'word': 'le'},\n {'entity_group': 'NC', 'score': 0.9995369911193848, 'word': 'gouvernement'},\n {'entity_group': 'V', 'score': 0.9993771314620972, 'word': 'ont'},\n {'entity_group': 'VPP', 'score': 0.9991101026535034, 'word': 'permis'},\n {'entity_group': 'DET', 'score': 0.9995885491371155, 'word': 'une'},\n {'entity_group': 'NC', 'score': 0.9995636343955994, 'word': 'protection'},\n {'entity_group': 'ADJ', 'score': 0.9991781711578369, 'word': 'forte'},\n {'entity_group': 'CC', 'score': 0.9991298317909241, 'word': 'et'},\n {'entity_group': 'ADJ', 'score': 0.9992275238037109, 'word': 'efficace'},\n {'entity_group': 'P+D', 'score': 0.9993300437927246, 'word': 'des'},\n {'entity_group': 'NC', 'score': 0.8353511393070221, 'word': 'ménages</s>'}]\n```\n",
    "card_content": "---\nlanguage: fr\nwidget:\n- text: Face à un choc inédit, les mesures mises en place par le gouvernement ont\n    permis une protection forte et efficace des ménages\n---\n\n## About\n\nThe  *french-camembert-postag-model* is a part of speech tagging model for French that was trained on the *free-french-treebank* dataset available on \n[github](https://github.com/nicolashernandez/free-french-treebank). The base tokenizer and model used for training is *'camembert-base'*.\n\n## Supported Tags\n\nIt uses the following tags:\n\n| Tag      |          Category              |  Extra Info |\n|----------|:------------------------------:|------------:|\n| ADJ      |           adjectif             |             |\n| ADJWH    |           adjectif             |             |\n| ADV      |           adverbe              |             |\n| ADVWH    |           adverbe              |             |\n| CC       |  conjonction de coordination   |             |\n| CLO      |            pronom              |     obj     |\n| CLR      |            pronom              |     refl    |\n| CLS      |            pronom              |     suj     |\n| CS       |  conjonction de subordination  |             |\n| DET      |          déterminant           |             |\n| DETWH    |          déterminant           |             |\n| ET       |          mot étranger          |             |\n| I        |          interjection          |             |\n| NC       |          nom commun            |             |\n| NPP      |          nom propre            |             |\n| P        |          préposition           |             |\n| P+D      |   préposition + déterminant    |             |\n| PONCT    |      signe de ponctuation      |             |\n| PREF     |            préfixe             |             |\n| PRO      |        autres pronoms          |             |\n| PROREL   |        autres pronoms          |     rel     |\n| PROWH    |        autres pronoms          |     int     |\n| U        |               ?                |             |\n| V        |             verbe              |             |\n| VIMP     |        verbe imperatif         |             |\n| VINF     |        verbe infinitif         |             |\n| VPP      |        participe passé         |             |\n| VPR      |        participe présent       |             |\n| VS       |        subjonctif              |             |\n\nMore information on the tags can be found here:\n\nhttp://alpage.inria.fr/statgram/frdep/Publications/crabbecandi-taln2008-final.pdf\n\n## Usage\n\nThe usage of this model follows the common transformers patterns. Here is a short example of its usage:\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\n\ntokenizer = AutoTokenizer.from_pretrained(\"gilf/french-camembert-postag-model\")\nmodel = AutoModelForTokenClassification.from_pretrained(\"gilf/french-camembert-postag-model\")\n\nfrom transformers import pipeline\n\nnlp_token_class = pipeline('ner', model=model, tokenizer=tokenizer, grouped_entities=True)\n\nnlp_token_class('Face à un choc inédit, les mesures mises en place par le gouvernement ont permis une protection forte et efficace des ménages')\n```\n\nThe lines above would display something like this on a Jupyter notebook:\n\n```\n[{'entity_group': 'NC', 'score': 0.5760144591331482, 'word': '<s>'},\n {'entity_group': 'U', 'score': 0.9946700930595398, 'word': 'Face'},\n {'entity_group': 'P', 'score': 0.999615490436554, 'word': 'à'},\n {'entity_group': 'DET', 'score': 0.9995906352996826, 'word': 'un'},\n {'entity_group': 'NC', 'score': 0.9995531439781189, 'word': 'choc'},\n {'entity_group': 'ADJ', 'score': 0.999183714389801, 'word': 'inédit'},\n {'entity_group': 'P', 'score': 0.3710663616657257, 'word': ','},\n {'entity_group': 'DET', 'score': 0.9995903968811035, 'word': 'les'},\n {'entity_group': 'NC', 'score': 0.9995649456977844, 'word': 'mesures'},\n {'entity_group': 'VPP', 'score': 0.9988670349121094, 'word': 'mises'},\n {'entity_group': 'P', 'score': 0.9996246099472046, 'word': 'en'},\n {'entity_group': 'NC', 'score': 0.9995329976081848, 'word': 'place'},\n {'entity_group': 'P', 'score': 0.9996233582496643, 'word': 'par'},\n {'entity_group': 'DET', 'score': 0.9995935559272766, 'word': 'le'},\n {'entity_group': 'NC', 'score': 0.9995369911193848, 'word': 'gouvernement'},\n {'entity_group': 'V', 'score': 0.9993771314620972, 'word': 'ont'},\n {'entity_group': 'VPP', 'score': 0.9991101026535034, 'word': 'permis'},\n {'entity_group': 'DET', 'score': 0.9995885491371155, 'word': 'une'},\n {'entity_group': 'NC', 'score': 0.9995636343955994, 'word': 'protection'},\n {'entity_group': 'ADJ', 'score': 0.9991781711578369, 'word': 'forte'},\n {'entity_group': 'CC', 'score': 0.9991298317909241, 'word': 'et'},\n {'entity_group': 'ADJ', 'score': 0.9992275238037109, 'word': 'efficace'},\n {'entity_group': 'P+D', 'score': 0.9993300437927246, 'word': 'des'},\n {'entity_group': 'NC', 'score': 0.8353511393070221, 'word': 'ménages</s>'}]\n```\n",
    "library_name": "transformers"
  },
  {
    "model_id": "obi/deid_roberta_i2b2",
    "model_name": "obi/deid_roberta_i2b2",
    "author": "obi",
    "downloads": 737342,
    "likes": 32,
    "tags": [
      "transformers",
      "pytorch",
      "safetensors",
      "roberta",
      "token-classification",
      "deidentification",
      "medical notes",
      "ehr",
      "phi",
      "en",
      "dataset:I2B2",
      "arxiv:1907.11692",
      "license:mit",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/obi/deid_roberta_i2b2",
    "dependencies": [
      [
        "transformers",
        null
      ],
      [
        "torch",
        null
      ],
      [
        "spacy",
        null
      ]
    ],
    "analysis_date": "2025-03-26T00:50:56.091901",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "roberta",
    "pipeline_tag": "token-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": [
        "en"
      ],
      "license": "mit",
      "tags": [
        "deidentification",
        "medical notes",
        "ehr",
        "phi"
      ],
      "datasets": [
        "I2B2"
      ],
      "metrics": [
        "F1",
        "Recall",
        "Precision"
      ],
      "thumbnail": "https://www.onebraveidea.org/wp-content/uploads/2019/07/OBI-Logo-Website.png",
      "widget": [
        {
          "text": "Physician Discharge Summary Admit date: 10/12/1982 Discharge date: 10/22/1982 Patient Information Jack Reacher, 54 y.o. male (DOB = 1/21/1928)."
        },
        {
          "text": "Home Address: 123 Park Drive, San Diego, CA, 03245. Home Phone: 202-555-0199 (home)."
        },
        {
          "text": "Hospital Care Team Service: Orthopedics Inpatient Attending: Roger C Kelly, MD Attending phys phone: (634)743-5135 Discharge Unit: HCS843 Primary Care Physician: Hassan V Kim, MD 512-832-5025."
        }
      ]
    },
    "card_text": "\n# Model Description\n\n* A RoBERTa [[Liu et al., 2019]](https://arxiv.org/pdf/1907.11692.pdf) model fine-tuned for de-identification of medical notes.\n* Sequence Labeling (token classification): The model was trained to predict protected health information (PHI/PII) entities (spans). A list of protected health information categories is given by [HIPAA](https://www.hhs.gov/hipaa/for-professionals/privacy/laws-regulations/index.html).\n* A token can either be classified as non-PHI or as one of the 11 PHI types. Token predictions are aggregated to spans by making use of BILOU tagging.\n* The PHI labels that were used for training and other details can be found here: [Annotation Guidelines](https://github.com/obi-ml-public/ehr_deidentification/blob/master/AnnotationGuidelines.md)\n* More details on how to use this model, the format of data and other useful information is present in the GitHub repo: [Robust DeID](https://github.com/obi-ml-public/ehr_deidentification).\n\n\n# How to use\n\n* A demo on how the model works (using model predictions to de-identify a medical note) is on this space: [Medical-Note-Deidentification](https://huggingface.co/spaces/obi/Medical-Note-Deidentification).\n* Steps on how this model can be used to run a forward pass can be found here: [Forward Pass](https://github.com/obi-ml-public/ehr_deidentification/tree/master/steps/forward_pass)\n* In brief, the steps are:\n    * Sentencize (the model aggregates the sentences back to the note level) and tokenize the dataset.\n    * Use the predict function of this model to gather the predictions (i.e., predictions for each token).\n    * Additionally, the model predictions can be used to remove PHI from the original note/text.\n    \n    \n# Dataset\n\n* The I2B2 2014 [[Stubbs and Uzuner, 2015]](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4978170/) dataset was used to train this model.\n\n|           | I2B2                  |            |  I2B2                |            |\n| --------- | --------------------- | ---------- | -------------------- | ---------- |\n|           | TRAIN SET - 790 NOTES |            | TEST SET - 514 NOTES |            |\n| PHI LABEL | COUNT                 | PERCENTAGE | COUNT                | PERCENTAGE |\n| DATE      | 7502                  | 43.69      | 4980                 | 44.14      |\n| STAFF     | 3149                  | 18.34      | 2004                 | 17.76      |\n| HOSP      | 1437                  | 8.37       | 875                  | 7.76       |\n| AGE       | 1233                  | 7.18       | 764                  | 6.77       |\n| LOC       | 1206                  | 7.02       | 856                  | 7.59       |\n| PATIENT   | 1316                  | 7.66       | 879                  | 7.79       |\n| PHONE     | 317                   | 1.85       | 217                  | 1.92       |\n| ID        | 881                   | 5.13       | 625                  | 5.54       |\n| PATORG    | 124                   | 0.72       | 82                   | 0.73       |\n| EMAIL     | 4                     | 0.02       | 1                    | 0.01       |\n| OTHERPHI  | 2                     | 0.01       | 0                    | 0          |\n| TOTAL     | 17171                 | 100        | 11283                | 100        |\n\n\n# Training procedure\n\n* Steps on how this model was trained can be found here: [Training](https://github.com/obi-ml-public/ehr_deidentification/tree/master/steps/train). The \"model_name_or_path\" was set to: \"roberta-large\".\n    * The dataset was sentencized with the en_core_sci_sm sentencizer from spacy.\n    * The dataset was then tokenized with a custom tokenizer built on top of the en_core_sci_sm tokenizer from spacy.\n    * For each sentence we added 32 tokens on the left (from previous sentences) and 32 tokens on the right (from the next sentences).\n    * The added tokens are not used for learning - i.e, the loss is not computed on these tokens - they are used as additional context.\n    * Each sequence contained a maximum of 128 tokens (including the 32 tokens added on). Longer sequences were split.\n    * The sentencized and tokenized dataset with the token level labels based on the BILOU notation was used to train the model.\n    * The model is fine-tuned from a pre-trained RoBERTa model.\n    \n* Training details:\n    * Input sequence length: 128\n    * Batch size: 32 (16 with 2 gradient accumulation steps)\n    * Optimizer: AdamW \n    * Learning rate: 5e-5\n    * Dropout: 0.1\n\n\n## Results\n\n# Questions?\n\nPost a Github issue on the repo: [Robust DeID](https://github.com/obi-ml-public/ehr_deidentification).\n",
    "card_content": "---\nlanguage:\n- en\nlicense: mit\ntags:\n- deidentification\n- medical notes\n- ehr\n- phi\ndatasets:\n- I2B2\nmetrics:\n- F1\n- Recall\n- Precision\nthumbnail: https://www.onebraveidea.org/wp-content/uploads/2019/07/OBI-Logo-Website.png\nwidget:\n- text: 'Physician Discharge Summary Admit date: 10/12/1982 Discharge date: 10/22/1982\n    Patient Information Jack Reacher, 54 y.o. male (DOB = 1/21/1928).'\n- text: 'Home Address: 123 Park Drive, San Diego, CA, 03245. Home Phone: 202-555-0199\n    (home).'\n- text: 'Hospital Care Team Service: Orthopedics Inpatient Attending: Roger C Kelly,\n    MD Attending phys phone: (634)743-5135 Discharge Unit: HCS843 Primary Care Physician:\n    Hassan V Kim, MD 512-832-5025.'\n---\n\n# Model Description\n\n* A RoBERTa [[Liu et al., 2019]](https://arxiv.org/pdf/1907.11692.pdf) model fine-tuned for de-identification of medical notes.\n* Sequence Labeling (token classification): The model was trained to predict protected health information (PHI/PII) entities (spans). A list of protected health information categories is given by [HIPAA](https://www.hhs.gov/hipaa/for-professionals/privacy/laws-regulations/index.html).\n* A token can either be classified as non-PHI or as one of the 11 PHI types. Token predictions are aggregated to spans by making use of BILOU tagging.\n* The PHI labels that were used for training and other details can be found here: [Annotation Guidelines](https://github.com/obi-ml-public/ehr_deidentification/blob/master/AnnotationGuidelines.md)\n* More details on how to use this model, the format of data and other useful information is present in the GitHub repo: [Robust DeID](https://github.com/obi-ml-public/ehr_deidentification).\n\n\n# How to use\n\n* A demo on how the model works (using model predictions to de-identify a medical note) is on this space: [Medical-Note-Deidentification](https://huggingface.co/spaces/obi/Medical-Note-Deidentification).\n* Steps on how this model can be used to run a forward pass can be found here: [Forward Pass](https://github.com/obi-ml-public/ehr_deidentification/tree/master/steps/forward_pass)\n* In brief, the steps are:\n    * Sentencize (the model aggregates the sentences back to the note level) and tokenize the dataset.\n    * Use the predict function of this model to gather the predictions (i.e., predictions for each token).\n    * Additionally, the model predictions can be used to remove PHI from the original note/text.\n    \n    \n# Dataset\n\n* The I2B2 2014 [[Stubbs and Uzuner, 2015]](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4978170/) dataset was used to train this model.\n\n|           | I2B2                  |            |  I2B2                |            |\n| --------- | --------------------- | ---------- | -------------------- | ---------- |\n|           | TRAIN SET - 790 NOTES |            | TEST SET - 514 NOTES |            |\n| PHI LABEL | COUNT                 | PERCENTAGE | COUNT                | PERCENTAGE |\n| DATE      | 7502                  | 43.69      | 4980                 | 44.14      |\n| STAFF     | 3149                  | 18.34      | 2004                 | 17.76      |\n| HOSP      | 1437                  | 8.37       | 875                  | 7.76       |\n| AGE       | 1233                  | 7.18       | 764                  | 6.77       |\n| LOC       | 1206                  | 7.02       | 856                  | 7.59       |\n| PATIENT   | 1316                  | 7.66       | 879                  | 7.79       |\n| PHONE     | 317                   | 1.85       | 217                  | 1.92       |\n| ID        | 881                   | 5.13       | 625                  | 5.54       |\n| PATORG    | 124                   | 0.72       | 82                   | 0.73       |\n| EMAIL     | 4                     | 0.02       | 1                    | 0.01       |\n| OTHERPHI  | 2                     | 0.01       | 0                    | 0          |\n| TOTAL     | 17171                 | 100        | 11283                | 100        |\n\n\n# Training procedure\n\n* Steps on how this model was trained can be found here: [Training](https://github.com/obi-ml-public/ehr_deidentification/tree/master/steps/train). The \"model_name_or_path\" was set to: \"roberta-large\".\n    * The dataset was sentencized with the en_core_sci_sm sentencizer from spacy.\n    * The dataset was then tokenized with a custom tokenizer built on top of the en_core_sci_sm tokenizer from spacy.\n    * For each sentence we added 32 tokens on the left (from previous sentences) and 32 tokens on the right (from the next sentences).\n    * The added tokens are not used for learning - i.e, the loss is not computed on these tokens - they are used as additional context.\n    * Each sequence contained a maximum of 128 tokens (including the 32 tokens added on). Longer sequences were split.\n    * The sentencized and tokenized dataset with the token level labels based on the BILOU notation was used to train the model.\n    * The model is fine-tuned from a pre-trained RoBERTa model.\n    \n* Training details:\n    * Input sequence length: 128\n    * Batch size: 32 (16 with 2 gradient accumulation steps)\n    * Optimizer: AdamW \n    * Learning rate: 5e-5\n    * Dropout: 0.1\n\n\n## Results\n\n# Questions?\n\nPost a Github issue on the repo: [Robust DeID](https://github.com/obi-ml-public/ehr_deidentification).\n",
    "library_name": "transformers"
  },
  {
    "model_id": "flair/ner-english-large",
    "model_name": "flair/ner-english-large",
    "author": "flair",
    "downloads": 690412,
    "likes": 44,
    "tags": [
      "flair",
      "pytorch",
      "token-classification",
      "sequence-tagger-model",
      "en",
      "dataset:conll2003",
      "arxiv:2011.06993",
      "region:us"
    ],
    "card_url": "https://huggingface.co/flair/ner-english-large",
    "dependencies": [
      [
        "flair",
        null
      ]
    ],
    "analysis_date": "2025-03-26T00:50:57.113322",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "unknown",
    "pipeline_tag": "token-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": "en",
      "tags": [
        "flair",
        "token-classification",
        "sequence-tagger-model"
      ],
      "datasets": [
        "conll2003"
      ],
      "widget": [
        {
          "text": "George Washington went to Washington"
        }
      ]
    },
    "card_text": "\n## English NER in Flair (large model)\n\nThis is the large 4-class NER model for English that ships with [Flair](https://github.com/flairNLP/flair/).\n\nF1-Score: **94,36** (corrected CoNLL-03)\n\nPredicts 4 tags:\n\n| **tag**                        | **meaning** |\n|---------------------------------|-----------|\n| PER         | person name | \n| LOC         | location name | \n| ORG         | organization name | \n| MISC         | other name | \n\nBased on document-level XLM-R embeddings and [FLERT](https://arxiv.org/pdf/2011.06993v1.pdf/).\n\n---\n\n### Demo: How to use in Flair\n\nRequires: **[Flair](https://github.com/flairNLP/flair/)** (`pip install flair`)\n\n```python\nfrom flair.data import Sentence\nfrom flair.models import SequenceTagger\n\n# load tagger\ntagger = SequenceTagger.load(\"flair/ner-english-large\")\n\n# make example sentence\nsentence = Sentence(\"George Washington went to Washington\")\n\n# predict NER tags\ntagger.predict(sentence)\n\n# print sentence\nprint(sentence)\n\n# print predicted NER spans\nprint('The following NER tags are found:')\n# iterate over entities and print\nfor entity in sentence.get_spans('ner'):\n    print(entity)\n\n```\n\nThis yields the following output:\n```\nSpan [1,2]: \"George Washington\"   [− Labels: PER (1.0)]\nSpan [5]: \"Washington\"   [− Labels: LOC (1.0)]\n```\n\nSo, the entities \"*George Washington*\" (labeled as a **person**) and \"*Washington*\" (labeled as a **location**) are found in the sentence \"*George Washington went to Washington*\". \n\n\n---\n\n### Training: Script to train this model\n\nThe following Flair script was used to train this model: \n\n```python\nimport torch\n\n# 1. get the corpus\nfrom flair.datasets import CONLL_03\n\ncorpus = CONLL_03()\n\n# 2. what tag do we want to predict?\ntag_type = 'ner'\n\n# 3. make the tag dictionary from the corpus\ntag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)\n\n# 4. initialize fine-tuneable transformer embeddings WITH document context\nfrom flair.embeddings import TransformerWordEmbeddings\n\nembeddings = TransformerWordEmbeddings(\n    model='xlm-roberta-large',\n    layers=\"-1\",\n    subtoken_pooling=\"first\",\n    fine_tune=True,\n    use_context=True,\n)\n\n# 5. initialize bare-bones sequence tagger (no CRF, no RNN, no reprojection)\nfrom flair.models import SequenceTagger\n\ntagger = SequenceTagger(\n    hidden_size=256,\n    embeddings=embeddings,\n    tag_dictionary=tag_dictionary,\n    tag_type='ner',\n    use_crf=False,\n    use_rnn=False,\n    reproject_embeddings=False,\n)\n\n# 6. initialize trainer with AdamW optimizer\nfrom flair.trainers import ModelTrainer\n\ntrainer = ModelTrainer(tagger, corpus, optimizer=torch.optim.AdamW)\n\n# 7. run training with XLM parameters (20 epochs, small LR)\nfrom torch.optim.lr_scheduler import OneCycleLR\n\ntrainer.train('resources/taggers/ner-english-large',\n              learning_rate=5.0e-6,\n              mini_batch_size=4,\n              mini_batch_chunk_size=1,\n              max_epochs=20,\n              scheduler=OneCycleLR,\n              embeddings_storage_mode='none',\n              weight_decay=0.,\n              )\n\n)\n```\n\n\n\n---\n\n### Cite\n\nPlease cite the following paper when using this model.\n\n```\n@misc{schweter2020flert,\n    title={FLERT: Document-Level Features for Named Entity Recognition},\n    author={Stefan Schweter and Alan Akbik},\n    year={2020},\n    eprint={2011.06993},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL}\n}\n```\n\n---\n\n### Issues?\n\nThe Flair issue tracker is available [here](https://github.com/flairNLP/flair/issues/).\n",
    "card_content": "---\nlanguage: en\ntags:\n- flair\n- token-classification\n- sequence-tagger-model\ndatasets:\n- conll2003\nwidget:\n- text: George Washington went to Washington\n---\n\n## English NER in Flair (large model)\n\nThis is the large 4-class NER model for English that ships with [Flair](https://github.com/flairNLP/flair/).\n\nF1-Score: **94,36** (corrected CoNLL-03)\n\nPredicts 4 tags:\n\n| **tag**                        | **meaning** |\n|---------------------------------|-----------|\n| PER         | person name | \n| LOC         | location name | \n| ORG         | organization name | \n| MISC         | other name | \n\nBased on document-level XLM-R embeddings and [FLERT](https://arxiv.org/pdf/2011.06993v1.pdf/).\n\n---\n\n### Demo: How to use in Flair\n\nRequires: **[Flair](https://github.com/flairNLP/flair/)** (`pip install flair`)\n\n```python\nfrom flair.data import Sentence\nfrom flair.models import SequenceTagger\n\n# load tagger\ntagger = SequenceTagger.load(\"flair/ner-english-large\")\n\n# make example sentence\nsentence = Sentence(\"George Washington went to Washington\")\n\n# predict NER tags\ntagger.predict(sentence)\n\n# print sentence\nprint(sentence)\n\n# print predicted NER spans\nprint('The following NER tags are found:')\n# iterate over entities and print\nfor entity in sentence.get_spans('ner'):\n    print(entity)\n\n```\n\nThis yields the following output:\n```\nSpan [1,2]: \"George Washington\"   [− Labels: PER (1.0)]\nSpan [5]: \"Washington\"   [− Labels: LOC (1.0)]\n```\n\nSo, the entities \"*George Washington*\" (labeled as a **person**) and \"*Washington*\" (labeled as a **location**) are found in the sentence \"*George Washington went to Washington*\". \n\n\n---\n\n### Training: Script to train this model\n\nThe following Flair script was used to train this model: \n\n```python\nimport torch\n\n# 1. get the corpus\nfrom flair.datasets import CONLL_03\n\ncorpus = CONLL_03()\n\n# 2. what tag do we want to predict?\ntag_type = 'ner'\n\n# 3. make the tag dictionary from the corpus\ntag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)\n\n# 4. initialize fine-tuneable transformer embeddings WITH document context\nfrom flair.embeddings import TransformerWordEmbeddings\n\nembeddings = TransformerWordEmbeddings(\n    model='xlm-roberta-large',\n    layers=\"-1\",\n    subtoken_pooling=\"first\",\n    fine_tune=True,\n    use_context=True,\n)\n\n# 5. initialize bare-bones sequence tagger (no CRF, no RNN, no reprojection)\nfrom flair.models import SequenceTagger\n\ntagger = SequenceTagger(\n    hidden_size=256,\n    embeddings=embeddings,\n    tag_dictionary=tag_dictionary,\n    tag_type='ner',\n    use_crf=False,\n    use_rnn=False,\n    reproject_embeddings=False,\n)\n\n# 6. initialize trainer with AdamW optimizer\nfrom flair.trainers import ModelTrainer\n\ntrainer = ModelTrainer(tagger, corpus, optimizer=torch.optim.AdamW)\n\n# 7. run training with XLM parameters (20 epochs, small LR)\nfrom torch.optim.lr_scheduler import OneCycleLR\n\ntrainer.train('resources/taggers/ner-english-large',\n              learning_rate=5.0e-6,\n              mini_batch_size=4,\n              mini_batch_chunk_size=1,\n              max_epochs=20,\n              scheduler=OneCycleLR,\n              embeddings_storage_mode='none',\n              weight_decay=0.,\n              )\n\n)\n```\n\n\n\n---\n\n### Cite\n\nPlease cite the following paper when using this model.\n\n```\n@misc{schweter2020flert,\n    title={FLERT: Document-Level Features for Named Entity Recognition},\n    author={Stefan Schweter and Alan Akbik},\n    year={2020},\n    eprint={2011.06993},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL}\n}\n```\n\n---\n\n### Issues?\n\nThe Flair issue tracker is available [here](https://github.com/flairNLP/flair/issues/).\n",
    "library_name": "flair"
  },
  {
    "model_id": "MMG/xlm-roberta-large-ner-spanish",
    "model_name": "MMG/xlm-roberta-large-ner-spanish",
    "author": "MMG",
    "downloads": 674268,
    "likes": 28,
    "tags": [
      "transformers",
      "pytorch",
      "safetensors",
      "xlm-roberta",
      "token-classification",
      "es",
      "dataset:CoNLL-2002",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/MMG/xlm-roberta-large-ner-spanish",
    "dependencies": null,
    "analysis_date": "2025-03-26T00:50:58.026108",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "xlm-roberta",
    "pipeline_tag": "token-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": [
        "es"
      ],
      "datasets": [
        "CoNLL-2002"
      ],
      "widget": [
        {
          "text": "Las oficinas de MMG están en Las Rozas."
        }
      ]
    },
    "card_text": "\n# xlm-roberta-large-ner-spanish\n\nThis model is a XLM-Roberta-large model fine-tuned for Named Entity Recognition (NER) over the Spanish portion of the CoNLL-2002 dataset. Evaluating it over the test subset of this dataset, we get a F1-score of 89.17, being one of the best NER for Spanish available at the moment.",
    "card_content": "---\nlanguage:\n- es\ndatasets:\n- CoNLL-2002\nwidget:\n- text: Las oficinas de MMG están en Las Rozas.\n---\n\n# xlm-roberta-large-ner-spanish\n\nThis model is a XLM-Roberta-large model fine-tuned for Named Entity Recognition (NER) over the Spanish portion of the CoNLL-2002 dataset. Evaluating it over the test subset of this dataset, we get a F1-score of 89.17, being one of the best NER for Spanish available at the moment.",
    "library_name": "transformers"
  },
  {
    "model_id": "FacebookAI/xlm-roberta-large-finetuned-conll03-english",
    "model_name": "FacebookAI/xlm-roberta-large-finetuned-conll03-english",
    "author": "FacebookAI",
    "downloads": 649971,
    "likes": 167,
    "tags": [
      "transformers",
      "pytorch",
      "rust",
      "onnx",
      "safetensors",
      "xlm-roberta",
      "token-classification",
      "multilingual",
      "af",
      "am",
      "ar",
      "as",
      "az",
      "be",
      "bg",
      "bn",
      "br",
      "bs",
      "ca",
      "cs",
      "cy",
      "da",
      "de",
      "el",
      "en",
      "eo",
      "es",
      "et",
      "eu",
      "fa",
      "fi",
      "fr",
      "fy",
      "ga",
      "gd",
      "gl",
      "gu",
      "ha",
      "he",
      "hi",
      "hr",
      "hu",
      "hy",
      "id",
      "is",
      "it",
      "ja",
      "jv",
      "ka",
      "kk",
      "km",
      "kn",
      "ko",
      "ku",
      "ky",
      "la",
      "lo",
      "lt",
      "lv",
      "mg",
      "mk",
      "ml",
      "mn",
      "mr",
      "ms",
      "my",
      "ne",
      "nl",
      "no",
      "om",
      "or",
      "pa",
      "pl",
      "ps",
      "pt",
      "ro",
      "ru",
      "sa",
      "sd",
      "si",
      "sk",
      "sl",
      "so",
      "sq",
      "sr",
      "su",
      "sv",
      "sw",
      "ta",
      "te",
      "th",
      "tl",
      "tr",
      "ug",
      "uk",
      "ur",
      "uz",
      "vi",
      "xh",
      "yi",
      "zh",
      "arxiv:1911.02116",
      "arxiv:2008.03415",
      "arxiv:1910.09700",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/FacebookAI/xlm-roberta-large-finetuned-conll03-english",
    "dependencies": [
      [
        "transformers",
        null
      ]
    ],
    "analysis_date": "2025-03-26T00:50:59.354787",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "xlm-roberta",
    "pipeline_tag": "token-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": [
        "multilingual",
        "af",
        "am",
        "ar",
        "as",
        "az",
        "be",
        "bg",
        "bn",
        "br",
        "bs",
        "ca",
        "cs",
        "cy",
        "da",
        "de",
        "el",
        "en",
        "eo",
        "es",
        "et",
        "eu",
        "fa",
        "fi",
        "fr",
        "fy",
        "ga",
        "gd",
        "gl",
        "gu",
        "ha",
        "he",
        "hi",
        "hr",
        "hu",
        "hy",
        "id",
        "is",
        "it",
        "ja",
        "jv",
        "ka",
        "kk",
        "km",
        "kn",
        "ko",
        "ku",
        "ky",
        "la",
        "lo",
        "lt",
        "lv",
        "mg",
        "mk",
        "ml",
        "mn",
        "mr",
        "ms",
        "my",
        "ne",
        "nl",
        false,
        "om",
        "or",
        "pa",
        "pl",
        "ps",
        "pt",
        "ro",
        "ru",
        "sa",
        "sd",
        "si",
        "sk",
        "sl",
        "so",
        "sq",
        "sr",
        "su",
        "sv",
        "sw",
        "ta",
        "te",
        "th",
        "tl",
        "tr",
        "ug",
        "uk",
        "ur",
        "uz",
        "vi",
        "xh",
        "yi",
        "zh"
      ]
    },
    "card_text": "\n# xlm-roberta-large-finetuned-conll03-english\n\n#  Table of Contents\n\n1. [Model Details](#model-details)\n2. [Uses](#uses)\n3. [Bias, Risks, and Limitations](#bias-risks-and-limitations)\n4. [Training](#training)\n5. [Evaluation](#evaluation)\n6. [Environmental Impact](#environmental-impact)\n7. [Technical Specifications](#technical-specifications)\n8. [Citation](#citation)\n9. [Model Card Authors](#model-card-authors)\n10. [How To Get Started With the Model](#how-to-get-started-with-the-model)\n\n\n# Model Details\n\n## Model Description\n\nThe XLM-RoBERTa model was proposed in [Unsupervised Cross-lingual Representation Learning at Scale](https://arxiv.org/abs/1911.02116) by Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer and Veselin Stoyanov. It is based on Facebook's RoBERTa model released in 2019. It is a large multi-lingual language model, trained on 2.5TB of filtered CommonCrawl data. This model is [XLM-RoBERTa-large](https://huggingface.co/xlm-roberta-large) fine-tuned with the [conll2003](https://huggingface.co/datasets/conll2003) dataset in English.\n\n- **Developed by:** See [associated paper](https://arxiv.org/abs/1911.02116)\n- **Model type:** Multi-lingual language model\n- **Language(s) (NLP) or Countries (images):** XLM-RoBERTa is a multilingual model trained on 100 different languages; see [GitHub Repo](https://github.com/facebookresearch/fairseq/tree/main/examples/xlmr) for full list; model is fine-tuned on a dataset in English\n- **License:** More information needed\n- **Related Models:** [RoBERTa](https://huggingface.co/roberta-base), [XLM](https://huggingface.co/docs/transformers/model_doc/xlm)\n    - **Parent Model:** [XLM-RoBERTa-large](https://huggingface.co/xlm-roberta-large)\n- **Resources for more information:** \n  -[GitHub Repo](https://github.com/facebookresearch/fairseq/tree/main/examples/xlmr)\n  -[Associated Paper](https://arxiv.org/abs/1911.02116)\n\n# Uses\n\n## Direct Use\n\nThe model is a language model. The model can be used for token classification, a natural language understanding task in which a label is assigned to some tokens in a text. \n\n## Downstream Use\n\nPotential downstream use cases include Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. To learn more about token classification and other potential downstream use cases, see the Hugging Face [token classification docs](https://huggingface.co/tasks/token-classification).\n\n## Out-of-Scope Use\n\nThe model should not be used to intentionally create hostile or alienating environments for people. \n\n# Bias, Risks, and Limitations\n\n**CONTENT WARNING: Readers should be made aware that language generated by this model may be disturbing or offensive to some and may propagate historical and current stereotypes.**\n\nSignificant research has explored bias and fairness issues with language models (see, e.g., [Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)). In the context of tasks relevant to this model, [Mishra et al. (2020)](https://arxiv.org/pdf/2008.03415.pdf) explore social biases in NER systems for English and find that there is systematic bias in existing NER systems in that they fail to identify named entities from different demographic groups (though this paper did not look at BERT). For example, using a sample sentence from [Mishra et al. (2020)](https://arxiv.org/pdf/2008.03415.pdf):\n\n```python\n>>> from transformers import pipeline\n>>> tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-large-finetuned-conll03-english\")\n>>> model = AutoModelForTokenClassification.from_pretrained(\"xlm-roberta-large-finetuned-conll03-english\")\n>>> classifier = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n>>> classifier(\"Alya told Jasmine that Andrew could pay with cash..\")\n[{'end': 2,\n  'entity': 'I-PER',\n  'index': 1,\n  'score': 0.9997861,\n  'start': 0,\n  'word': '▁Al'},\n {'end': 4,\n  'entity': 'I-PER',\n  'index': 2,\n  'score': 0.9998591,\n  'start': 2,\n  'word': 'ya'},\n {'end': 16,\n  'entity': 'I-PER',\n  'index': 4,\n  'score': 0.99995816,\n  'start': 10,\n  'word': '▁Jasmin'},\n {'end': 17,\n  'entity': 'I-PER',\n  'index': 5,\n  'score': 0.9999584,\n  'start': 16,\n  'word': 'e'},\n {'end': 29,\n  'entity': 'I-PER',\n  'index': 7,\n  'score': 0.99998057,\n  'start': 23,\n  'word': '▁Andrew'}]\n```\n\n## Recommendations\n\nUsers (both direct and downstream) should be made aware of the risks, biases and limitations of the model.\n\n# Training\n\nSee the following resources for training data and training procedure details: \n- [XLM-RoBERTa-large model card](https://huggingface.co/xlm-roberta-large)\n- [CoNLL-2003 data card](https://huggingface.co/datasets/conll2003)\n- [Associated paper](https://arxiv.org/pdf/1911.02116.pdf)\n \n# Evaluation\n\nSee the [associated paper](https://arxiv.org/pdf/1911.02116.pdf) for evaluation details.\n\n# Environmental Impact\n\nCarbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).\n\n- **Hardware Type:** 500 32GB Nvidia V100 GPUs (from the [associated paper](https://arxiv.org/pdf/1911.02116.pdf))\n- **Hours used:** More information needed\n- **Cloud Provider:** More information needed\n- **Compute Region:** More information needed\n- **Carbon Emitted:** More information needed\n\n# Technical Specifications\n\nSee the [associated paper](https://arxiv.org/pdf/1911.02116.pdf) for further details.\n\n# Citation\n\n**BibTeX:**\n\n```bibtex\n@article{conneau2019unsupervised,\n  title={Unsupervised Cross-lingual Representation Learning at Scale},\n  author={Conneau, Alexis and Khandelwal, Kartikay and Goyal, Naman and Chaudhary, Vishrav and Wenzek, Guillaume and Guzm{\\'a}n, Francisco and Grave, Edouard and Ott, Myle and Zettlemoyer, Luke and Stoyanov, Veselin},\n  journal={arXiv preprint arXiv:1911.02116},\n  year={2019}\n}\n```\n\n**APA:**\n- Conneau, A., Khandelwal, K., Goyal, N., Chaudhary, V., Wenzek, G., Guzmán, F., ... & Stoyanov, V. (2019). Unsupervised cross-lingual representation learning at scale. arXiv preprint arXiv:1911.02116.\n\n# Model Card Authors\n\nThis model card was written by the team at Hugging Face.\n\n# How to Get Started with the Model\n\nUse the code below to get started with the model. You can use this model directly within a pipeline for NER.\n\n<details>\n<summary> Click to expand </summary>\n\n```python\n>>> from transformers import AutoTokenizer, AutoModelForTokenClassification\n>>> from transformers import pipeline\n>>> tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-large-finetuned-conll03-english\")\n>>> model = AutoModelForTokenClassification.from_pretrained(\"xlm-roberta-large-finetuned-conll03-english\")\n>>> classifier = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n>>> classifier(\"Hello I'm Omar and I live in Zürich.\")\n\n[{'end': 14,\n  'entity': 'I-PER',\n  'index': 5,\n  'score': 0.9999175,\n  'start': 10,\n  'word': '▁Omar'},\n {'end': 35,\n  'entity': 'I-LOC',\n  'index': 10,\n  'score': 0.9999906,\n  'start': 29,\n  'word': '▁Zürich'}]\n```\n\n</details>",
    "card_content": "---\nlanguage:\n- multilingual\n- af\n- am\n- ar\n- as\n- az\n- be\n- bg\n- bn\n- br\n- bs\n- ca\n- cs\n- cy\n- da\n- de\n- el\n- en\n- eo\n- es\n- et\n- eu\n- fa\n- fi\n- fr\n- fy\n- ga\n- gd\n- gl\n- gu\n- ha\n- he\n- hi\n- hr\n- hu\n- hy\n- id\n- is\n- it\n- ja\n- jv\n- ka\n- kk\n- km\n- kn\n- ko\n- ku\n- ky\n- la\n- lo\n- lt\n- lv\n- mg\n- mk\n- ml\n- mn\n- mr\n- ms\n- my\n- ne\n- nl\n- false\n- om\n- or\n- pa\n- pl\n- ps\n- pt\n- ro\n- ru\n- sa\n- sd\n- si\n- sk\n- sl\n- so\n- sq\n- sr\n- su\n- sv\n- sw\n- ta\n- te\n- th\n- tl\n- tr\n- ug\n- uk\n- ur\n- uz\n- vi\n- xh\n- yi\n- zh\n---\n\n# xlm-roberta-large-finetuned-conll03-english\n\n#  Table of Contents\n\n1. [Model Details](#model-details)\n2. [Uses](#uses)\n3. [Bias, Risks, and Limitations](#bias-risks-and-limitations)\n4. [Training](#training)\n5. [Evaluation](#evaluation)\n6. [Environmental Impact](#environmental-impact)\n7. [Technical Specifications](#technical-specifications)\n8. [Citation](#citation)\n9. [Model Card Authors](#model-card-authors)\n10. [How To Get Started With the Model](#how-to-get-started-with-the-model)\n\n\n# Model Details\n\n## Model Description\n\nThe XLM-RoBERTa model was proposed in [Unsupervised Cross-lingual Representation Learning at Scale](https://arxiv.org/abs/1911.02116) by Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer and Veselin Stoyanov. It is based on Facebook's RoBERTa model released in 2019. It is a large multi-lingual language model, trained on 2.5TB of filtered CommonCrawl data. This model is [XLM-RoBERTa-large](https://huggingface.co/xlm-roberta-large) fine-tuned with the [conll2003](https://huggingface.co/datasets/conll2003) dataset in English.\n\n- **Developed by:** See [associated paper](https://arxiv.org/abs/1911.02116)\n- **Model type:** Multi-lingual language model\n- **Language(s) (NLP) or Countries (images):** XLM-RoBERTa is a multilingual model trained on 100 different languages; see [GitHub Repo](https://github.com/facebookresearch/fairseq/tree/main/examples/xlmr) for full list; model is fine-tuned on a dataset in English\n- **License:** More information needed\n- **Related Models:** [RoBERTa](https://huggingface.co/roberta-base), [XLM](https://huggingface.co/docs/transformers/model_doc/xlm)\n    - **Parent Model:** [XLM-RoBERTa-large](https://huggingface.co/xlm-roberta-large)\n- **Resources for more information:** \n  -[GitHub Repo](https://github.com/facebookresearch/fairseq/tree/main/examples/xlmr)\n  -[Associated Paper](https://arxiv.org/abs/1911.02116)\n\n# Uses\n\n## Direct Use\n\nThe model is a language model. The model can be used for token classification, a natural language understanding task in which a label is assigned to some tokens in a text. \n\n## Downstream Use\n\nPotential downstream use cases include Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. To learn more about token classification and other potential downstream use cases, see the Hugging Face [token classification docs](https://huggingface.co/tasks/token-classification).\n\n## Out-of-Scope Use\n\nThe model should not be used to intentionally create hostile or alienating environments for people. \n\n# Bias, Risks, and Limitations\n\n**CONTENT WARNING: Readers should be made aware that language generated by this model may be disturbing or offensive to some and may propagate historical and current stereotypes.**\n\nSignificant research has explored bias and fairness issues with language models (see, e.g., [Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)). In the context of tasks relevant to this model, [Mishra et al. (2020)](https://arxiv.org/pdf/2008.03415.pdf) explore social biases in NER systems for English and find that there is systematic bias in existing NER systems in that they fail to identify named entities from different demographic groups (though this paper did not look at BERT). For example, using a sample sentence from [Mishra et al. (2020)](https://arxiv.org/pdf/2008.03415.pdf):\n\n```python\n>>> from transformers import pipeline\n>>> tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-large-finetuned-conll03-english\")\n>>> model = AutoModelForTokenClassification.from_pretrained(\"xlm-roberta-large-finetuned-conll03-english\")\n>>> classifier = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n>>> classifier(\"Alya told Jasmine that Andrew could pay with cash..\")\n[{'end': 2,\n  'entity': 'I-PER',\n  'index': 1,\n  'score': 0.9997861,\n  'start': 0,\n  'word': '▁Al'},\n {'end': 4,\n  'entity': 'I-PER',\n  'index': 2,\n  'score': 0.9998591,\n  'start': 2,\n  'word': 'ya'},\n {'end': 16,\n  'entity': 'I-PER',\n  'index': 4,\n  'score': 0.99995816,\n  'start': 10,\n  'word': '▁Jasmin'},\n {'end': 17,\n  'entity': 'I-PER',\n  'index': 5,\n  'score': 0.9999584,\n  'start': 16,\n  'word': 'e'},\n {'end': 29,\n  'entity': 'I-PER',\n  'index': 7,\n  'score': 0.99998057,\n  'start': 23,\n  'word': '▁Andrew'}]\n```\n\n## Recommendations\n\nUsers (both direct and downstream) should be made aware of the risks, biases and limitations of the model.\n\n# Training\n\nSee the following resources for training data and training procedure details: \n- [XLM-RoBERTa-large model card](https://huggingface.co/xlm-roberta-large)\n- [CoNLL-2003 data card](https://huggingface.co/datasets/conll2003)\n- [Associated paper](https://arxiv.org/pdf/1911.02116.pdf)\n \n# Evaluation\n\nSee the [associated paper](https://arxiv.org/pdf/1911.02116.pdf) for evaluation details.\n\n# Environmental Impact\n\nCarbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).\n\n- **Hardware Type:** 500 32GB Nvidia V100 GPUs (from the [associated paper](https://arxiv.org/pdf/1911.02116.pdf))\n- **Hours used:** More information needed\n- **Cloud Provider:** More information needed\n- **Compute Region:** More information needed\n- **Carbon Emitted:** More information needed\n\n# Technical Specifications\n\nSee the [associated paper](https://arxiv.org/pdf/1911.02116.pdf) for further details.\n\n# Citation\n\n**BibTeX:**\n\n```bibtex\n@article{conneau2019unsupervised,\n  title={Unsupervised Cross-lingual Representation Learning at Scale},\n  author={Conneau, Alexis and Khandelwal, Kartikay and Goyal, Naman and Chaudhary, Vishrav and Wenzek, Guillaume and Guzm{\\'a}n, Francisco and Grave, Edouard and Ott, Myle and Zettlemoyer, Luke and Stoyanov, Veselin},\n  journal={arXiv preprint arXiv:1911.02116},\n  year={2019}\n}\n```\n\n**APA:**\n- Conneau, A., Khandelwal, K., Goyal, N., Chaudhary, V., Wenzek, G., Guzmán, F., ... & Stoyanov, V. (2019). Unsupervised cross-lingual representation learning at scale. arXiv preprint arXiv:1911.02116.\n\n# Model Card Authors\n\nThis model card was written by the team at Hugging Face.\n\n# How to Get Started with the Model\n\nUse the code below to get started with the model. You can use this model directly within a pipeline for NER.\n\n<details>\n<summary> Click to expand </summary>\n\n```python\n>>> from transformers import AutoTokenizer, AutoModelForTokenClassification\n>>> from transformers import pipeline\n>>> tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-large-finetuned-conll03-english\")\n>>> model = AutoModelForTokenClassification.from_pretrained(\"xlm-roberta-large-finetuned-conll03-english\")\n>>> classifier = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n>>> classifier(\"Hello I'm Omar and I live in Zürich.\")\n\n[{'end': 14,\n  'entity': 'I-PER',\n  'index': 5,\n  'score': 0.9999175,\n  'start': 10,\n  'word': '▁Omar'},\n {'end': 35,\n  'entity': 'I-LOC',\n  'index': 10,\n  'score': 0.9999906,\n  'start': 29,\n  'word': '▁Zürich'}]\n```\n\n</details>",
    "library_name": "transformers"
  },
  {
    "model_id": "tsmatz/xlm-roberta-ner-japanese",
    "model_name": "tsmatz/xlm-roberta-ner-japanese",
    "author": "tsmatz",
    "downloads": 635227,
    "likes": 25,
    "tags": [
      "transformers",
      "pytorch",
      "safetensors",
      "xlm-roberta",
      "token-classification",
      "generated_from_trainer",
      "ner",
      "bert",
      "ja",
      "base_model:FacebookAI/xlm-roberta-base",
      "base_model:finetune:FacebookAI/xlm-roberta-base",
      "license:mit",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/tsmatz/xlm-roberta-ner-japanese",
    "dependencies": [
      [
        "transformers",
        "4.23.1"
      ],
      [
        "torch",
        "1.12.1"
      ],
      [
        "datasets",
        "2.6.1"
      ],
      [
        "tokenizers",
        "0.13.1"
      ]
    ],
    "analysis_date": "2025-03-26T00:51:00.811755",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "xlm-roberta",
    "pipeline_tag": "token-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": [
        "ja"
      ],
      "license": "mit",
      "tags": [
        "generated_from_trainer",
        "ner",
        "bert"
      ],
      "metrics": [
        "f1"
      ],
      "widget": [
        {
          "text": "鈴井は4月の陽気の良い日に、鈴をつけて北海道のトムラウシへと登った"
        },
        {
          "text": "中国では、中国共産党による一党統治が続く"
        }
      ],
      "base_model": "xlm-roberta-base",
      "model-index": [
        {
          "name": "xlm-roberta-ner-ja",
          "results": []
        }
      ]
    },
    "card_text": "\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# xlm-roberta-ner-japanese\n\n(Japanese caption : 日本語の固有表現抽出のモデル)\n\nThis model is a fine-tuned version of [xlm-roberta-base](https://huggingface.co/xlm-roberta-base) (pre-trained cross-lingual ```RobertaModel```) trained for named entity recognition (NER) token classification.\n\nThe model is fine-tuned on NER dataset provided by Stockmark Inc, in which data is collected from Japanese Wikipedia articles.<br>\nSee [here](https://github.com/stockmarkteam/ner-wikipedia-dataset) for the license of this dataset.\n\nEach token is labeled by :\n\n| Label id | Tag | Tag in Widget | Description |\n|---|---|---|---|\n| 0 | O | (None) | others or nothing |\n| 1 | PER | PER | person |\n| 2 | ORG | ORG | general corporation organization |\n| 3 | ORG-P | P | political organization |\n| 4 | ORG-O | O | other organization |\n| 5 | LOC | LOC | location |\n| 6 | INS | INS | institution, facility |\n| 7 | PRD | PRD | product |\n| 8 | EVT | EVT | event |\n\n## Intended uses\n\n```python\nfrom transformers import pipeline\n\nmodel_name = \"tsmatz/xlm-roberta-ner-japanese\"\nclassifier = pipeline(\"token-classification\", model=model_name)\nresult = classifier(\"鈴井は4月の陽気の良い日に、鈴をつけて北海道のトムラウシへと登った\")\nprint(result)\n```\n\n## Training procedure\n\nYou can download the source code for fine-tuning from [here](https://github.com/tsmatz/huggingface-finetune-japanese/blob/master/01-named-entity.ipynb).\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-05\n- train_batch_size: 12\n- eval_batch_size: 12\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 5\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss | F1     |\n|:-------------:|:-----:|:----:|:---------------:|:------:|\n| No log        | 1.0   | 446  | 0.1510          | 0.8457 |\n| No log        | 2.0   | 892  | 0.0626          | 0.9261 |\n| No log        | 3.0   | 1338 | 0.0366          | 0.9580 |\n| No log        | 4.0   | 1784 | 0.0196          | 0.9792 |\n| No log        | 5.0   | 2230 | 0.0173          | 0.9864 |\n\n\n### Framework versions\n\n- Transformers 4.23.1\n- Pytorch 1.12.1+cu102\n- Datasets 2.6.1\n- Tokenizers 0.13.1\n",
    "card_content": "---\nlanguage:\n- ja\nlicense: mit\ntags:\n- generated_from_trainer\n- ner\n- bert\nmetrics:\n- f1\nwidget:\n- text: 鈴井は4月の陽気の良い日に、鈴をつけて北海道のトムラウシへと登った\n- text: 中国では、中国共産党による一党統治が続く\nbase_model: xlm-roberta-base\nmodel-index:\n- name: xlm-roberta-ner-ja\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# xlm-roberta-ner-japanese\n\n(Japanese caption : 日本語の固有表現抽出のモデル)\n\nThis model is a fine-tuned version of [xlm-roberta-base](https://huggingface.co/xlm-roberta-base) (pre-trained cross-lingual ```RobertaModel```) trained for named entity recognition (NER) token classification.\n\nThe model is fine-tuned on NER dataset provided by Stockmark Inc, in which data is collected from Japanese Wikipedia articles.<br>\nSee [here](https://github.com/stockmarkteam/ner-wikipedia-dataset) for the license of this dataset.\n\nEach token is labeled by :\n\n| Label id | Tag | Tag in Widget | Description |\n|---|---|---|---|\n| 0 | O | (None) | others or nothing |\n| 1 | PER | PER | person |\n| 2 | ORG | ORG | general corporation organization |\n| 3 | ORG-P | P | political organization |\n| 4 | ORG-O | O | other organization |\n| 5 | LOC | LOC | location |\n| 6 | INS | INS | institution, facility |\n| 7 | PRD | PRD | product |\n| 8 | EVT | EVT | event |\n\n## Intended uses\n\n```python\nfrom transformers import pipeline\n\nmodel_name = \"tsmatz/xlm-roberta-ner-japanese\"\nclassifier = pipeline(\"token-classification\", model=model_name)\nresult = classifier(\"鈴井は4月の陽気の良い日に、鈴をつけて北海道のトムラウシへと登った\")\nprint(result)\n```\n\n## Training procedure\n\nYou can download the source code for fine-tuning from [here](https://github.com/tsmatz/huggingface-finetune-japanese/blob/master/01-named-entity.ipynb).\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 5e-05\n- train_batch_size: 12\n- eval_batch_size: 12\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 5\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss | F1     |\n|:-------------:|:-----:|:----:|:---------------:|:------:|\n| No log        | 1.0   | 446  | 0.1510          | 0.8457 |\n| No log        | 2.0   | 892  | 0.0626          | 0.9261 |\n| No log        | 3.0   | 1338 | 0.0366          | 0.9580 |\n| No log        | 4.0   | 1784 | 0.0196          | 0.9792 |\n| No log        | 5.0   | 2230 | 0.0173          | 0.9864 |\n\n\n### Framework versions\n\n- Transformers 4.23.1\n- Pytorch 1.12.1+cu102\n- Datasets 2.6.1\n- Tokenizers 0.13.1\n",
    "library_name": "transformers"
  },
  {
    "model_id": "EmergentMethods/gliner_medium_news-v2.1",
    "model_name": "EmergentMethods/gliner_medium_news-v2.1",
    "author": "EmergentMethods",
    "downloads": 577238,
    "likes": 74,
    "tags": [
      "gliner",
      "pytorch",
      "token-classification",
      "en",
      "dataset:EmergentMethods/AskNews-NER-v0",
      "arxiv:2406.10258",
      "license:apache-2.0",
      "region:us"
    ],
    "card_url": "https://huggingface.co/EmergentMethods/gliner_medium_news-v2.1",
    "dependencies": [
      [
        "gliner",
        null
      ]
    ],
    "analysis_date": "2025-03-26T00:51:02.268579",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "unknown",
    "pipeline_tag": "token-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": [
        "en"
      ],
      "license": "apache-2.0",
      "tags": [
        "gliner"
      ],
      "datasets": [
        "EmergentMethods/AskNews-NER-v0"
      ],
      "pipeline_tag": "token-classification"
    },
    "card_text": "# Model Card for gliner_medium_news-v2.1\n\nThis model is a fine-tune of [GLiNER](https://huggingface.co/urchade/gliner_medium-v2.1) aimed at improving accuracy across a broad range of topics, especially with respect to long-context news entity extraction. As shown in the table below, these fine-tunes improved upon the base GLiNER model zero-shot accuracy by up to 7.5% across 18 benchmark datasets.\n\n![results table](assets/zero-shot_18_table.png)\n\nThe underlying dataset, [AskNews-NER-v0](https://huggingface.co/datasets/EmergentMethods/AskNews-NER-v0) was engineered with the objective of diversifying global perspectives by enforcing country/language/topic/temporal diversity. All data used to fine-tune this model was synthetically generated. WizardLM 13B v1.2 was used for translation/summarization of open-web news articles, while Llama3 70b instruct was used for entity extraction. Both the diversification and fine-tuning methods are presented in a our paper on [ArXiv](https://arxiv.org/abs/2406.10258).\n\n# Usage\n\n```python\nfrom gliner import GLiNER\n\nmodel = GLiNER.from_pretrained(\"EmergentMethods/gliner_medium_news-v2.1\")\n\ntext = \"\"\"\nThe Chihuahua State Public Security Secretariat (SSPE) arrested 35-year-old Salomón C. T. in Ciudad Juárez, found in possession of a stolen vehicle, a white GMC Yukon, which was reported stolen in the city's streets. The arrest was made by intelligence and police analysis personnel during an investigation in the border city. The arrest is related to a previous detention on February 6, which involved armed men in a private vehicle. The detainee and the vehicle were turned over to the Chihuahua State Attorney General's Office for further investigation into the case. \n\"\"\"\n\nlabels = [\"person\", \"location\", \"date\", \"event\", \"facility\", \"vehicle\", \"number\", \"organization\"]\n\nentities = model.predict_entities(text, labels)\n\nfor entity in entities:\n    print(entity[\"text\"], \"=>\", entity[\"label\"])\n```\n\nOutput:\n\n```\nChihuahua State Public Security Secretariat => organization\nSSPE => organization\n35-year-old => number\nSalomón C. T. => person\nCiudad Juárez => location\nGMC Yukon => vehicle\nFebruary 6 => date\nChihuahua State Attorney General's Office => organization\n```\n\n## Model Details\n\n### Model Description\n\n<!-- Provide a longer summary of what this model is. -->\n\nThe synthetic data underlying this news fine-tune was pulled from the [AskNews API](https://docs.asknews.app). We enforced diveristy across country/language/topic/time.\n\nCountries:\n\n![country distribution](assets/countries_distribution.png)\n\nEntity types:\n\n![entities](assets/entity-types_limited.png)\n\nTopics:\n\n![topics](assets/topics_fig_connected.png)\n\n\n- **Developed by:** [Emergent Methods](https://emergentmethods.ai/)\n- **Funded by:** [Emergent Methods](https://emergentmethods.ai/)\n- **Shared by:** [Emergent Methods](https://emergentmethods.ai/)\n- **Model type:** microsoft/deberta\n- **Language(s) (NLP):** English (en) (English texts and translations from Spanish (es), Portuguese (pt), German (de), Russian (ru), French (fr), Arabic (ar), Italian (it), Ukrainian (uk), Norwegian (no), Swedish (sv), Danish (da)).\n- **License:** Apache 2.0\n- **Finetuned from model:** [GLiNER](https://huggingface.co/urchade/gliner_medium-v2.1)\n\n### Model Sources [optional]\n\n<!-- Provide the basic links for the model. -->\n\n- **Repository:** To be added\n- **Paper:** To be added\n- **Demo:** To be added\n\n## Uses\n\n<!-- Address questions around how the model is intended to be used, including the foreseeable users of the model and those affected by the model. -->\n\n### Direct Use\n\n<!-- This section is for the model use without fine-tuning or plugging into a larger ecosystem/app. -->\n\nAs the name suggests, this model is aimed at generalist entity extraction. Although we used news to fine-tune this model, it improved accuracy across 18 benchmark datasets by up to 7.5%. This means that the broad and diversified underlying dataset has helped it to recognize and extract more entity types.\n\nThis model is shockingly compact, and can be used for high-throughput production usecases. This is another reason we have licensed this as Apache 2.0. Currently, [AskNews](https://asknews.app) is using this fine-tune for entity extraction in their system.\n\n\n## Bias, Risks, and Limitations\n\n<!-- This section is meant to convey both technical and sociotechnical limitations. -->\n\nAlthough the goal of the dataset is to reduce bias, and improve diversity, it is still biased to western languages and countries. This limitation originates from the abilities of Llama2 for the translation and summary generations. Further, any bias originating in Llama2 training data will also be present in this dataset, since Llama2 was used to summarize the open-web articles. Further, any biases present in Llama3 will be present in the present dataaset since Llama3 was used to extract entities from the summaries.\n\n![countries distribution](figures/topics_fig_connected.png)\n\n\n## How to Get Started with the Model\n\nUse the code below to get started with the model.\n\n\n\n## Training Details\n\n<!-- This should link to a Dataset Card, perhaps with a short stub of information on what the training data is all about as well as documentation related to data pre-processing or additional filtering. -->\n\nThe training dataset is [AskNews-NER-v0](https://huggingface.co/datasets/EmergentMethods/AskNews-NER-v0).\n\nOther training details can be found in the [companion paper](https://linktoarxiv.org).\n\n\n\n## Environmental Impact\n\n<!-- Total emissions (in grams of CO2eq) and additional considerations, such as electricity usage, go here. Edit the suggested text below accordingly -->\n\n- **Hardware Type:** 1xA4500\n- **Hours used:** 10\n- **Carbon Emitted:** 0.6 kg (According to [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute))\n\n\n## Citation\n\n<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->\n\n**BibTeX:**\n\nTo be added\n\n**APA:**\n\nTo be added\n\n## Model Authors\n\nElin Törnquist, Emergent Methods elin at emergentmethods.ai\nRobert Caulk, Emergent Methods rob at emergentmethods.ai\n\n\n## Model Contact\n\nElin Törnquist, Emergent Methods elin at emergentmethods.ai\nRobert Caulk, Emergent Methods rob at emergentmethods.ai",
    "card_content": "---\nlanguage:\n- en\nlicense: apache-2.0\ntags:\n- gliner\ndatasets:\n- EmergentMethods/AskNews-NER-v0\npipeline_tag: token-classification\n---\n# Model Card for gliner_medium_news-v2.1\n\nThis model is a fine-tune of [GLiNER](https://huggingface.co/urchade/gliner_medium-v2.1) aimed at improving accuracy across a broad range of topics, especially with respect to long-context news entity extraction. As shown in the table below, these fine-tunes improved upon the base GLiNER model zero-shot accuracy by up to 7.5% across 18 benchmark datasets.\n\n![results table](assets/zero-shot_18_table.png)\n\nThe underlying dataset, [AskNews-NER-v0](https://huggingface.co/datasets/EmergentMethods/AskNews-NER-v0) was engineered with the objective of diversifying global perspectives by enforcing country/language/topic/temporal diversity. All data used to fine-tune this model was synthetically generated. WizardLM 13B v1.2 was used for translation/summarization of open-web news articles, while Llama3 70b instruct was used for entity extraction. Both the diversification and fine-tuning methods are presented in a our paper on [ArXiv](https://arxiv.org/abs/2406.10258).\n\n# Usage\n\n```python\nfrom gliner import GLiNER\n\nmodel = GLiNER.from_pretrained(\"EmergentMethods/gliner_medium_news-v2.1\")\n\ntext = \"\"\"\nThe Chihuahua State Public Security Secretariat (SSPE) arrested 35-year-old Salomón C. T. in Ciudad Juárez, found in possession of a stolen vehicle, a white GMC Yukon, which was reported stolen in the city's streets. The arrest was made by intelligence and police analysis personnel during an investigation in the border city. The arrest is related to a previous detention on February 6, which involved armed men in a private vehicle. The detainee and the vehicle were turned over to the Chihuahua State Attorney General's Office for further investigation into the case. \n\"\"\"\n\nlabels = [\"person\", \"location\", \"date\", \"event\", \"facility\", \"vehicle\", \"number\", \"organization\"]\n\nentities = model.predict_entities(text, labels)\n\nfor entity in entities:\n    print(entity[\"text\"], \"=>\", entity[\"label\"])\n```\n\nOutput:\n\n```\nChihuahua State Public Security Secretariat => organization\nSSPE => organization\n35-year-old => number\nSalomón C. T. => person\nCiudad Juárez => location\nGMC Yukon => vehicle\nFebruary 6 => date\nChihuahua State Attorney General's Office => organization\n```\n\n## Model Details\n\n### Model Description\n\n<!-- Provide a longer summary of what this model is. -->\n\nThe synthetic data underlying this news fine-tune was pulled from the [AskNews API](https://docs.asknews.app). We enforced diveristy across country/language/topic/time.\n\nCountries:\n\n![country distribution](assets/countries_distribution.png)\n\nEntity types:\n\n![entities](assets/entity-types_limited.png)\n\nTopics:\n\n![topics](assets/topics_fig_connected.png)\n\n\n- **Developed by:** [Emergent Methods](https://emergentmethods.ai/)\n- **Funded by:** [Emergent Methods](https://emergentmethods.ai/)\n- **Shared by:** [Emergent Methods](https://emergentmethods.ai/)\n- **Model type:** microsoft/deberta\n- **Language(s) (NLP):** English (en) (English texts and translations from Spanish (es), Portuguese (pt), German (de), Russian (ru), French (fr), Arabic (ar), Italian (it), Ukrainian (uk), Norwegian (no), Swedish (sv), Danish (da)).\n- **License:** Apache 2.0\n- **Finetuned from model:** [GLiNER](https://huggingface.co/urchade/gliner_medium-v2.1)\n\n### Model Sources [optional]\n\n<!-- Provide the basic links for the model. -->\n\n- **Repository:** To be added\n- **Paper:** To be added\n- **Demo:** To be added\n\n## Uses\n\n<!-- Address questions around how the model is intended to be used, including the foreseeable users of the model and those affected by the model. -->\n\n### Direct Use\n\n<!-- This section is for the model use without fine-tuning or plugging into a larger ecosystem/app. -->\n\nAs the name suggests, this model is aimed at generalist entity extraction. Although we used news to fine-tune this model, it improved accuracy across 18 benchmark datasets by up to 7.5%. This means that the broad and diversified underlying dataset has helped it to recognize and extract more entity types.\n\nThis model is shockingly compact, and can be used for high-throughput production usecases. This is another reason we have licensed this as Apache 2.0. Currently, [AskNews](https://asknews.app) is using this fine-tune for entity extraction in their system.\n\n\n## Bias, Risks, and Limitations\n\n<!-- This section is meant to convey both technical and sociotechnical limitations. -->\n\nAlthough the goal of the dataset is to reduce bias, and improve diversity, it is still biased to western languages and countries. This limitation originates from the abilities of Llama2 for the translation and summary generations. Further, any bias originating in Llama2 training data will also be present in this dataset, since Llama2 was used to summarize the open-web articles. Further, any biases present in Llama3 will be present in the present dataaset since Llama3 was used to extract entities from the summaries.\n\n![countries distribution](figures/topics_fig_connected.png)\n\n\n## How to Get Started with the Model\n\nUse the code below to get started with the model.\n\n\n\n## Training Details\n\n<!-- This should link to a Dataset Card, perhaps with a short stub of information on what the training data is all about as well as documentation related to data pre-processing or additional filtering. -->\n\nThe training dataset is [AskNews-NER-v0](https://huggingface.co/datasets/EmergentMethods/AskNews-NER-v0).\n\nOther training details can be found in the [companion paper](https://linktoarxiv.org).\n\n\n\n## Environmental Impact\n\n<!-- Total emissions (in grams of CO2eq) and additional considerations, such as electricity usage, go here. Edit the suggested text below accordingly -->\n\n- **Hardware Type:** 1xA4500\n- **Hours used:** 10\n- **Carbon Emitted:** 0.6 kg (According to [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute))\n\n\n## Citation\n\n<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->\n\n**BibTeX:**\n\nTo be added\n\n**APA:**\n\nTo be added\n\n## Model Authors\n\nElin Törnquist, Emergent Methods elin at emergentmethods.ai\nRobert Caulk, Emergent Methods rob at emergentmethods.ai\n\n\n## Model Contact\n\nElin Törnquist, Emergent Methods elin at emergentmethods.ai\nRobert Caulk, Emergent Methods rob at emergentmethods.ai",
    "library_name": "gliner"
  },
  {
    "model_id": "dslim/bert-base-NER-uncased",
    "model_name": "dslim/bert-base-NER-uncased",
    "author": "dslim",
    "downloads": 514890,
    "likes": 32,
    "tags": [
      "transformers",
      "pytorch",
      "tf",
      "jax",
      "safetensors",
      "bert",
      "token-classification",
      "license:mit",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/dslim/bert-base-NER-uncased",
    "dependencies": null,
    "analysis_date": "2025-03-26T00:51:03.312320",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "bert",
    "pipeline_tag": "token-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "license": "mit"
    },
    "card_text": "",
    "card_content": "---\nlicense: mit\n---\n",
    "library_name": "transformers"
  },
  {
    "model_id": "EvanD/xlm-roberta-base-romanian-ner-ronec",
    "model_name": "EvanD/xlm-roberta-base-romanian-ner-ronec",
    "author": "EvanD",
    "downloads": 388672,
    "likes": 3,
    "tags": [
      "transformers",
      "pytorch",
      "xlm-roberta",
      "token-classification",
      "named-entity-recognition",
      "sequence-tagger-model",
      "ro",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/EvanD/xlm-roberta-base-romanian-ner-ronec",
    "dependencies": [
      [
        "transformers",
        null
      ]
    ],
    "analysis_date": "2025-03-26T00:51:04.633027",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "xlm-roberta",
    "pipeline_tag": "token-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": [
        "ro"
      ],
      "tags": [
        "named-entity-recognition",
        "sequence-tagger-model"
      ],
      "pipeline_tag": "token-classification",
      "widget": [
        {
          "text": "Numele meu este Amadeus Wolfgang și locuiesc în Berlin"
        }
      ],
      "inference": {
        "parameters": {
          "aggregation_strategy": "simple",
          "grouped_entities": true
        }
      }
    },
    "card_text": "\nxlm-roberta model trained on [ronec](https://github.com/dumitrescustefan/ronec) dataset, performing 95 f1-Macro on test set.\n\n| Test metric            | Results             |\n|------------------------|--------------------------|\n| test_f1_mac_ronec      | 0.9547659158706665       |\n| test_loss_ronec        | 0.16371206939220428      |\n| test_prec_mac_ronec    | 0.8663718700408936       |\n| test_rec_mac_ronec     | 0.8695588111877441       |\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\nfrom transformers import pipeline\n\ntokenizer = AutoTokenizer.from_pretrained(\"EvanD/xlm-roberta-base-romanian-ner-ronec\")\nner_model = AutoModelForTokenClassification.from_pretrained(\"EvanD/xlm-roberta-base-romanian-ner-ronec\")\n\nnlp = pipeline(\"ner\", model=ner_model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\nexample = \"Numele meu este Amadeus Wolfgang și locuiesc în Berlin\"\n\nner_results = nlp(example)\nprint(ner_results)\n\n# [\n#     {\n#         'entity_group': 'PER',\n#         'score': 0.9966806,\n#         'word': 'Amadeus Wolfgang',\n#         'start': 16,\n#         'end': 32\n#     },\n#     {'entity_group': 'GPE',\n#      'score': 0.99694663,\n#      'word': 'Berlin',\n#      'start': 48,\n#      'end': 54\n#      }\n# ]\n```",
    "card_content": "---\nlanguage:\n- ro\ntags:\n- named-entity-recognition\n- sequence-tagger-model\npipeline_tag: token-classification\nwidget:\n- text: Numele meu este Amadeus Wolfgang și locuiesc în Berlin\ninference:\n  parameters:\n    aggregation_strategy: simple\n    grouped_entities: true\n---\n\nxlm-roberta model trained on [ronec](https://github.com/dumitrescustefan/ronec) dataset, performing 95 f1-Macro on test set.\n\n| Test metric            | Results             |\n|------------------------|--------------------------|\n| test_f1_mac_ronec      | 0.9547659158706665       |\n| test_loss_ronec        | 0.16371206939220428      |\n| test_prec_mac_ronec    | 0.8663718700408936       |\n| test_rec_mac_ronec     | 0.8695588111877441       |\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\nfrom transformers import pipeline\n\ntokenizer = AutoTokenizer.from_pretrained(\"EvanD/xlm-roberta-base-romanian-ner-ronec\")\nner_model = AutoModelForTokenClassification.from_pretrained(\"EvanD/xlm-roberta-base-romanian-ner-ronec\")\n\nnlp = pipeline(\"ner\", model=ner_model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\nexample = \"Numele meu este Amadeus Wolfgang și locuiesc în Berlin\"\n\nner_results = nlp(example)\nprint(ner_results)\n\n# [\n#     {\n#         'entity_group': 'PER',\n#         'score': 0.9966806,\n#         'word': 'Amadeus Wolfgang',\n#         'start': 16,\n#         'end': 32\n#     },\n#     {'entity_group': 'GPE',\n#      'score': 0.99694663,\n#      'word': 'Berlin',\n#      'start': 48,\n#      'end': 54\n#      }\n# ]\n```",
    "library_name": "transformers"
  },
  {
    "model_id": "flair/ner-french",
    "model_name": "flair/ner-french",
    "author": "flair",
    "downloads": 343206,
    "likes": 14,
    "tags": [
      "flair",
      "pytorch",
      "token-classification",
      "sequence-tagger-model",
      "fr",
      "dataset:conll2003",
      "region:us"
    ],
    "card_url": "https://huggingface.co/flair/ner-french",
    "dependencies": [
      [
        "flair",
        null
      ]
    ],
    "analysis_date": "2025-03-26T00:51:05.767815",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "unknown",
    "pipeline_tag": "token-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": "fr",
      "tags": [
        "flair",
        "token-classification",
        "sequence-tagger-model"
      ],
      "datasets": [
        "conll2003"
      ],
      "widget": [
        {
          "text": "George Washington est allé à Washington"
        }
      ]
    },
    "card_text": "\n## French NER in Flair (default model)\n\nThis is the standard 4-class NER model for French that ships with [Flair](https://github.com/flairNLP/flair/).\n\nF1-Score: **90,61** (WikiNER)\n\nPredicts 4 tags:\n\n| **tag**                        | **meaning** |\n|---------------------------------|-----------|\n| PER         | person name | \n| LOC         | location name | \n| ORG         | organization name | \n| MISC         | other name | \n\nBased on [Flair embeddings](https://www.aclweb.org/anthology/C18-1139/) and LSTM-CRF.\n\n---\n\n### Demo: How to use in Flair\n\nRequires: **[Flair](https://github.com/flairNLP/flair/)** (`pip install flair`)\n\n```python\nfrom flair.data import Sentence\nfrom flair.models import SequenceTagger\n\n# load tagger\ntagger = SequenceTagger.load(\"flair/ner-french\")\n\n# make example sentence\nsentence = Sentence(\"George Washington est allé à Washington\")\n\n# predict NER tags\ntagger.predict(sentence)\n\n# print sentence\nprint(sentence)\n\n# print predicted NER spans\nprint('The following NER tags are found:')\n# iterate over entities and print\nfor entity in sentence.get_spans('ner'):\n    print(entity)\n\n```\n\nThis yields the following output:\n```\nSpan [1,2]: \"George Washington\"   [− Labels: PER (0.7394)]\nSpan [6]: \"Washington\"   [− Labels: LOC (0.9161)]\n```\n\nSo, the entities \"*George Washington*\" (labeled as a **person**) and \"*Washington*\" (labeled as a **location**) are found in the sentence \"*George Washington est allé à Washington*\". \n\n\n---\n\n### Training: Script to train this model\n\nThe following Flair script was used to train this model: \n\n```python\nfrom flair.data import Corpus\nfrom flair.datasets import WIKINER_FRENCH\nfrom flair.embeddings import WordEmbeddings, StackedEmbeddings, FlairEmbeddings\n\n# 1. get the corpus\ncorpus: Corpus = WIKINER_FRENCH()\n\n# 2. what tag do we want to predict?\ntag_type = 'ner'\n\n# 3. make the tag dictionary from the corpus\ntag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)\n\n# 4. initialize each embedding we use\nembedding_types = [\n\n    # GloVe embeddings\n    WordEmbeddings('fr'),\n\n    # contextual string embeddings, forward\n    FlairEmbeddings('fr-forward'),\n\n    # contextual string embeddings, backward\n    FlairEmbeddings('fr-backward'),\n]\n\n# embedding stack consists of Flair and GloVe embeddings\nembeddings = StackedEmbeddings(embeddings=embedding_types)\n\n# 5. initialize sequence tagger\nfrom flair.models import SequenceTagger\n\ntagger = SequenceTagger(hidden_size=256,\n                        embeddings=embeddings,\n                        tag_dictionary=tag_dictionary,\n                        tag_type=tag_type)\n\n# 6. initialize trainer\nfrom flair.trainers import ModelTrainer\n\ntrainer = ModelTrainer(tagger, corpus)\n\n# 7. run training\ntrainer.train('resources/taggers/ner-french',\n              train_with_dev=True,\n              max_epochs=150)\n```\n\n\n\n---\n\n### Cite\n\nPlease cite the following paper when using this model.\n\n```\n@inproceedings{akbik2018coling,\n  title={Contextual String Embeddings for Sequence Labeling},\n  author={Akbik, Alan and Blythe, Duncan and Vollgraf, Roland},\n  booktitle = {{COLING} 2018, 27th International Conference on Computational Linguistics},\n  pages     = {1638--1649},\n  year      = {2018}\n}\n```\n\n\n---\n\n### Issues?\n\nThe Flair issue tracker is available [here](https://github.com/flairNLP/flair/issues/).\n",
    "card_content": "---\nlanguage: fr\ntags:\n- flair\n- token-classification\n- sequence-tagger-model\ndatasets:\n- conll2003\nwidget:\n- text: George Washington est allé à Washington\n---\n\n## French NER in Flair (default model)\n\nThis is the standard 4-class NER model for French that ships with [Flair](https://github.com/flairNLP/flair/).\n\nF1-Score: **90,61** (WikiNER)\n\nPredicts 4 tags:\n\n| **tag**                        | **meaning** |\n|---------------------------------|-----------|\n| PER         | person name | \n| LOC         | location name | \n| ORG         | organization name | \n| MISC         | other name | \n\nBased on [Flair embeddings](https://www.aclweb.org/anthology/C18-1139/) and LSTM-CRF.\n\n---\n\n### Demo: How to use in Flair\n\nRequires: **[Flair](https://github.com/flairNLP/flair/)** (`pip install flair`)\n\n```python\nfrom flair.data import Sentence\nfrom flair.models import SequenceTagger\n\n# load tagger\ntagger = SequenceTagger.load(\"flair/ner-french\")\n\n# make example sentence\nsentence = Sentence(\"George Washington est allé à Washington\")\n\n# predict NER tags\ntagger.predict(sentence)\n\n# print sentence\nprint(sentence)\n\n# print predicted NER spans\nprint('The following NER tags are found:')\n# iterate over entities and print\nfor entity in sentence.get_spans('ner'):\n    print(entity)\n\n```\n\nThis yields the following output:\n```\nSpan [1,2]: \"George Washington\"   [− Labels: PER (0.7394)]\nSpan [6]: \"Washington\"   [− Labels: LOC (0.9161)]\n```\n\nSo, the entities \"*George Washington*\" (labeled as a **person**) and \"*Washington*\" (labeled as a **location**) are found in the sentence \"*George Washington est allé à Washington*\". \n\n\n---\n\n### Training: Script to train this model\n\nThe following Flair script was used to train this model: \n\n```python\nfrom flair.data import Corpus\nfrom flair.datasets import WIKINER_FRENCH\nfrom flair.embeddings import WordEmbeddings, StackedEmbeddings, FlairEmbeddings\n\n# 1. get the corpus\ncorpus: Corpus = WIKINER_FRENCH()\n\n# 2. what tag do we want to predict?\ntag_type = 'ner'\n\n# 3. make the tag dictionary from the corpus\ntag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)\n\n# 4. initialize each embedding we use\nembedding_types = [\n\n    # GloVe embeddings\n    WordEmbeddings('fr'),\n\n    # contextual string embeddings, forward\n    FlairEmbeddings('fr-forward'),\n\n    # contextual string embeddings, backward\n    FlairEmbeddings('fr-backward'),\n]\n\n# embedding stack consists of Flair and GloVe embeddings\nembeddings = StackedEmbeddings(embeddings=embedding_types)\n\n# 5. initialize sequence tagger\nfrom flair.models import SequenceTagger\n\ntagger = SequenceTagger(hidden_size=256,\n                        embeddings=embeddings,\n                        tag_dictionary=tag_dictionary,\n                        tag_type=tag_type)\n\n# 6. initialize trainer\nfrom flair.trainers import ModelTrainer\n\ntrainer = ModelTrainer(tagger, corpus)\n\n# 7. run training\ntrainer.train('resources/taggers/ner-french',\n              train_with_dev=True,\n              max_epochs=150)\n```\n\n\n\n---\n\n### Cite\n\nPlease cite the following paper when using this model.\n\n```\n@inproceedings{akbik2018coling,\n  title={Contextual String Embeddings for Sequence Labeling},\n  author={Akbik, Alan and Blythe, Duncan and Vollgraf, Roland},\n  booktitle = {{COLING} 2018, 27th International Conference on Computational Linguistics},\n  pages     = {1638--1649},\n  year      = {2018}\n}\n```\n\n\n---\n\n### Issues?\n\nThe Flair issue tracker is available [here](https://github.com/flairNLP/flair/issues/).\n",
    "library_name": "flair"
  },
  {
    "model_id": "oliverguhr/fullstop-punctuation-multilang-large",
    "model_name": "oliverguhr/fullstop-punctuation-multilang-large",
    "author": "oliverguhr",
    "downloads": 319747,
    "likes": 161,
    "tags": [
      "transformers",
      "pytorch",
      "tf",
      "onnx",
      "safetensors",
      "xlm-roberta",
      "token-classification",
      "punctuation prediction",
      "punctuation",
      "en",
      "de",
      "fr",
      "it",
      "multilingual",
      "dataset:wmt/europarl",
      "license:mit",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/oliverguhr/fullstop-punctuation-multilang-large",
    "dependencies": [
      [
        "deepmultilingualpunctuation",
        null
      ]
    ],
    "analysis_date": "2025-03-26T00:51:07.002237",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "xlm-roberta",
    "pipeline_tag": "token-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": [
        "en",
        "de",
        "fr",
        "it",
        "multilingual"
      ],
      "license": "mit",
      "tags": [
        "punctuation prediction",
        "punctuation"
      ],
      "datasets": "wmt/europarl",
      "metrics": [
        "f1"
      ],
      "widget": [
        {
          "text": "Ho sentito che ti sei laureata il che mi fa molto piacere",
          "example_title": "Italian"
        },
        {
          "text": "Tous les matins vers quatre heures mon père ouvrait la porte de ma chambre",
          "example_title": "French"
        },
        {
          "text": "Ist das eine Frage Frau Müller",
          "example_title": "German"
        },
        {
          "text": "Yet she blushed as if with guilt when Cynthia reading her thoughts said to her one day Molly you're very glad to get rid of us are not you",
          "example_title": "English"
        }
      ]
    },
    "card_text": "\nThis model predicts the punctuation of English, Italian, French and German texts. We developed it to restore the punctuation of transcribed spoken language. \n\nThis multilanguage model was trained on the [Europarl Dataset](https://huggingface.co/datasets/wmt/europarl) provided by the [SEPP-NLG Shared Task](https://sites.google.com/view/sentence-segmentation). *Please note that this dataset consists of political speeches. Therefore the model might perform differently on texts from other domains.*\n\nThe model restores the following punctuation markers: **\".\" \",\" \"?\" \"-\" \":\"**\n## Sample Code\nWe provide a simple python package that allows you to process text of any length.\n\n## Install \n\nTo get started install the package from [pypi](https://pypi.org/project/deepmultilingualpunctuation/):\n\n```bash\npip install deepmultilingualpunctuation\n```\n### Restore Punctuation\n```python\nfrom deepmultilingualpunctuation import PunctuationModel\n\nmodel = PunctuationModel()\ntext = \"My name is Clara and I live in Berkeley California Ist das eine Frage Frau Müller\"\nresult = model.restore_punctuation(text)\nprint(result)\n```\n\n**output**\n> My name is Clara and I live in Berkeley, California. Ist das eine Frage, Frau Müller?\n\n\n### Predict Labels \n```python\nfrom deepmultilingualpunctuation import PunctuationModel\n\nmodel = PunctuationModel()\ntext = \"My name is Clara and I live in Berkeley California Ist das eine Frage Frau Müller\"\nclean_text = model.preprocess(text)\nlabled_words = model.predict(clean_text)\nprint(labled_words)\n```\n\n**output**\n\n> [['My', '0', 0.9999887], ['name', '0', 0.99998665], ['is', '0', 0.9998579], ['Clara', '0', 0.6752215], ['and', '0', 0.99990904], ['I', '0', 0.9999877], ['live', '0', 0.9999839], ['in', '0', 0.9999515], ['Berkeley', ',', 0.99800044], ['California', '.', 0.99534047], ['Ist', '0', 0.99998784], ['das', '0', 0.99999154], ['eine', '0', 0.9999918], ['Frage', ',', 0.99622655], ['Frau', '0', 0.9999889], ['Müller', '?', 0.99863917]]\n\n\n\n\n## Results \n\nThe performance differs for the single punctuation markers as hyphens and colons, in many cases, are optional and can be substituted by either a comma or a full stop. The model achieves the following F1 scores for the different languages:\n\n| Label         | EN    | DE    | FR    | IT    |\n| ------------- | ----- | ----- | ----- | ----- |\n| 0             | 0.991 | 0.997 | 0.992 | 0.989 |\n| .             | 0.948 | 0.961 | 0.945 | 0.942 |\n| ?             | 0.890 | 0.893 | 0.871 | 0.832 |\n| ,             | 0.819 | 0.945 | 0.831 | 0.798 |\n| :             | 0.575 | 0.652 | 0.620 | 0.588 |\n| -             | 0.425 | 0.435 | 0.431 | 0.421 |\n| macro average | 0.775 | 0.814 | 0.782 | 0.762 |\n\n## Languages\n\n### Models\n\n| Languages                                  | Model                                                        |\n| ------------------------------------------ | ------------------------------------------------------------ |\n| English, Italian, French and German        | [oliverguhr/fullstop-punctuation-multilang-large](https://huggingface.co/oliverguhr/fullstop-punctuation-multilang-large) |\n| English, Italian, French, German and Dutch | [oliverguhr/fullstop-punctuation-multilingual-sonar-base](https://huggingface.co/oliverguhr/fullstop-punctuation-multilingual-sonar-base) |\n| Dutch                                      | [oliverguhr/fullstop-dutch-sonar-punctuation-prediction](https://huggingface.co/oliverguhr/fullstop-dutch-sonar-punctuation-prediction) |\n\n### Community Models\n\n| Languages                                  | Model                                                        |\n| ------------------------------------------ | ------------------------------------------------------------ |\n|English, German, French, Spanish, Bulgarian, Italian, Polish, Dutch, Czech, Portugese, Slovak, Slovenian| [kredor/punctuate-all](https://huggingface.co/kredor/punctuate-all)                                                             |\n| Catalan                                    | [softcatala/fullstop-catalan-punctuation-prediction](https://huggingface.co/softcatala/fullstop-catalan-punctuation-prediction) |\n| Welsh | [techiaith/fullstop-welsh-punctuation-prediction](https://huggingface.co/techiaith/fullstop-welsh-punctuation-prediction) |\n\nYou can use different models by setting the model parameter:\n\n```python\nmodel = PunctuationModel(model = \"oliverguhr/fullstop-dutch-punctuation-prediction\")\n```\n\n## Where do I find the code and can I train my own model?\n\nYes you can! For complete code of the reareach project take a look at [this repository](https://github.com/oliverguhr/fullstop-deep-punctuation-prediction).\n\nThere is also an guide on [how to fine tune this model for you data / language](https://github.com/oliverguhr/fullstop-deep-punctuation-prediction/blob/main/other_languages/readme.md). \n\n\n## References\n```\n@article{guhr-EtAl:2021:fullstop,\n  title={FullStop: Multilingual Deep Models for Punctuation Prediction},\n  author    = {Guhr, Oliver  and  Schumann, Anne-Kathrin  and  Bahrmann, Frank  and  Böhme, Hans Joachim},\n  booktitle      = {Proceedings of the Swiss Text Analytics Conference 2021},\n  month          = {June},\n  year           = {2021},\n  address        = {Winterthur, Switzerland},\n  publisher      = {CEUR Workshop Proceedings},  \n  url       = {http://ceur-ws.org/Vol-2957/sepp_paper4.pdf}\n}\n```",
    "card_content": "---\nlanguage:\n- en\n- de\n- fr\n- it\n- multilingual\nlicense: mit\ntags:\n- punctuation prediction\n- punctuation\ndatasets: wmt/europarl\nmetrics:\n- f1\nwidget:\n- text: Ho sentito che ti sei laureata il che mi fa molto piacere\n  example_title: Italian\n- text: Tous les matins vers quatre heures mon père ouvrait la porte de ma chambre\n  example_title: French\n- text: Ist das eine Frage Frau Müller\n  example_title: German\n- text: Yet she blushed as if with guilt when Cynthia reading her thoughts said to\n    her one day Molly you're very glad to get rid of us are not you\n  example_title: English\n---\n\nThis model predicts the punctuation of English, Italian, French and German texts. We developed it to restore the punctuation of transcribed spoken language. \n\nThis multilanguage model was trained on the [Europarl Dataset](https://huggingface.co/datasets/wmt/europarl) provided by the [SEPP-NLG Shared Task](https://sites.google.com/view/sentence-segmentation). *Please note that this dataset consists of political speeches. Therefore the model might perform differently on texts from other domains.*\n\nThe model restores the following punctuation markers: **\".\" \",\" \"?\" \"-\" \":\"**\n## Sample Code\nWe provide a simple python package that allows you to process text of any length.\n\n## Install \n\nTo get started install the package from [pypi](https://pypi.org/project/deepmultilingualpunctuation/):\n\n```bash\npip install deepmultilingualpunctuation\n```\n### Restore Punctuation\n```python\nfrom deepmultilingualpunctuation import PunctuationModel\n\nmodel = PunctuationModel()\ntext = \"My name is Clara and I live in Berkeley California Ist das eine Frage Frau Müller\"\nresult = model.restore_punctuation(text)\nprint(result)\n```\n\n**output**\n> My name is Clara and I live in Berkeley, California. Ist das eine Frage, Frau Müller?\n\n\n### Predict Labels \n```python\nfrom deepmultilingualpunctuation import PunctuationModel\n\nmodel = PunctuationModel()\ntext = \"My name is Clara and I live in Berkeley California Ist das eine Frage Frau Müller\"\nclean_text = model.preprocess(text)\nlabled_words = model.predict(clean_text)\nprint(labled_words)\n```\n\n**output**\n\n> [['My', '0', 0.9999887], ['name', '0', 0.99998665], ['is', '0', 0.9998579], ['Clara', '0', 0.6752215], ['and', '0', 0.99990904], ['I', '0', 0.9999877], ['live', '0', 0.9999839], ['in', '0', 0.9999515], ['Berkeley', ',', 0.99800044], ['California', '.', 0.99534047], ['Ist', '0', 0.99998784], ['das', '0', 0.99999154], ['eine', '0', 0.9999918], ['Frage', ',', 0.99622655], ['Frau', '0', 0.9999889], ['Müller', '?', 0.99863917]]\n\n\n\n\n## Results \n\nThe performance differs for the single punctuation markers as hyphens and colons, in many cases, are optional and can be substituted by either a comma or a full stop. The model achieves the following F1 scores for the different languages:\n\n| Label         | EN    | DE    | FR    | IT    |\n| ------------- | ----- | ----- | ----- | ----- |\n| 0             | 0.991 | 0.997 | 0.992 | 0.989 |\n| .             | 0.948 | 0.961 | 0.945 | 0.942 |\n| ?             | 0.890 | 0.893 | 0.871 | 0.832 |\n| ,             | 0.819 | 0.945 | 0.831 | 0.798 |\n| :             | 0.575 | 0.652 | 0.620 | 0.588 |\n| -             | 0.425 | 0.435 | 0.431 | 0.421 |\n| macro average | 0.775 | 0.814 | 0.782 | 0.762 |\n\n## Languages\n\n### Models\n\n| Languages                                  | Model                                                        |\n| ------------------------------------------ | ------------------------------------------------------------ |\n| English, Italian, French and German        | [oliverguhr/fullstop-punctuation-multilang-large](https://huggingface.co/oliverguhr/fullstop-punctuation-multilang-large) |\n| English, Italian, French, German and Dutch | [oliverguhr/fullstop-punctuation-multilingual-sonar-base](https://huggingface.co/oliverguhr/fullstop-punctuation-multilingual-sonar-base) |\n| Dutch                                      | [oliverguhr/fullstop-dutch-sonar-punctuation-prediction](https://huggingface.co/oliverguhr/fullstop-dutch-sonar-punctuation-prediction) |\n\n### Community Models\n\n| Languages                                  | Model                                                        |\n| ------------------------------------------ | ------------------------------------------------------------ |\n|English, German, French, Spanish, Bulgarian, Italian, Polish, Dutch, Czech, Portugese, Slovak, Slovenian| [kredor/punctuate-all](https://huggingface.co/kredor/punctuate-all)                                                             |\n| Catalan                                    | [softcatala/fullstop-catalan-punctuation-prediction](https://huggingface.co/softcatala/fullstop-catalan-punctuation-prediction) |\n| Welsh | [techiaith/fullstop-welsh-punctuation-prediction](https://huggingface.co/techiaith/fullstop-welsh-punctuation-prediction) |\n\nYou can use different models by setting the model parameter:\n\n```python\nmodel = PunctuationModel(model = \"oliverguhr/fullstop-dutch-punctuation-prediction\")\n```\n\n## Where do I find the code and can I train my own model?\n\nYes you can! For complete code of the reareach project take a look at [this repository](https://github.com/oliverguhr/fullstop-deep-punctuation-prediction).\n\nThere is also an guide on [how to fine tune this model for you data / language](https://github.com/oliverguhr/fullstop-deep-punctuation-prediction/blob/main/other_languages/readme.md). \n\n\n## References\n```\n@article{guhr-EtAl:2021:fullstop,\n  title={FullStop: Multilingual Deep Models for Punctuation Prediction},\n  author    = {Guhr, Oliver  and  Schumann, Anne-Kathrin  and  Bahrmann, Frank  and  Böhme, Hans Joachim},\n  booktitle      = {Proceedings of the Swiss Text Analytics Conference 2021},\n  month          = {June},\n  year           = {2021},\n  address        = {Winterthur, Switzerland},\n  publisher      = {CEUR Workshop Proceedings},  \n  url       = {http://ceur-ws.org/Vol-2957/sepp_paper4.pdf}\n}\n```",
    "library_name": "transformers"
  },
  {
    "model_id": "Davlan/bert-base-multilingual-cased-ner-hrl",
    "model_name": "Davlan/bert-base-multilingual-cased-ner-hrl",
    "author": "Davlan",
    "downloads": 282213,
    "likes": 70,
    "tags": [
      "transformers",
      "pytorch",
      "tf",
      "onnx",
      "safetensors",
      "bert",
      "token-classification",
      "license:afl-3.0",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/Davlan/bert-base-multilingual-cased-ner-hrl",
    "dependencies": [
      [
        "transformers",
        null
      ]
    ],
    "analysis_date": "2025-03-26T00:51:08.171718",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "bert",
    "pipeline_tag": "token-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "license": "afl-3.0"
    },
    "card_text": "Hugging Face's logo\n---\nlanguage: \n- ar\n- de\n- en\n- es\n- fr\n- it\n- lv\n- nl\n- pt\n- zh\n- multilingual\n\n---\n# bert-base-multilingual-cased-ner-hrl\n## Model description\n**bert-base-multilingual-cased-ner-hrl** is a **Named Entity Recognition** model for 10 high resourced languages (Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese and Chinese) based on a fine-tuned  mBERT base model. It has been trained to recognize three types of entities: location (LOC), organizations (ORG), and person (PER). \nSpecifically, this model is a *bert-base-multilingual-cased* model that was fine-tuned on an aggregation of 10 high-resourced languages\n## Intended uses & limitations\n#### How to use\nYou can use this model with Transformers *pipeline* for NER.\n```python\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\nfrom transformers import pipeline\ntokenizer = AutoTokenizer.from_pretrained(\"Davlan/bert-base-multilingual-cased-ner-hrl\")\nmodel = AutoModelForTokenClassification.from_pretrained(\"Davlan/bert-base-multilingual-cased-ner-hrl\")\nnlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\nexample = \"Nader Jokhadar had given Syria the lead with a well-struck header in the seventh minute.\"\nner_results = nlp(example)\nprint(ner_results)\n```\n#### Limitations and bias\nThis model is limited by its training dataset of entity-annotated news articles from a specific span of time. This may not generalize well for all use cases in different domains.  \n## Training data\nThe training data for the 10 languages are from: \n\nLanguage|Dataset\n-|-\nArabic | [ANERcorp](https://camel.abudhabi.nyu.edu/anercorp/)\nGerman | [conll 2003](https://www.clips.uantwerpen.be/conll2003/ner/)\nEnglish | [conll 2003](https://www.clips.uantwerpen.be/conll2003/ner/)\nSpanish | [conll 2002](https://www.clips.uantwerpen.be/conll2002/ner/)\nFrench | [Europeana Newspapers](https://github.com/EuropeanaNewspapers/ner-corpora/tree/master/enp_FR.bnf.bio)\nItalian | [Italian I-CAB](https://ontotext.fbk.eu/icab.html)\nLatvian | [Latvian NER](https://github.com/LUMII-AILab/FullStack/tree/master/NamedEntities)\nDutch | [conll 2002](https://www.clips.uantwerpen.be/conll2002/ner/)\nPortuguese |[Paramopama + Second Harem](https://github.com/davidsbatista/NER-datasets/tree/master/Portuguese)\nChinese | [MSRA](https://huggingface.co/datasets/msra_ner)\n\nThe training dataset distinguishes between the beginning and continuation of an entity so that if there are back-to-back entities of the same type, the model can output where the second entity begins. As in the dataset, each token will be classified as one of the following classes:\nAbbreviation|Description\n-|-\nO|Outside of a named entity\nB-PER |Beginning of a person’s name right after another person’s name\nI-PER |Person’s name\nB-ORG |Beginning of an organisation right after another organisation\nI-ORG |Organisation\nB-LOC |Beginning of a location right after another location\nI-LOC |Location\n## Training procedure\nThis model was trained on NVIDIA V100 GPU with recommended hyperparameters from HuggingFace code.",
    "card_content": "---\nlicense: afl-3.0\n---\nHugging Face's logo\n---\nlanguage: \n- ar\n- de\n- en\n- es\n- fr\n- it\n- lv\n- nl\n- pt\n- zh\n- multilingual\n\n---\n# bert-base-multilingual-cased-ner-hrl\n## Model description\n**bert-base-multilingual-cased-ner-hrl** is a **Named Entity Recognition** model for 10 high resourced languages (Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese and Chinese) based on a fine-tuned  mBERT base model. It has been trained to recognize three types of entities: location (LOC), organizations (ORG), and person (PER). \nSpecifically, this model is a *bert-base-multilingual-cased* model that was fine-tuned on an aggregation of 10 high-resourced languages\n## Intended uses & limitations\n#### How to use\nYou can use this model with Transformers *pipeline* for NER.\n```python\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\nfrom transformers import pipeline\ntokenizer = AutoTokenizer.from_pretrained(\"Davlan/bert-base-multilingual-cased-ner-hrl\")\nmodel = AutoModelForTokenClassification.from_pretrained(\"Davlan/bert-base-multilingual-cased-ner-hrl\")\nnlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\nexample = \"Nader Jokhadar had given Syria the lead with a well-struck header in the seventh minute.\"\nner_results = nlp(example)\nprint(ner_results)\n```\n#### Limitations and bias\nThis model is limited by its training dataset of entity-annotated news articles from a specific span of time. This may not generalize well for all use cases in different domains.  \n## Training data\nThe training data for the 10 languages are from: \n\nLanguage|Dataset\n-|-\nArabic | [ANERcorp](https://camel.abudhabi.nyu.edu/anercorp/)\nGerman | [conll 2003](https://www.clips.uantwerpen.be/conll2003/ner/)\nEnglish | [conll 2003](https://www.clips.uantwerpen.be/conll2003/ner/)\nSpanish | [conll 2002](https://www.clips.uantwerpen.be/conll2002/ner/)\nFrench | [Europeana Newspapers](https://github.com/EuropeanaNewspapers/ner-corpora/tree/master/enp_FR.bnf.bio)\nItalian | [Italian I-CAB](https://ontotext.fbk.eu/icab.html)\nLatvian | [Latvian NER](https://github.com/LUMII-AILab/FullStack/tree/master/NamedEntities)\nDutch | [conll 2002](https://www.clips.uantwerpen.be/conll2002/ner/)\nPortuguese |[Paramopama + Second Harem](https://github.com/davidsbatista/NER-datasets/tree/master/Portuguese)\nChinese | [MSRA](https://huggingface.co/datasets/msra_ner)\n\nThe training dataset distinguishes between the beginning and continuation of an entity so that if there are back-to-back entities of the same type, the model can output where the second entity begins. As in the dataset, each token will be classified as one of the following classes:\nAbbreviation|Description\n-|-\nO|Outside of a named entity\nB-PER |Beginning of a person’s name right after another person’s name\nI-PER |Person’s name\nB-ORG |Beginning of an organisation right after another organisation\nI-ORG |Organisation\nB-LOC |Beginning of a location right after another location\nI-LOC |Location\n## Training procedure\nThis model was trained on NVIDIA V100 GPU with recommended hyperparameters from HuggingFace code.",
    "library_name": "transformers"
  },
  {
    "model_id": "flair/ner-german-large",
    "model_name": "flair/ner-german-large",
    "author": "flair",
    "downloads": 261931,
    "likes": 39,
    "tags": [
      "flair",
      "pytorch",
      "token-classification",
      "sequence-tagger-model",
      "de",
      "dataset:conll2003",
      "arxiv:2011.06993",
      "region:us"
    ],
    "card_url": "https://huggingface.co/flair/ner-german-large",
    "dependencies": [
      [
        "flair",
        null
      ],
      [
        "torch",
        null
      ]
    ],
    "analysis_date": "2025-03-26T00:51:09.465056",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "unknown",
    "pipeline_tag": "token-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": "de",
      "tags": [
        "flair",
        "token-classification",
        "sequence-tagger-model"
      ],
      "datasets": [
        "conll2003"
      ],
      "widget": [
        {
          "text": "George Washington ging nach Washington"
        }
      ]
    },
    "card_text": "\n## German NER in Flair (large model)\n\nThis is the large 4-class NER model for German that ships with [Flair](https://github.com/flairNLP/flair/).\n\nF1-Score: **92,31** (CoNLL-03 German revised)\n\nPredicts 4 tags:\n\n| **tag**                        | **meaning** |\n|---------------------------------|-----------|\n| PER         | person name | \n| LOC         | location name | \n| ORG         | organization name | \n| MISC         | other name | \n\nBased on document-level XLM-R embeddings and [FLERT](https://arxiv.org/pdf/2011.06993v1.pdf).\n\n---\n\n### Demo: How to use in Flair\n\nRequires: **[Flair](https://github.com/flairNLP/flair/)** (`pip install flair`)\n\n```python\nfrom flair.data import Sentence\nfrom flair.models import SequenceTagger\n\n# load tagger\ntagger = SequenceTagger.load(\"flair/ner-german-large\")\n\n# make example sentence\nsentence = Sentence(\"George Washington ging nach Washington\")\n\n# predict NER tags\ntagger.predict(sentence)\n\n# print sentence\nprint(sentence)\n\n# print predicted NER spans\nprint('The following NER tags are found:')\n# iterate over entities and print\nfor entity in sentence.get_spans('ner'):\n    print(entity)\n\n```\n\nThis yields the following output:\n```\nSpan [1,2]: \"George Washington\"   [− Labels: PER (1.0)]\nSpan [5]: \"Washington\"   [− Labels: LOC (1.0)]\n```\n\nSo, the entities \"*George Washington*\" (labeled as a **person**) and \"*Washington*\" (labeled as a **location**) are found in the sentence \"*George Washington ging nach Washington*\". \n\n\n---\n\n### Training: Script to train this model\n\nThe following Flair script was used to train this model: \n\n```python\nimport torch\n\n# 1. get the corpus\nfrom flair.datasets import CONLL_03_GERMAN\n\ncorpus = CONLL_03_GERMAN()\n\n# 2. what tag do we want to predict?\ntag_type = 'ner'\n\n# 3. make the tag dictionary from the corpus\ntag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)\n\n# 4. initialize fine-tuneable transformer embeddings WITH document context\nfrom flair.embeddings import TransformerWordEmbeddings\n\nembeddings = TransformerWordEmbeddings(\n    model='xlm-roberta-large',\n    layers=\"-1\",\n    subtoken_pooling=\"first\",\n    fine_tune=True,\n    use_context=True,\n)\n\n# 5. initialize bare-bones sequence tagger (no CRF, no RNN, no reprojection)\nfrom flair.models import SequenceTagger\n\ntagger = SequenceTagger(\n    hidden_size=256,\n    embeddings=embeddings,\n    tag_dictionary=tag_dictionary,\n    tag_type='ner',\n    use_crf=False,\n    use_rnn=False,\n    reproject_embeddings=False,\n)\n\n# 6. initialize trainer with AdamW optimizer\nfrom flair.trainers import ModelTrainer\n\ntrainer = ModelTrainer(tagger, corpus, optimizer=torch.optim.AdamW)\n\n# 7. run training with XLM parameters (20 epochs, small LR)\nfrom torch.optim.lr_scheduler import OneCycleLR\n\ntrainer.train('resources/taggers/ner-german-large',\n              learning_rate=5.0e-6,\n              mini_batch_size=4,\n              mini_batch_chunk_size=1,\n              max_epochs=20,\n              scheduler=OneCycleLR,\n              embeddings_storage_mode='none',\n              weight_decay=0.,\n              )\n\n)\n```\n\n\n\n---\n\n### Cite\n\nPlease cite the following paper when using this model.\n\n```\n@misc{schweter2020flert,\n    title={FLERT: Document-Level Features for Named Entity Recognition},\n    author={Stefan Schweter and Alan Akbik},\n    year={2020},\n    eprint={2011.06993},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL}\n}\n```\n\n---\n\n### Issues?\n\nThe Flair issue tracker is available [here](https://github.com/flairNLP/flair/issues/).\n",
    "card_content": "---\nlanguage: de\ntags:\n- flair\n- token-classification\n- sequence-tagger-model\ndatasets:\n- conll2003\nwidget:\n- text: George Washington ging nach Washington\n---\n\n## German NER in Flair (large model)\n\nThis is the large 4-class NER model for German that ships with [Flair](https://github.com/flairNLP/flair/).\n\nF1-Score: **92,31** (CoNLL-03 German revised)\n\nPredicts 4 tags:\n\n| **tag**                        | **meaning** |\n|---------------------------------|-----------|\n| PER         | person name | \n| LOC         | location name | \n| ORG         | organization name | \n| MISC         | other name | \n\nBased on document-level XLM-R embeddings and [FLERT](https://arxiv.org/pdf/2011.06993v1.pdf).\n\n---\n\n### Demo: How to use in Flair\n\nRequires: **[Flair](https://github.com/flairNLP/flair/)** (`pip install flair`)\n\n```python\nfrom flair.data import Sentence\nfrom flair.models import SequenceTagger\n\n# load tagger\ntagger = SequenceTagger.load(\"flair/ner-german-large\")\n\n# make example sentence\nsentence = Sentence(\"George Washington ging nach Washington\")\n\n# predict NER tags\ntagger.predict(sentence)\n\n# print sentence\nprint(sentence)\n\n# print predicted NER spans\nprint('The following NER tags are found:')\n# iterate over entities and print\nfor entity in sentence.get_spans('ner'):\n    print(entity)\n\n```\n\nThis yields the following output:\n```\nSpan [1,2]: \"George Washington\"   [− Labels: PER (1.0)]\nSpan [5]: \"Washington\"   [− Labels: LOC (1.0)]\n```\n\nSo, the entities \"*George Washington*\" (labeled as a **person**) and \"*Washington*\" (labeled as a **location**) are found in the sentence \"*George Washington ging nach Washington*\". \n\n\n---\n\n### Training: Script to train this model\n\nThe following Flair script was used to train this model: \n\n```python\nimport torch\n\n# 1. get the corpus\nfrom flair.datasets import CONLL_03_GERMAN\n\ncorpus = CONLL_03_GERMAN()\n\n# 2. what tag do we want to predict?\ntag_type = 'ner'\n\n# 3. make the tag dictionary from the corpus\ntag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)\n\n# 4. initialize fine-tuneable transformer embeddings WITH document context\nfrom flair.embeddings import TransformerWordEmbeddings\n\nembeddings = TransformerWordEmbeddings(\n    model='xlm-roberta-large',\n    layers=\"-1\",\n    subtoken_pooling=\"first\",\n    fine_tune=True,\n    use_context=True,\n)\n\n# 5. initialize bare-bones sequence tagger (no CRF, no RNN, no reprojection)\nfrom flair.models import SequenceTagger\n\ntagger = SequenceTagger(\n    hidden_size=256,\n    embeddings=embeddings,\n    tag_dictionary=tag_dictionary,\n    tag_type='ner',\n    use_crf=False,\n    use_rnn=False,\n    reproject_embeddings=False,\n)\n\n# 6. initialize trainer with AdamW optimizer\nfrom flair.trainers import ModelTrainer\n\ntrainer = ModelTrainer(tagger, corpus, optimizer=torch.optim.AdamW)\n\n# 7. run training with XLM parameters (20 epochs, small LR)\nfrom torch.optim.lr_scheduler import OneCycleLR\n\ntrainer.train('resources/taggers/ner-german-large',\n              learning_rate=5.0e-6,\n              mini_batch_size=4,\n              mini_batch_chunk_size=1,\n              max_epochs=20,\n              scheduler=OneCycleLR,\n              embeddings_storage_mode='none',\n              weight_decay=0.,\n              )\n\n)\n```\n\n\n\n---\n\n### Cite\n\nPlease cite the following paper when using this model.\n\n```\n@misc{schweter2020flert,\n    title={FLERT: Document-Level Features for Named Entity Recognition},\n    author={Stefan Schweter and Alan Akbik},\n    year={2020},\n    eprint={2011.06993},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL}\n}\n```\n\n---\n\n### Issues?\n\nThe Flair issue tracker is available [here](https://github.com/flairNLP/flair/issues/).\n",
    "library_name": "flair"
  },
  {
    "model_id": "blaze999/Medical-NER",
    "model_name": "blaze999/Medical-NER",
    "author": "blaze999",
    "downloads": 259065,
    "likes": 202,
    "tags": [
      "transformers",
      "safetensors",
      "deberta-v2",
      "token-classification",
      "generated_from_trainer",
      "medical",
      "base_model:microsoft/deberta-v3-base",
      "base_model:finetune:microsoft/deberta-v3-base",
      "license:mit",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/blaze999/Medical-NER",
    "dependencies": [
      [
        "transformers",
        "4.37.0"
      ],
      [
        "pytorch",
        "2.1.2"
      ],
      [
        "datasets",
        "2.1.0"
      ],
      [
        "tokenizers",
        "0.15.1"
      ]
    ],
    "analysis_date": "2025-03-26T00:51:11.098018",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "deberta-v2",
    "pipeline_tag": "token-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "license": "mit",
      "tags": [
        "generated_from_trainer",
        "medical"
      ],
      "base_model": "microsoft/deberta-v3-base",
      "widget": [
        {
          "text": "63 year old woman with history of CAD presented to ER",
          "example_title": "Example-1"
        },
        {
          "text": "63 year old woman diagnosed with CAD",
          "example_title": "Example-2"
        },
        {
          "text": "A 48 year-old female presented with vaginal bleeding and abnormal Pap smears. Upon diagnosis of invasive non-keratinizing SCC of the cervix, she underwent a radical hysterectomy with salpingo-oophorectomy which demonstrated positive spread to the pelvic lymph nodes and the parametrium. Pathological examination revealed that the tumour also extensively involved the lower uterine segment.",
          "example_title": "example 3"
        }
      ],
      "pipeline_tag": "token-classification",
      "model-index": [
        {
          "name": "deberta-med-ner-2",
          "results": []
        }
      ]
    },
    "card_text": "\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n\n\n\n# deberta-med-ner-2\n\nThis model is a fine-tuned version of [DeBERTa](https://huggingface.co/microsoft/deberta-v3-base) on the PubMED Dataset.\n\n## Model description\n\nMedical NER Model finetuned on BERT to recognize 41 Medical entities.\n\n\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 8\n- eval_batch_size: 16\n- seed: 42\n- gradient_accumulation_steps: 2\n- total_train_batch_size: 16\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: cosine\n- lr_scheduler_warmup_ratio: 0.1\n- num_epochs: 30\n- mixed_precision_training: Native AMP\n\n\n\n## Usage\nThe easiest way is to load the inference api from huggingface and second method is through the pipeline object offered by transformers library.\n```python\n# Use a pipeline as a high-level helper\nfrom transformers import pipeline\npipe = pipeline(\"token-classification\", model=\"Clinical-AI-Apollo/Medical-NER\", aggregation_strategy='simple')\nresult = pipe('45 year old woman diagnosed with CAD')\n\n\n\n# Load model directly\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\n\ntokenizer = AutoTokenizer.from_pretrained(\"Clinical-AI-Apollo/Medical-NER\")\nmodel = AutoModelForTokenClassification.from_pretrained(\"Clinical-AI-Apollo/Medical-NER\")\n```\n\n### Author\n\nAuthor: [Saketh Mattupalli](https://huggingface.co/blaze999)\n\n### Framework versions\n\n- Transformers 4.37.0\n- Pytorch 2.1.2\n- Datasets 2.1.0\n- Tokenizers 0.15.1",
    "card_content": "---\nlicense: mit\ntags:\n- generated_from_trainer\n- medical\nbase_model: microsoft/deberta-v3-base\nwidget:\n- text: 63 year old woman with history of CAD presented to ER\n  example_title: Example-1\n- text: 63 year old woman diagnosed with CAD\n  example_title: Example-2\n- text: A 48 year-old female presented with vaginal bleeding and abnormal Pap smears.\n    Upon diagnosis of invasive non-keratinizing SCC of the cervix, she underwent a\n    radical hysterectomy with salpingo-oophorectomy which demonstrated positive spread\n    to the pelvic lymph nodes and the parametrium. Pathological examination revealed\n    that the tumour also extensively involved the lower uterine segment.\n  example_title: example 3\npipeline_tag: token-classification\nmodel-index:\n- name: deberta-med-ner-2\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n\n\n\n# deberta-med-ner-2\n\nThis model is a fine-tuned version of [DeBERTa](https://huggingface.co/microsoft/deberta-v3-base) on the PubMED Dataset.\n\n## Model description\n\nMedical NER Model finetuned on BERT to recognize 41 Medical entities.\n\n\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 8\n- eval_batch_size: 16\n- seed: 42\n- gradient_accumulation_steps: 2\n- total_train_batch_size: 16\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: cosine\n- lr_scheduler_warmup_ratio: 0.1\n- num_epochs: 30\n- mixed_precision_training: Native AMP\n\n\n\n## Usage\nThe easiest way is to load the inference api from huggingface and second method is through the pipeline object offered by transformers library.\n```python\n# Use a pipeline as a high-level helper\nfrom transformers import pipeline\npipe = pipeline(\"token-classification\", model=\"Clinical-AI-Apollo/Medical-NER\", aggregation_strategy='simple')\nresult = pipe('45 year old woman diagnosed with CAD')\n\n\n\n# Load model directly\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\n\ntokenizer = AutoTokenizer.from_pretrained(\"Clinical-AI-Apollo/Medical-NER\")\nmodel = AutoModelForTokenClassification.from_pretrained(\"Clinical-AI-Apollo/Medical-NER\")\n```\n\n### Author\n\nAuthor: [Saketh Mattupalli](https://huggingface.co/blaze999)\n\n### Framework versions\n\n- Transformers 4.37.0\n- Pytorch 2.1.2\n- Datasets 2.1.0\n- Tokenizers 0.15.1",
    "library_name": "transformers"
  },
  {
    "model_id": "ml6team/keyphrase-extraction-distilbert-inspec",
    "model_name": "ml6team/keyphrase-extraction-distilbert-inspec",
    "author": "ml6team",
    "downloads": 231177,
    "likes": 27,
    "tags": [
      "transformers",
      "pytorch",
      "distilbert",
      "token-classification",
      "keyphrase-extraction",
      "en",
      "dataset:midas/inspec",
      "arxiv:2112.08547",
      "license:mit",
      "model-index",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/ml6team/keyphrase-extraction-distilbert-inspec",
    "dependencies": [
      [
        "transformers",
        null
      ],
      [
        "numpy",
        null
      ]
    ],
    "analysis_date": "2025-03-26T00:51:12.400294",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "distilbert",
    "pipeline_tag": "token-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": "en",
      "license": "mit",
      "tags": [
        "keyphrase-extraction"
      ],
      "datasets": [
        "midas/inspec"
      ],
      "metrics": [
        "seqeval"
      ],
      "widget": [
        {
          "text": "Keyphrase extraction is a technique in text analysis where you extract the important keyphrases from a document. Thanks to these keyphrases humans can understand the content of a text very quickly and easily without reading it completely. Keyphrase extraction was first done primarily by human annotators, who read the text in detail and then wrote down the most important keyphrases. The disadvantage is that if you work with a lot of documents, this process can take a lot of time.\nHere is where Artificial Intelligence comes in. Currently, classical machine learning methods, that use statistical and linguistic features, are widely used for the extraction process. Now with deep learning, it is possible to capture the semantic meaning of a text even better than these classical methods. Classical methods look at the frequency, occurrence and order of words in the text, whereas these neural approaches can capture long-term semantic dependencies and context of words in a text.",
          "example_title": "Example 1"
        },
        {
          "text": "In this work, we explore how to learn task specific language models aimed towards learning rich representation of keyphrases from text documents. We experiment with different masking strategies for pre-training transformer language models (LMs) in discriminative as well as generative settings. In the discriminative setting, we introduce a new pre-training objective - Keyphrase Boundary Infilling with Replacement (KBIR), showing large gains in performance (up to 9.26 points in F1) over SOTA, when LM pre-trained using KBIR is fine-tuned for the task of keyphrase extraction. In the generative setting, we introduce a new pre-training setup for BART - KeyBART, that reproduces the keyphrases related to the input text in the CatSeq format, instead of the denoised original input. This also led to gains in performance (up to 4.33 points inF1@M) over SOTA for keyphrase generation. Additionally, we also fine-tune the pre-trained language models on named entity recognition(NER), question answering (QA), relation extraction (RE), abstractive summarization and achieve comparable performance with that of the SOTA, showing that learning rich representation of keyphrases is indeed beneficial for many other fundamental NLP tasks.",
          "example_title": "Example 2"
        }
      ],
      "model-index": [
        {
          "name": "DeDeckerThomas/keyphrase-extraction-distilbert-inspec",
          "results": [
            {
              "task": {
                "type": "keyphrase-extraction",
                "name": "Keyphrase Extraction"
              },
              "dataset": {
                "name": "inspec",
                "type": "midas/inspec"
              },
              "metrics": [
                {
                  "type": "F1 (Seqeval)",
                  "value": 0.509,
                  "name": "F1 (Seqeval)"
                },
                {
                  "type": "F1@M",
                  "value": 0.49,
                  "name": "F1@M"
                }
              ]
            }
          ]
        }
      ]
    },
    "card_text": "# 🔑 Keyphrase Extraction Model: distilbert-inspec\nKeyphrase extraction is a technique in text analysis where you extract the important keyphrases from a document. Thanks to these keyphrases humans can understand the content of a text very quickly and easily without reading it completely. Keyphrase extraction was first done primarily by human annotators, who read the text in detail and then wrote down the most important keyphrases. The disadvantage is that if you work with a lot of documents, this process can take a lot of time ⏳. \n\nHere is where Artificial Intelligence 🤖 comes in. Currently, classical machine learning methods, that use statistical and linguistic features, are widely used for the extraction process. Now with deep learning, it is possible to capture the semantic meaning of a text even better than these classical methods. Classical methods look at the frequency, occurrence and order of words in the text, whereas these neural approaches can capture long-term semantic dependencies and context of words in a text.\n\n\n## 📓 Model Description\nThis model uses [distilbert](https://huggingface.co/distilbert-base-uncased) as its base model and fine-tunes it on the [Inspec dataset](https://huggingface.co/datasets/midas/inspec).\n\nKeyphrase extraction models are transformer models fine-tuned as a token classification problem where each word in the document is classified as being part of a keyphrase or not.\n\n| Label | Description                     |\n| ----- | ------------------------------- |\n| B-KEY | At the beginning of a keyphrase |\n| I-KEY | Inside a keyphrase              |\n| O     | Outside a keyphrase             |\n\nKulkarni, Mayank, Debanjan Mahata, Ravneet Arora, and Rajarshi Bhowmik. \"Learning Rich Representation of Keyphrases from Text.\" arXiv preprint arXiv:2112.08547 (2021).\n\nSahrawat, Dhruva, Debanjan Mahata, Haimin Zhang, Mayank Kulkarni, Agniv Sharma, Rakesh Gosangi, Amanda Stent, Yaman Kumar, Rajiv Ratn Shah, and Roger Zimmermann. \"Keyphrase extraction as sequence labeling using contextualized embeddings.\" In European Conference on Information Retrieval, pp. 328-335. Springer, Cham, 2020.\n\n## ✋ Intended Uses & Limitations\n### 🛑 Limitations\n* This keyphrase extraction model is very domain-specific and will perform very well on abstracts of scientific papers. It's not recommended to use this model for other domains, but you are free to test it out.\n* Only works for English documents.\n\n### ❓ How To Use\n```python\nfrom transformers import (\n    TokenClassificationPipeline,\n    AutoModelForTokenClassification,\n    AutoTokenizer,\n)\nfrom transformers.pipelines import AggregationStrategy\nimport numpy as np\n\n# Define keyphrase extraction pipeline\nclass KeyphraseExtractionPipeline(TokenClassificationPipeline):\n    def __init__(self, model, *args, **kwargs):\n        super().__init__(\n            model=AutoModelForTokenClassification.from_pretrained(model),\n            tokenizer=AutoTokenizer.from_pretrained(model),\n            *args,\n            **kwargs\n        )\n\n    def postprocess(self, all_outputs):\n        results = super().postprocess(\n            all_outputs=all_outputs,\n            aggregation_strategy=AggregationStrategy.FIRST,\n        )\n        return np.unique([result.get(\"word\").strip() for result in results])\n\n```\n\n```python\n# Load pipeline\nmodel_name = \"ml6team/keyphrase-extraction-distilbert-inspec\"\nextractor = KeyphraseExtractionPipeline(model=model_name)\n```\n```python\n# Inference\ntext = \"\"\"\nKeyphrase extraction is a technique in text analysis where you extract the\nimportant keyphrases from a document. Thanks to these keyphrases humans can\nunderstand the content of a text very quickly and easily without reading it\ncompletely. Keyphrase extraction was first done primarily by human annotators,\nwho read the text in detail and then wrote down the most important keyphrases.\nThe disadvantage is that if you work with a lot of documents, this process\ncan take a lot of time. \n\nHere is where Artificial Intelligence comes in. Currently, classical machine\nlearning methods, that use statistical and linguistic features, are widely used\nfor the extraction process. Now with deep learning, it is possible to capture\nthe semantic meaning of a text even better than these classical methods.\nClassical methods look at the frequency, occurrence and order of words\nin the text, whereas these neural approaches can capture long-term\nsemantic dependencies and context of words in a text.\n\"\"\".replace(\"\\n\", \" \")\n\nkeyphrases = extractor(text)\n\nprint(keyphrases)\n```\n\n```\n# Output\n['artificial intelligence' 'classical machine learning' 'deep learning'\n 'keyphrase extraction' 'linguistic features' 'statistical'\n 'text analysis']\n```\n\n## 📚 Training Dataset\n[Inspec](https://huggingface.co/datasets/midas/inspec) is a keyphrase extraction/generation dataset consisting of 2000 English scientific papers from the scientific domains of Computers and Control and Information Technology published between 1998 to 2002. The keyphrases are annotated by professional indexers or editors.\n\nYou can find more information in the [paper](https://dl.acm.org/doi/10.3115/1119355.1119383).\n\n## 👷‍♂️ Training Procedure\n### Training Parameters\n\n| Parameter | Value |\n| --------- | ------|\n| Learning Rate | 1e-4 |\n| Epochs | 50 |\n| Early Stopping Patience | 3 |\n\n### Preprocessing\nThe documents in the dataset are already preprocessed into list of words with the corresponding labels. The only thing that must be done is tokenization and the realignment of the labels so that they correspond with the right subword tokens.\n\n```python\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer\n\n# Labels\nlabel_list = [\"B\", \"I\", \"O\"]\nlbl2idx = {\"B\": 0, \"I\": 1, \"O\": 2}\nidx2label = {0: \"B\", 1: \"I\", 2: \"O\"}\n\n# Tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\nmax_length = 512\n\n# Dataset parameters\ndataset_full_name = \"midas/inspec\"\ndataset_subset = \"raw\"\ndataset_document_column = \"document\"\ndataset_biotags_column = \"doc_bio_tags\"\n\ndef preprocess_fuction(all_samples_per_split):\n    tokenized_samples = tokenizer.batch_encode_plus(\n        all_samples_per_split[dataset_document_column],\n        padding=\"max_length\",\n        truncation=True,\n        is_split_into_words=True,\n        max_length=max_length,\n    )\n    total_adjusted_labels = []\n    for k in range(0, len(tokenized_samples[\"input_ids\"])):\n        prev_wid = -1\n        word_ids_list = tokenized_samples.word_ids(batch_index=k)\n        existing_label_ids = all_samples_per_split[dataset_biotags_column][k]\n        i = -1\n        adjusted_label_ids = []\n\n        for wid in word_ids_list:\n            if wid is None:\n                adjusted_label_ids.append(lbl2idx[\"O\"])\n            elif wid != prev_wid:\n                i = i + 1\n                adjusted_label_ids.append(lbl2idx[existing_label_ids[i]])\n                prev_wid = wid\n            else:\n                adjusted_label_ids.append(\n                    lbl2idx[\n                        f\"{'I' if existing_label_ids[i] == 'B' else existing_label_ids[i]}\"\n                    ]\n                )\n\n        total_adjusted_labels.append(adjusted_label_ids)\n    tokenized_samples[\"labels\"] = total_adjusted_labels\n    return tokenized_samples\n\n# Load dataset\ndataset = load_dataset(dataset_full_name, dataset_subset)\n\n# Preprocess dataset\ntokenized_dataset = dataset.map(preprocess_fuction, batched=True)\n    \n```\n\n### Postprocessing (Without Pipeline Function)\nIf you do not use the pipeline function, you must filter out the B and I labeled tokens. Each B and I will then be merged into a keyphrase. Finally, you need to strip the keyphrases to make sure all unnecessary spaces have been removed.\n```python\n# Define post_process functions\ndef concat_tokens_by_tag(keyphrases):\n    keyphrase_tokens = []\n    for id, label in keyphrases:\n        if label == \"B\":\n            keyphrase_tokens.append([id])\n        elif label == \"I\":\n            if len(keyphrase_tokens) > 0:\n                keyphrase_tokens[len(keyphrase_tokens) - 1].append(id)\n    return keyphrase_tokens\n\n\ndef extract_keyphrases(example, predictions, tokenizer, index=0):\n    keyphrases_list = [\n        (id, idx2label[label])\n        for id, label in zip(\n            np.array(example[\"input_ids\"]).squeeze().tolist(), predictions[index]\n        )\n        if idx2label[label] in [\"B\", \"I\"]\n    ]\n\n    processed_keyphrases = concat_tokens_by_tag(keyphrases_list)\n    extracted_kps = tokenizer.batch_decode(\n        processed_keyphrases,\n        skip_special_tokens=True,\n        clean_up_tokenization_spaces=True,\n    )\n    return np.unique([kp.strip() for kp in extracted_kps])\n\n```\n\n## 📝 Evaluation Results\n\nTraditional evaluation methods are the precision, recall and F1-score @k,m where k is the number that stands for the first k predicted keyphrases and m for the average amount of predicted keyphrases.\nThe model achieves the following results on the Inspec test set:\n\n| Dataset           | P@5  | R@5  | F1@5 | P@10 | R@10 | F1@10 | P@M  | R@M  | F1@M |\n|:-----------------:|:----:|:----:|:----:|:----:|:----:|:-----:|:----:|:----:|:----:|\n| Inspec Test Set   | 0.45 | 0.40 | 0.39 | 0.33 | 0.53 | 0.38  | 0.47 | 0.57 | 0.49 |\n\n## 🚨 Issues\nPlease feel free to start discussions in the Community Tab.",
    "card_content": "---\nlanguage: en\nlicense: mit\ntags:\n- keyphrase-extraction\ndatasets:\n- midas/inspec\nmetrics:\n- seqeval\nwidget:\n- text: 'Keyphrase extraction is a technique in text analysis where you extract the\n    important keyphrases from a document. Thanks to these keyphrases humans can understand\n    the content of a text very quickly and easily without reading it completely. Keyphrase\n    extraction was first done primarily by human annotators, who read the text in\n    detail and then wrote down the most important keyphrases. The disadvantage is\n    that if you work with a lot of documents, this process can take a lot of time.\n\n    Here is where Artificial Intelligence comes in. Currently, classical machine learning\n    methods, that use statistical and linguistic features, are widely used for the\n    extraction process. Now with deep learning, it is possible to capture the semantic\n    meaning of a text even better than these classical methods. Classical methods\n    look at the frequency, occurrence and order of words in the text, whereas these\n    neural approaches can capture long-term semantic dependencies and context of words\n    in a text.'\n  example_title: Example 1\n- text: In this work, we explore how to learn task specific language models aimed\n    towards learning rich representation of keyphrases from text documents. We experiment\n    with different masking strategies for pre-training transformer language models\n    (LMs) in discriminative as well as generative settings. In the discriminative\n    setting, we introduce a new pre-training objective - Keyphrase Boundary Infilling\n    with Replacement (KBIR), showing large gains in performance (up to 9.26 points\n    in F1) over SOTA, when LM pre-trained using KBIR is fine-tuned for the task of\n    keyphrase extraction. In the generative setting, we introduce a new pre-training\n    setup for BART - KeyBART, that reproduces the keyphrases related to the input\n    text in the CatSeq format, instead of the denoised original input. This also led\n    to gains in performance (up to 4.33 points inF1@M) over SOTA for keyphrase generation.\n    Additionally, we also fine-tune the pre-trained language models on named entity\n    recognition(NER), question answering (QA), relation extraction (RE), abstractive\n    summarization and achieve comparable performance with that of the SOTA, showing\n    that learning rich representation of keyphrases is indeed beneficial for many\n    other fundamental NLP tasks.\n  example_title: Example 2\nmodel-index:\n- name: DeDeckerThomas/keyphrase-extraction-distilbert-inspec\n  results:\n  - task:\n      type: keyphrase-extraction\n      name: Keyphrase Extraction\n    dataset:\n      name: inspec\n      type: midas/inspec\n    metrics:\n    - type: F1 (Seqeval)\n      value: 0.509\n      name: F1 (Seqeval)\n    - type: F1@M\n      value: 0.49\n      name: F1@M\n---\n# 🔑 Keyphrase Extraction Model: distilbert-inspec\nKeyphrase extraction is a technique in text analysis where you extract the important keyphrases from a document. Thanks to these keyphrases humans can understand the content of a text very quickly and easily without reading it completely. Keyphrase extraction was first done primarily by human annotators, who read the text in detail and then wrote down the most important keyphrases. The disadvantage is that if you work with a lot of documents, this process can take a lot of time ⏳. \n\nHere is where Artificial Intelligence 🤖 comes in. Currently, classical machine learning methods, that use statistical and linguistic features, are widely used for the extraction process. Now with deep learning, it is possible to capture the semantic meaning of a text even better than these classical methods. Classical methods look at the frequency, occurrence and order of words in the text, whereas these neural approaches can capture long-term semantic dependencies and context of words in a text.\n\n\n## 📓 Model Description\nThis model uses [distilbert](https://huggingface.co/distilbert-base-uncased) as its base model and fine-tunes it on the [Inspec dataset](https://huggingface.co/datasets/midas/inspec).\n\nKeyphrase extraction models are transformer models fine-tuned as a token classification problem where each word in the document is classified as being part of a keyphrase or not.\n\n| Label | Description                     |\n| ----- | ------------------------------- |\n| B-KEY | At the beginning of a keyphrase |\n| I-KEY | Inside a keyphrase              |\n| O     | Outside a keyphrase             |\n\nKulkarni, Mayank, Debanjan Mahata, Ravneet Arora, and Rajarshi Bhowmik. \"Learning Rich Representation of Keyphrases from Text.\" arXiv preprint arXiv:2112.08547 (2021).\n\nSahrawat, Dhruva, Debanjan Mahata, Haimin Zhang, Mayank Kulkarni, Agniv Sharma, Rakesh Gosangi, Amanda Stent, Yaman Kumar, Rajiv Ratn Shah, and Roger Zimmermann. \"Keyphrase extraction as sequence labeling using contextualized embeddings.\" In European Conference on Information Retrieval, pp. 328-335. Springer, Cham, 2020.\n\n## ✋ Intended Uses & Limitations\n### 🛑 Limitations\n* This keyphrase extraction model is very domain-specific and will perform very well on abstracts of scientific papers. It's not recommended to use this model for other domains, but you are free to test it out.\n* Only works for English documents.\n\n### ❓ How To Use\n```python\nfrom transformers import (\n    TokenClassificationPipeline,\n    AutoModelForTokenClassification,\n    AutoTokenizer,\n)\nfrom transformers.pipelines import AggregationStrategy\nimport numpy as np\n\n# Define keyphrase extraction pipeline\nclass KeyphraseExtractionPipeline(TokenClassificationPipeline):\n    def __init__(self, model, *args, **kwargs):\n        super().__init__(\n            model=AutoModelForTokenClassification.from_pretrained(model),\n            tokenizer=AutoTokenizer.from_pretrained(model),\n            *args,\n            **kwargs\n        )\n\n    def postprocess(self, all_outputs):\n        results = super().postprocess(\n            all_outputs=all_outputs,\n            aggregation_strategy=AggregationStrategy.FIRST,\n        )\n        return np.unique([result.get(\"word\").strip() for result in results])\n\n```\n\n```python\n# Load pipeline\nmodel_name = \"ml6team/keyphrase-extraction-distilbert-inspec\"\nextractor = KeyphraseExtractionPipeline(model=model_name)\n```\n```python\n# Inference\ntext = \"\"\"\nKeyphrase extraction is a technique in text analysis where you extract the\nimportant keyphrases from a document. Thanks to these keyphrases humans can\nunderstand the content of a text very quickly and easily without reading it\ncompletely. Keyphrase extraction was first done primarily by human annotators,\nwho read the text in detail and then wrote down the most important keyphrases.\nThe disadvantage is that if you work with a lot of documents, this process\ncan take a lot of time. \n\nHere is where Artificial Intelligence comes in. Currently, classical machine\nlearning methods, that use statistical and linguistic features, are widely used\nfor the extraction process. Now with deep learning, it is possible to capture\nthe semantic meaning of a text even better than these classical methods.\nClassical methods look at the frequency, occurrence and order of words\nin the text, whereas these neural approaches can capture long-term\nsemantic dependencies and context of words in a text.\n\"\"\".replace(\"\\n\", \" \")\n\nkeyphrases = extractor(text)\n\nprint(keyphrases)\n```\n\n```\n# Output\n['artificial intelligence' 'classical machine learning' 'deep learning'\n 'keyphrase extraction' 'linguistic features' 'statistical'\n 'text analysis']\n```\n\n## 📚 Training Dataset\n[Inspec](https://huggingface.co/datasets/midas/inspec) is a keyphrase extraction/generation dataset consisting of 2000 English scientific papers from the scientific domains of Computers and Control and Information Technology published between 1998 to 2002. The keyphrases are annotated by professional indexers or editors.\n\nYou can find more information in the [paper](https://dl.acm.org/doi/10.3115/1119355.1119383).\n\n## 👷‍♂️ Training Procedure\n### Training Parameters\n\n| Parameter | Value |\n| --------- | ------|\n| Learning Rate | 1e-4 |\n| Epochs | 50 |\n| Early Stopping Patience | 3 |\n\n### Preprocessing\nThe documents in the dataset are already preprocessed into list of words with the corresponding labels. The only thing that must be done is tokenization and the realignment of the labels so that they correspond with the right subword tokens.\n\n```python\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer\n\n# Labels\nlabel_list = [\"B\", \"I\", \"O\"]\nlbl2idx = {\"B\": 0, \"I\": 1, \"O\": 2}\nidx2label = {0: \"B\", 1: \"I\", 2: \"O\"}\n\n# Tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\nmax_length = 512\n\n# Dataset parameters\ndataset_full_name = \"midas/inspec\"\ndataset_subset = \"raw\"\ndataset_document_column = \"document\"\ndataset_biotags_column = \"doc_bio_tags\"\n\ndef preprocess_fuction(all_samples_per_split):\n    tokenized_samples = tokenizer.batch_encode_plus(\n        all_samples_per_split[dataset_document_column],\n        padding=\"max_length\",\n        truncation=True,\n        is_split_into_words=True,\n        max_length=max_length,\n    )\n    total_adjusted_labels = []\n    for k in range(0, len(tokenized_samples[\"input_ids\"])):\n        prev_wid = -1\n        word_ids_list = tokenized_samples.word_ids(batch_index=k)\n        existing_label_ids = all_samples_per_split[dataset_biotags_column][k]\n        i = -1\n        adjusted_label_ids = []\n\n        for wid in word_ids_list:\n            if wid is None:\n                adjusted_label_ids.append(lbl2idx[\"O\"])\n            elif wid != prev_wid:\n                i = i + 1\n                adjusted_label_ids.append(lbl2idx[existing_label_ids[i]])\n                prev_wid = wid\n            else:\n                adjusted_label_ids.append(\n                    lbl2idx[\n                        f\"{'I' if existing_label_ids[i] == 'B' else existing_label_ids[i]}\"\n                    ]\n                )\n\n        total_adjusted_labels.append(adjusted_label_ids)\n    tokenized_samples[\"labels\"] = total_adjusted_labels\n    return tokenized_samples\n\n# Load dataset\ndataset = load_dataset(dataset_full_name, dataset_subset)\n\n# Preprocess dataset\ntokenized_dataset = dataset.map(preprocess_fuction, batched=True)\n    \n```\n\n### Postprocessing (Without Pipeline Function)\nIf you do not use the pipeline function, you must filter out the B and I labeled tokens. Each B and I will then be merged into a keyphrase. Finally, you need to strip the keyphrases to make sure all unnecessary spaces have been removed.\n```python\n# Define post_process functions\ndef concat_tokens_by_tag(keyphrases):\n    keyphrase_tokens = []\n    for id, label in keyphrases:\n        if label == \"B\":\n            keyphrase_tokens.append([id])\n        elif label == \"I\":\n            if len(keyphrase_tokens) > 0:\n                keyphrase_tokens[len(keyphrase_tokens) - 1].append(id)\n    return keyphrase_tokens\n\n\ndef extract_keyphrases(example, predictions, tokenizer, index=0):\n    keyphrases_list = [\n        (id, idx2label[label])\n        for id, label in zip(\n            np.array(example[\"input_ids\"]).squeeze().tolist(), predictions[index]\n        )\n        if idx2label[label] in [\"B\", \"I\"]\n    ]\n\n    processed_keyphrases = concat_tokens_by_tag(keyphrases_list)\n    extracted_kps = tokenizer.batch_decode(\n        processed_keyphrases,\n        skip_special_tokens=True,\n        clean_up_tokenization_spaces=True,\n    )\n    return np.unique([kp.strip() for kp in extracted_kps])\n\n```\n\n## 📝 Evaluation Results\n\nTraditional evaluation methods are the precision, recall and F1-score @k,m where k is the number that stands for the first k predicted keyphrases and m for the average amount of predicted keyphrases.\nThe model achieves the following results on the Inspec test set:\n\n| Dataset           | P@5  | R@5  | F1@5 | P@10 | R@10 | F1@10 | P@M  | R@M  | F1@M |\n|:-----------------:|:----:|:----:|:----:|:----:|:----:|:-----:|:----:|:----:|:----:|\n| Inspec Test Set   | 0.45 | 0.40 | 0.39 | 0.33 | 0.53 | 0.38  | 0.47 | 0.57 | 0.49 |\n\n## 🚨 Issues\nPlease feel free to start discussions in the Community Tab.",
    "library_name": "transformers"
  },
  {
    "model_id": "flair/ner-english",
    "model_name": "flair/ner-english",
    "author": "flair",
    "downloads": 203136,
    "likes": 34,
    "tags": [
      "flair",
      "pytorch",
      "token-classification",
      "sequence-tagger-model",
      "en",
      "dataset:conll2003",
      "region:us"
    ],
    "card_url": "https://huggingface.co/flair/ner-english",
    "dependencies": [
      [
        "flair",
        null
      ]
    ],
    "analysis_date": "2025-03-26T00:51:13.423564",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "unknown",
    "pipeline_tag": "token-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": "en",
      "tags": [
        "flair",
        "token-classification",
        "sequence-tagger-model"
      ],
      "datasets": [
        "conll2003"
      ],
      "widget": [
        {
          "text": "George Washington went to Washington"
        }
      ]
    },
    "card_text": "\n## English NER in Flair (default model)\n\nThis is the standard 4-class NER model for English that ships with [Flair](https://github.com/flairNLP/flair/).\n\nF1-Score: **93,06** (corrected CoNLL-03)\n\nPredicts 4 tags:\n\n| **tag**                        | **meaning** |\n|---------------------------------|-----------|\n| PER         | person name | \n| LOC         | location name | \n| ORG         | organization name | \n| MISC         | other name | \n\nBased on [Flair embeddings](https://www.aclweb.org/anthology/C18-1139/) and LSTM-CRF.\n\n---\n\n### Demo: How to use in Flair\n\nRequires: **[Flair](https://github.com/flairNLP/flair/)** (`pip install flair`)\n\n```python\nfrom flair.data import Sentence\nfrom flair.models import SequenceTagger\n\n# load tagger\ntagger = SequenceTagger.load(\"flair/ner-english\")\n\n# make example sentence\nsentence = Sentence(\"George Washington went to Washington\")\n\n# predict NER tags\ntagger.predict(sentence)\n\n# print sentence\nprint(sentence)\n\n# print predicted NER spans\nprint('The following NER tags are found:')\n# iterate over entities and print\nfor entity in sentence.get_spans('ner'):\n    print(entity)\n\n```\n\nThis yields the following output:\n```\nSpan [1,2]: \"George Washington\"   [− Labels: PER (0.9968)]\nSpan [5]: \"Washington\"   [− Labels: LOC (0.9994)]\n```\n\nSo, the entities \"*George Washington*\" (labeled as a **person**) and \"*Washington*\" (labeled as a **location**) are found in the sentence \"*George Washington went to Washington*\". \n\n\n---\n\n### Training: Script to train this model\n\nThe following Flair script was used to train this model: \n\n```python\nfrom flair.data import Corpus\nfrom flair.datasets import CONLL_03\nfrom flair.embeddings import WordEmbeddings, StackedEmbeddings, FlairEmbeddings\n\n# 1. get the corpus\ncorpus: Corpus = CONLL_03()\n\n# 2. what tag do we want to predict?\ntag_type = 'ner'\n\n# 3. make the tag dictionary from the corpus\ntag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)\n\n# 4. initialize each embedding we use\nembedding_types = [\n\n    # GloVe embeddings\n    WordEmbeddings('glove'),\n\n    # contextual string embeddings, forward\n    FlairEmbeddings('news-forward'),\n\n    # contextual string embeddings, backward\n    FlairEmbeddings('news-backward'),\n]\n\n# embedding stack consists of Flair and GloVe embeddings\nembeddings = StackedEmbeddings(embeddings=embedding_types)\n\n# 5. initialize sequence tagger\nfrom flair.models import SequenceTagger\n\ntagger = SequenceTagger(hidden_size=256,\n                        embeddings=embeddings,\n                        tag_dictionary=tag_dictionary,\n                        tag_type=tag_type)\n\n# 6. initialize trainer\nfrom flair.trainers import ModelTrainer\n\ntrainer = ModelTrainer(tagger, corpus)\n\n# 7. run training\ntrainer.train('resources/taggers/ner-english',\n              train_with_dev=True,\n              max_epochs=150)\n```\n\n\n\n---\n\n### Cite\n\nPlease cite the following paper when using this model.\n\n```\n@inproceedings{akbik2018coling,\n  title={Contextual String Embeddings for Sequence Labeling},\n  author={Akbik, Alan and Blythe, Duncan and Vollgraf, Roland},\n  booktitle = {{COLING} 2018, 27th International Conference on Computational Linguistics},\n  pages     = {1638--1649},\n  year      = {2018}\n}\n```\n\n---\n\n### Issues?\n\nThe Flair issue tracker is available [here](https://github.com/flairNLP/flair/issues/).\n",
    "card_content": "---\nlanguage: en\ntags:\n- flair\n- token-classification\n- sequence-tagger-model\ndatasets:\n- conll2003\nwidget:\n- text: George Washington went to Washington\n---\n\n## English NER in Flair (default model)\n\nThis is the standard 4-class NER model for English that ships with [Flair](https://github.com/flairNLP/flair/).\n\nF1-Score: **93,06** (corrected CoNLL-03)\n\nPredicts 4 tags:\n\n| **tag**                        | **meaning** |\n|---------------------------------|-----------|\n| PER         | person name | \n| LOC         | location name | \n| ORG         | organization name | \n| MISC         | other name | \n\nBased on [Flair embeddings](https://www.aclweb.org/anthology/C18-1139/) and LSTM-CRF.\n\n---\n\n### Demo: How to use in Flair\n\nRequires: **[Flair](https://github.com/flairNLP/flair/)** (`pip install flair`)\n\n```python\nfrom flair.data import Sentence\nfrom flair.models import SequenceTagger\n\n# load tagger\ntagger = SequenceTagger.load(\"flair/ner-english\")\n\n# make example sentence\nsentence = Sentence(\"George Washington went to Washington\")\n\n# predict NER tags\ntagger.predict(sentence)\n\n# print sentence\nprint(sentence)\n\n# print predicted NER spans\nprint('The following NER tags are found:')\n# iterate over entities and print\nfor entity in sentence.get_spans('ner'):\n    print(entity)\n\n```\n\nThis yields the following output:\n```\nSpan [1,2]: \"George Washington\"   [− Labels: PER (0.9968)]\nSpan [5]: \"Washington\"   [− Labels: LOC (0.9994)]\n```\n\nSo, the entities \"*George Washington*\" (labeled as a **person**) and \"*Washington*\" (labeled as a **location**) are found in the sentence \"*George Washington went to Washington*\". \n\n\n---\n\n### Training: Script to train this model\n\nThe following Flair script was used to train this model: \n\n```python\nfrom flair.data import Corpus\nfrom flair.datasets import CONLL_03\nfrom flair.embeddings import WordEmbeddings, StackedEmbeddings, FlairEmbeddings\n\n# 1. get the corpus\ncorpus: Corpus = CONLL_03()\n\n# 2. what tag do we want to predict?\ntag_type = 'ner'\n\n# 3. make the tag dictionary from the corpus\ntag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)\n\n# 4. initialize each embedding we use\nembedding_types = [\n\n    # GloVe embeddings\n    WordEmbeddings('glove'),\n\n    # contextual string embeddings, forward\n    FlairEmbeddings('news-forward'),\n\n    # contextual string embeddings, backward\n    FlairEmbeddings('news-backward'),\n]\n\n# embedding stack consists of Flair and GloVe embeddings\nembeddings = StackedEmbeddings(embeddings=embedding_types)\n\n# 5. initialize sequence tagger\nfrom flair.models import SequenceTagger\n\ntagger = SequenceTagger(hidden_size=256,\n                        embeddings=embeddings,\n                        tag_dictionary=tag_dictionary,\n                        tag_type=tag_type)\n\n# 6. initialize trainer\nfrom flair.trainers import ModelTrainer\n\ntrainer = ModelTrainer(tagger, corpus)\n\n# 7. run training\ntrainer.train('resources/taggers/ner-english',\n              train_with_dev=True,\n              max_epochs=150)\n```\n\n\n\n---\n\n### Cite\n\nPlease cite the following paper when using this model.\n\n```\n@inproceedings{akbik2018coling,\n  title={Contextual String Embeddings for Sequence Labeling},\n  author={Akbik, Alan and Blythe, Duncan and Vollgraf, Roland},\n  booktitle = {{COLING} 2018, 27th International Conference on Computational Linguistics},\n  pages     = {1638--1649},\n  year      = {2018}\n}\n```\n\n---\n\n### Issues?\n\nThe Flair issue tracker is available [here](https://github.com/flairNLP/flair/issues/).\n",
    "library_name": "flair"
  },
  {
    "model_id": "vikp/layout_segmenter",
    "model_name": "vikp/layout_segmenter",
    "author": "vikp",
    "downloads": 202477,
    "likes": 15,
    "tags": [
      "transformers",
      "pytorch",
      "layoutlmv3",
      "token-classification",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/vikp/layout_segmenter",
    "dependencies": null,
    "analysis_date": "2025-03-26T00:51:14.393768",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "layoutlmv3",
    "pipeline_tag": "token-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {},
    "card_text": "Segments pdf page layout into blocks.  Based on layoutlmv3.\n\nUsed in [marker](https://github.com/VikParuchuri/marker).",
    "card_content": "---\n{}\n---\nSegments pdf page layout into blocks.  Based on layoutlmv3.\n\nUsed in [marker](https://github.com/VikParuchuri/marker).",
    "library_name": "transformers"
  },
  {
    "model_id": "Davlan/distilbert-base-multilingual-cased-ner-hrl",
    "model_name": "Davlan/distilbert-base-multilingual-cased-ner-hrl",
    "author": "Davlan",
    "downloads": 198855,
    "likes": 78,
    "tags": [
      "transformers",
      "pytorch",
      "tf",
      "safetensors",
      "distilbert",
      "token-classification",
      "license:afl-3.0",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/Davlan/distilbert-base-multilingual-cased-ner-hrl",
    "dependencies": [
      [
        "transformers",
        null
      ]
    ],
    "analysis_date": "2025-03-26T00:51:15.486454",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "distilbert",
    "pipeline_tag": "token-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "license": "afl-3.0"
    },
    "card_text": "Hugging Face's logo\n---\nlanguage: \n- ar\n- de\n- en\n- es\n- fr\n- it\n- lv\n- nl\n- pt\n- zh\n- multilingual\n\n---\n# distilbert-base-multilingual-cased-ner-hrl\n## Model description\n**distilbert-base-multilingual-cased-ner-hrl** is a **Named Entity Recognition** model for 10 high resourced languages (Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese and Chinese) based on a fine-tuned  Distiled BERT base model. It has been trained to recognize three types of entities: location (LOC), organizations (ORG), and person (PER). \nSpecifically, this model is a *distilbert-base-multilingual-cased* model that was fine-tuned on an aggregation of 10 high-resourced languages\n## Intended uses & limitations\n#### How to use\nYou can use this model with Transformers *pipeline* for NER.\n```python\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\nfrom transformers import pipeline\ntokenizer = AutoTokenizer.from_pretrained(\"Davlan/distilbert-base-multilingual-cased-ner-hrl\")\nmodel = AutoModelForTokenClassification.from_pretrained(\"Davlan/distilbert-base-multilingual-cased-ner-hrl\")\nnlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\nexample = \"Nader Jokhadar had given Syria the lead with a well-struck header in the seventh minute.\"\nner_results = nlp(example)\nprint(ner_results)\n```\n#### Limitations and bias\nThis model is limited by its training dataset of entity-annotated news articles from a specific span of time. This may not generalize well for all use cases in different domains.  \n## Training data\nThe training data for the 10 languages are from: \n\nLanguage|Dataset\n-|-\nArabic | [ANERcorp](https://camel.abudhabi.nyu.edu/anercorp/)\nGerman | [conll 2003](https://www.clips.uantwerpen.be/conll2003/ner/)\nEnglish | [conll 2003](https://www.clips.uantwerpen.be/conll2003/ner/)\nSpanish | [conll 2002](https://www.clips.uantwerpen.be/conll2002/ner/)\nFrench | [Europeana Newspapers](https://github.com/EuropeanaNewspapers/ner-corpora/tree/master/enp_FR.bnf.bio)\nItalian | [Italian I-CAB](https://ontotext.fbk.eu/icab.html)\nLatvian | [Latvian NER](https://github.com/LUMII-AILab/FullStack/tree/master/NamedEntities)\nDutch | [conll 2002](https://www.clips.uantwerpen.be/conll2002/ner/)\nPortuguese |[Paramopama + Second Harem](https://github.com/davidsbatista/NER-datasets/tree/master/Portuguese)\nChinese | [MSRA](https://huggingface.co/datasets/msra_ner)\n\nThe training dataset distinguishes between the beginning and continuation of an entity so that if there are back-to-back entities of the same type, the model can output where the second entity begins. As in the dataset, each token will be classified as one of the following classes:\nAbbreviation|Description\n-|-\nO|Outside of a named entity\nB-PER |Beginning of a person’s name right after another person’s name\nI-PER |Person’s name\nB-ORG |Beginning of an organisation right after another organisation\nI-ORG |Organisation\nB-LOC |Beginning of a location right after another location\nI-LOC |Location\n## Training procedure\nThis model was trained on NVIDIA V100 GPU with recommended hyperparameters from HuggingFace code.",
    "card_content": "---\nlicense: afl-3.0\n---\nHugging Face's logo\n---\nlanguage: \n- ar\n- de\n- en\n- es\n- fr\n- it\n- lv\n- nl\n- pt\n- zh\n- multilingual\n\n---\n# distilbert-base-multilingual-cased-ner-hrl\n## Model description\n**distilbert-base-multilingual-cased-ner-hrl** is a **Named Entity Recognition** model for 10 high resourced languages (Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese and Chinese) based on a fine-tuned  Distiled BERT base model. It has been trained to recognize three types of entities: location (LOC), organizations (ORG), and person (PER). \nSpecifically, this model is a *distilbert-base-multilingual-cased* model that was fine-tuned on an aggregation of 10 high-resourced languages\n## Intended uses & limitations\n#### How to use\nYou can use this model with Transformers *pipeline* for NER.\n```python\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\nfrom transformers import pipeline\ntokenizer = AutoTokenizer.from_pretrained(\"Davlan/distilbert-base-multilingual-cased-ner-hrl\")\nmodel = AutoModelForTokenClassification.from_pretrained(\"Davlan/distilbert-base-multilingual-cased-ner-hrl\")\nnlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\nexample = \"Nader Jokhadar had given Syria the lead with a well-struck header in the seventh minute.\"\nner_results = nlp(example)\nprint(ner_results)\n```\n#### Limitations and bias\nThis model is limited by its training dataset of entity-annotated news articles from a specific span of time. This may not generalize well for all use cases in different domains.  \n## Training data\nThe training data for the 10 languages are from: \n\nLanguage|Dataset\n-|-\nArabic | [ANERcorp](https://camel.abudhabi.nyu.edu/anercorp/)\nGerman | [conll 2003](https://www.clips.uantwerpen.be/conll2003/ner/)\nEnglish | [conll 2003](https://www.clips.uantwerpen.be/conll2003/ner/)\nSpanish | [conll 2002](https://www.clips.uantwerpen.be/conll2002/ner/)\nFrench | [Europeana Newspapers](https://github.com/EuropeanaNewspapers/ner-corpora/tree/master/enp_FR.bnf.bio)\nItalian | [Italian I-CAB](https://ontotext.fbk.eu/icab.html)\nLatvian | [Latvian NER](https://github.com/LUMII-AILab/FullStack/tree/master/NamedEntities)\nDutch | [conll 2002](https://www.clips.uantwerpen.be/conll2002/ner/)\nPortuguese |[Paramopama + Second Harem](https://github.com/davidsbatista/NER-datasets/tree/master/Portuguese)\nChinese | [MSRA](https://huggingface.co/datasets/msra_ner)\n\nThe training dataset distinguishes between the beginning and continuation of an entity so that if there are back-to-back entities of the same type, the model can output where the second entity begins. As in the dataset, each token will be classified as one of the following classes:\nAbbreviation|Description\n-|-\nO|Outside of a named entity\nB-PER |Beginning of a person’s name right after another person’s name\nI-PER |Person’s name\nB-ORG |Beginning of an organisation right after another organisation\nI-ORG |Organisation\nB-LOC |Beginning of a location right after another location\nI-LOC |Location\n## Training procedure\nThis model was trained on NVIDIA V100 GPU with recommended hyperparameters from HuggingFace code.",
    "library_name": "transformers"
  },
  {
    "model_id": "benjamin/wtp-canine-s-1l",
    "model_name": "benjamin/wtp-canine-s-1l",
    "author": "benjamin",
    "downloads": 193307,
    "likes": 5,
    "tags": [
      "transformers",
      "pytorch",
      "la-canine",
      "token-classification",
      "multilingual",
      "am",
      "ar",
      "az",
      "be",
      "bg",
      "bn",
      "ca",
      "ceb",
      "cs",
      "cy",
      "da",
      "de",
      "el",
      "en",
      "eo",
      "es",
      "et",
      "eu",
      "fa",
      "fi",
      "fr",
      "fy",
      "ga",
      "gd",
      "gl",
      "gu",
      "ha",
      "he",
      "hi",
      "hu",
      "hy",
      "id",
      "ig",
      "is",
      "it",
      "ja",
      "jv",
      "ka",
      "kk",
      "km",
      "kn",
      "ko",
      "ku",
      "ky",
      "la",
      "lt",
      "lv",
      "mg",
      "mk",
      "ml",
      "mn",
      "mr",
      "ms",
      "mt",
      "my",
      "ne",
      "nl",
      "no",
      "pa",
      "pl",
      "ps",
      "pt",
      "ro",
      "ru",
      "si",
      "sk",
      "sl",
      "sq",
      "sr",
      "sv",
      "ta",
      "te",
      "tg",
      "th",
      "tr",
      "uk",
      "ur",
      "uz",
      "vi",
      "xh",
      "yi",
      "yo",
      "zh",
      "zu",
      "license:mit",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/benjamin/wtp-canine-s-1l",
    "dependencies": null,
    "analysis_date": "2025-03-26T00:51:16.167859",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "la-canine",
    "pipeline_tag": "token-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": [
        "multilingual",
        "am",
        "ar",
        "az",
        "be",
        "bg",
        "bn",
        "ca",
        "ceb",
        "cs",
        "cy",
        "da",
        "de",
        "el",
        "en",
        "eo",
        "es",
        "et",
        "eu",
        "fa",
        "fi",
        "fr",
        "fy",
        "ga",
        "gd",
        "gl",
        "gu",
        "ha",
        "he",
        "hi",
        "hu",
        "hy",
        "id",
        "ig",
        "is",
        "it",
        "ja",
        "jv",
        "ka",
        "kk",
        "km",
        "kn",
        "ko",
        "ku",
        "ky",
        "la",
        "lt",
        "lv",
        "mg",
        "mk",
        "ml",
        "mn",
        "mr",
        "ms",
        "mt",
        "my",
        "ne",
        "nl",
        false,
        "pa",
        "pl",
        "ps",
        "pt",
        "ro",
        "ru",
        "si",
        "sk",
        "sl",
        "sq",
        "sr",
        "sv",
        "ta",
        "te",
        "tg",
        "th",
        "tr",
        "uk",
        "ur",
        "uz",
        "vi",
        "xh",
        "yi",
        "yo",
        "zh",
        "zu"
      ],
      "license": "mit"
    },
    "card_text": "\n# wtp-canine-s-1l\n\nModel for [`wtpsplit`](https://github.com/bminixhofer/wtpsplit).",
    "card_content": "---\nlanguage:\n- multilingual\n- am\n- ar\n- az\n- be\n- bg\n- bn\n- ca\n- ceb\n- cs\n- cy\n- da\n- de\n- el\n- en\n- eo\n- es\n- et\n- eu\n- fa\n- fi\n- fr\n- fy\n- ga\n- gd\n- gl\n- gu\n- ha\n- he\n- hi\n- hu\n- hy\n- id\n- ig\n- is\n- it\n- ja\n- jv\n- ka\n- kk\n- km\n- kn\n- ko\n- ku\n- ky\n- la\n- lt\n- lv\n- mg\n- mk\n- ml\n- mn\n- mr\n- ms\n- mt\n- my\n- ne\n- nl\n- false\n- pa\n- pl\n- ps\n- pt\n- ro\n- ru\n- si\n- sk\n- sl\n- sq\n- sr\n- sv\n- ta\n- te\n- tg\n- th\n- tr\n- uk\n- ur\n- uz\n- vi\n- xh\n- yi\n- yo\n- zh\n- zu\nlicense: mit\n---\n\n# wtp-canine-s-1l\n\nModel for [`wtpsplit`](https://github.com/bminixhofer/wtpsplit).",
    "library_name": "transformers"
  },
  {
    "model_id": "ckiplab/bert-base-chinese-ws",
    "model_name": "ckiplab/bert-base-chinese-ws",
    "author": "ckiplab",
    "downloads": 189256,
    "likes": 15,
    "tags": [
      "transformers",
      "pytorch",
      "jax",
      "bert",
      "token-classification",
      "zh",
      "license:gpl-3.0",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/ckiplab/bert-base-chinese-ws",
    "dependencies": [
      [
        "transformers",
        null
      ]
    ],
    "analysis_date": "2025-03-26T00:51:17.044506",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "bert",
    "pipeline_tag": "token-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": [
        "zh"
      ],
      "license": "gpl-3.0",
      "tags": [
        "pytorch",
        "token-classification",
        "bert",
        "zh"
      ],
      "thumbnail": "https://ckip.iis.sinica.edu.tw/files/ckip_logo.png"
    },
    "card_text": "\n# CKIP BERT Base Chinese\n\nThis project provides traditional Chinese transformers models (including ALBERT, BERT, GPT2) and NLP tools (including word segmentation, part-of-speech tagging, named entity recognition).\n\n這個專案提供了繁體中文的 transformers 模型（包含 ALBERT、BERT、GPT2）及自然語言處理工具（包含斷詞、詞性標記、實體辨識）。\n\n## Homepage\n\n- https://github.com/ckiplab/ckip-transformers\n\n## Contributers\n\n- [Mu Yang](https://muyang.pro) at [CKIP](https://ckip.iis.sinica.edu.tw) (Author & Maintainer)\n\n## Usage\n\nPlease use BertTokenizerFast as tokenizer instead of AutoTokenizer.\n\n請使用 BertTokenizerFast 而非 AutoTokenizer。\n\n```\nfrom transformers import (\n  BertTokenizerFast,\n  AutoModel,\n)\n\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')\nmodel = AutoModel.from_pretrained('ckiplab/bert-base-chinese-ws')\n```\n\nFor full usage and more information, please refer to https://github.com/ckiplab/ckip-transformers.\n\n有關完整使用方法及其他資訊，請參見 https://github.com/ckiplab/ckip-transformers 。\n",
    "card_content": "---\nlanguage:\n- zh\nlicense: gpl-3.0\ntags:\n- pytorch\n- token-classification\n- bert\n- zh\nthumbnail: https://ckip.iis.sinica.edu.tw/files/ckip_logo.png\n---\n\n# CKIP BERT Base Chinese\n\nThis project provides traditional Chinese transformers models (including ALBERT, BERT, GPT2) and NLP tools (including word segmentation, part-of-speech tagging, named entity recognition).\n\n這個專案提供了繁體中文的 transformers 模型（包含 ALBERT、BERT、GPT2）及自然語言處理工具（包含斷詞、詞性標記、實體辨識）。\n\n## Homepage\n\n- https://github.com/ckiplab/ckip-transformers\n\n## Contributers\n\n- [Mu Yang](https://muyang.pro) at [CKIP](https://ckip.iis.sinica.edu.tw) (Author & Maintainer)\n\n## Usage\n\nPlease use BertTokenizerFast as tokenizer instead of AutoTokenizer.\n\n請使用 BertTokenizerFast 而非 AutoTokenizer。\n\n```\nfrom transformers import (\n  BertTokenizerFast,\n  AutoModel,\n)\n\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')\nmodel = AutoModel.from_pretrained('ckiplab/bert-base-chinese-ws')\n```\n\nFor full usage and more information, please refer to https://github.com/ckiplab/ckip-transformers.\n\n有關完整使用方法及其他資訊，請參見 https://github.com/ckiplab/ckip-transformers 。\n",
    "library_name": "transformers"
  },
  {
    "model_id": "Jean-Baptiste/camembert-ner",
    "model_name": "Jean-Baptiste/camembert-ner",
    "author": "Jean-Baptiste",
    "downloads": 184148,
    "likes": 110,
    "tags": [
      "transformers",
      "pytorch",
      "onnx",
      "safetensors",
      "camembert",
      "token-classification",
      "fr",
      "dataset:Jean-Baptiste/wikiner_fr",
      "license:mit",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/Jean-Baptiste/camembert-ner",
    "dependencies": [
      [
        "transformers",
        null
      ]
    ],
    "analysis_date": "2025-03-26T00:51:20.003769",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "camembert",
    "pipeline_tag": "token-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": "fr",
      "license": "mit",
      "datasets": [
        "Jean-Baptiste/wikiner_fr"
      ],
      "widget": [
        {
          "text": "Je m'appelle jean-baptiste et je vis à montréal"
        },
        {
          "text": "george washington est allé à washington"
        }
      ]
    },
    "card_text": "\n# camembert-ner: model fine-tuned from camemBERT for NER task.\n\n## Introduction\n\n[camembert-ner] is a NER model that was fine-tuned from camemBERT on wikiner-fr dataset.\nModel was trained on wikiner-fr dataset (~170 634  sentences).\nModel was validated on emails/chat data and overperformed other models on this type of data specifically. \nIn particular the model seems to work better on entity that don't start with an upper case.\n\n## Training data\nTraining data was classified as follow:\n\nAbbreviation|Description\n-|-\nO |Outside of a named entity\nMISC |Miscellaneous entity\nPER |Person’s name\nORG |Organization\nLOC |Location\n\n\n## How to use camembert-ner with HuggingFace\n\n##### Load camembert-ner and its sub-word tokenizer :\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\n\ntokenizer = AutoTokenizer.from_pretrained(\"Jean-Baptiste/camembert-ner\")\nmodel = AutoModelForTokenClassification.from_pretrained(\"Jean-Baptiste/camembert-ner\")\n\n\n##### Process text sample (from wikipedia)\n\nfrom transformers import pipeline\n\nnlp = pipeline('ner', model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\nnlp(\"Apple est créée le 1er avril 1976 dans le garage de la maison d'enfance de Steve Jobs à Los Altos en Californie par Steve Jobs, Steve Wozniak et Ronald Wayne14, puis constituée sous forme de société le 3 janvier 1977 à l'origine sous le nom d'Apple Computer, mais pour ses 30 ans et pour refléter la diversification de ses produits, le mot « computer » est retiré le 9 janvier 2015.\")\n\n\n[{'entity_group': 'ORG',\n  'score': 0.9472818374633789,\n  'word': 'Apple',\n  'start': 0,\n  'end': 5},\n {'entity_group': 'PER',\n  'score': 0.9838564991950989,\n  'word': 'Steve Jobs',\n  'start': 74,\n  'end': 85},\n {'entity_group': 'LOC',\n  'score': 0.9831605950991312,\n  'word': 'Los Altos',\n  'start': 87,\n  'end': 97},\n {'entity_group': 'LOC',\n  'score': 0.9834540486335754,\n  'word': 'Californie',\n  'start': 100,\n  'end': 111},\n {'entity_group': 'PER',\n  'score': 0.9841555754343668,\n  'word': 'Steve Jobs',\n  'start': 115,\n  'end': 126},\n {'entity_group': 'PER',\n  'score': 0.9843501806259155,\n  'word': 'Steve Wozniak',\n  'start': 127,\n  'end': 141},\n {'entity_group': 'PER',\n  'score': 0.9841533899307251,\n  'word': 'Ronald Wayne',\n  'start': 144,\n  'end': 157},\n {'entity_group': 'ORG',\n  'score': 0.9468960364659628,\n  'word': 'Apple Computer',\n  'start': 243,\n  'end': 257}]\n\n```\n\n\n## Model performances (metric: seqeval)\n\nOverall\n\nprecision|recall|f1\n-|-|-\n0.8859|0.8971|0.8914\n\nBy entity\n\nentity|precision|recall|f1\n-|-|-|-\nPER|0.9372|0.9598|0.9483 \nORG|0.8099|0.8265|0.8181\nLOC|0.8905|0.9005|0.8955\nMISC|0.8175|0.8117|0.8146\n\n\n\n\nFor those who could be interested, here is a short article on how I used the results of this model to train a LSTM model for signature detection in emails:\nhttps://medium.com/@jean-baptiste.polle/lstm-model-for-email-signature-detection-8e990384fefa\n",
    "card_content": "---\nlanguage: fr\nlicense: mit\ndatasets:\n- Jean-Baptiste/wikiner_fr\nwidget:\n- text: Je m'appelle jean-baptiste et je vis à montréal\n- text: george washington est allé à washington\n---\n\n# camembert-ner: model fine-tuned from camemBERT for NER task.\n\n## Introduction\n\n[camembert-ner] is a NER model that was fine-tuned from camemBERT on wikiner-fr dataset.\nModel was trained on wikiner-fr dataset (~170 634  sentences).\nModel was validated on emails/chat data and overperformed other models on this type of data specifically. \nIn particular the model seems to work better on entity that don't start with an upper case.\n\n## Training data\nTraining data was classified as follow:\n\nAbbreviation|Description\n-|-\nO |Outside of a named entity\nMISC |Miscellaneous entity\nPER |Person’s name\nORG |Organization\nLOC |Location\n\n\n## How to use camembert-ner with HuggingFace\n\n##### Load camembert-ner and its sub-word tokenizer :\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\n\ntokenizer = AutoTokenizer.from_pretrained(\"Jean-Baptiste/camembert-ner\")\nmodel = AutoModelForTokenClassification.from_pretrained(\"Jean-Baptiste/camembert-ner\")\n\n\n##### Process text sample (from wikipedia)\n\nfrom transformers import pipeline\n\nnlp = pipeline('ner', model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\nnlp(\"Apple est créée le 1er avril 1976 dans le garage de la maison d'enfance de Steve Jobs à Los Altos en Californie par Steve Jobs, Steve Wozniak et Ronald Wayne14, puis constituée sous forme de société le 3 janvier 1977 à l'origine sous le nom d'Apple Computer, mais pour ses 30 ans et pour refléter la diversification de ses produits, le mot « computer » est retiré le 9 janvier 2015.\")\n\n\n[{'entity_group': 'ORG',\n  'score': 0.9472818374633789,\n  'word': 'Apple',\n  'start': 0,\n  'end': 5},\n {'entity_group': 'PER',\n  'score': 0.9838564991950989,\n  'word': 'Steve Jobs',\n  'start': 74,\n  'end': 85},\n {'entity_group': 'LOC',\n  'score': 0.9831605950991312,\n  'word': 'Los Altos',\n  'start': 87,\n  'end': 97},\n {'entity_group': 'LOC',\n  'score': 0.9834540486335754,\n  'word': 'Californie',\n  'start': 100,\n  'end': 111},\n {'entity_group': 'PER',\n  'score': 0.9841555754343668,\n  'word': 'Steve Jobs',\n  'start': 115,\n  'end': 126},\n {'entity_group': 'PER',\n  'score': 0.9843501806259155,\n  'word': 'Steve Wozniak',\n  'start': 127,\n  'end': 141},\n {'entity_group': 'PER',\n  'score': 0.9841533899307251,\n  'word': 'Ronald Wayne',\n  'start': 144,\n  'end': 157},\n {'entity_group': 'ORG',\n  'score': 0.9468960364659628,\n  'word': 'Apple Computer',\n  'start': 243,\n  'end': 257}]\n\n```\n\n\n## Model performances (metric: seqeval)\n\nOverall\n\nprecision|recall|f1\n-|-|-\n0.8859|0.8971|0.8914\n\nBy entity\n\nentity|precision|recall|f1\n-|-|-|-\nPER|0.9372|0.9598|0.9483 \nORG|0.8099|0.8265|0.8181\nLOC|0.8905|0.9005|0.8955\nMISC|0.8175|0.8117|0.8146\n\n\n\n\nFor those who could be interested, here is a short article on how I used the results of this model to train a LSTM model for signature detection in emails:\nhttps://medium.com/@jean-baptiste.polle/lstm-model-for-email-signature-detection-8e990384fefa\n",
    "library_name": "transformers"
  },
  {
    "model_id": "Jean-Baptiste/camembert-ner-with-dates",
    "model_name": "Jean-Baptiste/camembert-ner-with-dates",
    "author": "Jean-Baptiste",
    "downloads": 177375,
    "likes": 43,
    "tags": [
      "transformers",
      "pytorch",
      "onnx",
      "safetensors",
      "camembert",
      "token-classification",
      "fr",
      "dataset:Jean-Baptiste/wikiner_fr",
      "license:mit",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/Jean-Baptiste/camembert-ner-with-dates",
    "dependencies": [
      [
        "transformers",
        null
      ]
    ],
    "analysis_date": "2025-03-26T00:51:21.015184",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "camembert",
    "pipeline_tag": "token-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": "fr",
      "license": "mit",
      "datasets": [
        "Jean-Baptiste/wikiner_fr"
      ],
      "widget": [
        {
          "text": "Je m'appelle jean-baptiste et j'habite à montréal depuis fevr 2012"
        }
      ]
    },
    "card_text": "\n# camembert-ner: model fine-tuned from camemBERT for NER task (including DATE tag).\n\n## Introduction\n\n[camembert-ner-with-dates] is an extension of french camembert-ner model with an additionnal tag for dates.\nModel was trained on enriched version of wikiner-fr dataset (~170 634  sentences).\n\nOn my test data (mix of chat and email), this model got an f1 score of ~83% (in comparison dateparser was ~70%).\nDateparser library can still be be used on the output of this model in order to convert text to python datetime object \n(https://dateparser.readthedocs.io/en/latest/).\n\n\n## How to use camembert-ner-with-dates with HuggingFace\n\n##### Load camembert-ner-with-dates and its sub-word tokenizer :\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\n\ntokenizer = AutoTokenizer.from_pretrained(\"Jean-Baptiste/camembert-ner-with-dates\")\nmodel = AutoModelForTokenClassification.from_pretrained(\"Jean-Baptiste/camembert-ner-with-dates\")\n\n\n##### Process text sample (from wikipedia)\n\nfrom transformers import pipeline\n\nnlp = pipeline('ner', model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\nnlp(\"Apple est créée le 1er avril 1976 dans le garage de la maison d'enfance de Steve Jobs à Los Altos en Californie par Steve Jobs, Steve Wozniak et Ronald Wayne14, puis constituée sous forme de société le 3 janvier 1977 à l'origine sous le nom d'Apple Computer, mais pour ses 30 ans et pour refléter la diversification de ses produits, le mot « computer » est retiré le 9 janvier 2015.\")\n\n\n[{'entity_group': 'ORG',\n  'score': 0.9776379466056824,\n  'word': 'Apple',\n  'start': 0,\n  'end': 5},\n {'entity_group': 'DATE',\n  'score': 0.9793774570737567,\n  'word': 'le 1er avril 1976 dans le',\n  'start': 15,\n  'end': 41},\n {'entity_group': 'PER',\n  'score': 0.9958226680755615,\n  'word': 'Steve Jobs',\n  'start': 74,\n  'end': 85},\n {'entity_group': 'LOC',\n  'score': 0.995087186495463,\n  'word': 'Los Altos',\n  'start': 87,\n  'end': 97},\n {'entity_group': 'LOC',\n  'score': 0.9953305125236511,\n  'word': 'Californie',\n  'start': 100,\n  'end': 111},\n {'entity_group': 'PER',\n  'score': 0.9961076378822327,\n  'word': 'Steve Jobs',\n  'start': 115,\n  'end': 126},\n {'entity_group': 'PER',\n  'score': 0.9960325956344604,\n  'word': 'Steve Wozniak',\n  'start': 127,\n  'end': 141},\n {'entity_group': 'PER',\n  'score': 0.9957776467005411,\n  'word': 'Ronald Wayne',\n  'start': 144,\n  'end': 157},\n {'entity_group': 'DATE',\n  'score': 0.994030773639679,\n  'word': 'le 3 janvier 1977 à',\n  'start': 198,\n  'end': 218},\n {'entity_group': 'ORG',\n  'score': 0.9720810294151306,\n  'word': \"d'Apple Computer\",\n  'start': 240,\n  'end': 257},\n {'entity_group': 'DATE',\n  'score': 0.9924157659212748,\n  'word': '30 ans et',\n  'start': 272,\n  'end': 282},\n {'entity_group': 'DATE',\n  'score': 0.9934852868318558,\n  'word': 'le 9 janvier 2015.',\n  'start': 363,\n  'end': 382}]\n\n```\n\n\n## Model performances (metric: seqeval)\n\nGlobal\n```\n'precision': 0.928\n'recall': 0.928\n'f1': 0.928\n```\n\nBy entity\n```\nLabel LOC: (precision:0.929, recall:0.932, f1:0.931, support:9510)\nLabel PER: (precision:0.952, recall:0.965, f1:0.959, support:9399)\nLabel MISC: (precision:0.878, recall:0.844, f1:0.860, support:5364)\nLabel ORG: (precision:0.848, recall:0.883, f1:0.865, support:2299)\nLabel DATE: Not relevant because of method used to add date tag on wikiner dataset (estimated f1 ~90%)\n\n\n ```\n\n",
    "card_content": "---\nlanguage: fr\nlicense: mit\ndatasets:\n- Jean-Baptiste/wikiner_fr\nwidget:\n- text: Je m'appelle jean-baptiste et j'habite à montréal depuis fevr 2012\n---\n\n# camembert-ner: model fine-tuned from camemBERT for NER task (including DATE tag).\n\n## Introduction\n\n[camembert-ner-with-dates] is an extension of french camembert-ner model with an additionnal tag for dates.\nModel was trained on enriched version of wikiner-fr dataset (~170 634  sentences).\n\nOn my test data (mix of chat and email), this model got an f1 score of ~83% (in comparison dateparser was ~70%).\nDateparser library can still be be used on the output of this model in order to convert text to python datetime object \n(https://dateparser.readthedocs.io/en/latest/).\n\n\n## How to use camembert-ner-with-dates with HuggingFace\n\n##### Load camembert-ner-with-dates and its sub-word tokenizer :\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\n\ntokenizer = AutoTokenizer.from_pretrained(\"Jean-Baptiste/camembert-ner-with-dates\")\nmodel = AutoModelForTokenClassification.from_pretrained(\"Jean-Baptiste/camembert-ner-with-dates\")\n\n\n##### Process text sample (from wikipedia)\n\nfrom transformers import pipeline\n\nnlp = pipeline('ner', model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\nnlp(\"Apple est créée le 1er avril 1976 dans le garage de la maison d'enfance de Steve Jobs à Los Altos en Californie par Steve Jobs, Steve Wozniak et Ronald Wayne14, puis constituée sous forme de société le 3 janvier 1977 à l'origine sous le nom d'Apple Computer, mais pour ses 30 ans et pour refléter la diversification de ses produits, le mot « computer » est retiré le 9 janvier 2015.\")\n\n\n[{'entity_group': 'ORG',\n  'score': 0.9776379466056824,\n  'word': 'Apple',\n  'start': 0,\n  'end': 5},\n {'entity_group': 'DATE',\n  'score': 0.9793774570737567,\n  'word': 'le 1er avril 1976 dans le',\n  'start': 15,\n  'end': 41},\n {'entity_group': 'PER',\n  'score': 0.9958226680755615,\n  'word': 'Steve Jobs',\n  'start': 74,\n  'end': 85},\n {'entity_group': 'LOC',\n  'score': 0.995087186495463,\n  'word': 'Los Altos',\n  'start': 87,\n  'end': 97},\n {'entity_group': 'LOC',\n  'score': 0.9953305125236511,\n  'word': 'Californie',\n  'start': 100,\n  'end': 111},\n {'entity_group': 'PER',\n  'score': 0.9961076378822327,\n  'word': 'Steve Jobs',\n  'start': 115,\n  'end': 126},\n {'entity_group': 'PER',\n  'score': 0.9960325956344604,\n  'word': 'Steve Wozniak',\n  'start': 127,\n  'end': 141},\n {'entity_group': 'PER',\n  'score': 0.9957776467005411,\n  'word': 'Ronald Wayne',\n  'start': 144,\n  'end': 157},\n {'entity_group': 'DATE',\n  'score': 0.994030773639679,\n  'word': 'le 3 janvier 1977 à',\n  'start': 198,\n  'end': 218},\n {'entity_group': 'ORG',\n  'score': 0.9720810294151306,\n  'word': \"d'Apple Computer\",\n  'start': 240,\n  'end': 257},\n {'entity_group': 'DATE',\n  'score': 0.9924157659212748,\n  'word': '30 ans et',\n  'start': 272,\n  'end': 282},\n {'entity_group': 'DATE',\n  'score': 0.9934852868318558,\n  'word': 'le 9 janvier 2015.',\n  'start': 363,\n  'end': 382}]\n\n```\n\n\n## Model performances (metric: seqeval)\n\nGlobal\n```\n'precision': 0.928\n'recall': 0.928\n'f1': 0.928\n```\n\nBy entity\n```\nLabel LOC: (precision:0.929, recall:0.932, f1:0.931, support:9510)\nLabel PER: (precision:0.952, recall:0.965, f1:0.959, support:9399)\nLabel MISC: (precision:0.878, recall:0.844, f1:0.860, support:5364)\nLabel ORG: (precision:0.848, recall:0.883, f1:0.865, support:2299)\nLabel DATE: Not relevant because of method used to add date tag on wikiner dataset (estimated f1 ~90%)\n\n\n ```\n\n",
    "library_name": "transformers"
  },
  {
    "model_id": "microsoft/llmlingua-2-bert-base-multilingual-cased-meetingbank",
    "model_name": "microsoft/llmlingua-2-bert-base-multilingual-cased-meetingbank",
    "author": "microsoft",
    "downloads": 172379,
    "likes": 26,
    "tags": [
      "transformers",
      "safetensors",
      "bert",
      "token-classification",
      "arxiv:2403.12968",
      "license:apache-2.0",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/microsoft/llmlingua-2-bert-base-multilingual-cased-meetingbank",
    "dependencies": [
      [
        "llmlingua",
        null
      ]
    ],
    "analysis_date": "2025-03-26T00:51:22.253682",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "bert",
    "pipeline_tag": "token-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "license": "apache-2.0"
    },
    "card_text": "\n# LLMLingua-2-Bert-base-Multilingual-Cased-MeetingBank\n\nThis model was introduced in the paper [**LLMLingua-2: Data Distillation for Efficient and Faithful Task-Agnostic Prompt Compression** (Pan et al, 2024)](https://arxiv.org/abs/2403.12968). It is a [BERT multilingual base model (cased)](https://huggingface.co/google-bert/bert-base-multilingual-cased) finetuned to perform token classification for task agnostic prompt compression. The probability `$p_{preserve}$` of each token `$x_i$` is used as the metric for compression. This model is trained on [the extractive text compression dataset](https://huggingface.co/datasets/microsoft/MeetingBank-LLMCompressed) constructed with the methodology proposed in the [**LLMLingua-2**](https://arxiv.org/abs/2403.12968), using training examples from [MeetingBank (Hu et al, 2023)](https://meetingbank.github.io/) as the seed data.\n\nYou can evaluate the model on downstream tasks such as question answering (QA) and summarization over compressed meeting transcripts using [this dataset](https://huggingface.co/datasets/microsoft/MeetingBank-QA-Summary).\n\nFor more details, please check the project page of [LLMLingua-2](https://llmlingua.com/llmlingua2.html) and [LLMLingua Series](https://llmlingua.com/).\n\n## Usage\n```python\nfrom llmlingua import PromptCompressor\n\ncompressor = PromptCompressor(\n    model_name=\"microsoft/llmlingua-2-bert-base-multilingual-cased-meetingbank\",\n    use_llmlingua2=True\n)\n\noriginal_prompt = \"\"\"John: So, um, I've been thinking about the project, you know, and I believe we need to, uh, make some changes. I mean, we want the project to succeed, right? So, like, I think we should consider maybe revising the timeline.\nSarah: I totally agree, John. I mean, we have to be realistic, you know. The timeline is, like, too tight. You know what I mean? We should definitely extend it.\n\"\"\"\nresults = compressor.compress_prompt_llmlingua2(\n    original_prompt,\n    rate=0.6,\n    force_tokens=['\\n', '.', '!', '?', ','],\n    chunk_end_tokens=['.', '\\n'],\n    return_word_label=True,\n    drop_consecutive=True\n)\n\nprint(results.keys())\nprint(f\"Compressed prompt: {results['compressed_prompt']}\")\nprint(f\"Original tokens: {results['origin_tokens']}\")\nprint(f\"Compressed tokens: {results['compressed_tokens']}\")\nprint(f\"Compression rate: {results['rate']}\")\n\n# get the annotated results over the original prompt\nword_sep = \"\\t\\t|\\t\\t\"\nlabel_sep = \" \"\nlines = results[\"fn_labeled_original_prompt\"].split(word_sep)\nannotated_results = []\nfor line in lines:\n    word, label = line.split(label_sep)\n    annotated_results.append((word, '+') if label == '1' else (word, '-')) # list of tuples: (word, label)\nprint(\"Annotated results:\")\nfor word, label in annotated_results[:10]:\n    print(f\"{word} {label}\")\n```\n\n## Citation\n```\n@article{wu2024llmlingua2,\n    title = \"{LLML}ingua-2: Data Distillation for Efficient and Faithful Task-Agnostic Prompt Compression\",\n    author = \"Zhuoshi Pan and Qianhui Wu and Huiqiang Jiang and Menglin Xia and Xufang Luo and Jue Zhang and Qingwei Lin and Victor Ruhle and Yuqing Yang and Chin-Yew Lin and H. Vicky Zhao and Lili Qiu and Dongmei Zhang\",\n    url = \"https://arxiv.org/abs/2403.12968\",\n    journal = \"ArXiv preprint\",\n    volume = \"abs/2403.12968\",\n    year = \"2024\",\n}\n```",
    "card_content": "---\nlicense: apache-2.0\n---\n\n# LLMLingua-2-Bert-base-Multilingual-Cased-MeetingBank\n\nThis model was introduced in the paper [**LLMLingua-2: Data Distillation for Efficient and Faithful Task-Agnostic Prompt Compression** (Pan et al, 2024)](https://arxiv.org/abs/2403.12968). It is a [BERT multilingual base model (cased)](https://huggingface.co/google-bert/bert-base-multilingual-cased) finetuned to perform token classification for task agnostic prompt compression. The probability `$p_{preserve}$` of each token `$x_i$` is used as the metric for compression. This model is trained on [the extractive text compression dataset](https://huggingface.co/datasets/microsoft/MeetingBank-LLMCompressed) constructed with the methodology proposed in the [**LLMLingua-2**](https://arxiv.org/abs/2403.12968), using training examples from [MeetingBank (Hu et al, 2023)](https://meetingbank.github.io/) as the seed data.\n\nYou can evaluate the model on downstream tasks such as question answering (QA) and summarization over compressed meeting transcripts using [this dataset](https://huggingface.co/datasets/microsoft/MeetingBank-QA-Summary).\n\nFor more details, please check the project page of [LLMLingua-2](https://llmlingua.com/llmlingua2.html) and [LLMLingua Series](https://llmlingua.com/).\n\n## Usage\n```python\nfrom llmlingua import PromptCompressor\n\ncompressor = PromptCompressor(\n    model_name=\"microsoft/llmlingua-2-bert-base-multilingual-cased-meetingbank\",\n    use_llmlingua2=True\n)\n\noriginal_prompt = \"\"\"John: So, um, I've been thinking about the project, you know, and I believe we need to, uh, make some changes. I mean, we want the project to succeed, right? So, like, I think we should consider maybe revising the timeline.\nSarah: I totally agree, John. I mean, we have to be realistic, you know. The timeline is, like, too tight. You know what I mean? We should definitely extend it.\n\"\"\"\nresults = compressor.compress_prompt_llmlingua2(\n    original_prompt,\n    rate=0.6,\n    force_tokens=['\\n', '.', '!', '?', ','],\n    chunk_end_tokens=['.', '\\n'],\n    return_word_label=True,\n    drop_consecutive=True\n)\n\nprint(results.keys())\nprint(f\"Compressed prompt: {results['compressed_prompt']}\")\nprint(f\"Original tokens: {results['origin_tokens']}\")\nprint(f\"Compressed tokens: {results['compressed_tokens']}\")\nprint(f\"Compression rate: {results['rate']}\")\n\n# get the annotated results over the original prompt\nword_sep = \"\\t\\t|\\t\\t\"\nlabel_sep = \" \"\nlines = results[\"fn_labeled_original_prompt\"].split(word_sep)\nannotated_results = []\nfor line in lines:\n    word, label = line.split(label_sep)\n    annotated_results.append((word, '+') if label == '1' else (word, '-')) # list of tuples: (word, label)\nprint(\"Annotated results:\")\nfor word, label in annotated_results[:10]:\n    print(f\"{word} {label}\")\n```\n\n## Citation\n```\n@article{wu2024llmlingua2,\n    title = \"{LLML}ingua-2: Data Distillation for Efficient and Faithful Task-Agnostic Prompt Compression\",\n    author = \"Zhuoshi Pan and Qianhui Wu and Huiqiang Jiang and Menglin Xia and Xufang Luo and Jue Zhang and Qingwei Lin and Victor Ruhle and Yuqing Yang and Chin-Yew Lin and H. Vicky Zhao and Lili Qiu and Dongmei Zhang\",\n    url = \"https://arxiv.org/abs/2403.12968\",\n    journal = \"ArXiv preprint\",\n    volume = \"abs/2403.12968\",\n    year = \"2024\",\n}\n```",
    "library_name": "transformers"
  },
  {
    "model_id": "flair/ner-english-ontonotes-large",
    "model_name": "flair/ner-english-ontonotes-large",
    "author": "flair",
    "downloads": 166781,
    "likes": 95,
    "tags": [
      "flair",
      "pytorch",
      "token-classification",
      "sequence-tagger-model",
      "en",
      "dataset:ontonotes",
      "arxiv:2011.06993",
      "region:us"
    ],
    "card_url": "https://huggingface.co/flair/ner-english-ontonotes-large",
    "dependencies": [
      [
        "flair",
        null
      ]
    ],
    "analysis_date": "2025-03-26T00:51:23.683406",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "unknown",
    "pipeline_tag": "token-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": "en",
      "tags": [
        "flair",
        "token-classification",
        "sequence-tagger-model"
      ],
      "datasets": [
        "ontonotes"
      ],
      "widget": [
        {
          "text": "On September 1st George won 1 dollar while watching Game of Thrones."
        }
      ]
    },
    "card_text": "\n## English NER in Flair (Ontonotes large model)\n\nThis is the large 18-class NER model for English that ships with [Flair](https://github.com/flairNLP/flair/).\n\nF1-Score: **90.93** (Ontonotes)\n\nPredicts 18 tags:\n\n| **tag**                        | **meaning** |\n|---------------------------------|-----------|\n| CARDINAL    | cardinal value | \n| DATE         | date value | \n| EVENT         | event name | \n| FAC         | building name | \n| GPE         | geo-political entity | \n| LANGUAGE         | language name | \n| LAW         | law name | \n| LOC         | location name | \n| MONEY         | money name | \n| NORP         | affiliation | \n| ORDINAL         | ordinal value | \n| ORG         | organization name | \n| PERCENT         | percent value | \n| PERSON         | person name | \n| PRODUCT         | product name | \n| QUANTITY         | quantity value | \n| TIME         | time value | \n| WORK_OF_ART         | name of work of art | \n\nBased on document-level XLM-R embeddings and [FLERT](https://arxiv.org/pdf/2011.06993v1.pdf/).\n\n---\n\n### Demo: How to use in Flair\n\nRequires: **[Flair](https://github.com/flairNLP/flair/)** (`pip install flair`)\n\n```python\nfrom flair.data import Sentence\nfrom flair.models import SequenceTagger\n\n# load tagger\ntagger = SequenceTagger.load(\"flair/ner-english-ontonotes-large\")\n\n# make example sentence\nsentence = Sentence(\"On September 1st George won 1 dollar while watching Game of Thrones.\")\n\n# predict NER tags\ntagger.predict(sentence)\n\n# print sentence\nprint(sentence)\n\n# print predicted NER spans\nprint('The following NER tags are found:')\n# iterate over entities and print\nfor entity in sentence.get_spans('ner'):\n    print(entity)\n\n```\n\nThis yields the following output:\n```\nSpan [2,3]: \"September 1st\"   [− Labels: DATE (1.0)]\nSpan [4]: \"George\"   [− Labels: PERSON (1.0)]\nSpan [6,7]: \"1 dollar\"   [− Labels: MONEY (1.0)]\nSpan [10,11,12]: \"Game of Thrones\"   [− Labels: WORK_OF_ART (1.0)]\n```\n\nSo, the entities \"*September 1st*\" (labeled as a **date**), \"*George*\" (labeled as a **person**), \"*1 dollar*\" (labeled as a **money**) and \"Game of Thrones\" (labeled as a **work of art**) are found in the sentence \"*On September 1st George Washington won 1 dollar while watching Game of Thrones*\". \n\n\n---\n\n### Training: Script to train this model\n\nThe following Flair script was used to train this model: \n\n```python\nfrom flair.data import Corpus\nfrom flair.datasets import ColumnCorpus\nfrom flair.embeddings import WordEmbeddings, StackedEmbeddings, FlairEmbeddings\n\n# 1. load the corpus (Ontonotes does not ship with Flair, you need to download and reformat into a column format yourself)\ncorpus: Corpus = ColumnCorpus(\n                \"resources/tasks/onto-ner\",\n                column_format={0: \"text\", 1: \"pos\", 2: \"upos\", 3: \"ner\"},\n                tag_to_bioes=\"ner\",\n            )\n\n# 2. what tag do we want to predict?\ntag_type = 'ner'\n\n# 3. make the tag dictionary from the corpus\ntag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)\n\n# 4. initialize fine-tuneable transformer embeddings WITH document context\nfrom flair.embeddings import TransformerWordEmbeddings\n\nembeddings = TransformerWordEmbeddings(\n    model='xlm-roberta-large',\n    layers=\"-1\",\n    subtoken_pooling=\"first\",\n    fine_tune=True,\n    use_context=True,\n)\n\n# 5. initialize bare-bones sequence tagger (no CRF, no RNN, no reprojection)\nfrom flair.models import SequenceTagger\n\ntagger = SequenceTagger(\n    hidden_size=256,\n    embeddings=embeddings,\n    tag_dictionary=tag_dictionary,\n    tag_type='ner',\n    use_crf=False,\n    use_rnn=False,\n    reproject_embeddings=False,\n)\n\n# 6. initialize trainer with AdamW optimizer\nfrom flair.trainers import ModelTrainer\n\ntrainer = ModelTrainer(tagger, corpus, optimizer=torch.optim.AdamW)\n\n# 7. run training with XLM parameters (20 epochs, small LR)\nfrom torch.optim.lr_scheduler import OneCycleLR\n\ntrainer.train('resources/taggers/ner-english-ontonotes-large',\n              learning_rate=5.0e-6,\n              mini_batch_size=4,\n              mini_batch_chunk_size=1,\n              max_epochs=20,\n              scheduler=OneCycleLR,\n              embeddings_storage_mode='none',\n              weight_decay=0.,\n              )\n```\n\n\n\n---\n\n### Cite\n\nPlease cite the following paper when using this model.\n\n```\n@misc{schweter2020flert,\n    title={FLERT: Document-Level Features for Named Entity Recognition},\n    author={Stefan Schweter and Alan Akbik},\n    year={2020},\n    eprint={2011.06993},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL}\n}\n```\n\n---\n\n### Issues?\n\nThe Flair issue tracker is available [here](https://github.com/flairNLP/flair/issues/).\n",
    "card_content": "---\nlanguage: en\ntags:\n- flair\n- token-classification\n- sequence-tagger-model\ndatasets:\n- ontonotes\nwidget:\n- text: On September 1st George won 1 dollar while watching Game of Thrones.\n---\n\n## English NER in Flair (Ontonotes large model)\n\nThis is the large 18-class NER model for English that ships with [Flair](https://github.com/flairNLP/flair/).\n\nF1-Score: **90.93** (Ontonotes)\n\nPredicts 18 tags:\n\n| **tag**                        | **meaning** |\n|---------------------------------|-----------|\n| CARDINAL    | cardinal value | \n| DATE         | date value | \n| EVENT         | event name | \n| FAC         | building name | \n| GPE         | geo-political entity | \n| LANGUAGE         | language name | \n| LAW         | law name | \n| LOC         | location name | \n| MONEY         | money name | \n| NORP         | affiliation | \n| ORDINAL         | ordinal value | \n| ORG         | organization name | \n| PERCENT         | percent value | \n| PERSON         | person name | \n| PRODUCT         | product name | \n| QUANTITY         | quantity value | \n| TIME         | time value | \n| WORK_OF_ART         | name of work of art | \n\nBased on document-level XLM-R embeddings and [FLERT](https://arxiv.org/pdf/2011.06993v1.pdf/).\n\n---\n\n### Demo: How to use in Flair\n\nRequires: **[Flair](https://github.com/flairNLP/flair/)** (`pip install flair`)\n\n```python\nfrom flair.data import Sentence\nfrom flair.models import SequenceTagger\n\n# load tagger\ntagger = SequenceTagger.load(\"flair/ner-english-ontonotes-large\")\n\n# make example sentence\nsentence = Sentence(\"On September 1st George won 1 dollar while watching Game of Thrones.\")\n\n# predict NER tags\ntagger.predict(sentence)\n\n# print sentence\nprint(sentence)\n\n# print predicted NER spans\nprint('The following NER tags are found:')\n# iterate over entities and print\nfor entity in sentence.get_spans('ner'):\n    print(entity)\n\n```\n\nThis yields the following output:\n```\nSpan [2,3]: \"September 1st\"   [− Labels: DATE (1.0)]\nSpan [4]: \"George\"   [− Labels: PERSON (1.0)]\nSpan [6,7]: \"1 dollar\"   [− Labels: MONEY (1.0)]\nSpan [10,11,12]: \"Game of Thrones\"   [− Labels: WORK_OF_ART (1.0)]\n```\n\nSo, the entities \"*September 1st*\" (labeled as a **date**), \"*George*\" (labeled as a **person**), \"*1 dollar*\" (labeled as a **money**) and \"Game of Thrones\" (labeled as a **work of art**) are found in the sentence \"*On September 1st George Washington won 1 dollar while watching Game of Thrones*\". \n\n\n---\n\n### Training: Script to train this model\n\nThe following Flair script was used to train this model: \n\n```python\nfrom flair.data import Corpus\nfrom flair.datasets import ColumnCorpus\nfrom flair.embeddings import WordEmbeddings, StackedEmbeddings, FlairEmbeddings\n\n# 1. load the corpus (Ontonotes does not ship with Flair, you need to download and reformat into a column format yourself)\ncorpus: Corpus = ColumnCorpus(\n                \"resources/tasks/onto-ner\",\n                column_format={0: \"text\", 1: \"pos\", 2: \"upos\", 3: \"ner\"},\n                tag_to_bioes=\"ner\",\n            )\n\n# 2. what tag do we want to predict?\ntag_type = 'ner'\n\n# 3. make the tag dictionary from the corpus\ntag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)\n\n# 4. initialize fine-tuneable transformer embeddings WITH document context\nfrom flair.embeddings import TransformerWordEmbeddings\n\nembeddings = TransformerWordEmbeddings(\n    model='xlm-roberta-large',\n    layers=\"-1\",\n    subtoken_pooling=\"first\",\n    fine_tune=True,\n    use_context=True,\n)\n\n# 5. initialize bare-bones sequence tagger (no CRF, no RNN, no reprojection)\nfrom flair.models import SequenceTagger\n\ntagger = SequenceTagger(\n    hidden_size=256,\n    embeddings=embeddings,\n    tag_dictionary=tag_dictionary,\n    tag_type='ner',\n    use_crf=False,\n    use_rnn=False,\n    reproject_embeddings=False,\n)\n\n# 6. initialize trainer with AdamW optimizer\nfrom flair.trainers import ModelTrainer\n\ntrainer = ModelTrainer(tagger, corpus, optimizer=torch.optim.AdamW)\n\n# 7. run training with XLM parameters (20 epochs, small LR)\nfrom torch.optim.lr_scheduler import OneCycleLR\n\ntrainer.train('resources/taggers/ner-english-ontonotes-large',\n              learning_rate=5.0e-6,\n              mini_batch_size=4,\n              mini_batch_chunk_size=1,\n              max_epochs=20,\n              scheduler=OneCycleLR,\n              embeddings_storage_mode='none',\n              weight_decay=0.,\n              )\n```\n\n\n\n---\n\n### Cite\n\nPlease cite the following paper when using this model.\n\n```\n@misc{schweter2020flert,\n    title={FLERT: Document-Level Features for Named Entity Recognition},\n    author={Stefan Schweter and Alan Akbik},\n    year={2020},\n    eprint={2011.06993},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL}\n}\n```\n\n---\n\n### Issues?\n\nThe Flair issue tracker is available [here](https://github.com/flairNLP/flair/issues/).\n",
    "library_name": "flair"
  },
  {
    "model_id": "flair/ner-english-ontonotes",
    "model_name": "flair/ner-english-ontonotes",
    "author": "flair",
    "downloads": 166474,
    "likes": 19,
    "tags": [
      "flair",
      "pytorch",
      "token-classification",
      "sequence-tagger-model",
      "en",
      "dataset:ontonotes",
      "region:us"
    ],
    "card_url": "https://huggingface.co/flair/ner-english-ontonotes",
    "dependencies": [
      [
        "flair",
        null
      ]
    ],
    "analysis_date": "2025-03-26T00:51:24.664173",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "unknown",
    "pipeline_tag": "token-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": "en",
      "tags": [
        "flair",
        "token-classification",
        "sequence-tagger-model"
      ],
      "datasets": [
        "ontonotes"
      ],
      "widget": [
        {
          "text": "On September 1st George Washington won 1 dollar."
        }
      ]
    },
    "card_text": "\n## English NER in Flair (Ontonotes default model)\n\nThis is the 18-class NER model for English that ships with [Flair](https://github.com/flairNLP/flair/).\n\nF1-Score: **89.27** (Ontonotes)\n\nPredicts 18 tags:\n\n| **tag**                        | **meaning** |\n|---------------------------------|-----------|\n| CARDINAL    | cardinal value | \n| DATE         | date value | \n| EVENT         | event name | \n| FAC         | building name | \n| GPE         | geo-political entity | \n| LANGUAGE         | language name | \n| LAW         | law name | \n| LOC         | location name | \n| MONEY         | money name | \n| NORP         | affiliation | \n| ORDINAL         | ordinal value | \n| ORG         | organization name | \n| PERCENT         | percent value | \n| PERSON         | person name | \n| PRODUCT         | product name | \n| QUANTITY         | quantity value | \n| TIME         | time value | \n| WORK_OF_ART         | name of work of art | \n\nBased on [Flair embeddings](https://www.aclweb.org/anthology/C18-1139/) and LSTM-CRF.\n\n---\n\n### Demo: How to use in Flair\n\nRequires: **[Flair](https://github.com/flairNLP/flair/)** (`pip install flair`)\n\n```python\nfrom flair.data import Sentence\nfrom flair.models import SequenceTagger\n\n# load tagger\ntagger = SequenceTagger.load(\"flair/ner-english-ontonotes\")\n\n# make example sentence\nsentence = Sentence(\"On September 1st George Washington won 1 dollar.\")\n\n# predict NER tags\ntagger.predict(sentence)\n\n# print sentence\nprint(sentence)\n\n# print predicted NER spans\nprint('The following NER tags are found:')\n# iterate over entities and print\nfor entity in sentence.get_spans('ner'):\n    print(entity)\n\n```\n\nThis yields the following output:\n```\nSpan [2,3]: \"September 1st\"   [− Labels: DATE (0.8824)]\nSpan [4,5]: \"George Washington\"   [− Labels: PERSON (0.9604)]\nSpan [7,8]: \"1 dollar\"   [− Labels: MONEY (0.9837)]\n```\n\nSo, the entities \"*September 1st*\" (labeled as a **date**), \"*George Washington*\" (labeled as a **person**) and \"*1 dollar*\" (labeled as a **money**) are found in the sentence \"*On September 1st George Washington won 1 dollar*\". \n\n\n---\n\n### Training: Script to train this model\n\nThe following Flair script was used to train this model: \n\n```python\nfrom flair.data import Corpus\nfrom flair.datasets import ColumnCorpus\nfrom flair.embeddings import WordEmbeddings, StackedEmbeddings, FlairEmbeddings\n\n# 1. load the corpus (Ontonotes does not ship with Flair, you need to download and reformat into a column format yourself)\ncorpus: Corpus = ColumnCorpus(\n                \"resources/tasks/onto-ner\",\n                column_format={0: \"text\", 1: \"pos\", 2: \"upos\", 3: \"ner\"},\n                tag_to_bioes=\"ner\",\n            )\n\n# 2. what tag do we want to predict?\ntag_type = 'ner'\n\n# 3. make the tag dictionary from the corpus\ntag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)\n\n# 4. initialize each embedding we use\nembedding_types = [\n\n    # GloVe embeddings\n    WordEmbeddings('en-crawl'),\n\n    # contextual string embeddings, forward\n    FlairEmbeddings('news-forward'),\n\n    # contextual string embeddings, backward\n    FlairEmbeddings('news-backward'),\n]\n\n# embedding stack consists of Flair and GloVe embeddings\nembeddings = StackedEmbeddings(embeddings=embedding_types)\n\n# 5. initialize sequence tagger\nfrom flair.models import SequenceTagger\n\ntagger = SequenceTagger(hidden_size=256,\n                        embeddings=embeddings,\n                        tag_dictionary=tag_dictionary,\n                        tag_type=tag_type)\n\n# 6. initialize trainer\nfrom flair.trainers import ModelTrainer\n\ntrainer = ModelTrainer(tagger, corpus)\n\n# 7. run training\ntrainer.train('resources/taggers/ner-english-ontonotes',\n              train_with_dev=True,\n              max_epochs=150)\n```\n\n\n\n---\n\n### Cite\n\nPlease cite the following paper when using this model.\n\n```\n@inproceedings{akbik2018coling,\n  title={Contextual String Embeddings for Sequence Labeling},\n  author={Akbik, Alan and Blythe, Duncan and Vollgraf, Roland},\n  booktitle = {{COLING} 2018, 27th International Conference on Computational Linguistics},\n  pages     = {1638--1649},\n  year      = {2018}\n}\n```\n\n---\n\n### Issues?\n\nThe Flair issue tracker is available [here](https://github.com/flairNLP/flair/issues/).\n",
    "card_content": "---\nlanguage: en\ntags:\n- flair\n- token-classification\n- sequence-tagger-model\ndatasets:\n- ontonotes\nwidget:\n- text: On September 1st George Washington won 1 dollar.\n---\n\n## English NER in Flair (Ontonotes default model)\n\nThis is the 18-class NER model for English that ships with [Flair](https://github.com/flairNLP/flair/).\n\nF1-Score: **89.27** (Ontonotes)\n\nPredicts 18 tags:\n\n| **tag**                        | **meaning** |\n|---------------------------------|-----------|\n| CARDINAL    | cardinal value | \n| DATE         | date value | \n| EVENT         | event name | \n| FAC         | building name | \n| GPE         | geo-political entity | \n| LANGUAGE         | language name | \n| LAW         | law name | \n| LOC         | location name | \n| MONEY         | money name | \n| NORP         | affiliation | \n| ORDINAL         | ordinal value | \n| ORG         | organization name | \n| PERCENT         | percent value | \n| PERSON         | person name | \n| PRODUCT         | product name | \n| QUANTITY         | quantity value | \n| TIME         | time value | \n| WORK_OF_ART         | name of work of art | \n\nBased on [Flair embeddings](https://www.aclweb.org/anthology/C18-1139/) and LSTM-CRF.\n\n---\n\n### Demo: How to use in Flair\n\nRequires: **[Flair](https://github.com/flairNLP/flair/)** (`pip install flair`)\n\n```python\nfrom flair.data import Sentence\nfrom flair.models import SequenceTagger\n\n# load tagger\ntagger = SequenceTagger.load(\"flair/ner-english-ontonotes\")\n\n# make example sentence\nsentence = Sentence(\"On September 1st George Washington won 1 dollar.\")\n\n# predict NER tags\ntagger.predict(sentence)\n\n# print sentence\nprint(sentence)\n\n# print predicted NER spans\nprint('The following NER tags are found:')\n# iterate over entities and print\nfor entity in sentence.get_spans('ner'):\n    print(entity)\n\n```\n\nThis yields the following output:\n```\nSpan [2,3]: \"September 1st\"   [− Labels: DATE (0.8824)]\nSpan [4,5]: \"George Washington\"   [− Labels: PERSON (0.9604)]\nSpan [7,8]: \"1 dollar\"   [− Labels: MONEY (0.9837)]\n```\n\nSo, the entities \"*September 1st*\" (labeled as a **date**), \"*George Washington*\" (labeled as a **person**) and \"*1 dollar*\" (labeled as a **money**) are found in the sentence \"*On September 1st George Washington won 1 dollar*\". \n\n\n---\n\n### Training: Script to train this model\n\nThe following Flair script was used to train this model: \n\n```python\nfrom flair.data import Corpus\nfrom flair.datasets import ColumnCorpus\nfrom flair.embeddings import WordEmbeddings, StackedEmbeddings, FlairEmbeddings\n\n# 1. load the corpus (Ontonotes does not ship with Flair, you need to download and reformat into a column format yourself)\ncorpus: Corpus = ColumnCorpus(\n                \"resources/tasks/onto-ner\",\n                column_format={0: \"text\", 1: \"pos\", 2: \"upos\", 3: \"ner\"},\n                tag_to_bioes=\"ner\",\n            )\n\n# 2. what tag do we want to predict?\ntag_type = 'ner'\n\n# 3. make the tag dictionary from the corpus\ntag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)\n\n# 4. initialize each embedding we use\nembedding_types = [\n\n    # GloVe embeddings\n    WordEmbeddings('en-crawl'),\n\n    # contextual string embeddings, forward\n    FlairEmbeddings('news-forward'),\n\n    # contextual string embeddings, backward\n    FlairEmbeddings('news-backward'),\n]\n\n# embedding stack consists of Flair and GloVe embeddings\nembeddings = StackedEmbeddings(embeddings=embedding_types)\n\n# 5. initialize sequence tagger\nfrom flair.models import SequenceTagger\n\ntagger = SequenceTagger(hidden_size=256,\n                        embeddings=embeddings,\n                        tag_dictionary=tag_dictionary,\n                        tag_type=tag_type)\n\n# 6. initialize trainer\nfrom flair.trainers import ModelTrainer\n\ntrainer = ModelTrainer(tagger, corpus)\n\n# 7. run training\ntrainer.train('resources/taggers/ner-english-ontonotes',\n              train_with_dev=True,\n              max_epochs=150)\n```\n\n\n\n---\n\n### Cite\n\nPlease cite the following paper when using this model.\n\n```\n@inproceedings{akbik2018coling,\n  title={Contextual String Embeddings for Sequence Labeling},\n  author={Akbik, Alan and Blythe, Duncan and Vollgraf, Roland},\n  booktitle = {{COLING} 2018, 27th International Conference on Computational Linguistics},\n  pages     = {1638--1649},\n  year      = {2018}\n}\n```\n\n---\n\n### Issues?\n\nThe Flair issue tracker is available [here](https://github.com/flairNLP/flair/issues/).\n",
    "library_name": "flair"
  },
  {
    "model_id": "segment-any-text/sat-12l-sm",
    "model_name": "segment-any-text/sat-12l-sm",
    "author": "segment-any-text",
    "downloads": 165183,
    "likes": 20,
    "tags": [
      "transformers",
      "onnx",
      "safetensors",
      "xlm-token",
      "token-classification",
      "multilingual",
      "am",
      "ar",
      "az",
      "be",
      "bg",
      "bn",
      "ca",
      "ceb",
      "cs",
      "cy",
      "da",
      "de",
      "el",
      "en",
      "eo",
      "es",
      "et",
      "eu",
      "fa",
      "fi",
      "fr",
      "fy",
      "ga",
      "gd",
      "gl",
      "gu",
      "ha",
      "he",
      "hi",
      "hu",
      "hy",
      "id",
      "ig",
      "is",
      "it",
      "ja",
      "jv",
      "ka",
      "kk",
      "km",
      "kn",
      "ko",
      "ku",
      "ky",
      "la",
      "lt",
      "lv",
      "mg",
      "mk",
      "ml",
      "mn",
      "mr",
      "ms",
      "mt",
      "my",
      "ne",
      "nl",
      "no",
      "pa",
      "pl",
      "ps",
      "pt",
      "ro",
      "ru",
      "si",
      "sk",
      "sl",
      "sq",
      "sr",
      "sv",
      "ta",
      "te",
      "tg",
      "th",
      "tr",
      "uk",
      "ur",
      "uz",
      "vi",
      "xh",
      "yi",
      "yo",
      "zh",
      "zu",
      "arxiv:2406.16678",
      "license:mit",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/segment-any-text/sat-12l-sm",
    "dependencies": [
      [
        "wtpsplit",
        null
      ]
    ],
    "analysis_date": "2025-03-26T00:51:25.500856",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "xlm-token",
    "pipeline_tag": "token-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": [
        "multilingual",
        "am",
        "ar",
        "az",
        "be",
        "bg",
        "bn",
        "ca",
        "ceb",
        "cs",
        "cy",
        "da",
        "de",
        "el",
        "en",
        "eo",
        "es",
        "et",
        "eu",
        "fa",
        "fi",
        "fr",
        "fy",
        "ga",
        "gd",
        "gl",
        "gu",
        "ha",
        "he",
        "hi",
        "hu",
        "hy",
        "id",
        "ig",
        "is",
        "it",
        "ja",
        "jv",
        "ka",
        "kk",
        "km",
        "kn",
        "ko",
        "ku",
        "ky",
        "la",
        "lt",
        "lv",
        "mg",
        "mk",
        "ml",
        "mn",
        "mr",
        "ms",
        "mt",
        "my",
        "ne",
        "nl",
        false,
        "pa",
        "pl",
        "ps",
        "pt",
        "ro",
        "ru",
        "si",
        "sk",
        "sl",
        "sq",
        "sr",
        "sv",
        "ta",
        "te",
        "tg",
        "th",
        "tr",
        "uk",
        "ur",
        "uz",
        "vi",
        "xh",
        "yi",
        "yo",
        "zh",
        "zu"
      ],
      "license": "mit",
      "library": [
        "wtpsplit"
      ]
    },
    "card_text": "\n# sat-12l-sm\n\nModel for [`wtpsplit`](https://github.com/segment-any-text/wtpsplit).\n\nState-of-the-art sentence segmentation with 12 Transfomer layers.\n\n\nFor details, see our [`Segment any Text` paper](arxiv.org/abs/2406.16678)",
    "card_content": "---\nlanguage:\n- multilingual\n- am\n- ar\n- az\n- be\n- bg\n- bn\n- ca\n- ceb\n- cs\n- cy\n- da\n- de\n- el\n- en\n- eo\n- es\n- et\n- eu\n- fa\n- fi\n- fr\n- fy\n- ga\n- gd\n- gl\n- gu\n- ha\n- he\n- hi\n- hu\n- hy\n- id\n- ig\n- is\n- it\n- ja\n- jv\n- ka\n- kk\n- km\n- kn\n- ko\n- ku\n- ky\n- la\n- lt\n- lv\n- mg\n- mk\n- ml\n- mn\n- mr\n- ms\n- mt\n- my\n- ne\n- nl\n- false\n- pa\n- pl\n- ps\n- pt\n- ro\n- ru\n- si\n- sk\n- sl\n- sq\n- sr\n- sv\n- ta\n- te\n- tg\n- th\n- tr\n- uk\n- ur\n- uz\n- vi\n- xh\n- yi\n- yo\n- zh\n- zu\nlicense: mit\nlibrary:\n- wtpsplit\n---\n\n# sat-12l-sm\n\nModel for [`wtpsplit`](https://github.com/segment-any-text/wtpsplit).\n\nState-of-the-art sentence segmentation with 12 Transfomer layers.\n\n\nFor details, see our [`Segment any Text` paper](arxiv.org/abs/2406.16678)",
    "library_name": "transformers"
  },
  {
    "model_id": "sociocom/MedNERN-CR-JA",
    "model_name": "sociocom/MedNERN-CR-JA",
    "author": "sociocom",
    "downloads": 164122,
    "likes": 3,
    "tags": [
      "transformers",
      "pytorch",
      "safetensors",
      "bert",
      "token-classification",
      "NER",
      "medical documents",
      "ja",
      "dataset:MedTxt-CR-JA-training-v2.xml",
      "doi:10.57967/hf/0620",
      "license:cc-by-4.0",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/sociocom/MedNERN-CR-JA",
    "dependencies": [
      [
        "transformers",
        null
      ],
      [
        "torch",
        null
      ],
      [
        "numpy",
        null
      ],
      [
        "pandas",
        null
      ],
      [
        "tqdm",
        null
      ],
      [
        "scikit-learn",
        null
      ]
    ],
    "analysis_date": "2025-03-26T00:51:27.137470",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "bert",
    "pipeline_tag": "token-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": [
        "ja"
      ],
      "license": [
        "cc-by-4.0"
      ],
      "tags": [
        "NER",
        "medical documents"
      ],
      "datasets": [
        "MedTxt-CR-JA-training-v2.xml"
      ],
      "metrics": [
        "NTCIR-16 Real-MedNLP subtask 1"
      ]
    },
    "card_text": "\nThis is a model for named entity recognition of Japanese medical documents.\n\n# Introduction\n\nThis repository contains the base model and a support predict script for using the model and providing a XML tagged text output.\n\nThe original model was trained on the [MedTxt-CR-JA](https://sociocom.naist.jp/medtxt/cr) dataset, so the provided prediction code outputs XML tags in the same format.\n\nThe script also provide the normalization method for the output entities, which is not embedded in the model.\n\nIf you want to re-train or update the model, we provide additional support scripts in [this GitHub repository](https://github.com/sociocom/MedNERN-CR-JA).\nIssues and suggestions can also be submitted there.\n\n### A note about loading the model using standard HuggingFace methods\nThis model should also be loadable using standard HuggingFace `from_pretrained` methods. However, the model by itself only outputs labels in the format \"LABEL_0\", \"LABEL1\", etc.\n\nThe conversion of model outputs to the actual labels (\"<m-key>, \"<m-val>\", \"<timex-3>\" etc.) is not yet embedded into the model, so the extra `id_to_tags.pkl` file is necessary \nto make the conversion. It contains a mapping from the model output ids to the actual labels.\n\nSuch process can be done manually if needed, but the `predict.py` script already does that.\n\nWe are currently working to better standardize the model to HuggingFace's standards.\n\n## How to use\n\nClone the repository and install the requirements:\n\n``` \npip install -r requirements.txt\n```\n\nThe code has been developed tested with Python 3.9 in MacOS 14.1 (M1 MacBook Pro).\n\n### Prediction\n\nThe prediction script will output the results in the same XML format as the input file. It can be run with the following\ncommand:\n\n```\npython3 predict.py\n```\n\nThe default parameters will take the model located in `pytorch_model.bin` and the input file `text.txt`.\nThe resulting predictions will be output to the screen.\n\nTo select a different model or input file, use the `-m` and `-i` parameters, respectively:\n\n```\npython3 predict.py -m <model_path> -i <your_input_file>.txt\n```\n\nThe input file can be a single text file or a folder containing multiple `.txt` files, for batch processing. For example:\n\n```\npython3 predict.py -m <model_path> -i <your_input_folder>\n```\n\n\n### Entity normalization\n\nThis model supports entity normalization via dictionary matching. The dictionary is a list of medical terms or\ndrugs and their standard forms.\n\nTwo different dictionaries are used for drug and disease normalization, stored in the `dictionaries` folder as\n`drug_dict.csv` and `disease_dict.csv`, respectively.\n\nTo enable normalization you can add the `--normalize` flag to the `predict.py` command.\n\n```\npython3 predict.py -m <model_path> --normalize\n```\n\nNormalization will add the `norm` attribute to the output XML tags. This attribute can be empty if a normalized form of\nthe term is not found.\n\nThe provided disease normalization dictionary (`dictionaties/disease_dict.csv`) is based on\nthe [Manbyo Dictionary](https://sociocom.naist.jp/manbyo-dic-en/) and provides normalization to the standard ICD code\nfor the diseases.\n\nThe default drug dictionary (`dictionaties/drug_dict.csv`) is based on\nthe [Hyakuyaku Dictionary](https://sociocom.naist.jp/hyakuyaku-dic-en/).\n\nThe dictionary is a CSV file with three columns: the first column is the surface form term and the third column contain\nits standard form. The second column is not used.\n\n### Replacing the default dictionaries\n\nUser can freely change the dictionary to fit their needs by passing the path to a custom dictionary file.\nThe dictionary file must have at least a column containing the list of surface forms and a column containing the list of\nnormalized forms.\n\nThe parameters `--drug_dict` and `--disease_dict` can be used to specify the path to the drug and disease dictionaries,\nrespectively.\nWhen doing so, the respective parameters informing the column index of the surface form and normalized form must also be\nprovided.\nYou don't need to replace both dictionaries at the same time, you can replace only one of them.\n\nE.g.:\n\n```\npython3 predict.py --normalize --drug_dict dictionaries/drug_dict.csv --drug_surface_form 0 --drug_norm_form 2 --disease_dict dictionaries/disease_dict.csv --disease_surface_form 0 --disease_norm_form 2\n```\n\n### Input Example\n\n```\n肥大型心筋症、心房細動に対してＷＦ投与が開始となった。\n治療経過中に非持続性心室頻拍が認められたためアミオダロンが併用となった。\n```\n\n### Output Example\n\n```\n<d certainty=\"positive\" norm=\"I422\">肥大型心筋症、心房細動</d>に対して<m-key state=\"executed\" norm=\"ワルファリンカリウム\">ＷＦ</m-key>投与が開始となった。\n<timex3 type=\"med\">治療経過中</timex3>に<d certainty=\"positive\" norm=\"I472\">非持続性心室頻拍</d>が認められたため<m-key state=\"executed\" norm=\"アミオダロン塩酸塩\">アミオダロン</m-key>が併用となった。\n```\n\n## Publication\n\nThis model can be cited as:\n\n```\n@misc {social_computing_lab_2023,\n\tauthor       = { {Social Computing Lab} },\n\ttitle        = { MedNERN-CR-JA (Revision 13dbcb6) },\n\tyear         = 2023,\n\turl          = { https://huggingface.co/sociocom/MedNERN-CR-JA },\n\tdoi          = { 10.57967/hf/0620 },\n\tpublisher    = { Hugging Face }\n}\n```\n",
    "card_content": "---\nlanguage:\n- ja\nlicense:\n- cc-by-4.0\ntags:\n- NER\n- medical documents\ndatasets:\n- MedTxt-CR-JA-training-v2.xml\nmetrics:\n- NTCIR-16 Real-MedNLP subtask 1\n---\n\nThis is a model for named entity recognition of Japanese medical documents.\n\n# Introduction\n\nThis repository contains the base model and a support predict script for using the model and providing a XML tagged text output.\n\nThe original model was trained on the [MedTxt-CR-JA](https://sociocom.naist.jp/medtxt/cr) dataset, so the provided prediction code outputs XML tags in the same format.\n\nThe script also provide the normalization method for the output entities, which is not embedded in the model.\n\nIf you want to re-train or update the model, we provide additional support scripts in [this GitHub repository](https://github.com/sociocom/MedNERN-CR-JA).\nIssues and suggestions can also be submitted there.\n\n### A note about loading the model using standard HuggingFace methods\nThis model should also be loadable using standard HuggingFace `from_pretrained` methods. However, the model by itself only outputs labels in the format \"LABEL_0\", \"LABEL1\", etc.\n\nThe conversion of model outputs to the actual labels (\"<m-key>, \"<m-val>\", \"<timex-3>\" etc.) is not yet embedded into the model, so the extra `id_to_tags.pkl` file is necessary \nto make the conversion. It contains a mapping from the model output ids to the actual labels.\n\nSuch process can be done manually if needed, but the `predict.py` script already does that.\n\nWe are currently working to better standardize the model to HuggingFace's standards.\n\n## How to use\n\nClone the repository and install the requirements:\n\n``` \npip install -r requirements.txt\n```\n\nThe code has been developed tested with Python 3.9 in MacOS 14.1 (M1 MacBook Pro).\n\n### Prediction\n\nThe prediction script will output the results in the same XML format as the input file. It can be run with the following\ncommand:\n\n```\npython3 predict.py\n```\n\nThe default parameters will take the model located in `pytorch_model.bin` and the input file `text.txt`.\nThe resulting predictions will be output to the screen.\n\nTo select a different model or input file, use the `-m` and `-i` parameters, respectively:\n\n```\npython3 predict.py -m <model_path> -i <your_input_file>.txt\n```\n\nThe input file can be a single text file or a folder containing multiple `.txt` files, for batch processing. For example:\n\n```\npython3 predict.py -m <model_path> -i <your_input_folder>\n```\n\n\n### Entity normalization\n\nThis model supports entity normalization via dictionary matching. The dictionary is a list of medical terms or\ndrugs and their standard forms.\n\nTwo different dictionaries are used for drug and disease normalization, stored in the `dictionaries` folder as\n`drug_dict.csv` and `disease_dict.csv`, respectively.\n\nTo enable normalization you can add the `--normalize` flag to the `predict.py` command.\n\n```\npython3 predict.py -m <model_path> --normalize\n```\n\nNormalization will add the `norm` attribute to the output XML tags. This attribute can be empty if a normalized form of\nthe term is not found.\n\nThe provided disease normalization dictionary (`dictionaties/disease_dict.csv`) is based on\nthe [Manbyo Dictionary](https://sociocom.naist.jp/manbyo-dic-en/) and provides normalization to the standard ICD code\nfor the diseases.\n\nThe default drug dictionary (`dictionaties/drug_dict.csv`) is based on\nthe [Hyakuyaku Dictionary](https://sociocom.naist.jp/hyakuyaku-dic-en/).\n\nThe dictionary is a CSV file with three columns: the first column is the surface form term and the third column contain\nits standard form. The second column is not used.\n\n### Replacing the default dictionaries\n\nUser can freely change the dictionary to fit their needs by passing the path to a custom dictionary file.\nThe dictionary file must have at least a column containing the list of surface forms and a column containing the list of\nnormalized forms.\n\nThe parameters `--drug_dict` and `--disease_dict` can be used to specify the path to the drug and disease dictionaries,\nrespectively.\nWhen doing so, the respective parameters informing the column index of the surface form and normalized form must also be\nprovided.\nYou don't need to replace both dictionaries at the same time, you can replace only one of them.\n\nE.g.:\n\n```\npython3 predict.py --normalize --drug_dict dictionaries/drug_dict.csv --drug_surface_form 0 --drug_norm_form 2 --disease_dict dictionaries/disease_dict.csv --disease_surface_form 0 --disease_norm_form 2\n```\n\n### Input Example\n\n```\n肥大型心筋症、心房細動に対してＷＦ投与が開始となった。\n治療経過中に非持続性心室頻拍が認められたためアミオダロンが併用となった。\n```\n\n### Output Example\n\n```\n<d certainty=\"positive\" norm=\"I422\">肥大型心筋症、心房細動</d>に対して<m-key state=\"executed\" norm=\"ワルファリンカリウム\">ＷＦ</m-key>投与が開始となった。\n<timex3 type=\"med\">治療経過中</timex3>に<d certainty=\"positive\" norm=\"I472\">非持続性心室頻拍</d>が認められたため<m-key state=\"executed\" norm=\"アミオダロン塩酸塩\">アミオダロン</m-key>が併用となった。\n```\n\n## Publication\n\nThis model can be cited as:\n\n```\n@misc {social_computing_lab_2023,\n\tauthor       = { {Social Computing Lab} },\n\ttitle        = { MedNERN-CR-JA (Revision 13dbcb6) },\n\tyear         = 2023,\n\turl          = { https://huggingface.co/sociocom/MedNERN-CR-JA },\n\tdoi          = { 10.57967/hf/0620 },\n\tpublisher    = { Hugging Face }\n}\n```\n",
    "library_name": "transformers"
  },
  {
    "model_id": "Jean-Baptiste/roberta-large-ner-english",
    "model_name": "Jean-Baptiste/roberta-large-ner-english",
    "author": "Jean-Baptiste",
    "downloads": 163982,
    "likes": 70,
    "tags": [
      "transformers",
      "pytorch",
      "tf",
      "onnx",
      "safetensors",
      "roberta",
      "token-classification",
      "en",
      "dataset:conll2003",
      "license:mit",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/Jean-Baptiste/roberta-large-ner-english",
    "dependencies": [
      [
        "transformers",
        null
      ]
    ],
    "analysis_date": "2025-03-26T00:51:29.816422",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "roberta",
    "pipeline_tag": "token-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": "en",
      "license": "mit",
      "datasets": [
        "conll2003"
      ],
      "widget": [
        {
          "text": "My name is jean-baptiste and I live in montreal"
        },
        {
          "text": "My name is clara and I live in berkeley, california."
        },
        {
          "text": "My name is wolfgang and I live in berlin"
        }
      ],
      "train-eval-index": [
        {
          "config": "conll2003",
          "task": "token-classification",
          "task_id": "entity_extraction",
          "splits": {
            "eval_split": "validation"
          },
          "col_mapping": {
            "tokens": "tokens",
            "ner_tags": "tags"
          }
        }
      ]
    },
    "card_text": "\n# roberta-large-ner-english: model fine-tuned from roberta-large for NER task\n\n## Introduction\n\n[roberta-large-ner-english] is an english NER model that was fine-tuned from roberta-large on conll2003 dataset. \nModel was validated on emails/chat data and outperformed other models on this type of data specifically. \nIn particular the model seems to work better on entity that don't start with an upper case.\n\n\n## Training data\n\nTraining data was classified as follow:\n\nAbbreviation|Description\n-|-\nO |Outside of a named entity\nMISC |Miscellaneous entity\nPER |Person’s name\nORG |Organization\nLOC |Location\n\nIn order to simplify, the prefix B- or I- from original conll2003 was removed.\nI used the train and test dataset from original conll2003 for training and the \"validation\" dataset for validation. This resulted in a dataset of size:\n\nTrain | Validation \n-|-\n17494 | 3250\n\n## How to use roberta-large-ner-english with HuggingFace\n\n##### Load roberta-large-ner-english and its sub-word tokenizer :\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\n\ntokenizer = AutoTokenizer.from_pretrained(\"Jean-Baptiste/roberta-large-ner-english\")\nmodel = AutoModelForTokenClassification.from_pretrained(\"Jean-Baptiste/roberta-large-ner-english\")\n\n\n##### Process text sample (from wikipedia)\n\nfrom transformers import pipeline\n\nnlp = pipeline('ner', model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\nnlp(\"Apple was founded in 1976 by Steve Jobs, Steve Wozniak and Ronald Wayne to develop and sell Wozniak's Apple I personal computer\")\n\n\n[{'entity_group': 'ORG',\n  'score': 0.99381506,\n  'word': ' Apple',\n  'start': 0,\n  'end': 5},\n {'entity_group': 'PER',\n  'score': 0.99970853,\n  'word': ' Steve Jobs',\n  'start': 29,\n  'end': 39},\n {'entity_group': 'PER',\n  'score': 0.99981767,\n  'word': ' Steve Wozniak',\n  'start': 41,\n  'end': 54},\n {'entity_group': 'PER',\n  'score': 0.99956465,\n  'word': ' Ronald Wayne',\n  'start': 59,\n  'end': 71},\n {'entity_group': 'PER',\n  'score': 0.9997918,\n  'word': ' Wozniak',\n  'start': 92,\n  'end': 99},\n {'entity_group': 'MISC',\n  'score': 0.99956393,\n  'word': ' Apple I',\n  'start': 102,\n  'end': 109}]\n```\n\n\n## Model performances \n\nModel performances computed on conll2003 validation dataset (computed on the tokens predictions)\n\nentity|precision|recall|f1\n-|-|-|-\nPER|0.9914|0.9927|0.9920 \nORG|0.9627|0.9661|0.9644\nLOC|0.9795|0.9862|0.9828\nMISC|0.9292|0.9262|0.9277\nOverall|0.9740|0.9766|0.9753\n\n\nOn private dataset (email, chat, informal discussion), computed on word predictions:\n\nentity|precision|recall|f1\n-|-|-|-\nPER|0.8823|0.9116|0.8967\nORG|0.7694|0.7292|0.7487\nLOC|0.8619|0.7768|0.8171\n\nBy comparison on the same private dataset, Spacy (en_core_web_trf-3.2.0) was giving:\n\nentity|precision|recall|f1\n-|-|-|-\nPER|0.9146|0.8287|0.8695\nORG|0.7655|0.6437|0.6993\nLOC|0.8727|0.6180|0.7236\n\n\n\nFor those who could be interested, here is a short article on how I used the results of this model to train a LSTM model for signature detection in emails:\nhttps://medium.com/@jean-baptiste.polle/lstm-model-for-email-signature-detection-8e990384fefa\n",
    "card_content": "---\nlanguage: en\nlicense: mit\ndatasets:\n- conll2003\nwidget:\n- text: My name is jean-baptiste and I live in montreal\n- text: My name is clara and I live in berkeley, california.\n- text: My name is wolfgang and I live in berlin\ntrain-eval-index:\n- config: conll2003\n  task: token-classification\n  task_id: entity_extraction\n  splits:\n    eval_split: validation\n  col_mapping:\n    tokens: tokens\n    ner_tags: tags\n---\n\n# roberta-large-ner-english: model fine-tuned from roberta-large for NER task\n\n## Introduction\n\n[roberta-large-ner-english] is an english NER model that was fine-tuned from roberta-large on conll2003 dataset. \nModel was validated on emails/chat data and outperformed other models on this type of data specifically. \nIn particular the model seems to work better on entity that don't start with an upper case.\n\n\n## Training data\n\nTraining data was classified as follow:\n\nAbbreviation|Description\n-|-\nO |Outside of a named entity\nMISC |Miscellaneous entity\nPER |Person’s name\nORG |Organization\nLOC |Location\n\nIn order to simplify, the prefix B- or I- from original conll2003 was removed.\nI used the train and test dataset from original conll2003 for training and the \"validation\" dataset for validation. This resulted in a dataset of size:\n\nTrain | Validation \n-|-\n17494 | 3250\n\n## How to use roberta-large-ner-english with HuggingFace\n\n##### Load roberta-large-ner-english and its sub-word tokenizer :\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\n\ntokenizer = AutoTokenizer.from_pretrained(\"Jean-Baptiste/roberta-large-ner-english\")\nmodel = AutoModelForTokenClassification.from_pretrained(\"Jean-Baptiste/roberta-large-ner-english\")\n\n\n##### Process text sample (from wikipedia)\n\nfrom transformers import pipeline\n\nnlp = pipeline('ner', model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\nnlp(\"Apple was founded in 1976 by Steve Jobs, Steve Wozniak and Ronald Wayne to develop and sell Wozniak's Apple I personal computer\")\n\n\n[{'entity_group': 'ORG',\n  'score': 0.99381506,\n  'word': ' Apple',\n  'start': 0,\n  'end': 5},\n {'entity_group': 'PER',\n  'score': 0.99970853,\n  'word': ' Steve Jobs',\n  'start': 29,\n  'end': 39},\n {'entity_group': 'PER',\n  'score': 0.99981767,\n  'word': ' Steve Wozniak',\n  'start': 41,\n  'end': 54},\n {'entity_group': 'PER',\n  'score': 0.99956465,\n  'word': ' Ronald Wayne',\n  'start': 59,\n  'end': 71},\n {'entity_group': 'PER',\n  'score': 0.9997918,\n  'word': ' Wozniak',\n  'start': 92,\n  'end': 99},\n {'entity_group': 'MISC',\n  'score': 0.99956393,\n  'word': ' Apple I',\n  'start': 102,\n  'end': 109}]\n```\n\n\n## Model performances \n\nModel performances computed on conll2003 validation dataset (computed on the tokens predictions)\n\nentity|precision|recall|f1\n-|-|-|-\nPER|0.9914|0.9927|0.9920 \nORG|0.9627|0.9661|0.9644\nLOC|0.9795|0.9862|0.9828\nMISC|0.9292|0.9262|0.9277\nOverall|0.9740|0.9766|0.9753\n\n\nOn private dataset (email, chat, informal discussion), computed on word predictions:\n\nentity|precision|recall|f1\n-|-|-|-\nPER|0.8823|0.9116|0.8967\nORG|0.7694|0.7292|0.7487\nLOC|0.8619|0.7768|0.8171\n\nBy comparison on the same private dataset, Spacy (en_core_web_trf-3.2.0) was giving:\n\nentity|precision|recall|f1\n-|-|-|-\nPER|0.9146|0.8287|0.8695\nORG|0.7655|0.6437|0.6993\nLOC|0.8727|0.6180|0.7236\n\n\n\nFor those who could be interested, here is a short article on how I used the results of this model to train a LSTM model for signature detection in emails:\nhttps://medium.com/@jean-baptiste.polle/lstm-model-for-email-signature-detection-8e990384fefa\n",
    "library_name": "transformers"
  },
  {
    "model_id": "dslim/bert-large-NER",
    "model_name": "dslim/bert-large-NER",
    "author": "dslim",
    "downloads": 161598,
    "likes": 148,
    "tags": [
      "transformers",
      "pytorch",
      "tf",
      "jax",
      "onnx",
      "safetensors",
      "bert",
      "token-classification",
      "en",
      "dataset:conll2003",
      "arxiv:1810.04805",
      "license:mit",
      "model-index",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/dslim/bert-large-NER",
    "dependencies": [
      [
        "transformers",
        null
      ]
    ],
    "analysis_date": "2025-03-26T00:51:30.931033",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "bert",
    "pipeline_tag": "token-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": "en",
      "license": "mit",
      "datasets": [
        "conll2003"
      ],
      "model-index": [
        {
          "name": "dslim/bert-large-NER",
          "results": [
            {
              "task": {
                "type": "token-classification",
                "name": "Token Classification"
              },
              "dataset": {
                "name": "conll2003",
                "type": "conll2003",
                "config": "conll2003",
                "split": "test"
              },
              "metrics": [
                {
                  "type": "accuracy",
                  "value": 0.9031688753722759,
                  "name": "Accuracy",
                  "verified": true
                },
                {
                  "type": "precision",
                  "value": 0.920025068328604,
                  "name": "Precision",
                  "verified": true
                },
                {
                  "type": "recall",
                  "value": 0.9193688678588825,
                  "name": "Recall",
                  "verified": true
                },
                {
                  "type": "f1",
                  "value": 0.9196968510445761,
                  "name": "F1",
                  "verified": true
                },
                {
                  "type": "loss",
                  "value": 0.5085050463676453,
                  "name": "loss",
                  "verified": true
                }
              ]
            }
          ]
        }
      ]
    },
    "card_text": "# bert-large-NER\n\nIf my open source models have been useful to you, please consider supporting me in building small, useful AI models for everyone (and help me afford med school / help out my parents financially). Thanks!\n\n<a href=\"https://www.buymeacoffee.com/dslim\" target=\"_blank\"><img src=\"https://cdn.buymeacoffee.com/buttons/v2/arial-yellow.png\" alt=\"Buy Me A Coffee\" style=\"height: 60px !important;width: 217px !important;\" ></a>\n\n## Model description\n\n**bert-large-NER** is a fine-tuned BERT model that is ready to use for **Named Entity Recognition** and achieves **state-of-the-art performance** for the NER task. It has been trained to recognize four types of entities: location (LOC), organizations (ORG), person (PER) and Miscellaneous (MISC). \n\nSpecifically, this model is a *bert-large-cased* model that was fine-tuned on the English version of the standard [CoNLL-2003 Named Entity Recognition](https://www.aclweb.org/anthology/W03-0419.pdf) dataset. \n\nIf you'd like to use a smaller BERT model fine-tuned on the same dataset, a [**bert-base-NER**](https://huggingface.co/dslim/bert-base-NER/) version is also available. \n\n\n## Intended uses & limitations\n\n#### How to use\n\nYou can use this model with Transformers *pipeline* for NER.\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\nfrom transformers import pipeline\n\ntokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-large-NER\")\nmodel = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-large-NER\")\n\nnlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\nexample = \"My name is Wolfgang and I live in Berlin\"\n\nner_results = nlp(example)\nprint(ner_results)\n```\n\n#### Limitations and bias\n\nThis model is limited by its training dataset of entity-annotated news articles from a specific span of time. This may not generalize well for all use cases in different domains. Furthermore, the model occassionally tags subword tokens as entities and post-processing of results may be necessary to handle those cases. \n\n## Training data\n\nThis model was fine-tuned on English version of the standard [CoNLL-2003 Named Entity Recognition](https://www.aclweb.org/anthology/W03-0419.pdf) dataset. \n\nThe training dataset distinguishes between the beginning and continuation of an entity so that if there are back-to-back entities of the same type, the model can output where the second entity begins. As in the dataset, each token will be classified as one of the following classes:\n\nAbbreviation|Description\n-|-\nO|Outside of a named entity\nB-MIS |Beginning of a miscellaneous entity right after another miscellaneous entity\nI-MIS | Miscellaneous entity\nB-PER |Beginning of a person’s name right after another person’s name\nI-PER |Person’s name\nB-ORG |Beginning of an organization right after another organization\nI-ORG |organization\nB-LOC |Beginning of a location right after another location\nI-LOC |Location\n\n\n### CoNLL-2003 English Dataset Statistics\nThis dataset was derived from the Reuters corpus which consists of Reuters news stories. You can read more about how this dataset was created in the CoNLL-2003 paper. \n#### # of training examples per entity type\nDataset|LOC|MISC|ORG|PER\n-|-|-|-|-\nTrain|7140|3438|6321|6600\nDev|1837|922|1341|1842\nTest|1668|702|1661|1617\n#### # of articles/sentences/tokens per dataset\nDataset |Articles |Sentences |Tokens\n-|-|-|-\nTrain |946 |14,987 |203,621\nDev |216 |3,466 |51,362\nTest |231 |3,684 |46,435\n\n## Training procedure\n\nThis model was trained on a single NVIDIA V100 GPU with recommended hyperparameters from the [original BERT paper](https://arxiv.org/pdf/1810.04805) which trained & evaluated the model on CoNLL-2003 NER task. \n\n## Eval results\nmetric|dev|test\n-|-|-\nf1 |95.7 |91.7\nprecision |95.3 |91.2\nrecall |96.1 |92.3\n\nThe test metrics are a little lower than the official Google BERT results which encoded document context & experimented with CRF. More on replicating the original results [here](https://github.com/google-research/bert/issues/223).\n\n### BibTeX entry and citation info\n\n```\n@article{DBLP:journals/corr/abs-1810-04805,\n  author    = {Jacob Devlin and\n               Ming{-}Wei Chang and\n               Kenton Lee and\n               Kristina Toutanova},\n  title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language\n               Understanding},\n  journal   = {CoRR},\n  volume    = {abs/1810.04805},\n  year      = {2018},\n  url       = {http://arxiv.org/abs/1810.04805},\n  archivePrefix = {arXiv},\n  eprint    = {1810.04805},\n  timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n```\n```\n@inproceedings{tjong-kim-sang-de-meulder-2003-introduction,\n    title = \"Introduction to the {C}o{NLL}-2003 Shared Task: Language-Independent Named Entity Recognition\",\n    author = \"Tjong Kim Sang, Erik F.  and\n      De Meulder, Fien\",\n    booktitle = \"Proceedings of the Seventh Conference on Natural Language Learning at {HLT}-{NAACL} 2003\",\n    year = \"2003\",\n    url = \"https://www.aclweb.org/anthology/W03-0419\",\n    pages = \"142--147\",\n}\n```\n",
    "card_content": "---\nlanguage: en\nlicense: mit\ndatasets:\n- conll2003\nmodel-index:\n- name: dslim/bert-large-NER\n  results:\n  - task:\n      type: token-classification\n      name: Token Classification\n    dataset:\n      name: conll2003\n      type: conll2003\n      config: conll2003\n      split: test\n    metrics:\n    - type: accuracy\n      value: 0.9031688753722759\n      name: Accuracy\n      verified: true\n    - type: precision\n      value: 0.920025068328604\n      name: Precision\n      verified: true\n    - type: recall\n      value: 0.9193688678588825\n      name: Recall\n      verified: true\n    - type: f1\n      value: 0.9196968510445761\n      name: F1\n      verified: true\n    - type: loss\n      value: 0.5085050463676453\n      name: loss\n      verified: true\n---\n# bert-large-NER\n\nIf my open source models have been useful to you, please consider supporting me in building small, useful AI models for everyone (and help me afford med school / help out my parents financially). Thanks!\n\n<a href=\"https://www.buymeacoffee.com/dslim\" target=\"_blank\"><img src=\"https://cdn.buymeacoffee.com/buttons/v2/arial-yellow.png\" alt=\"Buy Me A Coffee\" style=\"height: 60px !important;width: 217px !important;\" ></a>\n\n## Model description\n\n**bert-large-NER** is a fine-tuned BERT model that is ready to use for **Named Entity Recognition** and achieves **state-of-the-art performance** for the NER task. It has been trained to recognize four types of entities: location (LOC), organizations (ORG), person (PER) and Miscellaneous (MISC). \n\nSpecifically, this model is a *bert-large-cased* model that was fine-tuned on the English version of the standard [CoNLL-2003 Named Entity Recognition](https://www.aclweb.org/anthology/W03-0419.pdf) dataset. \n\nIf you'd like to use a smaller BERT model fine-tuned on the same dataset, a [**bert-base-NER**](https://huggingface.co/dslim/bert-base-NER/) version is also available. \n\n\n## Intended uses & limitations\n\n#### How to use\n\nYou can use this model with Transformers *pipeline* for NER.\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\nfrom transformers import pipeline\n\ntokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-large-NER\")\nmodel = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-large-NER\")\n\nnlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\nexample = \"My name is Wolfgang and I live in Berlin\"\n\nner_results = nlp(example)\nprint(ner_results)\n```\n\n#### Limitations and bias\n\nThis model is limited by its training dataset of entity-annotated news articles from a specific span of time. This may not generalize well for all use cases in different domains. Furthermore, the model occassionally tags subword tokens as entities and post-processing of results may be necessary to handle those cases. \n\n## Training data\n\nThis model was fine-tuned on English version of the standard [CoNLL-2003 Named Entity Recognition](https://www.aclweb.org/anthology/W03-0419.pdf) dataset. \n\nThe training dataset distinguishes between the beginning and continuation of an entity so that if there are back-to-back entities of the same type, the model can output where the second entity begins. As in the dataset, each token will be classified as one of the following classes:\n\nAbbreviation|Description\n-|-\nO|Outside of a named entity\nB-MIS |Beginning of a miscellaneous entity right after another miscellaneous entity\nI-MIS | Miscellaneous entity\nB-PER |Beginning of a person’s name right after another person’s name\nI-PER |Person’s name\nB-ORG |Beginning of an organization right after another organization\nI-ORG |organization\nB-LOC |Beginning of a location right after another location\nI-LOC |Location\n\n\n### CoNLL-2003 English Dataset Statistics\nThis dataset was derived from the Reuters corpus which consists of Reuters news stories. You can read more about how this dataset was created in the CoNLL-2003 paper. \n#### # of training examples per entity type\nDataset|LOC|MISC|ORG|PER\n-|-|-|-|-\nTrain|7140|3438|6321|6600\nDev|1837|922|1341|1842\nTest|1668|702|1661|1617\n#### # of articles/sentences/tokens per dataset\nDataset |Articles |Sentences |Tokens\n-|-|-|-\nTrain |946 |14,987 |203,621\nDev |216 |3,466 |51,362\nTest |231 |3,684 |46,435\n\n## Training procedure\n\nThis model was trained on a single NVIDIA V100 GPU with recommended hyperparameters from the [original BERT paper](https://arxiv.org/pdf/1810.04805) which trained & evaluated the model on CoNLL-2003 NER task. \n\n## Eval results\nmetric|dev|test\n-|-|-\nf1 |95.7 |91.7\nprecision |95.3 |91.2\nrecall |96.1 |92.3\n\nThe test metrics are a little lower than the official Google BERT results which encoded document context & experimented with CRF. More on replicating the original results [here](https://github.com/google-research/bert/issues/223).\n\n### BibTeX entry and citation info\n\n```\n@article{DBLP:journals/corr/abs-1810-04805,\n  author    = {Jacob Devlin and\n               Ming{-}Wei Chang and\n               Kenton Lee and\n               Kristina Toutanova},\n  title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language\n               Understanding},\n  journal   = {CoRR},\n  volume    = {abs/1810.04805},\n  year      = {2018},\n  url       = {http://arxiv.org/abs/1810.04805},\n  archivePrefix = {arXiv},\n  eprint    = {1810.04805},\n  timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n```\n```\n@inproceedings{tjong-kim-sang-de-meulder-2003-introduction,\n    title = \"Introduction to the {C}o{NLL}-2003 Shared Task: Language-Independent Named Entity Recognition\",\n    author = \"Tjong Kim Sang, Erik F.  and\n      De Meulder, Fien\",\n    booktitle = \"Proceedings of the Seventh Conference on Natural Language Learning at {HLT}-{NAACL} 2003\",\n    year = \"2003\",\n    url = \"https://www.aclweb.org/anthology/W03-0419\",\n    pages = \"142--147\",\n}\n```\n",
    "library_name": "transformers"
  },
  {
    "model_id": "tner/roberta-large-ontonotes5",
    "model_name": "tner/roberta-large-ontonotes5",
    "author": "tner",
    "downloads": 153436,
    "likes": 16,
    "tags": [
      "transformers",
      "pytorch",
      "roberta",
      "token-classification",
      "dataset:tner/ontonotes5",
      "model-index",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/tner/roberta-large-ontonotes5",
    "dependencies": [
      [
        "tner",
        null
      ]
    ],
    "analysis_date": "2025-03-26T00:51:32.069222",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "roberta",
    "pipeline_tag": "token-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "datasets": [
        "tner/ontonotes5"
      ],
      "metrics": [
        "f1",
        "precision",
        "recall"
      ],
      "pipeline_tag": "token-classification",
      "widget": [
        {
          "text": "Jacob Collier is a Grammy awarded artist from England.",
          "example_title": "NER Example 1"
        }
      ],
      "model-index": [
        {
          "name": "tner/roberta-large-ontonotes5",
          "results": [
            {
              "task": {
                "type": "token-classification",
                "name": "Token Classification"
              },
              "dataset": {
                "name": "tner/ontonotes5",
                "type": "tner/ontonotes5",
                "args": "tner/ontonotes5"
              },
              "metrics": [
                {
                  "type": "f1",
                  "value": 0.908632361399938,
                  "name": "F1"
                },
                {
                  "type": "precision",
                  "value": 0.905148095909732,
                  "name": "Precision"
                },
                {
                  "type": "recall",
                  "value": 0.9121435551212579,
                  "name": "Recall"
                },
                {
                  "type": "f1_macro",
                  "value": 0.8265477704565624,
                  "name": "F1 (macro)"
                },
                {
                  "type": "precision_macro",
                  "value": 0.8170668848546687,
                  "name": "Precision (macro)"
                },
                {
                  "type": "recall_macro",
                  "value": 0.8387672780349001,
                  "name": "Recall (macro)"
                },
                {
                  "type": "f1_entity_span",
                  "value": 0.9284544931640193,
                  "name": "F1 (entity span)"
                },
                {
                  "type": "precision_entity_span",
                  "value": 0.9248942172073342,
                  "name": "Precision (entity span)"
                },
                {
                  "type": "recall_entity_span",
                  "value": 0.9320422848005685,
                  "name": "Recall (entity span)"
                }
              ]
            }
          ]
        }
      ]
    },
    "card_text": "# tner/roberta-large-ontonotes5\n\nThis model is a fine-tuned version of [roberta-large](https://huggingface.co/roberta-large) on the \n[tner/ontonotes5](https://huggingface.co/datasets/tner/ontonotes5) dataset.\nModel fine-tuning is done via [T-NER](https://github.com/asahi417/tner)'s hyper-parameter search (see the repository\nfor more detail). It achieves the following results on the test set:\n- F1 (micro): 0.908632361399938\n- Precision (micro): 0.905148095909732\n- Recall (micro): 0.9121435551212579\n- F1 (macro): 0.8265477704565624\n- Precision (macro): 0.8170668848546687\n- Recall (macro): 0.8387672780349001\n\nThe per-entity breakdown of the F1 score on the test set are below:\n- cardinal_number: 0.8605277329025309\n- date: 0.872996300863132\n- event: 0.7424242424242424\n- facility: 0.7732342007434945\n- geopolitical_area: 0.9687148323205043\n- group: 0.9470588235294117\n- language: 0.7499999999999999\n- law: 0.6666666666666666\n- location: 0.7593582887700535\n- money: 0.901098901098901\n- ordinal_number: 0.85785536159601\n- organization: 0.9227360841872057\n- percent: 0.9171428571428571\n- person: 0.9556004036326943\n- product: 0.7857142857142858\n- quantity: 0.7945205479452055\n- time: 0.6870588235294116\n- work_of_art: 0.7151515151515151 \n\nFor F1 scores, the confidence interval is obtained by bootstrap as below:\n- F1 (micro): \n    - 90%: [0.9039454247544766, 0.9128956119702822]\n    - 95%: [0.9030263216115454, 0.9138350859566045] \n- F1 (macro): \n    - 90%: [0.9039454247544766, 0.9128956119702822]\n    - 95%: [0.9030263216115454, 0.9138350859566045] \n\nFull evaluation can be found at [metric file of NER](https://huggingface.co/tner/roberta-large-ontonotes5/raw/main/eval/metric.json) \nand [metric file of entity span](https://huggingface.co/tner/roberta-large-ontonotes5/raw/main/eval/metric_span.json).\n\n### Usage\nThis model can be used through the [tner library](https://github.com/asahi417/tner). Install the library via pip   \n```shell\npip install tner\n```\nand activate model as below.\n```python\nfrom tner import TransformersNER\nmodel = TransformersNER(\"tner/roberta-large-ontonotes5\")\nmodel.predict([\"Jacob Collier is a Grammy awarded English artist from London\"])\n```\nIt can be used via transformers library but it is not recommended as CRF layer is not supported at the moment.\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n - dataset: ['tner/ontonotes5']\n - dataset_split: train\n - dataset_name: None\n - local_dataset: None\n - model: roberta-large\n - crf: True\n - max_length: 128\n - epoch: 15\n - batch_size: 64\n - lr: 1e-05\n - random_seed: 42\n - gradient_accumulation_steps: 1\n - weight_decay: None\n - lr_warmup_step_ratio: 0.1\n - max_grad_norm: 10.0\n\nThe full configuration can be found at [fine-tuning parameter file](https://huggingface.co/tner/roberta-large-ontonotes5/raw/main/trainer_config.json).\n\n### Reference\nIf you use any resource from T-NER, please consider to cite our [paper](https://aclanthology.org/2021.eacl-demos.7/).\n\n```\n\n@inproceedings{ushio-camacho-collados-2021-ner,\n    title = \"{T}-{NER}: An All-Round Python Library for Transformer-based Named Entity Recognition\",\n    author = \"Ushio, Asahi  and\n      Camacho-Collados, Jose\",\n    booktitle = \"Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations\",\n    month = apr,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.eacl-demos.7\",\n    doi = \"10.18653/v1/2021.eacl-demos.7\",\n    pages = \"53--62\",\n    abstract = \"Language model (LM) pretraining has led to consistent improvements in many NLP downstream tasks, including named entity recognition (NER). In this paper, we present T-NER (Transformer-based Named Entity Recognition), a Python library for NER LM finetuning. In addition to its practical utility, T-NER facilitates the study and investigation of the cross-domain and cross-lingual generalization ability of LMs finetuned on NER. Our library also provides a web app where users can get model predictions interactively for arbitrary text, which facilitates qualitative model evaluation for non-expert programmers. We show the potential of the library by compiling nine public NER datasets into a unified format and evaluating the cross-domain and cross- lingual performance across the datasets. The results from our initial experiments show that in-domain performance is generally competitive across datasets. However, cross-domain generalization is challenging even with a large pretrained LM, which has nevertheless capacity to learn domain-specific features if fine- tuned on a combined dataset. To facilitate future research, we also release all our LM checkpoints via the Hugging Face model hub.\",\n}\n\n```\n",
    "card_content": "---\ndatasets:\n- tner/ontonotes5\nmetrics:\n- f1\n- precision\n- recall\npipeline_tag: token-classification\nwidget:\n- text: Jacob Collier is a Grammy awarded artist from England.\n  example_title: NER Example 1\nmodel-index:\n- name: tner/roberta-large-ontonotes5\n  results:\n  - task:\n      type: token-classification\n      name: Token Classification\n    dataset:\n      name: tner/ontonotes5\n      type: tner/ontonotes5\n      args: tner/ontonotes5\n    metrics:\n    - type: f1\n      value: 0.908632361399938\n      name: F1\n    - type: precision\n      value: 0.905148095909732\n      name: Precision\n    - type: recall\n      value: 0.9121435551212579\n      name: Recall\n    - type: f1_macro\n      value: 0.8265477704565624\n      name: F1 (macro)\n    - type: precision_macro\n      value: 0.8170668848546687\n      name: Precision (macro)\n    - type: recall_macro\n      value: 0.8387672780349001\n      name: Recall (macro)\n    - type: f1_entity_span\n      value: 0.9284544931640193\n      name: F1 (entity span)\n    - type: precision_entity_span\n      value: 0.9248942172073342\n      name: Precision (entity span)\n    - type: recall_entity_span\n      value: 0.9320422848005685\n      name: Recall (entity span)\n---\n# tner/roberta-large-ontonotes5\n\nThis model is a fine-tuned version of [roberta-large](https://huggingface.co/roberta-large) on the \n[tner/ontonotes5](https://huggingface.co/datasets/tner/ontonotes5) dataset.\nModel fine-tuning is done via [T-NER](https://github.com/asahi417/tner)'s hyper-parameter search (see the repository\nfor more detail). It achieves the following results on the test set:\n- F1 (micro): 0.908632361399938\n- Precision (micro): 0.905148095909732\n- Recall (micro): 0.9121435551212579\n- F1 (macro): 0.8265477704565624\n- Precision (macro): 0.8170668848546687\n- Recall (macro): 0.8387672780349001\n\nThe per-entity breakdown of the F1 score on the test set are below:\n- cardinal_number: 0.8605277329025309\n- date: 0.872996300863132\n- event: 0.7424242424242424\n- facility: 0.7732342007434945\n- geopolitical_area: 0.9687148323205043\n- group: 0.9470588235294117\n- language: 0.7499999999999999\n- law: 0.6666666666666666\n- location: 0.7593582887700535\n- money: 0.901098901098901\n- ordinal_number: 0.85785536159601\n- organization: 0.9227360841872057\n- percent: 0.9171428571428571\n- person: 0.9556004036326943\n- product: 0.7857142857142858\n- quantity: 0.7945205479452055\n- time: 0.6870588235294116\n- work_of_art: 0.7151515151515151 \n\nFor F1 scores, the confidence interval is obtained by bootstrap as below:\n- F1 (micro): \n    - 90%: [0.9039454247544766, 0.9128956119702822]\n    - 95%: [0.9030263216115454, 0.9138350859566045] \n- F1 (macro): \n    - 90%: [0.9039454247544766, 0.9128956119702822]\n    - 95%: [0.9030263216115454, 0.9138350859566045] \n\nFull evaluation can be found at [metric file of NER](https://huggingface.co/tner/roberta-large-ontonotes5/raw/main/eval/metric.json) \nand [metric file of entity span](https://huggingface.co/tner/roberta-large-ontonotes5/raw/main/eval/metric_span.json).\n\n### Usage\nThis model can be used through the [tner library](https://github.com/asahi417/tner). Install the library via pip   \n```shell\npip install tner\n```\nand activate model as below.\n```python\nfrom tner import TransformersNER\nmodel = TransformersNER(\"tner/roberta-large-ontonotes5\")\nmodel.predict([\"Jacob Collier is a Grammy awarded English artist from London\"])\n```\nIt can be used via transformers library but it is not recommended as CRF layer is not supported at the moment.\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n - dataset: ['tner/ontonotes5']\n - dataset_split: train\n - dataset_name: None\n - local_dataset: None\n - model: roberta-large\n - crf: True\n - max_length: 128\n - epoch: 15\n - batch_size: 64\n - lr: 1e-05\n - random_seed: 42\n - gradient_accumulation_steps: 1\n - weight_decay: None\n - lr_warmup_step_ratio: 0.1\n - max_grad_norm: 10.0\n\nThe full configuration can be found at [fine-tuning parameter file](https://huggingface.co/tner/roberta-large-ontonotes5/raw/main/trainer_config.json).\n\n### Reference\nIf you use any resource from T-NER, please consider to cite our [paper](https://aclanthology.org/2021.eacl-demos.7/).\n\n```\n\n@inproceedings{ushio-camacho-collados-2021-ner,\n    title = \"{T}-{NER}: An All-Round Python Library for Transformer-based Named Entity Recognition\",\n    author = \"Ushio, Asahi  and\n      Camacho-Collados, Jose\",\n    booktitle = \"Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations\",\n    month = apr,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.eacl-demos.7\",\n    doi = \"10.18653/v1/2021.eacl-demos.7\",\n    pages = \"53--62\",\n    abstract = \"Language model (LM) pretraining has led to consistent improvements in many NLP downstream tasks, including named entity recognition (NER). In this paper, we present T-NER (Transformer-based Named Entity Recognition), a Python library for NER LM finetuning. In addition to its practical utility, T-NER facilitates the study and investigation of the cross-domain and cross-lingual generalization ability of LMs finetuned on NER. Our library also provides a web app where users can get model predictions interactively for arbitrary text, which facilitates qualitative model evaluation for non-expert programmers. We show the potential of the library by compiling nine public NER datasets into a unified format and evaluating the cross-domain and cross- lingual performance across the datasets. The results from our initial experiments show that in-domain performance is generally competitive across datasets. However, cross-domain generalization is challenging even with a large pretrained LM, which has nevertheless capacity to learn domain-specific features if fine- tuned on a combined dataset. To facilitate future research, we also release all our LM checkpoints via the Hugging Face model hub.\",\n}\n\n```\n",
    "library_name": "transformers"
  },
  {
    "model_id": "Angelakeke/RaTE-NER-Deberta",
    "model_name": "Angelakeke/RaTE-NER-Deberta",
    "author": "Angelakeke",
    "downloads": 139585,
    "likes": 6,
    "tags": [
      "transformers",
      "safetensors",
      "deberta-v2",
      "token-classification",
      "medical",
      "radiology",
      "en",
      "license:mit",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/Angelakeke/RaTE-NER-Deberta",
    "dependencies": [
      [
        "transformers",
        null
      ],
      [
        "torch",
        null
      ]
    ],
    "analysis_date": "2025-03-26T00:51:33.207467",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "deberta-v2",
    "pipeline_tag": "token-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": [
        "en"
      ],
      "license": "mit",
      "tags": [
        "medical",
        "radiology"
      ],
      "pipeline_tag": "token-classification",
      "widget": [
        {
          "text": "No suspicious focal mass lesion is seen in the left kidney.",
          "example_title": "Example in radiopaedia"
        }
      ],
      "model-index": [
        {
          "name": "rate-ner-rad",
          "results": []
        }
      ]
    },
    "card_text": "\n# RaTE-NER-Deberta\n\nThis model is a fine-tuned version of [DeBERTa](https://huggingface.co/microsoft/deberta-v3-base) on the [RaTE-NER](https://huggingface.co/datasets/Angelakeke/RaTE-NER/) dataset.\n\n## Model description\n\nThis model is trained to serve the RaTEScore metric, if you are interested in our pipeline, please refer to our [paper](https://aclanthology.org/2024.emnlp-main.836.pdf) and [Github](https://github.com/Angelakeke/RaTEScore).\n\nThis model also can be used to extract  **Abnormality, Non-Abnormality, Anatomy, Disease, Non-Disease**\nin medical radiology reports.\n\n## Usage\n\n<details>\n  <summary> Click to expand the usage of this model. </summary>\n<pre><code>\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\nimport torch\ndef post_process(tokenized_text, predicted_entities, tokenizer):\n    entity_spans = []\n    start = end = None\n    entity_type = None\n    for i, (token, label) in enumerate(zip(tokenized_text, predicted_entities[:len(tokenized_text)])):\n        if token in [\"[CLS]\", \"[SEP]\"]:\n            continue\n        if label != \"O\" and i < len(predicted_entities) - 1:\n            if label.startswith(\"B-\") and predicted_entities[i+1].startswith(\"I-\"):\n                start = i\n                entity_type = label[2:]\n            elif label.startswith(\"B-\") and predicted_entities[i+1].startswith(\"B-\"):\n                start = i\n                end = i\n                entity_spans.append((start, end, label[2:]))\n                start = i\n                entity_type = label[2:]\n            elif label.startswith(\"B-\") and predicted_entities[i+1].startswith(\"O\"):\n                start = i\n                end = i\n                entity_spans.append((start, end, label[2:]))\n                start = end = None\n                entity_type = None\n            elif label.startswith(\"I-\") and predicted_entities[i+1].startswith(\"B-\"):\n                end = i\n                if start is not None:\n                    entity_spans.append((start, end, entity_type))\n                start = i\n                entity_type = label[2:]\n            elif label.startswith(\"I-\") and predicted_entities[i+1].startswith(\"O\"):\n                end = i\n                if start is not None:\n                    entity_spans.append((start, end, entity_type))\n                start = end = None\n                entity_type = None\n    if start is not None and end is None:\n        end = len(tokenized_text) - 2\n        entity_spans.append((start, end, entity_type))\n    save_pair = []\n    for start, end, entity_type in entity_spans:\n        entity_str = tokenizer.convert_tokens_to_string(tokenized_text[start:end+1])\n        save_pair.append((entity_str, entity_type))\n    return save_pair\n\ndef run_ner(texts, idx2label, tokenizer, model, device):\n    inputs = tokenizer(texts, \n                    max_length=512,\n                    padding=True, \n                    truncation=True, \n                    return_tensors=\"pt\").to(device)\n    with torch.no_grad():\n        outputs = model(**inputs)\n    predicted_labels = torch.argmax(outputs.logits, dim=2).tolist()\n    save_pairs = []\n    for i in range(len(texts)):\n        predicted_entities = [idx2label[label] for label in predicted_labels[i]]\n        non_pad_mask = inputs[\"input_ids\"][i] != tokenizer.pad_token_id\n        non_pad_length = non_pad_mask.sum().item()\n        non_pad_input_ids = inputs[\"input_ids\"][i][:non_pad_length]\n        tokenized_text = tokenizer.convert_ids_to_tokens(non_pad_input_ids)\n        save_pair = post_process(tokenized_text, predicted_entities, tokenizer)\n        if i == 0:\n            save_pairs = save_pair\n        else:\n            save_pairs.extend(save_pair)\n    return save_pairs\n\nner_labels = ['B-ABNORMALITY', 'I-ABNORMALITY', \n              'B-NON-ABNORMALITY', 'I-NON-ABNORMALITY', \n              'B-DISEASE', 'I-DISEASE', \n              'B-NON-DISEASE', 'I-NON-DISEASE', \n              'B-ANATOMY', 'I-ANATOMY', \n              'O']\nidx2label = {i: label for i, label in enumerate(ner_labels)}\n\ntokenizer = AutoTokenizer.from_pretrained('Angelakeke/RaTE-NER-Deberta')\nmodel = AutoModelForTokenClassification.from_pretrained('Angelakeke/RaTE-NER-Deberta')\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\nmodel.eval()\n\nWe recommend to inference by sentences.\n\ntext = \"\"\n\ntexts = text.split('. ')\nsave_pair = run_ner(texts, idx2label, tokenizer, model, device)\n\n </code></pre>\n\n</details>\n\n\n## Author\n\nAuthor: [Weike Zhao](https://angelakeke.github.io/)\n\nIf you have any questions, please feel free to contact zwk0629@sjtu.edu.cn.\n\n## Citation\n```bibtex\n@inproceedings{zhao2024ratescore,\n  title={RaTEScore: A Metric for Radiology Report Generation},\n  author={Zhao, Weike and Wu, Chaoyi and Zhang, Xiaoman and Zhang, Ya and Wang, Yanfeng and Xie, Weidi},\n  booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},\n  pages={15004--15019},\n  year={2024}\n}\n```\n",
    "card_content": "---\nlanguage:\n- en\nlicense: mit\ntags:\n- medical\n- radiology\npipeline_tag: token-classification\nwidget:\n- text: No suspicious focal mass lesion is seen in the left kidney.\n  example_title: Example in radiopaedia\nmodel-index:\n- name: rate-ner-rad\n  results: []\n---\n\n# RaTE-NER-Deberta\n\nThis model is a fine-tuned version of [DeBERTa](https://huggingface.co/microsoft/deberta-v3-base) on the [RaTE-NER](https://huggingface.co/datasets/Angelakeke/RaTE-NER/) dataset.\n\n## Model description\n\nThis model is trained to serve the RaTEScore metric, if you are interested in our pipeline, please refer to our [paper](https://aclanthology.org/2024.emnlp-main.836.pdf) and [Github](https://github.com/Angelakeke/RaTEScore).\n\nThis model also can be used to extract  **Abnormality, Non-Abnormality, Anatomy, Disease, Non-Disease**\nin medical radiology reports.\n\n## Usage\n\n<details>\n  <summary> Click to expand the usage of this model. </summary>\n<pre><code>\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\nimport torch\ndef post_process(tokenized_text, predicted_entities, tokenizer):\n    entity_spans = []\n    start = end = None\n    entity_type = None\n    for i, (token, label) in enumerate(zip(tokenized_text, predicted_entities[:len(tokenized_text)])):\n        if token in [\"[CLS]\", \"[SEP]\"]:\n            continue\n        if label != \"O\" and i < len(predicted_entities) - 1:\n            if label.startswith(\"B-\") and predicted_entities[i+1].startswith(\"I-\"):\n                start = i\n                entity_type = label[2:]\n            elif label.startswith(\"B-\") and predicted_entities[i+1].startswith(\"B-\"):\n                start = i\n                end = i\n                entity_spans.append((start, end, label[2:]))\n                start = i\n                entity_type = label[2:]\n            elif label.startswith(\"B-\") and predicted_entities[i+1].startswith(\"O\"):\n                start = i\n                end = i\n                entity_spans.append((start, end, label[2:]))\n                start = end = None\n                entity_type = None\n            elif label.startswith(\"I-\") and predicted_entities[i+1].startswith(\"B-\"):\n                end = i\n                if start is not None:\n                    entity_spans.append((start, end, entity_type))\n                start = i\n                entity_type = label[2:]\n            elif label.startswith(\"I-\") and predicted_entities[i+1].startswith(\"O\"):\n                end = i\n                if start is not None:\n                    entity_spans.append((start, end, entity_type))\n                start = end = None\n                entity_type = None\n    if start is not None and end is None:\n        end = len(tokenized_text) - 2\n        entity_spans.append((start, end, entity_type))\n    save_pair = []\n    for start, end, entity_type in entity_spans:\n        entity_str = tokenizer.convert_tokens_to_string(tokenized_text[start:end+1])\n        save_pair.append((entity_str, entity_type))\n    return save_pair\n\ndef run_ner(texts, idx2label, tokenizer, model, device):\n    inputs = tokenizer(texts, \n                    max_length=512,\n                    padding=True, \n                    truncation=True, \n                    return_tensors=\"pt\").to(device)\n    with torch.no_grad():\n        outputs = model(**inputs)\n    predicted_labels = torch.argmax(outputs.logits, dim=2).tolist()\n    save_pairs = []\n    for i in range(len(texts)):\n        predicted_entities = [idx2label[label] for label in predicted_labels[i]]\n        non_pad_mask = inputs[\"input_ids\"][i] != tokenizer.pad_token_id\n        non_pad_length = non_pad_mask.sum().item()\n        non_pad_input_ids = inputs[\"input_ids\"][i][:non_pad_length]\n        tokenized_text = tokenizer.convert_ids_to_tokens(non_pad_input_ids)\n        save_pair = post_process(tokenized_text, predicted_entities, tokenizer)\n        if i == 0:\n            save_pairs = save_pair\n        else:\n            save_pairs.extend(save_pair)\n    return save_pairs\n\nner_labels = ['B-ABNORMALITY', 'I-ABNORMALITY', \n              'B-NON-ABNORMALITY', 'I-NON-ABNORMALITY', \n              'B-DISEASE', 'I-DISEASE', \n              'B-NON-DISEASE', 'I-NON-DISEASE', \n              'B-ANATOMY', 'I-ANATOMY', \n              'O']\nidx2label = {i: label for i, label in enumerate(ner_labels)}\n\ntokenizer = AutoTokenizer.from_pretrained('Angelakeke/RaTE-NER-Deberta')\nmodel = AutoModelForTokenClassification.from_pretrained('Angelakeke/RaTE-NER-Deberta')\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\nmodel.eval()\n\nWe recommend to inference by sentences.\n\ntext = \"\"\n\ntexts = text.split('. ')\nsave_pair = run_ner(texts, idx2label, tokenizer, model, device)\n\n </code></pre>\n\n</details>\n\n\n## Author\n\nAuthor: [Weike Zhao](https://angelakeke.github.io/)\n\nIf you have any questions, please feel free to contact zwk0629@sjtu.edu.cn.\n\n## Citation\n```bibtex\n@inproceedings{zhao2024ratescore,\n  title={RaTEScore: A Metric for Radiology Report Generation},\n  author={Zhao, Weike and Wu, Chaoyi and Zhang, Xiaoman and Zhang, Ya and Wang, Yanfeng and Xie, Weidi},\n  booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},\n  pages={15004--15019},\n  year={2024}\n}\n```\n",
    "library_name": "transformers"
  },
  {
    "model_id": "ckiplab/bert-base-chinese-pos",
    "model_name": "ckiplab/bert-base-chinese-pos",
    "author": "ckiplab",
    "downloads": 129049,
    "likes": 16,
    "tags": [
      "transformers",
      "pytorch",
      "jax",
      "bert",
      "token-classification",
      "zh",
      "license:gpl-3.0",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/ckiplab/bert-base-chinese-pos",
    "dependencies": [
      [
        "transformers",
        null
      ]
    ],
    "analysis_date": "2025-03-26T00:51:34.040205",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "bert",
    "pipeline_tag": "token-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": [
        "zh"
      ],
      "license": "gpl-3.0",
      "tags": [
        "pytorch",
        "token-classification",
        "bert",
        "zh"
      ],
      "thumbnail": "https://ckip.iis.sinica.edu.tw/files/ckip_logo.png"
    },
    "card_text": "\n# CKIP BERT Base Chinese\n\nThis project provides traditional Chinese transformers models (including ALBERT, BERT, GPT2) and NLP tools (including word segmentation, part-of-speech tagging, named entity recognition).\n\n這個專案提供了繁體中文的 transformers 模型（包含 ALBERT、BERT、GPT2）及自然語言處理工具（包含斷詞、詞性標記、實體辨識）。\n\n## Homepage\n\n- https://github.com/ckiplab/ckip-transformers\n\n## Contributers\n\n- [Mu Yang](https://muyang.pro) at [CKIP](https://ckip.iis.sinica.edu.tw) (Author & Maintainer)\n\n## Usage\n\nPlease use BertTokenizerFast as tokenizer instead of AutoTokenizer.\n\n請使用 BertTokenizerFast 而非 AutoTokenizer。\n\n```\nfrom transformers import (\n  BertTokenizerFast,\n  AutoModel,\n)\n\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')\nmodel = AutoModel.from_pretrained('ckiplab/bert-base-chinese-pos')\n```\n\nFor full usage and more information, please refer to https://github.com/ckiplab/ckip-transformers.\n\n有關完整使用方法及其他資訊，請參見 https://github.com/ckiplab/ckip-transformers 。\n",
    "card_content": "---\nlanguage:\n- zh\nlicense: gpl-3.0\ntags:\n- pytorch\n- token-classification\n- bert\n- zh\nthumbnail: https://ckip.iis.sinica.edu.tw/files/ckip_logo.png\n---\n\n# CKIP BERT Base Chinese\n\nThis project provides traditional Chinese transformers models (including ALBERT, BERT, GPT2) and NLP tools (including word segmentation, part-of-speech tagging, named entity recognition).\n\n這個專案提供了繁體中文的 transformers 模型（包含 ALBERT、BERT、GPT2）及自然語言處理工具（包含斷詞、詞性標記、實體辨識）。\n\n## Homepage\n\n- https://github.com/ckiplab/ckip-transformers\n\n## Contributers\n\n- [Mu Yang](https://muyang.pro) at [CKIP](https://ckip.iis.sinica.edu.tw) (Author & Maintainer)\n\n## Usage\n\nPlease use BertTokenizerFast as tokenizer instead of AutoTokenizer.\n\n請使用 BertTokenizerFast 而非 AutoTokenizer。\n\n```\nfrom transformers import (\n  BertTokenizerFast,\n  AutoModel,\n)\n\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')\nmodel = AutoModel.from_pretrained('ckiplab/bert-base-chinese-pos')\n```\n\nFor full usage and more information, please refer to https://github.com/ckiplab/ckip-transformers.\n\n有關完整使用方法及其他資訊，請參見 https://github.com/ckiplab/ckip-transformers 。\n",
    "library_name": "transformers"
  },
  {
    "model_id": "globis-university/deberta-v3-japanese-large",
    "model_name": "globis-university/deberta-v3-japanese-large",
    "author": "globis-university",
    "downloads": 119232,
    "likes": 2,
    "tags": [
      "transformers",
      "pytorch",
      "deberta-v2",
      "token-classification",
      "ja",
      "dataset:globis-university/aozorabunko-clean",
      "dataset:oscar-corpus/OSCAR-2301",
      "dataset:Wikipedia",
      "dataset:WikiBooks",
      "dataset:CC-100",
      "dataset:allenai/c4",
      "arxiv:2302.03169",
      "license:cc-by-sa-4.0",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/globis-university/deberta-v3-japanese-large",
    "dependencies": [
      [
        "transformers",
        null
      ]
    ],
    "analysis_date": "2025-03-26T00:51:35.743198",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "deberta-v2",
    "pipeline_tag": "token-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": [
        "ja"
      ],
      "license": "cc-by-sa-4.0",
      "library_name": "transformers",
      "datasets": [
        "globis-university/aozorabunko-clean",
        "oscar-corpus/OSCAR-2301",
        "Wikipedia",
        "WikiBooks",
        "CC-100",
        "allenai/c4"
      ]
    },
    "card_text": "\n# What’s this?\n日本語リソースで学習した [DeBERTa V3](https://huggingface.co/microsoft/deberta-v3-large) モデルです。\n\n以下のような特徴を持ちます:\n\n- 定評のある [DeBERTa V3](https://huggingface.co/microsoft/deberta-v3-large) を用いたモデル\n- 日本語特化\n- 推論時に形態素解析器を用いない\n- 単語境界をある程度尊重する (`の都合上` や `の判定負けを喫し` のような複数語のトークンを生じさせない)\n\n---\nThis is a model based on [DeBERTa V3](https://huggingface.co/microsoft/deberta-v3-large) pre-trained on Japanese resources.\n\nThe model has the following features:\n- Based on the well-known [DeBERTa V3](https://huggingface.co/microsoft/deberta-v3-large) model\n- Specialized for the Japanese language\n- Does not use a morphological analyzer during inference\n- Respects word boundaries to some extent (does not produce tokens spanning multiple words like `の都合上` or `の判定負けを喫し`)\n\n# How to use\n```python\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\n\nmodel_name = 'globis-university/deberta-v3-japanese-large'\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForTokenClassification.from_pretrained(model_name)\n```\n\n# Tokenizer\n[工藤氏によって示された手法](https://qiita.com/taku910/items/fbaeab4684665952d5a9)で学習しました。\n\n以下のことを意識しています:\n\n- 推論時の形態素解析器なし\n- トークンが単語の境界を跨がない (辞書: `unidic-cwj-202302`)\n- Hugging Faceで使いやすい\n- 大きすぎない語彙数\n\n本家の DeBERTa V3 は大きな語彙数で学習されていることに特徴がありますが、反面埋め込み層のパラメータ数が大きくなりすぎる ([microsoft/deberta-v3-base](https://huggingface.co/microsoft/deberta-v3-base) モデルの場合で埋め込み層が全体の 54%) ことから、本モデルでは小さめの語彙数を採用しています。\n\n注意点として、 `xsmall` 、 `base` 、 `large` の 3 つのモデルのうち、前者二つは unigram アルゴリズムで学習しているが、 `large` モデルのみ BPE アルゴリズムで学習している。\n深い理由はなく、  `large` モデルのみ語彙サイズを増やすために独立して学習を行ったが、なぜか unigram アルゴリズムでの学習がうまくいかなかったことが原因である。\n原因の探究よりモデルの完成を優先して、 BPE アルゴリズムに切り替えた。\n\n---\nThe tokenizer is trained using [the method introduced by Kudo](https://qiita.com/taku910/items/fbaeab4684665952d5a9).\n\nKey points include:\n- No morphological analyzer needed during inference\n- Tokens do not cross word boundaries (dictionary: `unidic-cwj-202302`)\n- Easy to use with Hugging Face\n- Smaller vocabulary size\n\nAlthough the original DeBERTa V3 is characterized by a large vocabulary size, which can result in a significant increase in the number of parameters in the embedding layer (for the [microsoft/deberta-v3-base](https://huggingface.co/microsoft/deberta-v3-base) model, the embedding layer accounts for 54% of the total), this model adopts a smaller vocabulary size to address this.\n\nNote that, among the three models: xsmall, base, and large, the first two were trained using the unigram algorithm, while only the large model was trained using the BPE algorithm.\nThe reason for this is simple: while the large model was independently trained to increase its vocabulary size, for some reason, training with the unigram algorithm was not successful.\nThus, prioritizing the completion of the model over investigating the cause, we switched to the BPE algorithm.\n\n# Data\n| Dataset Name  | Notes | File Size (with metadata) | Factor |\n| ------------- | ----- | ------------------------- | ---------- |\n| Wikipedia     | 2023/07; [WikiExtractor](https://github.com/attardi/wikiextractor) | 3.5GB | x2 |\n| Wikipedia     | 2023/07; [cl-tohoku's method](https://github.com/cl-tohoku/bert-japanese/blob/main/make_corpus_wiki.py) | 4.8GB | x2 |\n| WikiBooks     | 2023/07; [cl-tohoku's method](https://github.com/cl-tohoku/bert-japanese/blob/main/make_corpus_wiki.py) | 43MB | x2 |\n| Aozora Bunko  | 2023/07; [globis-university/aozorabunko-clean](https://huggingface.co/globis-university/globis-university/aozorabunko-clean) | 496MB | x4 |\n| CC-100        | ja | 90GB | x1 |\n| mC4           | ja; extracted 10%, with Wikipedia-like focus via [DSIR](https://arxiv.org/abs/2302.03169) | 91GB | x1 |\n| OSCAR 2023    | ja; extracted 10%, with Wikipedia-like focus via [DSIR](https://arxiv.org/abs/2302.03169) | 26GB | x1 |\n\n# Training parameters\n- Number of devices: 8\n- Batch size: 8 x 8\n- Learning rate: 6.4e-5\n- Maximum sequence length: 512\n- Optimizer: AdamW\n- Learning rate scheduler: Linear schedule with warmup\n- Training steps: 2,000,000\n- Warmup steps: 100,000\n- Precision: Mixed (fp16)\n- Vocabulary size: 48,000\n\n# Evaluation\n| Model | #params | JSTS | JNLI | JSQuAD | JCQA |\n| ----- | ------- | ---- | ---- | ------ | ---- |\n| ≤ small | | | | | |\n| [izumi-lab/deberta-v2-small-japanese](https://huggingface.co/izumi-lab/deberta-v2-small-japanese) | 17.8M | 0.890/0.846 | 0.880 | - | 0.737 |\n| [globis-university/deberta-v3-japanese-xsmall](https://huggingface.co/globis-university/deberta-v3-japanese-xsmall) | 33.7M | **0.916**/**0.880** | **0.913** | **0.869**/**0.938** | **0.821** |\n| base | | | | |\n| [cl-tohoku/bert-base-japanese-v3](https://huggingface.co/cl-tohoku/bert-base-japanese-v3) | 111M | 0.919/0.881 | 0.907 | 0.880/0.946 | 0.848 |\n| [nlp-waseda/roberta-base-japanese](https://huggingface.co/nlp-waseda/roberta-base-japanese) | 111M | 0.913/0.873 | 0.895 | 0.864/0.927 | 0.840 |\n| [izumi-lab/deberta-v2-base-japanese](https://huggingface.co/izumi-lab/deberta-v2-base-japanese) | 110M | 0.919/0.882 | 0.912 | - | 0.859 |\n| [ku-nlp/deberta-v2-base-japanese](https://huggingface.co/ku-nlp/deberta-v2-base-japanese) | 112M | 0.922/0.886 | 0.922 | **0.899**/**0.951** | - |\n| [ku-nlp/deberta-v3-base-japanese](https://huggingface.co/ku-nlp/deberta-v3-base-japanese) | 160M | **0.927**/0.891 | **0.927** | 0.896/- | - |\n| [globis-university/deberta-v3-japanese-base](https://huggingface.co/globis-university/deberta-v3-japanese-base) | 110M | 0.925/**0.895** | 0.921 | 0.890/0.950 | **0.886** |\n| large | | | | | |\n| [cl-tohoku/bert-large-japanese-v2](https://huggingface.co/cl-tohoku/bert-large-japanese-v2) | 337M | 0.926/0.893 | **0.929** | 0.893/0.956 | 0.893 |\n| [nlp-waseda/roberta-large-japanese](https://huggingface.co/nlp-waseda/roberta-large-japanese) | 337M | **0.930**/**0.896** | 0.924 | 0.884/0.940 | **0.907** |\n| [nlp-waseda/roberta-large-japanese-seq512](https://huggingface.co/nlp-waseda/roberta-large-japanese-seq512) | 337M | 0.926/0.892 | 0.926 | **0.918**/**0.963** | 0.891 |\n| [ku-nlp/deberta-v2-large-japanese](https://huggingface.co/ku-nlp/deberta-v2-large-japanese) | 339M | 0.925/0.892 | 0.924 | 0.912/0.959 | - |\n| [**globis-university/deberta-v3-japanese-large**](https://huggingface.co/globis-university/deberta-v3-japanese-large) | 352M | 0.928/**0.896** | 0.924 | 0.896/0.956 | 0.900 |\n\n## License\nCC BY SA 4.0\n\n## Acknowledgement\n計算リソースに [ABCI](https://abci.ai/) を利用させていただきました。ありがとうございます。\n\n---\nWe used [ABCI](https://abci.ai/) for computing resources. Thank you.",
    "card_content": "---\nlanguage:\n- ja\nlicense: cc-by-sa-4.0\nlibrary_name: transformers\ndatasets:\n- globis-university/aozorabunko-clean\n- oscar-corpus/OSCAR-2301\n- Wikipedia\n- WikiBooks\n- CC-100\n- allenai/c4\n---\n\n# What’s this?\n日本語リソースで学習した [DeBERTa V3](https://huggingface.co/microsoft/deberta-v3-large) モデルです。\n\n以下のような特徴を持ちます:\n\n- 定評のある [DeBERTa V3](https://huggingface.co/microsoft/deberta-v3-large) を用いたモデル\n- 日本語特化\n- 推論時に形態素解析器を用いない\n- 単語境界をある程度尊重する (`の都合上` や `の判定負けを喫し` のような複数語のトークンを生じさせない)\n\n---\nThis is a model based on [DeBERTa V3](https://huggingface.co/microsoft/deberta-v3-large) pre-trained on Japanese resources.\n\nThe model has the following features:\n- Based on the well-known [DeBERTa V3](https://huggingface.co/microsoft/deberta-v3-large) model\n- Specialized for the Japanese language\n- Does not use a morphological analyzer during inference\n- Respects word boundaries to some extent (does not produce tokens spanning multiple words like `の都合上` or `の判定負けを喫し`)\n\n# How to use\n```python\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\n\nmodel_name = 'globis-university/deberta-v3-japanese-large'\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForTokenClassification.from_pretrained(model_name)\n```\n\n# Tokenizer\n[工藤氏によって示された手法](https://qiita.com/taku910/items/fbaeab4684665952d5a9)で学習しました。\n\n以下のことを意識しています:\n\n- 推論時の形態素解析器なし\n- トークンが単語の境界を跨がない (辞書: `unidic-cwj-202302`)\n- Hugging Faceで使いやすい\n- 大きすぎない語彙数\n\n本家の DeBERTa V3 は大きな語彙数で学習されていることに特徴がありますが、反面埋め込み層のパラメータ数が大きくなりすぎる ([microsoft/deberta-v3-base](https://huggingface.co/microsoft/deberta-v3-base) モデルの場合で埋め込み層が全体の 54%) ことから、本モデルでは小さめの語彙数を採用しています。\n\n注意点として、 `xsmall` 、 `base` 、 `large` の 3 つのモデルのうち、前者二つは unigram アルゴリズムで学習しているが、 `large` モデルのみ BPE アルゴリズムで学習している。\n深い理由はなく、  `large` モデルのみ語彙サイズを増やすために独立して学習を行ったが、なぜか unigram アルゴリズムでの学習がうまくいかなかったことが原因である。\n原因の探究よりモデルの完成を優先して、 BPE アルゴリズムに切り替えた。\n\n---\nThe tokenizer is trained using [the method introduced by Kudo](https://qiita.com/taku910/items/fbaeab4684665952d5a9).\n\nKey points include:\n- No morphological analyzer needed during inference\n- Tokens do not cross word boundaries (dictionary: `unidic-cwj-202302`)\n- Easy to use with Hugging Face\n- Smaller vocabulary size\n\nAlthough the original DeBERTa V3 is characterized by a large vocabulary size, which can result in a significant increase in the number of parameters in the embedding layer (for the [microsoft/deberta-v3-base](https://huggingface.co/microsoft/deberta-v3-base) model, the embedding layer accounts for 54% of the total), this model adopts a smaller vocabulary size to address this.\n\nNote that, among the three models: xsmall, base, and large, the first two were trained using the unigram algorithm, while only the large model was trained using the BPE algorithm.\nThe reason for this is simple: while the large model was independently trained to increase its vocabulary size, for some reason, training with the unigram algorithm was not successful.\nThus, prioritizing the completion of the model over investigating the cause, we switched to the BPE algorithm.\n\n# Data\n| Dataset Name  | Notes | File Size (with metadata) | Factor |\n| ------------- | ----- | ------------------------- | ---------- |\n| Wikipedia     | 2023/07; [WikiExtractor](https://github.com/attardi/wikiextractor) | 3.5GB | x2 |\n| Wikipedia     | 2023/07; [cl-tohoku's method](https://github.com/cl-tohoku/bert-japanese/blob/main/make_corpus_wiki.py) | 4.8GB | x2 |\n| WikiBooks     | 2023/07; [cl-tohoku's method](https://github.com/cl-tohoku/bert-japanese/blob/main/make_corpus_wiki.py) | 43MB | x2 |\n| Aozora Bunko  | 2023/07; [globis-university/aozorabunko-clean](https://huggingface.co/globis-university/globis-university/aozorabunko-clean) | 496MB | x4 |\n| CC-100        | ja | 90GB | x1 |\n| mC4           | ja; extracted 10%, with Wikipedia-like focus via [DSIR](https://arxiv.org/abs/2302.03169) | 91GB | x1 |\n| OSCAR 2023    | ja; extracted 10%, with Wikipedia-like focus via [DSIR](https://arxiv.org/abs/2302.03169) | 26GB | x1 |\n\n# Training parameters\n- Number of devices: 8\n- Batch size: 8 x 8\n- Learning rate: 6.4e-5\n- Maximum sequence length: 512\n- Optimizer: AdamW\n- Learning rate scheduler: Linear schedule with warmup\n- Training steps: 2,000,000\n- Warmup steps: 100,000\n- Precision: Mixed (fp16)\n- Vocabulary size: 48,000\n\n# Evaluation\n| Model | #params | JSTS | JNLI | JSQuAD | JCQA |\n| ----- | ------- | ---- | ---- | ------ | ---- |\n| ≤ small | | | | | |\n| [izumi-lab/deberta-v2-small-japanese](https://huggingface.co/izumi-lab/deberta-v2-small-japanese) | 17.8M | 0.890/0.846 | 0.880 | - | 0.737 |\n| [globis-university/deberta-v3-japanese-xsmall](https://huggingface.co/globis-university/deberta-v3-japanese-xsmall) | 33.7M | **0.916**/**0.880** | **0.913** | **0.869**/**0.938** | **0.821** |\n| base | | | | |\n| [cl-tohoku/bert-base-japanese-v3](https://huggingface.co/cl-tohoku/bert-base-japanese-v3) | 111M | 0.919/0.881 | 0.907 | 0.880/0.946 | 0.848 |\n| [nlp-waseda/roberta-base-japanese](https://huggingface.co/nlp-waseda/roberta-base-japanese) | 111M | 0.913/0.873 | 0.895 | 0.864/0.927 | 0.840 |\n| [izumi-lab/deberta-v2-base-japanese](https://huggingface.co/izumi-lab/deberta-v2-base-japanese) | 110M | 0.919/0.882 | 0.912 | - | 0.859 |\n| [ku-nlp/deberta-v2-base-japanese](https://huggingface.co/ku-nlp/deberta-v2-base-japanese) | 112M | 0.922/0.886 | 0.922 | **0.899**/**0.951** | - |\n| [ku-nlp/deberta-v3-base-japanese](https://huggingface.co/ku-nlp/deberta-v3-base-japanese) | 160M | **0.927**/0.891 | **0.927** | 0.896/- | - |\n| [globis-university/deberta-v3-japanese-base](https://huggingface.co/globis-university/deberta-v3-japanese-base) | 110M | 0.925/**0.895** | 0.921 | 0.890/0.950 | **0.886** |\n| large | | | | | |\n| [cl-tohoku/bert-large-japanese-v2](https://huggingface.co/cl-tohoku/bert-large-japanese-v2) | 337M | 0.926/0.893 | **0.929** | 0.893/0.956 | 0.893 |\n| [nlp-waseda/roberta-large-japanese](https://huggingface.co/nlp-waseda/roberta-large-japanese) | 337M | **0.930**/**0.896** | 0.924 | 0.884/0.940 | **0.907** |\n| [nlp-waseda/roberta-large-japanese-seq512](https://huggingface.co/nlp-waseda/roberta-large-japanese-seq512) | 337M | 0.926/0.892 | 0.926 | **0.918**/**0.963** | 0.891 |\n| [ku-nlp/deberta-v2-large-japanese](https://huggingface.co/ku-nlp/deberta-v2-large-japanese) | 339M | 0.925/0.892 | 0.924 | 0.912/0.959 | - |\n| [**globis-university/deberta-v3-japanese-large**](https://huggingface.co/globis-university/deberta-v3-japanese-large) | 352M | 0.928/**0.896** | 0.924 | 0.896/0.956 | 0.900 |\n\n## License\nCC BY SA 4.0\n\n## Acknowledgement\n計算リソースに [ABCI](https://abci.ai/) を利用させていただきました。ありがとうございます。\n\n---\nWe used [ABCI](https://abci.ai/) for computing resources. Thank you.",
    "library_name": "transformers"
  },
  {
    "model_id": "yanekyuk/camembert-keyword-extractor",
    "model_name": "yanekyuk/camembert-keyword-extractor",
    "author": "yanekyuk",
    "downloads": 114394,
    "likes": 13,
    "tags": [
      "transformers",
      "pytorch",
      "camembert",
      "token-classification",
      "generated_from_trainer",
      "fr",
      "license:mit",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/yanekyuk/camembert-keyword-extractor",
    "dependencies": [
      [
        "transformers",
        "4.19.2"
      ],
      [
        "torch",
        "1.11.0"
      ],
      [
        "datasets",
        "2.2.2"
      ],
      [
        "tokenizers",
        "0.12.1"
      ]
    ],
    "analysis_date": "2025-03-26T00:51:37.144217",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "camembert",
    "pipeline_tag": "token-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": [
        "fr"
      ],
      "license": "mit",
      "tags": [
        "generated_from_trainer"
      ],
      "metrics": [
        "precision",
        "recall",
        "accuracy",
        "f1"
      ],
      "widget": [
        {
          "text": "Le président de la République appelle en outre les Français à faire le choix d'une \"majorité stable et sérieuse pour les protéger face aux crises et pour agir pour l'avenir\". \"Je vois dans le projet de Jean-Luc Mélenchon ou de Madame Le Pen un projet de désordre et de soumission. Ils expliquent qu'il faut sortir de nos alliances, de l'Europe, et bâtir des alliances stratégiques avec la Russie. C'est la soumission à la Russie\", assure-t-il."
        },
        {
          "text": "Top départ à l’ouverture des bureaux de vote. La Polynésie et les Français résidant à l'étranger, dont certains ont déjà pu voter en ligne, sont invités aux urnes ce week-end pour le premier tour des législatives, samedi 4 juin pour le continent américain et les Caraïbes, et dimanche 5 juin pour le reste du monde. En France métropolitaine, les premier et second tours auront lieu les 12 et 19 juin."
        },
        {
          "text": "Le ministère a aussi indiqué que des missiles russes ont frappé un centre d'entraînement d'artillerie dans la région de Soumy où travaillaient des instructeurs étrangers. Il a jouté qu'une autre frappe avait détruit une position de \"mercenaires étrangers\" dans la région d'Odessa."
        },
        {
          "text": "Le malaise est profond et ressemble à une crise existentielle. Fait rarissime au Quai d’Orsay, six syndicats et un collectif de 500 jeunes diplomates du ministère des Affaires étrangères ont appelé à la grève, jeudi 2 juin, pour protester contre la réforme de la haute fonction publique qui, à terme, entraînera la disparition des deux corps historiques de la diplomatie française : celui de ministre plénipotentiaire (ambassadeur) et celui de conseiller des affaires étrangères."
        },
        {
          "text": "Ils se font passer pour des recruteurs de Lockheed Martin ou du géant britannique de la défense et de l’aérospatial BAE Systems. Ces soi-disant chasseurs de tête font miroiter des perspectives lucratives de carrière et des postes à responsabilité. Mais ce n’est que du vent. En réalité, il s’agit de cyberespions nord-coréens cherchant à voler des secrets industriels de groupes de défense ou du secteur de l’aérospatial, révèle Eset, une société slovaque de sécurité informatique, dans un rapport publié mardi 31 mai."
        }
      ],
      "model-index": [
        {
          "name": "camembert-keyword-extractor",
          "results": []
        }
      ]
    },
    "card_text": "\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# camembert-keyword-extractor\n\nThis model is a fine-tuned version of [camembert-base](https://huggingface.co/camembert-base) on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.2199\n- Precision: 0.6743\n- Recall: 0.6979\n- Accuracy: 0.9346\n- F1: 0.6859\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 16\n- eval_batch_size: 16\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 8\n- mixed_precision_training: Native AMP\n\n### Training results\n\n| Training Loss | Epoch | Step  | Validation Loss | Precision | Recall | Accuracy | F1     |\n|:-------------:|:-----:|:-----:|:---------------:|:---------:|:------:|:--------:|:------:|\n| 0.1747        | 1.0   | 1875  | 0.1780          | 0.5935    | 0.7116 | 0.9258   | 0.6472 |\n| 0.1375        | 2.0   | 3750  | 0.1588          | 0.6505    | 0.7032 | 0.9334   | 0.6759 |\n| 0.1147        | 3.0   | 5625  | 0.1727          | 0.6825    | 0.6689 | 0.9355   | 0.6756 |\n| 0.0969        | 4.0   | 7500  | 0.1759          | 0.6886    | 0.6621 | 0.9350   | 0.6751 |\n| 0.0837        | 5.0   | 9375  | 0.1967          | 0.6688    | 0.7112 | 0.9348   | 0.6893 |\n| 0.0746        | 6.0   | 11250 | 0.2088          | 0.6646    | 0.7114 | 0.9334   | 0.6872 |\n| 0.0666        | 7.0   | 13125 | 0.2169          | 0.6713    | 0.7054 | 0.9347   | 0.6879 |\n| 0.0634        | 8.0   | 15000 | 0.2199          | 0.6743    | 0.6979 | 0.9346   | 0.6859 |\n\n\n### Framework versions\n\n- Transformers 4.19.2\n- Pytorch 1.11.0+cu113\n- Datasets 2.2.2\n- Tokenizers 0.12.1\n",
    "card_content": "---\nlanguage:\n- fr\nlicense: mit\ntags:\n- generated_from_trainer\nmetrics:\n- precision\n- recall\n- accuracy\n- f1\nwidget:\n- text: Le président de la République appelle en outre les Français à faire le choix\n    d'une \"majorité stable et sérieuse pour les protéger face aux crises et pour agir\n    pour l'avenir\". \"Je vois dans le projet de Jean-Luc Mélenchon ou de Madame Le\n    Pen un projet de désordre et de soumission. Ils expliquent qu'il faut sortir de\n    nos alliances, de l'Europe, et bâtir des alliances stratégiques avec la Russie.\n    C'est la soumission à la Russie\", assure-t-il.\n- text: Top départ à l’ouverture des bureaux de vote. La Polynésie et les Français\n    résidant à l'étranger, dont certains ont déjà pu voter en ligne, sont invités\n    aux urnes ce week-end pour le premier tour des législatives, samedi 4 juin pour\n    le continent américain et les Caraïbes, et dimanche 5 juin pour le reste du monde.\n    En France métropolitaine, les premier et second tours auront lieu les 12 et 19\n    juin.\n- text: Le ministère a aussi indiqué que des missiles russes ont frappé un centre\n    d'entraînement d'artillerie dans la région de Soumy où travaillaient des instructeurs\n    étrangers. Il a jouté qu'une autre frappe avait détruit une position de \"mercenaires\n    étrangers\" dans la région d'Odessa.\n- text: 'Le malaise est profond et ressemble à une crise existentielle. Fait rarissime\n    au Quai d’Orsay, six syndicats et un collectif de 500 jeunes diplomates du ministère\n    des Affaires étrangères ont appelé à la grève, jeudi 2 juin, pour protester contre\n    la réforme de la haute fonction publique qui, à terme, entraînera la disparition\n    des deux corps historiques de la diplomatie française : celui de ministre plénipotentiaire\n    (ambassadeur) et celui de conseiller des affaires étrangères.'\n- text: Ils se font passer pour des recruteurs de Lockheed Martin ou du géant britannique\n    de la défense et de l’aérospatial BAE Systems. Ces soi-disant chasseurs de tête\n    font miroiter des perspectives lucratives de carrière et des postes à responsabilité.\n    Mais ce n’est que du vent. En réalité, il s’agit de cyberespions nord-coréens\n    cherchant à voler des secrets industriels de groupes de défense ou du secteur\n    de l’aérospatial, révèle Eset, une société slovaque de sécurité informatique,\n    dans un rapport publié mardi 31 mai.\nmodel-index:\n- name: camembert-keyword-extractor\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# camembert-keyword-extractor\n\nThis model is a fine-tuned version of [camembert-base](https://huggingface.co/camembert-base) on an unknown dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.2199\n- Precision: 0.6743\n- Recall: 0.6979\n- Accuracy: 0.9346\n- F1: 0.6859\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 16\n- eval_batch_size: 16\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 8\n- mixed_precision_training: Native AMP\n\n### Training results\n\n| Training Loss | Epoch | Step  | Validation Loss | Precision | Recall | Accuracy | F1     |\n|:-------------:|:-----:|:-----:|:---------------:|:---------:|:------:|:--------:|:------:|\n| 0.1747        | 1.0   | 1875  | 0.1780          | 0.5935    | 0.7116 | 0.9258   | 0.6472 |\n| 0.1375        | 2.0   | 3750  | 0.1588          | 0.6505    | 0.7032 | 0.9334   | 0.6759 |\n| 0.1147        | 3.0   | 5625  | 0.1727          | 0.6825    | 0.6689 | 0.9355   | 0.6756 |\n| 0.0969        | 4.0   | 7500  | 0.1759          | 0.6886    | 0.6621 | 0.9350   | 0.6751 |\n| 0.0837        | 5.0   | 9375  | 0.1967          | 0.6688    | 0.7112 | 0.9348   | 0.6893 |\n| 0.0746        | 6.0   | 11250 | 0.2088          | 0.6646    | 0.7114 | 0.9334   | 0.6872 |\n| 0.0666        | 7.0   | 13125 | 0.2169          | 0.6713    | 0.7054 | 0.9347   | 0.6879 |\n| 0.0634        | 8.0   | 15000 | 0.2199          | 0.6743    | 0.6979 | 0.9346   | 0.6859 |\n\n\n### Framework versions\n\n- Transformers 4.19.2\n- Pytorch 1.11.0+cu113\n- Datasets 2.2.2\n- Tokenizers 0.12.1\n",
    "library_name": "transformers"
  },
  {
    "model_id": "flair/pos-english",
    "model_name": "flair/pos-english",
    "author": "flair",
    "downloads": 108429,
    "likes": 28,
    "tags": [
      "flair",
      "pytorch",
      "token-classification",
      "sequence-tagger-model",
      "en",
      "dataset:ontonotes",
      "region:us"
    ],
    "card_url": "https://huggingface.co/flair/pos-english",
    "dependencies": [
      [
        "flair",
        null
      ]
    ],
    "analysis_date": "2025-03-26T00:51:38.543220",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "unknown",
    "pipeline_tag": "token-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": "en",
      "tags": [
        "flair",
        "token-classification",
        "sequence-tagger-model"
      ],
      "datasets": [
        "ontonotes"
      ],
      "inference": false
    },
    "card_text": "\n## English Part-of-Speech Tagging in Flair (default model)\n\nThis is the standard part-of-speech tagging model for English that ships with [Flair](https://github.com/flairNLP/flair/).\n\nF1-Score: **98,19** (Ontonotes)\n\nPredicts fine-grained POS tags:\n\n| **tag**                        | **meaning** |\n|---------------------------------|-----------|\n|ADD        | Email |\n|AFX        | Affix |\n|CC         | Coordinating conjunction  |\n|CD         | Cardinal number |\n|DT         | Determiner |\n|EX         | Existential there |\n|FW         | Foreign word |\n|HYPH       | Hyphen |\n|IN        | Preposition or subordinating conjunction |\n|JJ         | Adjective |\n|JJR        |Adjective, comparative |\n|JJS        | Adjective, superlative |\n|LS         | List item marker  |\n|MD         | Modal |\n|NFP        | Superfluous punctuation |\n|NN        | Noun, singular or mass |\n|NNP        |Proper noun, singular |\n|NNPS       | Proper noun, plural |\n|NNS        |Noun, plural |\n|PDT        | Predeterminer |\n|POS        | Possessive ending |\n|PRP        | Personal pronoun |\n|PRP$       | Possessive pronoun |\n|RB         | Adverb |\n|RBR        | Adverb, comparative |\n|RBS        | Adverb, superlative |\n|RP         | Particle |\n|SYM        | Symbol |\n|TO         | to |\n|UH         | Interjection |\n|VB         | Verb, base form |\n|VBD       | Verb, past tense |\n|VBG        | Verb, gerund or present participle |\n|VBN        | Verb, past participle |\n|VBP        | Verb, non-3rd person singular present |\n|VBZ        | Verb, 3rd person singular present |\n|WDT        | Wh-determiner |\n|WP        | Wh-pronoun |\n|WP$        | Possessive wh-pronoun |\n|WRB        | Wh-adverb |\n|XX         | Unknown |\n\n\n\nBased on [Flair embeddings](https://www.aclweb.org/anthology/C18-1139/) and LSTM-CRF.\n\n---\n\n### Demo: How to use in Flair\n\nRequires: **[Flair](https://github.com/flairNLP/flair/)** (`pip install flair`)\n\n```python\nfrom flair.data import Sentence\nfrom flair.models import SequenceTagger\n\n# load tagger\ntagger = SequenceTagger.load(\"flair/pos-english\")\n\n# make example sentence\nsentence = Sentence(\"I love Berlin.\")\n\n# predict NER tags\ntagger.predict(sentence)\n\n# print sentence\nprint(sentence)\n\n# print predicted NER spans\nprint('The following NER tags are found:')\n# iterate over entities and print\nfor entity in sentence.get_spans('pos'):\n    print(entity)\n\n```\n\nThis yields the following output:\n```\nSpan [1]: \"I\"   [− Labels: PRP (1.0)]\nSpan [2]: \"love\"   [− Labels: VBP (1.0)]\nSpan [3]: \"Berlin\"   [− Labels: NNP (0.9999)]\nSpan [4]: \".\"   [− Labels: . (1.0)]\n\n```\n\nSo, the word \"*I*\" is labeled as a **pronoun** (PRP),  \"*love*\" is labeled as a **verb** (VBP) and \"*Berlin*\" is labeled as a **proper noun** (NNP) in the sentence \"*I love Berlin*\". \n\n\n---\n\n### Training: Script to train this model\n\nThe following Flair script was used to train this model: \n\n```python\nfrom flair.data import Corpus\nfrom flair.datasets import ColumnCorpus\nfrom flair.embeddings import WordEmbeddings, StackedEmbeddings, FlairEmbeddings\n\n# 1. load the corpus (Ontonotes does not ship with Flair, you need to download and reformat into a column format yourself)\ncorpus: Corpus = ColumnCorpus(\n                \"resources/tasks/onto-ner\",\n                column_format={0: \"text\", 1: \"pos\", 2: \"upos\", 3: \"ner\"},\n                tag_to_bioes=\"ner\",\n            )\n\n# 2. what tag do we want to predict?\ntag_type = 'pos'\n\n# 3. make the tag dictionary from the corpus\ntag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)\n\n# 4. initialize each embedding we use\nembedding_types = [\n\n    # contextual string embeddings, forward\n    FlairEmbeddings('news-forward'),\n\n    # contextual string embeddings, backward\n    FlairEmbeddings('news-backward'),\n]\n\n# embedding stack consists of Flair and GloVe embeddings\nembeddings = StackedEmbeddings(embeddings=embedding_types)\n\n# 5. initialize sequence tagger\nfrom flair.models import SequenceTagger\n\ntagger = SequenceTagger(hidden_size=256,\n                        embeddings=embeddings,\n                        tag_dictionary=tag_dictionary,\n                        tag_type=tag_type)\n\n# 6. initialize trainer\nfrom flair.trainers import ModelTrainer\n\ntrainer = ModelTrainer(tagger, corpus)\n\n# 7. run training\ntrainer.train('resources/taggers/pos-english',\n              train_with_dev=True,\n              max_epochs=150)\n```\n\n\n\n---\n\n### Cite\n\nPlease cite the following paper when using this model.\n\n```\n@inproceedings{akbik2018coling,\n  title={Contextual String Embeddings for Sequence Labeling},\n  author={Akbik, Alan and Blythe, Duncan and Vollgraf, Roland},\n  booktitle = {{COLING} 2018, 27th International Conference on Computational Linguistics},\n  pages     = {1638--1649},\n  year      = {2018}\n}\n```\n\n---\n\n### Issues?\n\nThe Flair issue tracker is available [here](https://github.com/flairNLP/flair/issues/).\n",
    "card_content": "---\nlanguage: en\ntags:\n- flair\n- token-classification\n- sequence-tagger-model\ndatasets:\n- ontonotes\ninference: false\n---\n\n## English Part-of-Speech Tagging in Flair (default model)\n\nThis is the standard part-of-speech tagging model for English that ships with [Flair](https://github.com/flairNLP/flair/).\n\nF1-Score: **98,19** (Ontonotes)\n\nPredicts fine-grained POS tags:\n\n| **tag**                        | **meaning** |\n|---------------------------------|-----------|\n|ADD        | Email |\n|AFX        | Affix |\n|CC         | Coordinating conjunction  |\n|CD         | Cardinal number |\n|DT         | Determiner |\n|EX         | Existential there |\n|FW         | Foreign word |\n|HYPH       | Hyphen |\n|IN        | Preposition or subordinating conjunction |\n|JJ         | Adjective |\n|JJR        |Adjective, comparative |\n|JJS        | Adjective, superlative |\n|LS         | List item marker  |\n|MD         | Modal |\n|NFP        | Superfluous punctuation |\n|NN        | Noun, singular or mass |\n|NNP        |Proper noun, singular |\n|NNPS       | Proper noun, plural |\n|NNS        |Noun, plural |\n|PDT        | Predeterminer |\n|POS        | Possessive ending |\n|PRP        | Personal pronoun |\n|PRP$       | Possessive pronoun |\n|RB         | Adverb |\n|RBR        | Adverb, comparative |\n|RBS        | Adverb, superlative |\n|RP         | Particle |\n|SYM        | Symbol |\n|TO         | to |\n|UH         | Interjection |\n|VB         | Verb, base form |\n|VBD       | Verb, past tense |\n|VBG        | Verb, gerund or present participle |\n|VBN        | Verb, past participle |\n|VBP        | Verb, non-3rd person singular present |\n|VBZ        | Verb, 3rd person singular present |\n|WDT        | Wh-determiner |\n|WP        | Wh-pronoun |\n|WP$        | Possessive wh-pronoun |\n|WRB        | Wh-adverb |\n|XX         | Unknown |\n\n\n\nBased on [Flair embeddings](https://www.aclweb.org/anthology/C18-1139/) and LSTM-CRF.\n\n---\n\n### Demo: How to use in Flair\n\nRequires: **[Flair](https://github.com/flairNLP/flair/)** (`pip install flair`)\n\n```python\nfrom flair.data import Sentence\nfrom flair.models import SequenceTagger\n\n# load tagger\ntagger = SequenceTagger.load(\"flair/pos-english\")\n\n# make example sentence\nsentence = Sentence(\"I love Berlin.\")\n\n# predict NER tags\ntagger.predict(sentence)\n\n# print sentence\nprint(sentence)\n\n# print predicted NER spans\nprint('The following NER tags are found:')\n# iterate over entities and print\nfor entity in sentence.get_spans('pos'):\n    print(entity)\n\n```\n\nThis yields the following output:\n```\nSpan [1]: \"I\"   [− Labels: PRP (1.0)]\nSpan [2]: \"love\"   [− Labels: VBP (1.0)]\nSpan [3]: \"Berlin\"   [− Labels: NNP (0.9999)]\nSpan [4]: \".\"   [− Labels: . (1.0)]\n\n```\n\nSo, the word \"*I*\" is labeled as a **pronoun** (PRP),  \"*love*\" is labeled as a **verb** (VBP) and \"*Berlin*\" is labeled as a **proper noun** (NNP) in the sentence \"*I love Berlin*\". \n\n\n---\n\n### Training: Script to train this model\n\nThe following Flair script was used to train this model: \n\n```python\nfrom flair.data import Corpus\nfrom flair.datasets import ColumnCorpus\nfrom flair.embeddings import WordEmbeddings, StackedEmbeddings, FlairEmbeddings\n\n# 1. load the corpus (Ontonotes does not ship with Flair, you need to download and reformat into a column format yourself)\ncorpus: Corpus = ColumnCorpus(\n                \"resources/tasks/onto-ner\",\n                column_format={0: \"text\", 1: \"pos\", 2: \"upos\", 3: \"ner\"},\n                tag_to_bioes=\"ner\",\n            )\n\n# 2. what tag do we want to predict?\ntag_type = 'pos'\n\n# 3. make the tag dictionary from the corpus\ntag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)\n\n# 4. initialize each embedding we use\nembedding_types = [\n\n    # contextual string embeddings, forward\n    FlairEmbeddings('news-forward'),\n\n    # contextual string embeddings, backward\n    FlairEmbeddings('news-backward'),\n]\n\n# embedding stack consists of Flair and GloVe embeddings\nembeddings = StackedEmbeddings(embeddings=embedding_types)\n\n# 5. initialize sequence tagger\nfrom flair.models import SequenceTagger\n\ntagger = SequenceTagger(hidden_size=256,\n                        embeddings=embeddings,\n                        tag_dictionary=tag_dictionary,\n                        tag_type=tag_type)\n\n# 6. initialize trainer\nfrom flair.trainers import ModelTrainer\n\ntrainer = ModelTrainer(tagger, corpus)\n\n# 7. run training\ntrainer.train('resources/taggers/pos-english',\n              train_with_dev=True,\n              max_epochs=150)\n```\n\n\n\n---\n\n### Cite\n\nPlease cite the following paper when using this model.\n\n```\n@inproceedings{akbik2018coling,\n  title={Contextual String Embeddings for Sequence Labeling},\n  author={Akbik, Alan and Blythe, Duncan and Vollgraf, Roland},\n  booktitle = {{COLING} 2018, 27th International Conference on Computational Linguistics},\n  pages     = {1638--1649},\n  year      = {2018}\n}\n```\n\n---\n\n### Issues?\n\nThe Flair issue tracker is available [here](https://github.com/flairNLP/flair/issues/).\n",
    "library_name": "flair"
  },
  {
    "model_id": "syssec-utd/py312-pylingual-v1-segmenter",
    "model_name": "syssec-utd/py312-pylingual-v1-segmenter",
    "author": "syssec-utd",
    "downloads": 101741,
    "likes": 0,
    "tags": [
      "transformers",
      "tensorboard",
      "safetensors",
      "roberta",
      "token-classification",
      "generated_from_trainer",
      "base_model:syssec-utd/py312-pylingual-v1-mlm",
      "base_model:finetune:syssec-utd/py312-pylingual-v1-mlm",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/syssec-utd/py312-pylingual-v1-segmenter",
    "dependencies": [
      [
        "transformers",
        "4.48.2"
      ],
      [
        "torch",
        "2.2.1"
      ],
      [
        "datasets",
        "2.18.0"
      ],
      [
        "tokenizers",
        "0.21.0"
      ]
    ],
    "analysis_date": "2025-03-26T00:51:39.918036",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "roberta",
    "pipeline_tag": "token-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "library_name": "transformers",
      "tags": [
        "generated_from_trainer"
      ],
      "metrics": [
        "precision",
        "recall",
        "f1",
        "accuracy"
      ],
      "base_model": "syssec-utd/py312-pylingual-v1-mlm",
      "model-index": [
        {
          "name": "py312-pylingual-v1-segmenter",
          "results": []
        }
      ]
    },
    "card_text": "\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# py312-pylingual-v1-segmenter\n\nThis model is a fine-tuned version of [syssec-utd/py312-pylingual-v1-mlm](https://huggingface.co/syssec-utd/py312-pylingual-v1-mlm) on the syssec-utd/segmentation-py312-pylingual-v1-tokenized dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.0053\n- Precision: 0.9923\n- Recall: 0.9935\n- F1: 0.9929\n- Accuracy: 0.9982\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 48\n- eval_batch_size: 8\n- seed: 42\n- distributed_type: multi-GPU\n- optimizer: Use OptimizerNames.ADAMW_TORCH with betas=(0.9,0.999) and epsilon=1e-08 and optimizer_args=No additional optimizer arguments\n- lr_scheduler_type: linear\n- num_epochs: 2\n- mixed_precision_training: Native AMP\n\n### Training results\n\n| Training Loss | Epoch | Step   | Validation Loss | Precision | Recall | F1     | Accuracy |\n|:-------------:|:-----:|:------:|:---------------:|:---------:|:------:|:------:|:--------:|\n| 0.006         | 1.0   | 163232 | 0.0042          | 0.9929    | 0.9938 | 0.9934 | 0.9984   |\n| 0.0035        | 2.0   | 326464 | 0.0053          | 0.9923    | 0.9935 | 0.9929 | 0.9982   |\n\n\n### Framework versions\n\n- Transformers 4.48.2\n- Pytorch 2.2.1+cu121\n- Datasets 2.18.0\n- Tokenizers 0.21.0\n",
    "card_content": "---\nlibrary_name: transformers\ntags:\n- generated_from_trainer\nmetrics:\n- precision\n- recall\n- f1\n- accuracy\nbase_model: syssec-utd/py312-pylingual-v1-mlm\nmodel-index:\n- name: py312-pylingual-v1-segmenter\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# py312-pylingual-v1-segmenter\n\nThis model is a fine-tuned version of [syssec-utd/py312-pylingual-v1-mlm](https://huggingface.co/syssec-utd/py312-pylingual-v1-mlm) on the syssec-utd/segmentation-py312-pylingual-v1-tokenized dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.0053\n- Precision: 0.9923\n- Recall: 0.9935\n- F1: 0.9929\n- Accuracy: 0.9982\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 48\n- eval_batch_size: 8\n- seed: 42\n- distributed_type: multi-GPU\n- optimizer: Use OptimizerNames.ADAMW_TORCH with betas=(0.9,0.999) and epsilon=1e-08 and optimizer_args=No additional optimizer arguments\n- lr_scheduler_type: linear\n- num_epochs: 2\n- mixed_precision_training: Native AMP\n\n### Training results\n\n| Training Loss | Epoch | Step   | Validation Loss | Precision | Recall | F1     | Accuracy |\n|:-------------:|:-----:|:------:|:---------------:|:---------:|:------:|:------:|:--------:|\n| 0.006         | 1.0   | 163232 | 0.0042          | 0.9929    | 0.9938 | 0.9934 | 0.9984   |\n| 0.0035        | 2.0   | 326464 | 0.0053          | 0.9923    | 0.9935 | 0.9929 | 0.9982   |\n\n\n### Framework versions\n\n- Transformers 4.48.2\n- Pytorch 2.2.1+cu121\n- Datasets 2.18.0\n- Tokenizers 0.21.0\n",
    "library_name": "transformers"
  },
  {
    "model_id": "savasy/bert-base-turkish-ner-cased",
    "model_name": "savasy/bert-base-turkish-ner-cased",
    "author": "savasy",
    "downloads": 98317,
    "likes": 18,
    "tags": [
      "transformers",
      "pytorch",
      "jax",
      "safetensors",
      "bert",
      "token-classification",
      "tr",
      "arxiv:2401.17396",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/savasy/bert-base-turkish-ner-cased",
    "dependencies": [
      [
        "transformers",
        null
      ]
    ],
    "analysis_date": "2025-03-26T00:51:40.997333",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "bert",
    "pipeline_tag": "token-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": "tr"
    },
    "card_text": "\n# For Turkish language, here is an easy-to-use NER application. \n ** Türkçe için kolay bir python  NER (Bert + Transfer Learning)  (İsim Varlık Tanıma) modeli... \n\n\n\n# Citation\n\nPlease cite if you use it in your study\n\n\n```\n\n@misc{yildirim2024finetuning,\n      title={Fine-tuning Transformer-based Encoder for Turkish Language Understanding Tasks}, \n      author={Savas Yildirim},\n      year={2024},\n      eprint={2401.17396},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n\n\n\n@book{yildirim2021mastering,\n  title={Mastering Transformers: Build state-of-the-art models from scratch with advanced natural language processing techniques},\n  author={Yildirim, Savas and Asgari-Chenaghlu, Meysam},\n  year={2021},\n  publisher={Packt Publishing Ltd}\n}\n```\n\n\n# other detail\n\n\nThanks to @stefan-it, I applied the followings for training\n\n\ncd tr-data\n\nfor file in train.txt dev.txt test.txt labels.txt\ndo\n  wget https://schweter.eu/storage/turkish-bert-wikiann/$file\ndone\n\ncd ..\nIt will download the pre-processed datasets with training, dev and test splits and put them in a tr-data folder.\n\nRun pre-training\nAfter downloading the dataset, pre-training can be started. Just set the following environment variables:\n```\nexport MAX_LENGTH=128\nexport BERT_MODEL=dbmdz/bert-base-turkish-cased \nexport OUTPUT_DIR=tr-new-model\nexport BATCH_SIZE=32\nexport NUM_EPOCHS=3\nexport SAVE_STEPS=625\nexport SEED=1\n```\nThen run pre-training:\n```\npython3 run_ner_old.py --data_dir ./tr-data3 \\\n--model_type bert \\\n--labels ./tr-data/labels.txt \\\n--model_name_or_path $BERT_MODEL \\\n--output_dir $OUTPUT_DIR-$SEED \\\n--max_seq_length $MAX_LENGTH \\\n--num_train_epochs $NUM_EPOCHS \\\n--per_gpu_train_batch_size $BATCH_SIZE \\\n--save_steps $SAVE_STEPS \\\n--seed $SEED \\\n--do_train \\\n--do_eval \\\n--do_predict \\\n--fp16\n```\n\n\n# Usage\n\n```\nfrom transformers import pipeline, AutoModelForTokenClassification, AutoTokenizer\nmodel = AutoModelForTokenClassification.from_pretrained(\"savasy/bert-base-turkish-ner-cased\")\ntokenizer = AutoTokenizer.from_pretrained(\"savasy/bert-base-turkish-ner-cased\")\nner=pipeline('ner', model=model, tokenizer=tokenizer)\nner(\"Mustafa Kemal Atatürk 19 Mayıs 1919'da Samsun'a ayak bastı.\")\n```\n# Some results\nData1:  For the data above\nEval Results:\n\n* precision = 0.916400580551524\n* recall = 0.9342309684101502\n* f1 = 0.9252298787412536\n* loss = 0.11335893666411284\n\nTest Results:\n* precision = 0.9192058759362955\n* recall = 0.9303010230367262\n* f1 = 0.9247201697271198\n* loss = 0.11182546521618497\n\n\n\nData2:\nhttps://github.com/stefan-it/turkish-bert/files/4558187/nerdata.txt\nThe performance for the data given by @kemalaraz is as follows\n\nsavas@savas-lenova:~/Desktop/trans/tr-new-model-1$ cat eval_results.txt\n* precision = 0.9461980692049029\n* recall = 0.959309358847465\n* f1 = 0.9527086063783312\n* loss = 0.037054269206847804\n\nsavas@savas-lenova:~/Desktop/trans/tr-new-model-1$ cat test_results.txt\n* precision = 0.9458370635631155\n* recall = 0.9588201928530913\n* f1 = 0.952284378344882\n* loss = 0.035431676572445225\n\n",
    "card_content": "---\nlanguage: tr\n---\n\n# For Turkish language, here is an easy-to-use NER application. \n ** Türkçe için kolay bir python  NER (Bert + Transfer Learning)  (İsim Varlık Tanıma) modeli... \n\n\n\n# Citation\n\nPlease cite if you use it in your study\n\n\n```\n\n@misc{yildirim2024finetuning,\n      title={Fine-tuning Transformer-based Encoder for Turkish Language Understanding Tasks}, \n      author={Savas Yildirim},\n      year={2024},\n      eprint={2401.17396},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n\n\n\n@book{yildirim2021mastering,\n  title={Mastering Transformers: Build state-of-the-art models from scratch with advanced natural language processing techniques},\n  author={Yildirim, Savas and Asgari-Chenaghlu, Meysam},\n  year={2021},\n  publisher={Packt Publishing Ltd}\n}\n```\n\n\n# other detail\n\n\nThanks to @stefan-it, I applied the followings for training\n\n\ncd tr-data\n\nfor file in train.txt dev.txt test.txt labels.txt\ndo\n  wget https://schweter.eu/storage/turkish-bert-wikiann/$file\ndone\n\ncd ..\nIt will download the pre-processed datasets with training, dev and test splits and put them in a tr-data folder.\n\nRun pre-training\nAfter downloading the dataset, pre-training can be started. Just set the following environment variables:\n```\nexport MAX_LENGTH=128\nexport BERT_MODEL=dbmdz/bert-base-turkish-cased \nexport OUTPUT_DIR=tr-new-model\nexport BATCH_SIZE=32\nexport NUM_EPOCHS=3\nexport SAVE_STEPS=625\nexport SEED=1\n```\nThen run pre-training:\n```\npython3 run_ner_old.py --data_dir ./tr-data3 \\\n--model_type bert \\\n--labels ./tr-data/labels.txt \\\n--model_name_or_path $BERT_MODEL \\\n--output_dir $OUTPUT_DIR-$SEED \\\n--max_seq_length $MAX_LENGTH \\\n--num_train_epochs $NUM_EPOCHS \\\n--per_gpu_train_batch_size $BATCH_SIZE \\\n--save_steps $SAVE_STEPS \\\n--seed $SEED \\\n--do_train \\\n--do_eval \\\n--do_predict \\\n--fp16\n```\n\n\n# Usage\n\n```\nfrom transformers import pipeline, AutoModelForTokenClassification, AutoTokenizer\nmodel = AutoModelForTokenClassification.from_pretrained(\"savasy/bert-base-turkish-ner-cased\")\ntokenizer = AutoTokenizer.from_pretrained(\"savasy/bert-base-turkish-ner-cased\")\nner=pipeline('ner', model=model, tokenizer=tokenizer)\nner(\"Mustafa Kemal Atatürk 19 Mayıs 1919'da Samsun'a ayak bastı.\")\n```\n# Some results\nData1:  For the data above\nEval Results:\n\n* precision = 0.916400580551524\n* recall = 0.9342309684101502\n* f1 = 0.9252298787412536\n* loss = 0.11335893666411284\n\nTest Results:\n* precision = 0.9192058759362955\n* recall = 0.9303010230367262\n* f1 = 0.9247201697271198\n* loss = 0.11182546521618497\n\n\n\nData2:\nhttps://github.com/stefan-it/turkish-bert/files/4558187/nerdata.txt\nThe performance for the data given by @kemalaraz is as follows\n\nsavas@savas-lenova:~/Desktop/trans/tr-new-model-1$ cat eval_results.txt\n* precision = 0.9461980692049029\n* recall = 0.959309358847465\n* f1 = 0.9527086063783312\n* loss = 0.037054269206847804\n\nsavas@savas-lenova:~/Desktop/trans/tr-new-model-1$ cat test_results.txt\n* precision = 0.9458370635631155\n* recall = 0.9588201928530913\n* f1 = 0.952284378344882\n* loss = 0.035431676572445225\n\n",
    "library_name": "transformers"
  },
  {
    "model_id": "QCRI/bert-base-multilingual-cased-pos-english",
    "model_name": "QCRI/bert-base-multilingual-cased-pos-english",
    "author": "QCRI",
    "downloads": 97500,
    "likes": 39,
    "tags": [
      "transformers",
      "pytorch",
      "bert",
      "token-classification",
      "part-of-speech",
      "finetuned",
      "en",
      "license:cc-by-nc-3.0",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/QCRI/bert-base-multilingual-cased-pos-english",
    "dependencies": [
      [
        "transformers",
        null
      ]
    ],
    "analysis_date": "2025-03-26T00:51:42.098210",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "bert",
    "pipeline_tag": "token-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": [
        "en"
      ],
      "license": "cc-by-nc-3.0",
      "tags": [
        "part-of-speech",
        "finetuned"
      ]
    },
    "card_text": "\n# BERT-base-multilingual-cased finetuned for Part-of-Speech tagging\n\nThis is a multilingual BERT model fine tuned for part-of-speech tagging for English. It is trained using the Penn TreeBank (Marcus et al., 1993) and achieves an F1-score of 96.69.\n\n## Usage\nA *transformers* pipeline can be used to run the model:\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification, TokenClassificationPipeline\n\nmodel_name = \"QCRI/bert-base-multilingual-cased-pos-english\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForTokenClassification.from_pretrained(model_name)\n\npipeline = TokenClassificationPipeline(model=model, tokenizer=tokenizer)\noutputs = pipeline(\"A test example\")\nprint(outputs)\n```\n\n\n## Citation\nThis model was used for all the part-of-speech tagging based results in *Analyzing Encoded Concepts in Transformer Language Models*, published at NAACL'22. If you find this model useful for your own work, please use the following citation:\n\n```bib\n@inproceedings{sajjad-NAACL,\n  title={Analyzing Encoded Concepts in Transformer Language Models},\n  author={Hassan Sajjad, Nadir Durrani, Fahim Dalvi, Firoj Alam, Abdul Rafae Khan and Jia Xu},\n  booktitle={North American Chapter of the Association of Computational Linguistics: Human Language Technologies (NAACL)},\n  series={NAACL~'22},\n  year={2022},\n  address={Seattle}\n}\n```",
    "card_content": "---\nlanguage:\n- en\nlicense: cc-by-nc-3.0\ntags:\n- part-of-speech\n- finetuned\n---\n\n# BERT-base-multilingual-cased finetuned for Part-of-Speech tagging\n\nThis is a multilingual BERT model fine tuned for part-of-speech tagging for English. It is trained using the Penn TreeBank (Marcus et al., 1993) and achieves an F1-score of 96.69.\n\n## Usage\nA *transformers* pipeline can be used to run the model:\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification, TokenClassificationPipeline\n\nmodel_name = \"QCRI/bert-base-multilingual-cased-pos-english\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForTokenClassification.from_pretrained(model_name)\n\npipeline = TokenClassificationPipeline(model=model, tokenizer=tokenizer)\noutputs = pipeline(\"A test example\")\nprint(outputs)\n```\n\n\n## Citation\nThis model was used for all the part-of-speech tagging based results in *Analyzing Encoded Concepts in Transformer Language Models*, published at NAACL'22. If you find this model useful for your own work, please use the following citation:\n\n```bib\n@inproceedings{sajjad-NAACL,\n  title={Analyzing Encoded Concepts in Transformer Language Models},\n  author={Hassan Sajjad, Nadir Durrani, Fahim Dalvi, Firoj Alam, Abdul Rafae Khan and Jia Xu},\n  booktitle={North American Chapter of the Association of Computational Linguistics: Human Language Technologies (NAACL)},\n  series={NAACL~'22},\n  year={2022},\n  address={Seattle}\n}\n```",
    "library_name": "transformers"
  },
  {
    "model_id": "pdelobelle/robbert-v2-dutch-ner",
    "model_name": "pdelobelle/robbert-v2-dutch-ner",
    "author": "pdelobelle",
    "downloads": 89779,
    "likes": 3,
    "tags": [
      "transformers",
      "pytorch",
      "jax",
      "roberta",
      "token-classification",
      "Dutch",
      "Flemish",
      "RoBERTa",
      "RobBERT",
      "nl",
      "license:mit",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/pdelobelle/robbert-v2-dutch-ner",
    "dependencies": [
      [
        "transformers",
        null
      ]
    ],
    "analysis_date": "2025-03-26T00:51:42.980188",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "roberta",
    "pipeline_tag": "token-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": "nl",
      "license": "mit",
      "tags": [
        "Dutch",
        "Flemish",
        "RoBERTa",
        "RobBERT"
      ],
      "datasets": [
        "oscar",
        "oscar (NL)",
        "dbrd",
        "lassy-ud",
        "europarl-mono",
        "conll2002"
      ],
      "thumbnail": "https://github.com/iPieter/RobBERT/raw/master/res/robbert_logo.png",
      "widget": [
        {
          "text": "Mijn naam is RobBERT en ik ben een taalmodel van de KU Leuven."
        }
      ]
    },
    "card_text": "\n<p align=\"center\"> \n    <img src=\"https://github.com/iPieter/RobBERT/raw/master/res/robbert_logo_with_name.png\" alt=\"RobBERT: A Dutch RoBERTa-based Language Model\" width=\"75%\">\n </p>\n\n# RobBERT: Dutch RoBERTa-based Language Model.\n\n[RobBERT](https://github.com/iPieter/RobBERT) is the state-of-the-art Dutch BERT model. It is a large pre-trained general Dutch language model that can be fine-tuned on a given dataset to perform any text classification, regression or token-tagging task. As such, it has been successfully used by many [researchers](https://scholar.google.com/scholar?oi=bibs&hl=en&cites=7180110604335112086) and [practitioners](https://huggingface.co/models?search=robbert) for achieving state-of-the-art performance for a wide range of Dutch natural language processing tasks,",
    "card_content": "---\nlanguage: nl\nlicense: mit\ntags:\n- Dutch\n- Flemish\n- RoBERTa\n- RobBERT\ndatasets:\n- oscar\n- oscar (NL)\n- dbrd\n- lassy-ud\n- europarl-mono\n- conll2002\nthumbnail: https://github.com/iPieter/RobBERT/raw/master/res/robbert_logo.png\nwidget:\n- text: Mijn naam is RobBERT en ik ben een taalmodel van de KU Leuven.\n---\n\n<p align=\"center\"> \n    <img src=\"https://github.com/iPieter/RobBERT/raw/master/res/robbert_logo_with_name.png\" alt=\"RobBERT: A Dutch RoBERTa-based Language Model\" width=\"75%\">\n </p>\n\n# RobBERT: Dutch RoBERTa-based Language Model.\n\n[RobBERT](https://github.com/iPieter/RobBERT) is the state-of-the-art Dutch BERT model. It is a large pre-trained general Dutch language model that can be fine-tuned on a given dataset to perform any text classification, regression or token-tagging task. As such, it has been successfully used by many [researchers](https://scholar.google.com/scholar?oi=bibs&hl=en&cites=7180110604335112086) and [practitioners](https://huggingface.co/models?search=robbert) for achieving state-of-the-art performance for a wide range of Dutch natural language processing tasks,",
    "library_name": "transformers"
  },
  {
    "model_id": "vblagoje/bert-english-uncased-finetuned-pos",
    "model_name": "vblagoje/bert-english-uncased-finetuned-pos",
    "author": "vblagoje",
    "downloads": 87787,
    "likes": 39,
    "tags": [
      "transformers",
      "pytorch",
      "jax",
      "safetensors",
      "bert",
      "token-classification",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/vblagoje/bert-english-uncased-finetuned-pos",
    "dependencies": null,
    "analysis_date": "2025-03-26T00:51:43.639185",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "bert",
    "pipeline_tag": "token-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {},
    "card_text": "## Part-of-Speech (PoS) Tags\n\nBelow are the Part-of-Speech (PoS) tags used in the model:\n\n| **Tag**   | **Meaning**                                          | **Examples**                  |\n|-----------|------------------------------------------------------|--------------------------------|\n| ADP       | Adposition (prepositions or postpositions)           | in, on, by                     |\n| ADJ       | Adjective                                            | significant, global            |\n| ADV       | Adverb                                               | quickly, often                 |\n| AUX       | Auxiliary verb                                       | is, was                        |\n| CCONJ     | Coordinating conjunction                             | and, but                       |\n| DET       | Determiner                                           | the, a                         |\n| INTJ      | Interjection                                         | oh, wow                        |\n| NOUN      | Noun                                                 | man, city                      |\n| NUM       | Number                                               | one, 2022                      |\n| PART      | Particle                                              | 's, to                         |\n| PRON      | Pronoun                                               | he, which                      |\n| PROPN     | Proper noun                                          | Neil Armstrong, Paris          |\n| PUNCT     | Punctuation mark                                     | ,, .                           |\n| SCONJ     | Subordinating conjunction                            | because, although              |\n| SYM       | Symbol                                               | $, %                           |\n| VERB      | Verb                                                 | run, is                        |\n| X         | Other (generally words that do not fit into other categories) | [not defined]                 |\n\n",
    "card_content": "---\n{}\n---\n## Part-of-Speech (PoS) Tags\n\nBelow are the Part-of-Speech (PoS) tags used in the model:\n\n| **Tag**   | **Meaning**                                          | **Examples**                  |\n|-----------|------------------------------------------------------|--------------------------------|\n| ADP       | Adposition (prepositions or postpositions)           | in, on, by                     |\n| ADJ       | Adjective                                            | significant, global            |\n| ADV       | Adverb                                               | quickly, often                 |\n| AUX       | Auxiliary verb                                       | is, was                        |\n| CCONJ     | Coordinating conjunction                             | and, but                       |\n| DET       | Determiner                                           | the, a                         |\n| INTJ      | Interjection                                         | oh, wow                        |\n| NOUN      | Noun                                                 | man, city                      |\n| NUM       | Number                                               | one, 2022                      |\n| PART      | Particle                                              | 's, to                         |\n| PRON      | Pronoun                                               | he, which                      |\n| PROPN     | Proper noun                                          | Neil Armstrong, Paris          |\n| PUNCT     | Punctuation mark                                     | ,, .                           |\n| SCONJ     | Subordinating conjunction                            | because, although              |\n| SYM       | Symbol                                               | $, %                           |\n| VERB      | Verb                                                 | run, is                        |\n| X         | Other (generally words that do not fit into other categories) | [not defined]                 |\n\n",
    "library_name": "transformers"
  },
  {
    "model_id": "ckiplab/albert-tiny-chinese-ws",
    "model_name": "ckiplab/albert-tiny-chinese-ws",
    "author": "ckiplab",
    "downloads": 87485,
    "likes": 6,
    "tags": [
      "transformers",
      "pytorch",
      "albert",
      "token-classification",
      "zh",
      "license:gpl-3.0",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/ckiplab/albert-tiny-chinese-ws",
    "dependencies": [
      [
        "transformers",
        null
      ]
    ],
    "analysis_date": "2025-03-26T00:51:45.002711",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "albert",
    "pipeline_tag": "token-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": [
        "zh"
      ],
      "license": "gpl-3.0",
      "tags": [
        "pytorch",
        "token-classification",
        "albert",
        "zh"
      ],
      "thumbnail": "https://ckip.iis.sinica.edu.tw/files/ckip_logo.png"
    },
    "card_text": "\n# CKIP ALBERT Tiny Chinese\n\nThis project provides traditional Chinese transformers models (including ALBERT, BERT, GPT2) and NLP tools (including word segmentation, part-of-speech tagging, named entity recognition).\n\n這個專案提供了繁體中文的 transformers 模型（包含 ALBERT、BERT、GPT2）及自然語言處理工具（包含斷詞、詞性標記、實體辨識）。\n\n## Homepage\n\n- https://github.com/ckiplab/ckip-transformers\n\n## Contributers\n\n- [Mu Yang](https://muyang.pro) at [CKIP](https://ckip.iis.sinica.edu.tw) (Author & Maintainer)\n\n## Usage\n\nPlease use BertTokenizerFast as tokenizer instead of AutoTokenizer.\n\n請使用 BertTokenizerFast 而非 AutoTokenizer。\n\n```\nfrom transformers import (\n  BertTokenizerFast,\n  AutoModel,\n)\n\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')\nmodel = AutoModel.from_pretrained('ckiplab/albert-tiny-chinese-ws')\n```\n\nFor full usage and more information, please refer to https://github.com/ckiplab/ckip-transformers.\n\n有關完整使用方法及其他資訊，請參見 https://github.com/ckiplab/ckip-transformers 。\n",
    "card_content": "---\nlanguage:\n- zh\nlicense: gpl-3.0\ntags:\n- pytorch\n- token-classification\n- albert\n- zh\nthumbnail: https://ckip.iis.sinica.edu.tw/files/ckip_logo.png\n---\n\n# CKIP ALBERT Tiny Chinese\n\nThis project provides traditional Chinese transformers models (including ALBERT, BERT, GPT2) and NLP tools (including word segmentation, part-of-speech tagging, named entity recognition).\n\n這個專案提供了繁體中文的 transformers 模型（包含 ALBERT、BERT、GPT2）及自然語言處理工具（包含斷詞、詞性標記、實體辨識）。\n\n## Homepage\n\n- https://github.com/ckiplab/ckip-transformers\n\n## Contributers\n\n- [Mu Yang](https://muyang.pro) at [CKIP](https://ckip.iis.sinica.edu.tw) (Author & Maintainer)\n\n## Usage\n\nPlease use BertTokenizerFast as tokenizer instead of AutoTokenizer.\n\n請使用 BertTokenizerFast 而非 AutoTokenizer。\n\n```\nfrom transformers import (\n  BertTokenizerFast,\n  AutoModel,\n)\n\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')\nmodel = AutoModel.from_pretrained('ckiplab/albert-tiny-chinese-ws')\n```\n\nFor full usage and more information, please refer to https://github.com/ckiplab/ckip-transformers.\n\n有關完整使用方法及其他資訊，請參見 https://github.com/ckiplab/ckip-transformers 。\n",
    "library_name": "transformers"
  },
  {
    "model_id": "cahya/bert-base-indonesian-NER",
    "model_name": "cahya/bert-base-indonesian-NER",
    "author": "cahya",
    "downloads": 82039,
    "likes": 13,
    "tags": [
      "transformers",
      "pytorch",
      "jax",
      "bert",
      "token-classification",
      "id",
      "license:mit",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/cahya/bert-base-indonesian-NER",
    "dependencies": null,
    "analysis_date": "2025-03-26T00:51:45.536356",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "bert",
    "pipeline_tag": "token-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": [
        "id"
      ],
      "license": "mit",
      "pipeline_tag": "token-classification"
    },
    "card_text": "",
    "card_content": "---\nlanguage:\n- id\nlicense: mit\npipeline_tag: token-classification\n---\n",
    "library_name": "transformers"
  },
  {
    "model_id": "llm-book/bert-base-japanese-v3-ner-wikipedia-dataset",
    "model_name": "llm-book/bert-base-japanese-v3-ner-wikipedia-dataset",
    "author": "llm-book",
    "downloads": 80247,
    "likes": 9,
    "tags": [
      "transformers",
      "pytorch",
      "bert",
      "token-classification",
      "ja",
      "dataset:llm-book/ner-wikipedia-dataset",
      "license:apache-2.0",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/llm-book/bert-base-japanese-v3-ner-wikipedia-dataset",
    "dependencies": [
      [
        "transformers",
        null
      ]
    ],
    "analysis_date": "2025-03-26T00:51:46.358597",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "bert",
    "pipeline_tag": "token-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": [
        "ja"
      ],
      "license": "apache-2.0",
      "library_name": "transformers",
      "datasets": [
        "llm-book/ner-wikipedia-dataset"
      ],
      "metrics": [
        "seqeval",
        "precision",
        "recall",
        "f1"
      ],
      "pipeline_tag": "token-classification"
    },
    "card_text": "\n# llm-book/bert-base-japanese-v3-ner-wikipedia-dataset\n\n「[大規模言語モデル入門](https://www.amazon.co.jp/dp/4297136333)」の第6章で紹介している固有表現認識のモデルです。\n[cl-tohoku/bert-base-japanese-v3](https://huggingface.co/cl-tohoku/bert-base-japanese-v3)を[llm-book/ner-wikipedia-dataset](https://huggingface.co/datasets/llm-book/ner-wikipedia-dataset)でファインチューニングして構築されています。\n\n## 関連リンク\n\n* [GitHubリポジトリ](https://github.com/ghmagazine/llm-book)\n* [Colabノートブック](https://colab.research.google.com/github/ghmagazine/llm-book/blob/main/chapter6/6-named-entity-recognition.ipynb)\n* [データセット](https://huggingface.co/datasets/llm-book/ner-wikipedia-dataset)\n* [大規模言語モデル入門（Amazon.co.jp）](https://www.amazon.co.jp/dp/4297136333/)\n* [大規模言語モデル入門（gihyo.jp）](https://gihyo.jp/book/2023/978-4-297-13633-8)\n\n## 使い方\n```python\nfrom transformers import pipeline\nfrom pprint import pprint\n\nner_pipeline = pipeline(\n    model=\"llm-book/bert-base-japanese-v3-ner-wikipedia-dataset\",\n    aggregation_strategy=\"simple\",\n)\ntext = \"大谷翔平は岩手県水沢市出身のプロ野球選手\"\n# text中の固有表現を抽出\npprint(ner_pipeline(text))\n# [{'end': None,\n#   'entity_group': '人名',\n#   'score': 0.99823624,\n#   'start': None,\n#   'word': '大谷 翔平'},\n#  {'end': None,\n#   'entity_group': '地名',\n#   'score': 0.9986874,\n#   'start': None,\n#   'word': '岩手 県 水沢 市'}]\n```\n\n## ライセンス\n\n[Apache License 2.0](https://www.apache.org/licenses/LICENSE-2.0)",
    "card_content": "---\nlanguage:\n- ja\nlicense: apache-2.0\nlibrary_name: transformers\ndatasets:\n- llm-book/ner-wikipedia-dataset\nmetrics:\n- seqeval\n- precision\n- recall\n- f1\npipeline_tag: token-classification\n---\n\n# llm-book/bert-base-japanese-v3-ner-wikipedia-dataset\n\n「[大規模言語モデル入門](https://www.amazon.co.jp/dp/4297136333)」の第6章で紹介している固有表現認識のモデルです。\n[cl-tohoku/bert-base-japanese-v3](https://huggingface.co/cl-tohoku/bert-base-japanese-v3)を[llm-book/ner-wikipedia-dataset](https://huggingface.co/datasets/llm-book/ner-wikipedia-dataset)でファインチューニングして構築されています。\n\n## 関連リンク\n\n* [GitHubリポジトリ](https://github.com/ghmagazine/llm-book)\n* [Colabノートブック](https://colab.research.google.com/github/ghmagazine/llm-book/blob/main/chapter6/6-named-entity-recognition.ipynb)\n* [データセット](https://huggingface.co/datasets/llm-book/ner-wikipedia-dataset)\n* [大規模言語モデル入門（Amazon.co.jp）](https://www.amazon.co.jp/dp/4297136333/)\n* [大規模言語モデル入門（gihyo.jp）](https://gihyo.jp/book/2023/978-4-297-13633-8)\n\n## 使い方\n```python\nfrom transformers import pipeline\nfrom pprint import pprint\n\nner_pipeline = pipeline(\n    model=\"llm-book/bert-base-japanese-v3-ner-wikipedia-dataset\",\n    aggregation_strategy=\"simple\",\n)\ntext = \"大谷翔平は岩手県水沢市出身のプロ野球選手\"\n# text中の固有表現を抽出\npprint(ner_pipeline(text))\n# [{'end': None,\n#   'entity_group': '人名',\n#   'score': 0.99823624,\n#   'start': None,\n#   'word': '大谷 翔平'},\n#  {'end': None,\n#   'entity_group': '地名',\n#   'score': 0.9986874,\n#   'start': None,\n#   'word': '岩手 県 水沢 市'}]\n```\n\n## ライセンス\n\n[Apache License 2.0](https://www.apache.org/licenses/LICENSE-2.0)",
    "library_name": "transformers"
  },
  {
    "model_id": "51la5/roberta-large-NER",
    "model_name": "51la5/roberta-large-NER",
    "author": "51la5",
    "downloads": 77564,
    "likes": 47,
    "tags": [
      "transformers",
      "pytorch",
      "rust",
      "xlm-roberta",
      "token-classification",
      "multilingual",
      "af",
      "am",
      "ar",
      "as",
      "az",
      "be",
      "bg",
      "bn",
      "br",
      "bs",
      "ca",
      "cs",
      "cy",
      "da",
      "de",
      "el",
      "en",
      "eo",
      "es",
      "et",
      "eu",
      "fa",
      "fi",
      "fr",
      "fy",
      "ga",
      "gd",
      "gl",
      "gu",
      "ha",
      "he",
      "hi",
      "hr",
      "hu",
      "hy",
      "id",
      "is",
      "it",
      "ja",
      "jv",
      "ka",
      "kk",
      "km",
      "kn",
      "ko",
      "ku",
      "ky",
      "la",
      "lo",
      "lt",
      "lv",
      "mg",
      "mk",
      "ml",
      "mn",
      "mr",
      "ms",
      "my",
      "ne",
      "nl",
      "no",
      "om",
      "or",
      "pa",
      "pl",
      "ps",
      "pt",
      "ro",
      "ru",
      "sa",
      "sd",
      "si",
      "sk",
      "sl",
      "so",
      "sq",
      "sr",
      "su",
      "sv",
      "sw",
      "ta",
      "te",
      "th",
      "tl",
      "tr",
      "ug",
      "uk",
      "ur",
      "uz",
      "vi",
      "xh",
      "yi",
      "zh",
      "arxiv:1911.02116",
      "arxiv:2008.03415",
      "arxiv:1910.09700",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/51la5/roberta-large-NER",
    "dependencies": [
      [
        "transformers",
        null
      ]
    ],
    "analysis_date": "2025-03-26T00:51:47.854635",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "xlm-roberta",
    "pipeline_tag": "token-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": [
        "multilingual",
        "af",
        "am",
        "ar",
        "as",
        "az",
        "be",
        "bg",
        "bn",
        "br",
        "bs",
        "ca",
        "cs",
        "cy",
        "da",
        "de",
        "el",
        "en",
        "eo",
        "es",
        "et",
        "eu",
        "fa",
        "fi",
        "fr",
        "fy",
        "ga",
        "gd",
        "gl",
        "gu",
        "ha",
        "he",
        "hi",
        "hr",
        "hu",
        "hy",
        "id",
        "is",
        "it",
        "ja",
        "jv",
        "ka",
        "kk",
        "km",
        "kn",
        "ko",
        "ku",
        "ky",
        "la",
        "lo",
        "lt",
        "lv",
        "mg",
        "mk",
        "ml",
        "mn",
        "mr",
        "ms",
        "my",
        "ne",
        "nl",
        false,
        "om",
        "or",
        "pa",
        "pl",
        "ps",
        "pt",
        "ro",
        "ru",
        "sa",
        "sd",
        "si",
        "sk",
        "sl",
        "so",
        "sq",
        "sr",
        "su",
        "sv",
        "sw",
        "ta",
        "te",
        "th",
        "tl",
        "tr",
        "ug",
        "uk",
        "ur",
        "uz",
        "vi",
        "xh",
        "yi",
        "zh"
      ]
    },
    "card_text": "\n# xlm-roberta-large-finetuned-conll03-english\n\n#  Table of Contents\n\n1. [Model Details](#model-details)\n2. [Uses](#uses)\n3. [Bias, Risks, and Limitations](#bias-risks-and-limitations)\n4. [Training](#training)\n5. [Evaluation](#evaluation)\n6. [Environmental Impact](#environmental-impact)\n7. [Technical Specifications](#technical-specifications)\n8. [Citation](#citation)\n9. [Model Card Authors](#model-card-authors)\n10. [How To Get Started With the Model](#how-to-get-started-with-the-model)\n\n\n# Model Details\n\n## Model Description\n\nThe XLM-RoBERTa model was proposed in [Unsupervised Cross-lingual Representation Learning at Scale](https://arxiv.org/abs/1911.02116) by Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer and Veselin Stoyanov. It is based on Facebook's RoBERTa model released in 2019. It is a large multi-lingual language model, trained on 2.5TB of filtered CommonCrawl data. This model is [XLM-RoBERTa-large](https://huggingface.co/xlm-roberta-large) fine-tuned with the [conll2003](https://huggingface.co/datasets/conll2003) dataset in English.\n\n- **Developed by:** See [associated paper](https://arxiv.org/abs/1911.02116)\n- **Model type:** Multi-lingual language model\n- **Language(s) (NLP) or Countries (images):** XLM-RoBERTa is a multilingual model trained on 100 different languages; see [GitHub Repo](https://github.com/facebookresearch/fairseq/tree/main/examples/xlmr) for full list; model is fine-tuned on a dataset in English\n- **License:** More information needed\n- **Related Models:** [RoBERTa](https://huggingface.co/roberta-base), [XLM](https://huggingface.co/docs/transformers/model_doc/xlm)\n    - **Parent Model:** [XLM-RoBERTa-large](https://huggingface.co/xlm-roberta-large)\n- **Resources for more information:** \n  -[GitHub Repo](https://github.com/facebookresearch/fairseq/tree/main/examples/xlmr)\n  -[Associated Paper](https://arxiv.org/abs/1911.02116)\n\n# Uses\n\n## Direct Use\n\nThe model is a language model. The model can be used for token classification, a natural language understanding task in which a label is assigned to some tokens in a text. \n\n## Downstream Use\n\nPotential downstream use cases include Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. To learn more about token classification and other potential downstream use cases, see the Hugging Face [token classification docs](https://huggingface.co/tasks/token-classification).\n\n## Out-of-Scope Use\n\nThe model should not be used to intentionally create hostile or alienating environments for people. \n\n# Bias, Risks, and Limitations\n\n**CONTENT WARNING: Readers should be made aware that language generated by this model may be disturbing or offensive to some and may propagate historical and current stereotypes.**\n\nSignificant research has explored bias and fairness issues with language models (see, e.g., [Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)). In the context of tasks relevant to this model, [Mishra et al. (2020)](https://arxiv.org/pdf/2008.03415.pdf) explore social biases in NER systems for English and find that there is systematic bias in existing NER systems in that they fail to identify named entities from different demographic groups (though this paper did not look at BERT). For example, using a sample sentence from [Mishra et al. (2020)](https://arxiv.org/pdf/2008.03415.pdf):\n\n```python\n>>> from transformers import pipeline\n>>> tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-large-finetuned-conll03-english\")\n>>> model = AutoModelForTokenClassification.from_pretrained(\"xlm-roberta-large-finetuned-conll03-english\")\n>>> classifier = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n>>> classifier(\"Alya told Jasmine that Andrew could pay with cash..\")\n[{'end': 2,\n  'entity': 'I-PER',\n  'index': 1,\n  'score': 0.9997861,\n  'start': 0,\n  'word': '▁Al'},\n {'end': 4,\n  'entity': 'I-PER',\n  'index': 2,\n  'score': 0.9998591,\n  'start': 2,\n  'word': 'ya'},\n {'end': 16,\n  'entity': 'I-PER',\n  'index': 4,\n  'score': 0.99995816,\n  'start': 10,\n  'word': '▁Jasmin'},\n {'end': 17,\n  'entity': 'I-PER',\n  'index': 5,\n  'score': 0.9999584,\n  'start': 16,\n  'word': 'e'},\n {'end': 29,\n  'entity': 'I-PER',\n  'index': 7,\n  'score': 0.99998057,\n  'start': 23,\n  'word': '▁Andrew'}]\n```\n\n## Recommendations\n\nUsers (both direct and downstream) should be made aware of the risks, biases and limitations of the model.\n\n# Training\n\nSee the following resources for training data and training procedure details: \n- [XLM-RoBERTa-large model card](https://huggingface.co/xlm-roberta-large)\n- [CoNLL-2003 data card](https://huggingface.co/datasets/conll2003)\n- [Associated paper](https://arxiv.org/pdf/1911.02116.pdf)\n \n# Evaluation\n\nSee the [associated paper](https://arxiv.org/pdf/1911.02116.pdf) for evaluation details.\n\n# Environmental Impact\n\nCarbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).\n\n- **Hardware Type:** 500 32GB Nvidia V100 GPUs (from the [associated paper](https://arxiv.org/pdf/1911.02116.pdf))\n- **Hours used:** More information needed\n- **Cloud Provider:** More information needed\n- **Compute Region:** More information needed\n- **Carbon Emitted:** More information needed\n\n# Technical Specifications\n\nSee the [associated paper](https://arxiv.org/pdf/1911.02116.pdf) for further details.\n\n# Citation\n\n**BibTeX:**\n\n```bibtex\n@article{conneau2019unsupervised,\n  title={Unsupervised Cross-lingual Representation Learning at Scale},\n  author={Conneau, Alexis and Khandelwal, Kartikay and Goyal, Naman and Chaudhary, Vishrav and Wenzek, Guillaume and Guzm{\\'a}n, Francisco and Grave, Edouard and Ott, Myle and Zettlemoyer, Luke and Stoyanov, Veselin},\n  journal={arXiv preprint arXiv:1911.02116},\n  year={2019}\n}\n```\n\n**APA:**\n- Conneau, A., Khandelwal, K., Goyal, N., Chaudhary, V., Wenzek, G., Guzmán, F., ... & Stoyanov, V. (2019). Unsupervised cross-lingual representation learning at scale. arXiv preprint arXiv:1911.02116.\n\n# Model Card Authors\n\nThis model card was written by the team at Hugging Face.\n\n# How to Get Started with the Model\n\nUse the code below to get started with the model. You can use this model directly within a pipeline for NER.\n\n<details>\n<summary> Click to expand </summary>\n\n```python\n>>> from transformers import AutoTokenizer, AutoModelForTokenClassification\n>>> from transformers import pipeline\n>>> tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-large-finetuned-conll03-english\")\n>>> model = AutoModelForTokenClassification.from_pretrained(\"xlm-roberta-large-finetuned-conll03-english\")\n>>> classifier = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n>>> classifier(\"Hello I'm Omar and I live in Zürich.\")\n\n[{'end': 14,\n  'entity': 'I-PER',\n  'index': 5,\n  'score': 0.9999175,\n  'start': 10,\n  'word': '▁Omar'},\n {'end': 35,\n  'entity': 'I-LOC',\n  'index': 10,\n  'score': 0.9999906,\n  'start': 29,\n  'word': '▁Zürich'}]\n```\n\n</details>",
    "card_content": "---\nlanguage:\n- multilingual\n- af\n- am\n- ar\n- as\n- az\n- be\n- bg\n- bn\n- br\n- bs\n- ca\n- cs\n- cy\n- da\n- de\n- el\n- en\n- eo\n- es\n- et\n- eu\n- fa\n- fi\n- fr\n- fy\n- ga\n- gd\n- gl\n- gu\n- ha\n- he\n- hi\n- hr\n- hu\n- hy\n- id\n- is\n- it\n- ja\n- jv\n- ka\n- kk\n- km\n- kn\n- ko\n- ku\n- ky\n- la\n- lo\n- lt\n- lv\n- mg\n- mk\n- ml\n- mn\n- mr\n- ms\n- my\n- ne\n- nl\n- false\n- om\n- or\n- pa\n- pl\n- ps\n- pt\n- ro\n- ru\n- sa\n- sd\n- si\n- sk\n- sl\n- so\n- sq\n- sr\n- su\n- sv\n- sw\n- ta\n- te\n- th\n- tl\n- tr\n- ug\n- uk\n- ur\n- uz\n- vi\n- xh\n- yi\n- zh\n---\n\n# xlm-roberta-large-finetuned-conll03-english\n\n#  Table of Contents\n\n1. [Model Details](#model-details)\n2. [Uses](#uses)\n3. [Bias, Risks, and Limitations](#bias-risks-and-limitations)\n4. [Training](#training)\n5. [Evaluation](#evaluation)\n6. [Environmental Impact](#environmental-impact)\n7. [Technical Specifications](#technical-specifications)\n8. [Citation](#citation)\n9. [Model Card Authors](#model-card-authors)\n10. [How To Get Started With the Model](#how-to-get-started-with-the-model)\n\n\n# Model Details\n\n## Model Description\n\nThe XLM-RoBERTa model was proposed in [Unsupervised Cross-lingual Representation Learning at Scale](https://arxiv.org/abs/1911.02116) by Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer and Veselin Stoyanov. It is based on Facebook's RoBERTa model released in 2019. It is a large multi-lingual language model, trained on 2.5TB of filtered CommonCrawl data. This model is [XLM-RoBERTa-large](https://huggingface.co/xlm-roberta-large) fine-tuned with the [conll2003](https://huggingface.co/datasets/conll2003) dataset in English.\n\n- **Developed by:** See [associated paper](https://arxiv.org/abs/1911.02116)\n- **Model type:** Multi-lingual language model\n- **Language(s) (NLP) or Countries (images):** XLM-RoBERTa is a multilingual model trained on 100 different languages; see [GitHub Repo](https://github.com/facebookresearch/fairseq/tree/main/examples/xlmr) for full list; model is fine-tuned on a dataset in English\n- **License:** More information needed\n- **Related Models:** [RoBERTa](https://huggingface.co/roberta-base), [XLM](https://huggingface.co/docs/transformers/model_doc/xlm)\n    - **Parent Model:** [XLM-RoBERTa-large](https://huggingface.co/xlm-roberta-large)\n- **Resources for more information:** \n  -[GitHub Repo](https://github.com/facebookresearch/fairseq/tree/main/examples/xlmr)\n  -[Associated Paper](https://arxiv.org/abs/1911.02116)\n\n# Uses\n\n## Direct Use\n\nThe model is a language model. The model can be used for token classification, a natural language understanding task in which a label is assigned to some tokens in a text. \n\n## Downstream Use\n\nPotential downstream use cases include Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. To learn more about token classification and other potential downstream use cases, see the Hugging Face [token classification docs](https://huggingface.co/tasks/token-classification).\n\n## Out-of-Scope Use\n\nThe model should not be used to intentionally create hostile or alienating environments for people. \n\n# Bias, Risks, and Limitations\n\n**CONTENT WARNING: Readers should be made aware that language generated by this model may be disturbing or offensive to some and may propagate historical and current stereotypes.**\n\nSignificant research has explored bias and fairness issues with language models (see, e.g., [Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)). In the context of tasks relevant to this model, [Mishra et al. (2020)](https://arxiv.org/pdf/2008.03415.pdf) explore social biases in NER systems for English and find that there is systematic bias in existing NER systems in that they fail to identify named entities from different demographic groups (though this paper did not look at BERT). For example, using a sample sentence from [Mishra et al. (2020)](https://arxiv.org/pdf/2008.03415.pdf):\n\n```python\n>>> from transformers import pipeline\n>>> tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-large-finetuned-conll03-english\")\n>>> model = AutoModelForTokenClassification.from_pretrained(\"xlm-roberta-large-finetuned-conll03-english\")\n>>> classifier = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n>>> classifier(\"Alya told Jasmine that Andrew could pay with cash..\")\n[{'end': 2,\n  'entity': 'I-PER',\n  'index': 1,\n  'score': 0.9997861,\n  'start': 0,\n  'word': '▁Al'},\n {'end': 4,\n  'entity': 'I-PER',\n  'index': 2,\n  'score': 0.9998591,\n  'start': 2,\n  'word': 'ya'},\n {'end': 16,\n  'entity': 'I-PER',\n  'index': 4,\n  'score': 0.99995816,\n  'start': 10,\n  'word': '▁Jasmin'},\n {'end': 17,\n  'entity': 'I-PER',\n  'index': 5,\n  'score': 0.9999584,\n  'start': 16,\n  'word': 'e'},\n {'end': 29,\n  'entity': 'I-PER',\n  'index': 7,\n  'score': 0.99998057,\n  'start': 23,\n  'word': '▁Andrew'}]\n```\n\n## Recommendations\n\nUsers (both direct and downstream) should be made aware of the risks, biases and limitations of the model.\n\n# Training\n\nSee the following resources for training data and training procedure details: \n- [XLM-RoBERTa-large model card](https://huggingface.co/xlm-roberta-large)\n- [CoNLL-2003 data card](https://huggingface.co/datasets/conll2003)\n- [Associated paper](https://arxiv.org/pdf/1911.02116.pdf)\n \n# Evaluation\n\nSee the [associated paper](https://arxiv.org/pdf/1911.02116.pdf) for evaluation details.\n\n# Environmental Impact\n\nCarbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).\n\n- **Hardware Type:** 500 32GB Nvidia V100 GPUs (from the [associated paper](https://arxiv.org/pdf/1911.02116.pdf))\n- **Hours used:** More information needed\n- **Cloud Provider:** More information needed\n- **Compute Region:** More information needed\n- **Carbon Emitted:** More information needed\n\n# Technical Specifications\n\nSee the [associated paper](https://arxiv.org/pdf/1911.02116.pdf) for further details.\n\n# Citation\n\n**BibTeX:**\n\n```bibtex\n@article{conneau2019unsupervised,\n  title={Unsupervised Cross-lingual Representation Learning at Scale},\n  author={Conneau, Alexis and Khandelwal, Kartikay and Goyal, Naman and Chaudhary, Vishrav and Wenzek, Guillaume and Guzm{\\'a}n, Francisco and Grave, Edouard and Ott, Myle and Zettlemoyer, Luke and Stoyanov, Veselin},\n  journal={arXiv preprint arXiv:1911.02116},\n  year={2019}\n}\n```\n\n**APA:**\n- Conneau, A., Khandelwal, K., Goyal, N., Chaudhary, V., Wenzek, G., Guzmán, F., ... & Stoyanov, V. (2019). Unsupervised cross-lingual representation learning at scale. arXiv preprint arXiv:1911.02116.\n\n# Model Card Authors\n\nThis model card was written by the team at Hugging Face.\n\n# How to Get Started with the Model\n\nUse the code below to get started with the model. You can use this model directly within a pipeline for NER.\n\n<details>\n<summary> Click to expand </summary>\n\n```python\n>>> from transformers import AutoTokenizer, AutoModelForTokenClassification\n>>> from transformers import pipeline\n>>> tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-large-finetuned-conll03-english\")\n>>> model = AutoModelForTokenClassification.from_pretrained(\"xlm-roberta-large-finetuned-conll03-english\")\n>>> classifier = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n>>> classifier(\"Hello I'm Omar and I live in Zürich.\")\n\n[{'end': 14,\n  'entity': 'I-PER',\n  'index': 5,\n  'score': 0.9999175,\n  'start': 10,\n  'word': '▁Omar'},\n {'end': 35,\n  'entity': 'I-LOC',\n  'index': 10,\n  'score': 0.9999906,\n  'start': 29,\n  'word': '▁Zürich'}]\n```\n\n</details>",
    "library_name": "transformers"
  }
]