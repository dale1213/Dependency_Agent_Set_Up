[
  {
    "model_id": "openai-community/gpt2",
    "model_name": "openai-community/gpt2",
    "author": "openai-community",
    "downloads": 17272637,
    "downloads_all_time": null,
    "likes": 2630,
    "tags": [
      "transformers",
      "pytorch",
      "tf",
      "jax",
      "tflite",
      "rust",
      "onnx",
      "safetensors",
      "gpt2",
      "text-generation",
      "exbert",
      "en",
      "doi:10.57967/hf/0039",
      "license:mit",
      "autotrain_compatible",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/openai-community/gpt2",
    "dependencies": [
      [
        "transformers",
        null
      ]
    ],
    "last_modified": "2024-02-19T10:57:45+00:00",
    "created_at": "2022-03-02T23:29:04+00:00",
    "analysis_date": "2025-03-22T00:40:59.848289",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "gpt2",
    "pipeline_tag": "text-generation",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": "en",
      "tags": [
        "exbert"
      ],
      "license": "mit"
    },
    "card_text": "\n\n# GPT-2\n\nTest the whole generation capabilities here: https://transformer.huggingface.co/doc/gpt2-large\n\nPretrained model on English language using a causal language modeling (CLM) objective. It was introduced in\n[this paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\nand first released at [this page](https://openai.com/blog/better-language-models/).\n\nDisclaimer: The team releasing GPT-2 also wrote a\n[model card](https://github.com/openai/gpt-2/blob/master/model_card.md) for their model. Content from this model card\nhas been written by the Hugging Face team to complete the information they provided and give specific examples of bias.\n\n## Model description\n\nGPT-2 is a transformers model pretrained on a very large corpus of English data in a self-supervised fashion. This\nmeans it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots\nof publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely,\nit was trained to guess the next word in sentences.\n\nMore precisely, inputs are sequences of continuous text of a certain length and the targets are the same sequence,\nshifted one token (word or piece of word) to the right. The model uses internally a mask-mechanism to make sure the\npredictions for the token `i` only uses the inputs from `1` to `i` but not the future tokens.\n\nThis way, the model learns an inner representation of the English language that can then be used to extract features\nuseful for downstream tasks. The model is best at what it was pretrained for however, which is generating texts from a\nprompt.\n\nThis is the **smallest** version of GPT-2, with 124M parameters. \n\n**Related Models:** [GPT-Large](https://huggingface.co/gpt2-large), [GPT-Medium](https://huggingface.co/gpt2-medium) and [GPT-XL](https://huggingface.co/gpt2-xl)\n\n## Intended uses & limitations\n\nYou can use the raw model for text generation or fine-tune it to a downstream task. See the\n[model hub](https://huggingface.co/models?filter=gpt2) to look for fine-tuned versions on a task that interests you.\n\n### How to use\n\nYou can use this model directly with a pipeline for text generation. Since the generation relies on some randomness, we\nset a seed for reproducibility:\n\n```python\n>>> from transformers import pipeline, set_seed\n>>> generator = pipeline('text-generation', model='gpt2')\n>>> set_seed(42)\n>>> generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)\n\n[{'generated_text': \"Hello, I'm a language model, a language for thinking, a language for expressing thoughts.\"},\n {'generated_text': \"Hello, I'm a language model, a compiler, a compiler library, I just want to know how I build this kind of stuff. I don\"},\n {'generated_text': \"Hello, I'm a language model, and also have more than a few of your own, but I understand that they're going to need some help\"},\n {'generated_text': \"Hello, I'm a language model, a system model. I want to know my language so that it might be more interesting, more user-friendly\"},\n {'generated_text': 'Hello, I\\'m a language model, not a language model\"\\n\\nThe concept of \"no-tricks\" comes in handy later with new'}]\n```\n\nHere is how to use this model to get the features of a given text in PyTorch:\n\n```python\nfrom transformers import GPT2Tokenizer, GPT2Model\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\nmodel = GPT2Model.from_pretrained('gpt2')\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n```\n\nand in TensorFlow:\n\n```python\nfrom transformers import GPT2Tokenizer, TFGPT2Model\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\nmodel = TFGPT2Model.from_pretrained('gpt2')\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\n```\n\n### Limitations and bias\n\nThe training data used for this model has not been released as a dataset one can browse. We know it contains a lot of\nunfiltered content from the internet, which is far from neutral. As the openAI team themselves point out in their\n[model card](https://github.com/openai/gpt-2/blob/master/model_card.md#out-of-scope-use-cases):\n\n> Because large-scale language models like GPT-2 do not distinguish fact from fiction, we don’t support use-cases\n> that require the generated text to be true.\n>\n> Additionally, language models like GPT-2 reflect the biases inherent to the systems they were trained on, so we do\n> not recommend that they be deployed into systems that interact with humans > unless the deployers first carry out a\n> study of biases relevant to the intended use-case. We found no statistically significant difference in gender, race,\n> and religious bias probes between 774M and 1.5B, implying all versions of GPT-2 should be approached with similar\n> levels of caution around use cases that are sensitive to biases around human attributes.\n\nHere's an example of how the model can have biased predictions:\n\n```python\n>>> from transformers import pipeline, set_seed\n>>> generator = pipeline('text-generation', model='gpt2')\n>>> set_seed(42)\n>>> generator(\"The White man worked as a\", max_length=10, num_return_sequences=5)\n\n[{'generated_text': 'The White man worked as a mannequin for'},\n {'generated_text': 'The White man worked as a maniser of the'},\n {'generated_text': 'The White man worked as a bus conductor by day'},\n {'generated_text': 'The White man worked as a plumber at the'},\n {'generated_text': 'The White man worked as a journalist. He had'}]\n\n>>> set_seed(42)\n>>> generator(\"The Black man worked as a\", max_length=10, num_return_sequences=5)\n\n[{'generated_text': 'The Black man worked as a man at a restaurant'},\n {'generated_text': 'The Black man worked as a car salesman in a'},\n {'generated_text': 'The Black man worked as a police sergeant at the'},\n {'generated_text': 'The Black man worked as a man-eating monster'},\n {'generated_text': 'The Black man worked as a slave, and was'}]\n```\n\nThis bias will also affect all fine-tuned versions of this model.\n\n## Training data\n\nThe OpenAI team wanted to train this model on a corpus as large as possible. To build it, they scraped all the web\npages from outbound links on Reddit which received at least 3 karma. Note that all Wikipedia pages were removed from\nthis dataset, so the model was not trained on any part of Wikipedia. The resulting dataset (called WebText) weights\n40GB of texts but has not been publicly released. You can find a list of the top 1,000 domains present in WebText\n[here](https://github.com/openai/gpt-2/blob/master/domains.txt).\n\n## Training procedure\n\n### Preprocessing\n\nThe texts are tokenized using a byte-level version of Byte Pair Encoding (BPE) (for unicode characters) and a\nvocabulary size of 50,257. The inputs are sequences of 1024 consecutive tokens.\n\nThe larger model was trained on 256 cloud TPU v3 cores. The training duration was not disclosed, nor were the exact\ndetails of training.\n\n## Evaluation results\n\nThe model achieves the following results without any fine-tuning (zero-shot):\n\n| Dataset  | LAMBADA | LAMBADA | CBT-CN | CBT-NE | WikiText2 | PTB    | enwiki8 | text8  | WikiText103 | 1BW   |\n|:--------:|:-------:|:-------:|:------:|:------:|:---------:|:------:|:-------:|:------:|:-----------:|:-----:|\n| (metric) | (PPL)   | (ACC)   | (ACC)  | (ACC)  | (PPL)     | (PPL)  | (BPB)   | (BPC)  | (PPL)       | (PPL) |\n|          | 35.13   | 45.99   | 87.65  | 83.4   | 29.41     | 65.85  | 1.16    | 1,17   | 37.50       | 75.20 |\n\n\n### BibTeX entry and citation info\n\n```bibtex\n@article{radford2019language,\n  title={Language Models are Unsupervised Multitask Learners},\n  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},\n  year={2019}\n}\n```\n\n<a href=\"https://huggingface.co/exbert/?model=gpt2\">\n\t<img width=\"300px\" src=\"https://cdn-media.huggingface.co/exbert/button.png\">\n</a>\n",
    "card_content": "---\nlanguage: en\ntags:\n- exbert\nlicense: mit\n---\n\n\n# GPT-2\n\nTest the whole generation capabilities here: https://transformer.huggingface.co/doc/gpt2-large\n\nPretrained model on English language using a causal language modeling (CLM) objective. It was introduced in\n[this paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\nand first released at [this page](https://openai.com/blog/better-language-models/).\n\nDisclaimer: The team releasing GPT-2 also wrote a\n[model card](https://github.com/openai/gpt-2/blob/master/model_card.md) for their model. Content from this model card\nhas been written by the Hugging Face team to complete the information they provided and give specific examples of bias.\n\n## Model description\n\nGPT-2 is a transformers model pretrained on a very large corpus of English data in a self-supervised fashion. This\nmeans it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots\nof publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely,\nit was trained to guess the next word in sentences.\n\nMore precisely, inputs are sequences of continuous text of a certain length and the targets are the same sequence,\nshifted one token (word or piece of word) to the right. The model uses internally a mask-mechanism to make sure the\npredictions for the token `i` only uses the inputs from `1` to `i` but not the future tokens.\n\nThis way, the model learns an inner representation of the English language that can then be used to extract features\nuseful for downstream tasks. The model is best at what it was pretrained for however, which is generating texts from a\nprompt.\n\nThis is the **smallest** version of GPT-2, with 124M parameters. \n\n**Related Models:** [GPT-Large](https://huggingface.co/gpt2-large), [GPT-Medium](https://huggingface.co/gpt2-medium) and [GPT-XL](https://huggingface.co/gpt2-xl)\n\n## Intended uses & limitations\n\nYou can use the raw model for text generation or fine-tune it to a downstream task. See the\n[model hub](https://huggingface.co/models?filter=gpt2) to look for fine-tuned versions on a task that interests you.\n\n### How to use\n\nYou can use this model directly with a pipeline for text generation. Since the generation relies on some randomness, we\nset a seed for reproducibility:\n\n```python\n>>> from transformers import pipeline, set_seed\n>>> generator = pipeline('text-generation', model='gpt2')\n>>> set_seed(42)\n>>> generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)\n\n[{'generated_text': \"Hello, I'm a language model, a language for thinking, a language for expressing thoughts.\"},\n {'generated_text': \"Hello, I'm a language model, a compiler, a compiler library, I just want to know how I build this kind of stuff. I don\"},\n {'generated_text': \"Hello, I'm a language model, and also have more than a few of your own, but I understand that they're going to need some help\"},\n {'generated_text': \"Hello, I'm a language model, a system model. I want to know my language so that it might be more interesting, more user-friendly\"},\n {'generated_text': 'Hello, I\\'m a language model, not a language model\"\\n\\nThe concept of \"no-tricks\" comes in handy later with new'}]\n```\n\nHere is how to use this model to get the features of a given text in PyTorch:\n\n```python\nfrom transformers import GPT2Tokenizer, GPT2Model\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\nmodel = GPT2Model.from_pretrained('gpt2')\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n```\n\nand in TensorFlow:\n\n```python\nfrom transformers import GPT2Tokenizer, TFGPT2Model\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\nmodel = TFGPT2Model.from_pretrained('gpt2')\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\n```\n\n### Limitations and bias\n\nThe training data used for this model has not been released as a dataset one can browse. We know it contains a lot of\nunfiltered content from the internet, which is far from neutral. As the openAI team themselves point out in their\n[model card](https://github.com/openai/gpt-2/blob/master/model_card.md#out-of-scope-use-cases):\n\n> Because large-scale language models like GPT-2 do not distinguish fact from fiction, we don’t support use-cases\n> that require the generated text to be true.\n>\n> Additionally, language models like GPT-2 reflect the biases inherent to the systems they were trained on, so we do\n> not recommend that they be deployed into systems that interact with humans > unless the deployers first carry out a\n> study of biases relevant to the intended use-case. We found no statistically significant difference in gender, race,\n> and religious bias probes between 774M and 1.5B, implying all versions of GPT-2 should be approached with similar\n> levels of caution around use cases that are sensitive to biases around human attributes.\n\nHere's an example of how the model can have biased predictions:\n\n```python\n>>> from transformers import pipeline, set_seed\n>>> generator = pipeline('text-generation', model='gpt2')\n>>> set_seed(42)\n>>> generator(\"The White man worked as a\", max_length=10, num_return_sequences=5)\n\n[{'generated_text': 'The White man worked as a mannequin for'},\n {'generated_text': 'The White man worked as a maniser of the'},\n {'generated_text': 'The White man worked as a bus conductor by day'},\n {'generated_text': 'The White man worked as a plumber at the'},\n {'generated_text': 'The White man worked as a journalist. He had'}]\n\n>>> set_seed(42)\n>>> generator(\"The Black man worked as a\", max_length=10, num_return_sequences=5)\n\n[{'generated_text': 'The Black man worked as a man at a restaurant'},\n {'generated_text': 'The Black man worked as a car salesman in a'},\n {'generated_text': 'The Black man worked as a police sergeant at the'},\n {'generated_text': 'The Black man worked as a man-eating monster'},\n {'generated_text': 'The Black man worked as a slave, and was'}]\n```\n\nThis bias will also affect all fine-tuned versions of this model.\n\n## Training data\n\nThe OpenAI team wanted to train this model on a corpus as large as possible. To build it, they scraped all the web\npages from outbound links on Reddit which received at least 3 karma. Note that all Wikipedia pages were removed from\nthis dataset, so the model was not trained on any part of Wikipedia. The resulting dataset (called WebText) weights\n40GB of texts but has not been publicly released. You can find a list of the top 1,000 domains present in WebText\n[here](https://github.com/openai/gpt-2/blob/master/domains.txt).\n\n## Training procedure\n\n### Preprocessing\n\nThe texts are tokenized using a byte-level version of Byte Pair Encoding (BPE) (for unicode characters) and a\nvocabulary size of 50,257. The inputs are sequences of 1024 consecutive tokens.\n\nThe larger model was trained on 256 cloud TPU v3 cores. The training duration was not disclosed, nor were the exact\ndetails of training.\n\n## Evaluation results\n\nThe model achieves the following results without any fine-tuning (zero-shot):\n\n| Dataset  | LAMBADA | LAMBADA | CBT-CN | CBT-NE | WikiText2 | PTB    | enwiki8 | text8  | WikiText103 | 1BW   |\n|:--------:|:-------:|:-------:|:------:|:------:|:---------:|:------:|:-------:|:------:|:-----------:|:-----:|\n| (metric) | (PPL)   | (ACC)   | (ACC)  | (ACC)  | (PPL)     | (PPL)  | (BPB)   | (BPC)  | (PPL)       | (PPL) |\n|          | 35.13   | 45.99   | 87.65  | 83.4   | 29.41     | 65.85  | 1.16    | 1,17   | 37.50       | 75.20 |\n\n\n### BibTeX entry and citation info\n\n```bibtex\n@article{radford2019language,\n  title={Language Models are Unsupervised Multitask Learners},\n  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},\n  year={2019}\n}\n```\n\n<a href=\"https://huggingface.co/exbert/?model=gpt2\">\n\t<img width=\"300px\" src=\"https://cdn-media.huggingface.co/exbert/button.png\">\n</a>\n",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModelForCausalLM",
      "custom_class": null,
      "pipeline_tag": "text-generation",
      "processor": "AutoTokenizer"
    },
    "safetensors": {
      "parameters": {
        "F32": 137022720
      },
      "total": 137022720
    },
    "model_index": null,
    "trending_score": null
  },
  {
    "model_id": "facebook/opt-125m",
    "model_name": "facebook/opt-125m",
    "author": "facebook",
    "downloads": 9106207,
    "downloads_all_time": null,
    "likes": 187,
    "tags": [
      "transformers",
      "pytorch",
      "tf",
      "jax",
      "opt",
      "text-generation",
      "en",
      "arxiv:2205.01068",
      "arxiv:2005.14165",
      "license:other",
      "autotrain_compatible",
      "text-generation-inference",
      "region:us"
    ],
    "card_url": "https://huggingface.co/facebook/opt-125m",
    "dependencies": [
      [
        "transformers",
        null
      ]
    ],
    "last_modified": "2023-09-15T13:10:03+00:00",
    "created_at": "2022-05-11T08:25:17+00:00",
    "analysis_date": "2025-03-22T00:41:00.929038",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "opt",
    "pipeline_tag": "text-generation",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": "en",
      "inference": false,
      "tags": [
        "text-generation",
        "opt"
      ],
      "license": "other",
      "commercial": false
    },
    "card_text": "\n# OPT : Open Pre-trained Transformer Language Models\n\nOPT was first introduced in [Open Pre-trained Transformer Language Models](https://arxiv.org/abs/2205.01068) and first released in [metaseq's repository](https://github.com/facebookresearch/metaseq) on May 3rd 2022 by Meta AI.\n\n**Disclaimer**: The team releasing OPT wrote an official model card, which is available in Appendix D of the [paper](https://arxiv.org/pdf/2205.01068.pdf). \nContent from **this** model card has been written by the Hugging Face team.\n\n## Intro\n\nTo quote the first two paragraphs of the [official paper](https://arxiv.org/abs/2205.01068)\n\n\n> Large language models trained on massive text collections have shown surprising emergent\n> capabilities to generate text and perform zero- and few-shot learning. While in some cases the public\n> can interact with these models through paid APIs, full model access is currently limited to only a\n> few highly resourced labs. This restricted access has limited researchers’ ability to study how and\n> why these large language models work, hindering progress on improving known challenges in areas\n> such as robustness, bias, and toxicity.\n\n> We present Open Pretrained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M\n> to 175B parameters, which we aim to fully and responsibly share with interested researchers. We train the OPT models to roughly match \n> the performance and sizes of the GPT-3 class of models, while also applying the latest best practices in data\n> collection and efficient training. Our aim in developing this suite of OPT models is to enable reproducible and responsible research at scale, and\n> to bring more voices to the table in studying the impact of these LLMs. Definitions of risk, harm, bias, and toxicity, etc., should be articulated by the\n> collective research community as a whole, which is only possible when models are available for study.\n\n## Model description\n\nOPT was predominantly pretrained with English text, but a small amount of non-English data is still present within the training corpus via CommonCrawl. The model was pretrained using a causal language modeling (CLM) objective.\nOPT belongs to the same family of decoder-only models like [GPT-3](https://arxiv.org/abs/2005.14165). As such, it was pretrained using the self-supervised causal language modedling objective.\n\nFor evaluation, OPT follows [GPT-3](https://arxiv.org/abs/2005.14165) by using their prompts and overall experimental setup. For more details, please read \nthe [official paper](https://arxiv.org/abs/2205.01068).\n## Intended uses & limitations\n\nThe pretrained-only model can be used for prompting for evaluation of downstream tasks as well as text generation.\nIn addition, the model can be fine-tuned on a downstream task using the [CLM example](https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling). For all other OPT checkpoints, please have a look at the [model hub](https://huggingface.co/models?filter=opt).\n\n### How to use\n\nYou can use this model directly with a pipeline for text generation.\n\n```python\n>>> from transformers import pipeline\n\n>>> generator = pipeline('text-generation', model=\"facebook/opt-125m\")\n>>> generator(\"What are we having for dinner?\")\n[{'generated_text': 'What are we having for dinner?\\nA nice dinner with a friend.\\nI'm not sure'}]\n```\n\nBy default, generation is deterministic. In order to use the top-k sampling, please set `do_sample` to `True`. \n\n```python\n>>> from transformers import pipeline, set_seed\n\n>>> set_seed(32)\n>>> generator = pipeline('text-generation', model=\"facebook/opt-125m\", do_sample=True)\n>>> generator(\"What are we having for dinner?\")\n[{'generated_text': 'What are we having for dinner?\\nCoffee, sausage and cream cheese at Chili's.'}]\n```\n\n### Limitations and bias\n\nAs mentioned in Meta AI's model card, given that the training data used for this model contains a lot of\nunfiltered content from the internet, which is far from neutral the model is strongly biased : \n\n> Like other large language models for which the diversity (or lack thereof) of training\n> data induces downstream impact on the quality of our model, OPT-175B has limitations in terms\n> of bias and safety. OPT-175B can also have quality issues in terms of generation diversity and\n> hallucination. In general, OPT-175B is not immune from the plethora of issues that plague modern\n> large language models. \n\nThis bias will also affect all fine-tuned versions of this model.\n\n## Training data\n\nThe Meta AI team wanted to train this model on a corpus as large as possible. It is composed of the union of the following 5 filtered datasets of textual documents: \n\n  - BookCorpus, which consists of more than 10K unpublished books,\n  - CC-Stories, which contains a subset of CommonCrawl data filtered to match the\nstory-like style of Winograd schemas,\n  - The Pile, from which * Pile-CC, OpenWebText2, USPTO, Project Gutenberg, OpenSubtitles, Wikipedia, DM Mathematics and HackerNews* were included. \n  - Pushshift.io Reddit dataset that was developed in Baumgartner et al. (2020) and processed in\nRoller et al. (2021)\n  - CCNewsV2 containing an updated version of the English portion of the CommonCrawl News\ndataset that was used in RoBERTa (Liu et al., 2019b)\n\nThe final training data contains 180B tokens corresponding to 800GB of data. The validation split was made of 200MB of the pretraining data, sampled proportionally\nto each dataset’s size in the pretraining corpus. \n\nThe dataset might contains offensive content as parts of the dataset are a subset of\npublic Common Crawl data, along with a subset of public Reddit data, which could contain sentences\nthat, if viewed directly, can be insulting, threatening, or might otherwise cause anxiety.\n\n### Collection process\n\nThe dataset was collected form internet, and went through classic data processing algorithms  and\nre-formatting practices, including removing repetitive/non-informative text like *Chapter One* or\n*This ebook by Project Gutenberg.*\n\n## Training procedure\n\n\n\n### Preprocessing\n\nThe texts are tokenized using the **GPT2** byte-level version of Byte Pair Encoding (BPE) (for unicode characters) and a\nvocabulary size of 50272. The inputs are sequences of 2048 consecutive tokens.\n\nThe 175B model was trained on 992 *80GB A100 GPUs*. The training duration was roughly ~33 days of continuous training.\n\n### BibTeX entry and citation info\n\n```bibtex\n@misc{zhang2022opt,\n      title={OPT: Open Pre-trained Transformer Language Models}, \n      author={Susan Zhang and Stephen Roller and Naman Goyal and Mikel Artetxe and Moya Chen and Shuohui Chen and Christopher Dewan and Mona Diab and Xian Li and Xi Victoria Lin and Todor Mihaylov and Myle Ott and Sam Shleifer and Kurt Shuster and Daniel Simig and Punit Singh Koura and Anjali Sridhar and Tianlu Wang and Luke Zettlemoyer},\n      year={2022},\n      eprint={2205.01068},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```",
    "card_content": "---\nlanguage: en\ninference: false\ntags:\n- text-generation\n- opt\nlicense: other\ncommercial: false\n---\n\n# OPT : Open Pre-trained Transformer Language Models\n\nOPT was first introduced in [Open Pre-trained Transformer Language Models](https://arxiv.org/abs/2205.01068) and first released in [metaseq's repository](https://github.com/facebookresearch/metaseq) on May 3rd 2022 by Meta AI.\n\n**Disclaimer**: The team releasing OPT wrote an official model card, which is available in Appendix D of the [paper](https://arxiv.org/pdf/2205.01068.pdf). \nContent from **this** model card has been written by the Hugging Face team.\n\n## Intro\n\nTo quote the first two paragraphs of the [official paper](https://arxiv.org/abs/2205.01068)\n\n\n> Large language models trained on massive text collections have shown surprising emergent\n> capabilities to generate text and perform zero- and few-shot learning. While in some cases the public\n> can interact with these models through paid APIs, full model access is currently limited to only a\n> few highly resourced labs. This restricted access has limited researchers’ ability to study how and\n> why these large language models work, hindering progress on improving known challenges in areas\n> such as robustness, bias, and toxicity.\n\n> We present Open Pretrained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M\n> to 175B parameters, which we aim to fully and responsibly share with interested researchers. We train the OPT models to roughly match \n> the performance and sizes of the GPT-3 class of models, while also applying the latest best practices in data\n> collection and efficient training. Our aim in developing this suite of OPT models is to enable reproducible and responsible research at scale, and\n> to bring more voices to the table in studying the impact of these LLMs. Definitions of risk, harm, bias, and toxicity, etc., should be articulated by the\n> collective research community as a whole, which is only possible when models are available for study.\n\n## Model description\n\nOPT was predominantly pretrained with English text, but a small amount of non-English data is still present within the training corpus via CommonCrawl. The model was pretrained using a causal language modeling (CLM) objective.\nOPT belongs to the same family of decoder-only models like [GPT-3](https://arxiv.org/abs/2005.14165). As such, it was pretrained using the self-supervised causal language modedling objective.\n\nFor evaluation, OPT follows [GPT-3](https://arxiv.org/abs/2005.14165) by using their prompts and overall experimental setup. For more details, please read \nthe [official paper](https://arxiv.org/abs/2205.01068).\n## Intended uses & limitations\n\nThe pretrained-only model can be used for prompting for evaluation of downstream tasks as well as text generation.\nIn addition, the model can be fine-tuned on a downstream task using the [CLM example](https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling). For all other OPT checkpoints, please have a look at the [model hub](https://huggingface.co/models?filter=opt).\n\n### How to use\n\nYou can use this model directly with a pipeline for text generation.\n\n```python\n>>> from transformers import pipeline\n\n>>> generator = pipeline('text-generation', model=\"facebook/opt-125m\")\n>>> generator(\"What are we having for dinner?\")\n[{'generated_text': 'What are we having for dinner?\\nA nice dinner with a friend.\\nI'm not sure'}]\n```\n\nBy default, generation is deterministic. In order to use the top-k sampling, please set `do_sample` to `True`. \n\n```python\n>>> from transformers import pipeline, set_seed\n\n>>> set_seed(32)\n>>> generator = pipeline('text-generation', model=\"facebook/opt-125m\", do_sample=True)\n>>> generator(\"What are we having for dinner?\")\n[{'generated_text': 'What are we having for dinner?\\nCoffee, sausage and cream cheese at Chili's.'}]\n```\n\n### Limitations and bias\n\nAs mentioned in Meta AI's model card, given that the training data used for this model contains a lot of\nunfiltered content from the internet, which is far from neutral the model is strongly biased : \n\n> Like other large language models for which the diversity (or lack thereof) of training\n> data induces downstream impact on the quality of our model, OPT-175B has limitations in terms\n> of bias and safety. OPT-175B can also have quality issues in terms of generation diversity and\n> hallucination. In general, OPT-175B is not immune from the plethora of issues that plague modern\n> large language models. \n\nThis bias will also affect all fine-tuned versions of this model.\n\n## Training data\n\nThe Meta AI team wanted to train this model on a corpus as large as possible. It is composed of the union of the following 5 filtered datasets of textual documents: \n\n  - BookCorpus, which consists of more than 10K unpublished books,\n  - CC-Stories, which contains a subset of CommonCrawl data filtered to match the\nstory-like style of Winograd schemas,\n  - The Pile, from which * Pile-CC, OpenWebText2, USPTO, Project Gutenberg, OpenSubtitles, Wikipedia, DM Mathematics and HackerNews* were included. \n  - Pushshift.io Reddit dataset that was developed in Baumgartner et al. (2020) and processed in\nRoller et al. (2021)\n  - CCNewsV2 containing an updated version of the English portion of the CommonCrawl News\ndataset that was used in RoBERTa (Liu et al., 2019b)\n\nThe final training data contains 180B tokens corresponding to 800GB of data. The validation split was made of 200MB of the pretraining data, sampled proportionally\nto each dataset’s size in the pretraining corpus. \n\nThe dataset might contains offensive content as parts of the dataset are a subset of\npublic Common Crawl data, along with a subset of public Reddit data, which could contain sentences\nthat, if viewed directly, can be insulting, threatening, or might otherwise cause anxiety.\n\n### Collection process\n\nThe dataset was collected form internet, and went through classic data processing algorithms  and\nre-formatting practices, including removing repetitive/non-informative text like *Chapter One* or\n*This ebook by Project Gutenberg.*\n\n## Training procedure\n\n\n\n### Preprocessing\n\nThe texts are tokenized using the **GPT2** byte-level version of Byte Pair Encoding (BPE) (for unicode characters) and a\nvocabulary size of 50272. The inputs are sequences of 2048 consecutive tokens.\n\nThe 175B model was trained on 992 *80GB A100 GPUs*. The training duration was roughly ~33 days of continuous training.\n\n### BibTeX entry and citation info\n\n```bibtex\n@misc{zhang2022opt,\n      title={OPT: Open Pre-trained Transformer Language Models}, \n      author={Susan Zhang and Stephen Roller and Naman Goyal and Mikel Artetxe and Moya Chen and Shuohui Chen and Christopher Dewan and Mona Diab and Xian Li and Xi Victoria Lin and Todor Mihaylov and Myle Ott and Sam Shleifer and Kurt Shuster and Daniel Simig and Punit Singh Koura and Anjali Sridhar and Tianlu Wang and Luke Zettlemoyer},\n      year={2022},\n      eprint={2205.01068},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModelForCausalLM",
      "custom_class": null,
      "pipeline_tag": "text-generation",
      "processor": "AutoTokenizer"
    },
    "safetensors": null,
    "model_index": null,
    "trending_score": null
  },
  {
    "model_id": "meta-llama/Llama-3.1-8B-Instruct",
    "model_name": "meta-llama/Llama-3.1-8B-Instruct",
    "author": "meta-llama",
    "downloads": 5795728,
    "downloads_all_time": null,
    "likes": 3764,
    "tags": [
      "transformers",
      "safetensors",
      "llama",
      "text-generation",
      "facebook",
      "meta",
      "pytorch",
      "llama-3",
      "conversational",
      "en",
      "de",
      "fr",
      "it",
      "pt",
      "hi",
      "es",
      "th",
      "arxiv:2204.05149",
      "base_model:meta-llama/Llama-3.1-8B",
      "base_model:finetune:meta-llama/Llama-3.1-8B",
      "license:llama3.1",
      "autotrain_compatible",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct",
    "dependencies": null,
    "last_modified": "2024-09-25T17:00:57+00:00",
    "created_at": "2024-07-18T08:56:00+00:00",
    "analysis_date": "2025-03-22T00:41:04.035909",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "llama",
    "pipeline_tag": "text-generation",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": [
        "en",
        "de",
        "fr",
        "it",
        "pt",
        "hi",
        "es",
        "th"
      ],
      "license": "llama3.1",
      "base_model": "meta-llama/Meta-Llama-3.1-8B",
      "pipeline_tag": "text-generation",
      "tags": [
        "facebook",
        "meta",
        "pytorch",
        "llama",
        "llama-3"
      ],
      "extra_gated_prompt": "### LLAMA 3.1 COMMUNITY LICENSE AGREEMENT\nLlama 3.1 Version Release Date: July 23, 2024\n\"Agreement\" means the terms and conditions for use, reproduction, distribution and modification of the  Llama Materials set forth herein.\n\"Documentation\" means the specifications, manuals and documentation accompanying Llama 3.1 distributed by Meta at https://llama.meta.com/doc/overview.\n\"Licensee\" or \"you\" means you, or your employer or any other person or entity (if you are entering into this Agreement on such person or entity’s behalf), of the age required under applicable laws, rules or regulations to provide legal consent and that has legal authority to bind your employer or such other person or entity if you are entering in this Agreement on their behalf.\n\"Llama 3.1\" means the foundational large language models and software and algorithms, including machine-learning model code, trained model weights, inference-enabling code, training-enabling code, fine-tuning enabling code and other elements of the foregoing distributed by Meta at https://llama.meta.com/llama-downloads.\n\"Llama Materials\" means, collectively, Meta’s proprietary Llama 3.1 and Documentation (and any portion thereof) made available under this Agreement.\n\"Meta\" or \"we\" means Meta Platforms Ireland Limited (if you are located in or, if you are an entity, your principal place of business is in the EEA or Switzerland) and Meta Platforms, Inc. (if you are located outside of the EEA or Switzerland).\n   \n1. License Rights and Redistribution.\na. Grant of Rights. You are granted a non-exclusive, worldwide, non-transferable and royalty-free limited license under Meta’s intellectual property or other rights owned by Meta embodied in the Llama Materials to use, reproduce, distribute, copy, create derivative works of, and make modifications to the Llama Materials.\nb. Redistribution and Use.\ni. If you distribute or make available the Llama Materials (or any derivative works thereof), or a product or service (including another AI model) that contains any of them, you shall (A) provide a copy of this Agreement with any such Llama Materials; and (B) prominently display “Built with Llama” on a related website, user interface, blogpost, about page, or product documentation. If you use the Llama Materials or any outputs or results of the Llama Materials to create, train, fine tune, or otherwise improve an AI model, which is distributed or made available, you shall also include “Llama” at the beginning of any such AI model name.\nii. If you receive Llama Materials, or any derivative works thereof, from a Licensee as part  of an integrated end user product, then Section 2 of this Agreement will not apply to you.\niii. You must retain in all copies of the Llama Materials that you distribute the following attribution notice within a “Notice” text file distributed as a part of such copies: “Llama 3.1 is licensed under the Llama 3.1 Community License, Copyright © Meta Platforms, Inc. All Rights Reserved.”\niv. Your use of the Llama Materials must comply with applicable laws and regulations (including trade compliance laws and regulations) and adhere to the Acceptable Use Policy for the Llama Materials (available at https://llama.meta.com/llama3_1/use-policy), which is hereby incorporated by reference into this Agreement.\n2. Additional Commercial Terms. If, on the Llama 3.1 version release date, the monthly active users of the products or services made available by or for Licensee, or Licensee’s affiliates, is greater than 700 million monthly active users in the preceding calendar month, you must request a license from Meta, which Meta may grant to you in its sole discretion, and you are not authorized to exercise any of the rights under this Agreement unless or until Meta otherwise expressly grants you such rights.\n3. Disclaimer of Warranty. UNLESS REQUIRED BY APPLICABLE LAW, THE LLAMA MATERIALS AND ANY OUTPUT AND RESULTS THEREFROM ARE PROVIDED ON AN “AS IS” BASIS, WITHOUT WARRANTIES OF ANY KIND, AND META DISCLAIMS ALL WARRANTIES OF ANY KIND, BOTH EXPRESS AND IMPLIED, INCLUDING, WITHOUT LIMITATION, ANY WARRANTIES OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE. YOU ARE SOLELY RESPONSIBLE FOR DETERMINING THE APPROPRIATENESS OF USING OR REDISTRIBUTING THE LLAMA MATERIALS AND ASSUME ANY RISKS ASSOCIATED WITH YOUR USE OF THE LLAMA MATERIALS AND ANY OUTPUT AND RESULTS.\n4. Limitation of Liability. IN NO EVENT WILL META OR ITS AFFILIATES BE LIABLE UNDER ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, TORT, NEGLIGENCE, PRODUCTS LIABILITY, OR OTHERWISE, ARISING OUT OF THIS AGREEMENT, FOR ANY LOST PROFITS OR ANY INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL, EXEMPLARY OR PUNITIVE DAMAGES, EVEN IF META OR ITS AFFILIATES HAVE BEEN ADVISED OF THE POSSIBILITY OF ANY OF THE FOREGOING.\n5. Intellectual Property.\na. No trademark licenses are granted under this Agreement, and in connection with the Llama Materials, neither Meta nor Licensee may use any name or mark owned by or associated with the other or any of its affiliates, except as required for reasonable and customary use in describing and redistributing the Llama Materials or as set forth in this Section 5(a). Meta hereby grants you a license to use “Llama” (the “Mark”) solely as required to comply with the last sentence of Section 1.b.i. You will comply with Meta’s brand guidelines (currently accessible at https://about.meta.com/brand/resources/meta/company-brand/ ). All goodwill arising out of your use of the Mark will inure to the benefit of Meta.\nb. Subject to Meta’s ownership of Llama Materials and derivatives made by or for Meta, with respect to any derivative works and modifications of the Llama Materials that are made by you, as between you and Meta, you are and will be the owner of such derivative works and modifications.\nc. If you institute litigation or other proceedings against Meta or any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Llama Materials or Llama 3.1 outputs or results, or any portion of any of the foregoing, constitutes infringement of intellectual property or other rights owned or licensable by you, then any licenses granted to you under this Agreement shall terminate as of the date such litigation or claim is filed or instituted. You will indemnify and hold harmless Meta from and against any claim by any third party arising out of or related to your use or distribution of the Llama Materials.\n6. Term and Termination. The term of this Agreement will commence upon your acceptance of this Agreement or access to the Llama Materials and will continue in full force and effect until terminated in accordance with the terms and conditions herein. Meta may terminate this Agreement if you are in breach of any term or condition of this Agreement. Upon termination of this Agreement, you shall delete and cease use of the Llama Materials. Sections 3, 4 and 7 shall survive the termination of this Agreement.\n7. Governing Law and Jurisdiction. This Agreement will be governed and construed under the laws of the State of California without regard to choice of law principles, and the UN Convention on Contracts for the International Sale of Goods does not apply to this Agreement. The courts of California shall have exclusive jurisdiction of any dispute arising out of this Agreement.\n### Llama 3.1 Acceptable Use Policy\nMeta is committed to promoting safe and fair use of its tools and features, including Llama 3.1. If you access or use Llama 3.1, you agree to this Acceptable Use Policy (“Policy”). The most recent copy of this policy can be found at [https://llama.meta.com/llama3_1/use-policy](https://llama.meta.com/llama3_1/use-policy)\n#### Prohibited Uses\nWe want everyone to use Llama 3.1 safely and responsibly. You agree you will not use, or allow others to use, Llama 3.1 to:\n 1. Violate the law or others’ rights, including to:\n    1. Engage in, promote, generate, contribute to, encourage, plan, incite, or further illegal or unlawful activity or content, such as:\n        1. Violence or terrorism\n        2. Exploitation or harm to children, including the solicitation, creation, acquisition, or dissemination of child exploitative content or failure to report Child Sexual Abuse Material\n        3. Human trafficking, exploitation, and sexual violence\n        4. The illegal distribution of information or materials to minors, including obscene materials, or failure to employ legally required age-gating in connection with such information or materials.\n        5. Sexual solicitation\n        6. Any other criminal activity\n    3. Engage in, promote, incite, or facilitate the harassment, abuse, threatening, or bullying of individuals or groups of individuals\n    4. Engage in, promote, incite, or facilitate discrimination or other unlawful or harmful conduct in the provision of employment, employment benefits, credit, housing, other economic benefits, or other essential goods and services\n    5. Engage in the unauthorized or unlicensed practice of any profession including, but not limited to, financial, legal, medical/health, or related professional practices\n    6. Collect, process, disclose, generate, or infer health, demographic, or other sensitive personal or private information about individuals without rights and consents required by applicable laws\n    7. Engage in or facilitate any action or generate any content that infringes, misappropriates, or otherwise violates any third-party rights, including the outputs or results of any products or services using the Llama Materials\n    8. Create, generate, or facilitate the creation of malicious code, malware, computer viruses or do anything else that could disable, overburden, interfere with or impair the proper working, integrity, operation or appearance of a website or computer system\n2. Engage in, promote, incite, facilitate, or assist in the planning or development of activities that present a risk of death or bodily harm to individuals, including use of Llama 3.1 related to the following:\n    1. Military, warfare, nuclear industries or applications, espionage, use for materials or activities that are subject to the International Traffic Arms Regulations (ITAR) maintained by the United States Department of State\n    2. Guns and illegal weapons (including weapon development)\n    3. Illegal drugs and regulated/controlled substances\n    4. Operation of critical infrastructure, transportation technologies, or heavy machinery\n    5. Self-harm or harm to others, including suicide, cutting, and eating disorders\n    6. Any content intended to incite or promote violence, abuse, or any infliction of bodily harm to an individual\n3. Intentionally deceive or mislead others, including use of Llama 3.1 related to the following:\n    1. Generating, promoting, or furthering fraud or the creation or promotion of disinformation\n    2. Generating, promoting, or furthering defamatory content, including the creation of defamatory statements, images, or other content\n    3. Generating, promoting, or further distributing spam\n    4. Impersonating another individual without consent, authorization, or legal right\n    5. Representing that the use of Llama 3.1 or outputs are human-generated\n    6. Generating or facilitating false online engagement, including fake reviews and other means of fake online engagement\n4. Fail to appropriately disclose to end users any known dangers of your AI system\nPlease report any violation of this Policy, software “bug,” or other problems that could lead to a violation of this Policy through one of the following means:\n    * Reporting issues with the model: [https://github.com/meta-llama/llama-models/issues](https://github.com/meta-llama/llama-models/issues)\n    * Reporting risky content generated by the model:\n    developers.facebook.com/llama_output_feedback\n    * Reporting bugs and security concerns: facebook.com/whitehat/info\n    * Reporting violations of the Acceptable Use Policy or unlicensed uses of Meta Llama 3: LlamaUseReport@meta.com",
      "extra_gated_fields": {
        "First Name": "text",
        "Last Name": "text",
        "Date of birth": "date_picker",
        "Country": "country",
        "Affiliation": "text",
        "Job title": {
          "type": "select",
          "options": [
            "Student",
            "Research Graduate",
            "AI researcher",
            "AI developer/engineer",
            "Reporter",
            "Other"
          ]
        },
        "geo": "ip_location",
        "By clicking Submit below I accept the terms of the license and acknowledge that the information I provide will be collected stored processed and shared in accordance with the Meta Privacy Policy": "checkbox"
      },
      "extra_gated_description": "The information you provide will be collected, stored, processed and shared in accordance with the [Meta Privacy Policy](https://www.facebook.com/privacy/policy/).",
      "extra_gated_button_content": "Submit"
    },
    "card_text": "\n## Model Information\n\nThe Meta Llama 3.1 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction tuned generative models in 8B, 70B and 405B sizes (text in/text out). The Llama 3.1 instruction tuned text only models (8B, 70B, 405B) are optimized for multilingual dialogue use cases and outperform many of the available open source and closed chat models on common industry benchmarks.\n\n**Model developer**: Meta\n\n**Model Architecture:** Llama 3.1 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety. \n\n\n<table>\n  <tr>\n   <td>\n   </td>\n   <td><strong>Training Data</strong>\n   </td>\n   <td><strong>Params</strong>\n   </td>\n   <td><strong>Input modalities</strong>\n   </td>\n   <td><strong>Output modalities</strong>\n   </td>\n   <td><strong>Context length</strong>\n   </td>\n   <td><strong>GQA</strong>\n   </td>\n   <td><strong>Token count</strong>\n   </td>\n   <td><strong>Knowledge cutoff</strong>\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"3\" >Llama 3.1 (text only)\n   </td>\n   <td rowspan=\"3\" >A new mix of publicly available online data.\n   </td>\n   <td>8B\n   </td>\n   <td>Multilingual Text\n   </td>\n   <td>Multilingual Text and code\n   </td>\n   <td>128k\n   </td>\n   <td>Yes\n   </td>\n   <td rowspan=\"3\" >15T+\n   </td>\n   <td rowspan=\"3\" >December 2023\n   </td>\n  </tr>\n  <tr>\n   <td>70B\n   </td>\n   <td>Multilingual Text\n   </td>\n   <td>Multilingual Text and code\n   </td>\n   <td>128k\n   </td>\n   <td>Yes\n   </td>\n  </tr>\n  <tr>\n   <td>405B\n   </td>\n   <td>Multilingual Text\n   </td>\n   <td>Multilingual Text and code\n   </td>\n   <td>128k\n   </td>\n   <td>Yes\n   </td>\n  </tr>\n</table>\n\n\n**Supported languages:** English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai.\n\n**Llama 3.1 family of models**. Token counts refer to pretraining data only. All model versions use Grouped-Query Attention (GQA) for improved inference scalability.\n\n**Model Release Date:** July 23, 2024.\n\n**Status:** This is a static model trained on an offline dataset. Future versions of the tuned models will be released as we improve model safety with community feedback.\n\n**License:** A custom commercial license, the Llama 3.1 Community License, is available at: [https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/LICENSE](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/LICENSE)\n\nWhere to send questions or comments about the model Instructions on how to provide feedback or comments on the model can be found in the model [README](https://github.com/meta-llama/llama3). For more technical information about generation parameters and recipes for how to use Llama 3.1 in applications, please go [here](https://github.com/meta-llama/llama-recipes). \n\n\n## Intended Use\n\n**Intended Use Cases** Llama 3.1 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks. The Llama 3.1 model collection also supports the ability to leverage the outputs of its models to improve other models including synthetic data generation and distillation. The Llama 3.1 Community License allows for these use cases. \n\n**Out-of-scope** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.1 Community License. Use in languages beyond those explicitly referenced as supported in this model card**.\n\n**<span style=\"text-decoration:underline;\">Note</span>: Llama 3.1 has been trained on a broader collection of languages than the 8 supported languages. Developers may fine-tune Llama 3.1 models for languages beyond the 8 supported languages provided they comply with the Llama 3.1 Community License and the Acceptable Use Policy and in such cases are responsible for ensuring that any uses of Llama 3.1 in additional languages is done in a safe and responsible manner.\n\n## How to use\n\nThis repository contains two versions of Meta-Llama-3.1-8B-Instruct, for use with transformers and with the original `llama` codebase.\n\n### Use with transformers\n\nStarting with `transformers >= 4.43.0` onward, you can run conversational inference using the Transformers `pipeline` abstraction or by leveraging the Auto classes with the `generate()` function.\n\nMake sure to update your transformers installation via `pip install --upgrade transformers`.\n\n```python\nimport transformers\nimport torch\n\nmodel_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model_id,\n    model_kwargs={\"torch_dtype\": torch.bfloat16},\n    device_map=\"auto\",\n)\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n    {\"role\": \"user\", \"content\": \"Who are you?\"},\n]\n\noutputs = pipeline(\n    messages,\n    max_new_tokens=256,\n)\nprint(outputs[0][\"generated_text\"][-1])\n```\n\nNote: You can also find detailed recipes on how to use the model locally, with `torch.compile()`, assisted generations, quantised and more at [`huggingface-llama-recipes`](https://github.com/huggingface/huggingface-llama-recipes)\n\n### Tool use with transformers\n\nLLaMA-3.1 supports multiple tool use formats. You can see a full guide to prompt formatting [here](https://llama.meta.com/docs/model-cards-and-prompt-formats/llama3_1/).\n\nTool use is also supported through [chat templates](https://huggingface.co/docs/transformers/main/chat_templating#advanced-tool-use--function-calling) in Transformers. \nHere is a quick example showing a single simple tool:\n\n```python\n# First, define a tool\ndef get_current_temperature(location: str) -> float:\n    \"\"\"\n    Get the current temperature at a location.\n    \n    Args:\n        location: The location to get the temperature for, in the format \"City, Country\"\n    Returns:\n        The current temperature at the specified location in the specified units, as a float.\n    \"\"\"\n    return 22.  # A real function should probably actually get the temperature!\n\n# Next, create a chat and apply the chat template\nmessages = [\n  {\"role\": \"system\", \"content\": \"You are a bot that responds to weather queries.\"},\n  {\"role\": \"user\", \"content\": \"Hey, what's the temperature in Paris right now?\"}\n]\n\ninputs = tokenizer.apply_chat_template(messages, tools=[get_current_temperature], add_generation_prompt=True)\n```\n\nYou can then generate text from this input as normal. If the model generates a tool call, you should add it to the chat like so:\n\n```python\ntool_call = {\"name\": \"get_current_temperature\", \"arguments\": {\"location\": \"Paris, France\"}}\nmessages.append({\"role\": \"assistant\", \"tool_calls\": [{\"type\": \"function\", \"function\": tool_call}]})\n```\n\nand then call the tool and append the result, with the `tool` role, like so:\n\n```python\nmessages.append({\"role\": \"tool\", \"name\": \"get_current_temperature\", \"content\": \"22.0\"})\n```\n\nAfter that, you can `generate()` again to let the model use the tool result in the chat. Note that this was a very brief introduction to tool calling - for more information,\nsee the [LLaMA prompt format docs](https://llama.meta.com/docs/model-cards-and-prompt-formats/llama3_1/) and the Transformers [tool use documentation](https://huggingface.co/docs/transformers/main/chat_templating#advanced-tool-use--function-calling).\n\n\n### Use with `llama`\n\nPlease, follow the instructions in the [repository](https://github.com/meta-llama/llama)\n\nTo download Original checkpoints, see the example command below leveraging `huggingface-cli`:\n\n```\nhuggingface-cli download meta-llama/Meta-Llama-3.1-8B-Instruct --include \"original/*\" --local-dir Meta-Llama-3.1-8B-Instruct\n```\n\n## Hardware and Software\n\n**Training Factors** We used custom training libraries, Meta's custom built GPU cluster, and production infrastructure for pretraining. Fine-tuning, annotation, and evaluation were also performed on production infrastructure.\n\n**Training utilized a cumulative of** 39.3M GPU hours of computation on H100-80GB (TDP of 700W) type hardware, per the table below. Training time is the total GPU time required for training each model and power consumption is the peak power capacity per GPU device used, adjusted for power usage efficiency. \n\n\n**Training Greenhouse Gas Emissions** Estimated total location-based greenhouse gas emissions were **11,390** tons CO2eq for training. Since 2020, Meta has maintained net zero greenhouse gas emissions in its global operations and matched 100% of its electricity use with renewable energy, therefore the total market-based greenhouse gas emissions for training were 0 tons CO2eq.\n\n\n<table>\n  <tr>\n   <td>\n   </td>\n   <td><strong>Training Time (GPU hours)</strong>\n   </td>\n   <td><strong>Training Power Consumption (W)</strong>\n   </td>\n   <td><strong>Training Location-Based Greenhouse Gas Emissions</strong>\n<p>\n<strong>(tons CO2eq)</strong>\n   </td>\n   <td><strong>Training Market-Based Greenhouse Gas Emissions</strong>\n<p>\n<strong>(tons CO2eq)</strong>\n   </td>\n  </tr>\n  <tr>\n   <td>Llama 3.1 8B\n   </td>\n   <td>1.46M\n   </td>\n   <td>700\n   </td>\n   <td>420\n   </td>\n   <td>0\n   </td>\n  </tr>\n  <tr>\n   <td>Llama 3.1 70B\n   </td>\n   <td>7.0M\n   </td>\n   <td>700\n   </td>\n   <td>2,040\n   </td>\n   <td>0\n   </td>\n  </tr>\n  <tr>\n   <td>Llama 3.1 405B\n   </td>\n   <td>30.84M\n   </td>\n   <td>700\n   </td>\n   <td>8,930\n   </td>\n   <td>0\n   </td>\n  </tr>\n  <tr>\n   <td>Total\n   </td>\n   <td>39.3M\n   <td>\n<ul>\n\n</ul>\n   </td>\n   <td>11,390\n   </td>\n   <td>0\n   </td>\n  </tr>\n</table>\n\n\n\nThe methodology used to determine training energy use and greenhouse gas emissions can be found [here](https://arxiv.org/pdf/2204.05149).  Since Meta is openly releasing these models, the training energy use and greenhouse gas emissions  will not be incurred by others.\n\n\n## Training Data\n\n**Overview:** Llama 3.1 was pretrained on ~15 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over 25M synthetically generated examples. \n\n**Data Freshness:** The pretraining data has a cutoff of December 2023.\n\n\n## Benchmark scores\n\nIn this section, we report the results for Llama 3.1 models on standard automatic benchmarks. For all the evaluations, we use our internal evaluations library. \n\n### Base pretrained models\n\n\n<table>\n  <tr>\n   <td><strong>Category</strong>\n   </td>\n   <td><strong>Benchmark</strong>\n   </td>\n   <td><strong># Shots</strong>\n   </td>\n   <td><strong>Metric</strong>\n   </td>\n   <td><strong>Llama 3 8B</strong>\n   </td>\n   <td><strong>Llama 3.1 8B</strong>\n   </td>\n   <td><strong>Llama 3 70B</strong>\n   </td>\n   <td><strong>Llama 3.1 70B</strong>\n   </td>\n   <td><strong>Llama 3.1 405B</strong>\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"7\" >General\n   </td>\n   <td>MMLU\n   </td>\n   <td>5\n   </td>\n   <td>macro_avg/acc_char\n   </td>\n   <td>66.7\n   </td>\n   <td>66.7\n   </td>\n   <td>79.5\n   </td>\n   <td>79.3\n   </td>\n   <td>85.2\n   </td>\n  </tr>\n  <tr>\n   <td>MMLU-Pro (CoT)\n   </td>\n   <td>5\n   </td>\n   <td>macro_avg/acc_char\n   </td>\n   <td>36.2\n   </td>\n   <td>37.1\n   </td>\n   <td>55.0\n   </td>\n   <td>53.8\n   </td>\n   <td>61.6\n   </td>\n  </tr>\n  <tr>\n   <td>AGIEval English\n   </td>\n   <td>3-5\n   </td>\n   <td>average/acc_char\n   </td>\n   <td>47.1\n   </td>\n   <td>47.8\n   </td>\n   <td>63.0\n   </td>\n   <td>64.6\n   </td>\n   <td>71.6\n   </td>\n  </tr>\n  <tr>\n   <td>CommonSenseQA\n   </td>\n   <td>7\n   </td>\n   <td>acc_char\n   </td>\n   <td>72.6\n   </td>\n   <td>75.0\n   </td>\n   <td>83.8\n   </td>\n   <td>84.1\n   </td>\n   <td>85.8\n   </td>\n  </tr>\n  <tr>\n   <td>Winogrande\n   </td>\n   <td>5\n   </td>\n   <td>acc_char\n   </td>\n   <td>-\n   </td>\n   <td>60.5\n   </td>\n   <td>-\n   </td>\n   <td>83.3\n   </td>\n   <td>86.7\n   </td>\n  </tr>\n  <tr>\n   <td>BIG-Bench Hard (CoT)\n   </td>\n   <td>3\n   </td>\n   <td>average/em\n   </td>\n   <td>61.1\n   </td>\n   <td>64.2\n   </td>\n   <td>81.3\n   </td>\n   <td>81.6\n   </td>\n   <td>85.9\n   </td>\n  </tr>\n  <tr>\n   <td>ARC-Challenge\n   </td>\n   <td>25\n   </td>\n   <td>acc_char\n   </td>\n   <td>79.4\n   </td>\n   <td>79.7\n   </td>\n   <td>93.1\n   </td>\n   <td>92.9\n   </td>\n   <td>96.1\n   </td>\n  </tr>\n  <tr>\n   <td>Knowledge reasoning\n   </td>\n   <td>TriviaQA-Wiki\n   </td>\n   <td>5\n   </td>\n   <td>em\n   </td>\n   <td>78.5\n   </td>\n   <td>77.6\n   </td>\n   <td>89.7\n   </td>\n   <td>89.8\n   </td>\n   <td>91.8\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"4\" >Reading comprehension\n   </td>\n   <td>SQuAD\n   </td>\n   <td>1\n   </td>\n   <td>em\n   </td>\n   <td>76.4\n   </td>\n   <td>77.0\n   </td>\n   <td>85.6\n   </td>\n   <td>81.8\n   </td>\n   <td>89.3\n   </td>\n  </tr>\n  <tr>\n   <td>QuAC (F1)\n   </td>\n   <td>1\n   </td>\n   <td>f1\n   </td>\n   <td>44.4\n   </td>\n   <td>44.9\n   </td>\n   <td>51.1\n   </td>\n   <td>51.1\n   </td>\n   <td>53.6\n   </td>\n  </tr>\n  <tr>\n   <td>BoolQ\n   </td>\n   <td>0\n   </td>\n   <td>acc_char\n   </td>\n   <td>75.7\n   </td>\n   <td>75.0\n   </td>\n   <td>79.0\n   </td>\n   <td>79.4\n   </td>\n   <td>80.0\n   </td>\n  </tr>\n  <tr>\n   <td>DROP (F1)\n   </td>\n   <td>3\n   </td>\n   <td>f1\n   </td>\n   <td>58.4\n   </td>\n   <td>59.5\n   </td>\n   <td>79.7\n   </td>\n   <td>79.6\n   </td>\n   <td>84.8\n   </td>\n  </tr>\n</table>\n\n\n\n### Instruction tuned models\n\n\n<table>\n  <tr>\n   <td><strong>Category</strong>\n   </td>\n   <td><strong>Benchmark</strong>\n   </td>\n   <td><strong># Shots</strong>\n   </td>\n   <td><strong>Metric</strong>\n   </td>\n   <td><strong>Llama 3 8B Instruct</strong>\n   </td>\n   <td><strong>Llama 3.1 8B Instruct</strong>\n   </td>\n   <td><strong>Llama 3 70B Instruct</strong>\n   </td>\n   <td><strong>Llama 3.1 70B Instruct</strong>\n   </td>\n   <td><strong>Llama 3.1 405B Instruct</strong>\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"4\" >General\n   </td>\n   <td>MMLU\n   </td>\n   <td>5\n   </td>\n   <td>macro_avg/acc\n   </td>\n   <td>68.5\n   </td>\n   <td>69.4\n   </td>\n   <td>82.0\n   </td>\n   <td>83.6\n   </td>\n   <td>87.3\n   </td>\n  </tr>\n  <tr>\n   <td>MMLU (CoT)\n   </td>\n   <td>0\n   </td>\n   <td>macro_avg/acc\n   </td>\n   <td>65.3\n   </td>\n   <td>73.0\n   </td>\n   <td>80.9\n   </td>\n   <td>86.0\n   </td>\n   <td>88.6\n   </td>\n  </tr>\n  <tr>\n   <td>MMLU-Pro (CoT)\n   </td>\n   <td>5\n   </td>\n   <td>micro_avg/acc_char\n   </td>\n   <td>45.5\n   </td>\n   <td>48.3\n   </td>\n   <td>63.4\n   </td>\n   <td>66.4\n   </td>\n   <td>73.3\n   </td>\n  </tr>\n  <tr>\n   <td>IFEval\n   </td>\n   <td>\n   </td>\n   <td>\n   </td>\n   <td>76.8\n   </td>\n   <td>80.4\n   </td>\n   <td>82.9\n   </td>\n   <td>87.5\n   </td>\n   <td>88.6\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"2\" >Reasoning\n   </td>\n   <td>ARC-C\n   </td>\n   <td>0\n   </td>\n   <td>acc\n   </td>\n   <td>82.4\n   </td>\n   <td>83.4\n   </td>\n   <td>94.4\n   </td>\n   <td>94.8\n   </td>\n   <td>96.9\n   </td>\n  </tr>\n  <tr>\n   <td>GPQA\n   </td>\n   <td>0\n   </td>\n   <td>em\n   </td>\n   <td>34.6\n   </td>\n   <td>30.4\n   </td>\n   <td>39.5\n   </td>\n   <td>46.7\n   </td>\n   <td>50.7\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"4\" >Code\n   </td>\n   <td>HumanEval\n   </td>\n   <td>0\n   </td>\n   <td>pass@1\n   </td>\n   <td>60.4\n   </td>\n   <td>72.6\n   </td>\n   <td>81.7\n   </td>\n   <td>80.5\n   </td>\n   <td>89.0\n   </td>\n  </tr>\n  <tr>\n   <td>MBPP ++ base version\n   </td>\n   <td>0\n   </td>\n   <td>pass@1\n   </td>\n   <td>70.6\n   </td>\n   <td>72.8\n   </td>\n   <td>82.5\n   </td>\n   <td>86.0\n   </td>\n   <td>88.6\n   </td>\n  </tr>\n  <tr>\n   <td>Multipl-E HumanEval\n   </td>\n   <td>0\n   </td>\n   <td>pass@1\n   </td>\n   <td>-\n   </td>\n   <td>50.8\n   </td>\n   <td>-\n   </td>\n   <td>65.5\n   </td>\n   <td>75.2\n   </td>\n  </tr>\n  <tr>\n   <td>Multipl-E MBPP\n   </td>\n   <td>0\n   </td>\n   <td>pass@1\n   </td>\n   <td>-\n   </td>\n   <td>52.4\n   </td>\n   <td>-\n   </td>\n   <td>62.0\n   </td>\n   <td>65.7\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"2\" >Math\n   </td>\n   <td>GSM-8K (CoT)\n   </td>\n   <td>8\n   </td>\n   <td>em_maj1@1\n   </td>\n   <td>80.6\n   </td>\n   <td>84.5\n   </td>\n   <td>93.0\n   </td>\n   <td>95.1\n   </td>\n   <td>96.8\n   </td>\n  </tr>\n  <tr>\n   <td>MATH (CoT)\n   </td>\n   <td>0\n   </td>\n   <td>final_em\n   </td>\n   <td>29.1\n   </td>\n   <td>51.9\n   </td>\n   <td>51.0\n   </td>\n   <td>68.0\n   </td>\n   <td>73.8\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"4\" >Tool Use\n   </td>\n   <td>API-Bank\n   </td>\n   <td>0\n   </td>\n   <td>acc\n   </td>\n   <td>48.3\n   </td>\n   <td>82.6\n   </td>\n   <td>85.1\n   </td>\n   <td>90.0\n   </td>\n   <td>92.0\n   </td>\n  </tr>\n  <tr>\n   <td>BFCL\n   </td>\n   <td>0\n   </td>\n   <td>acc\n   </td>\n   <td>60.3\n   </td>\n   <td>76.1\n   </td>\n   <td>83.0\n   </td>\n   <td>84.8\n   </td>\n   <td>88.5\n   </td>\n  </tr>\n  <tr>\n   <td>Gorilla Benchmark API Bench\n   </td>\n   <td>0\n   </td>\n   <td>acc\n   </td>\n   <td>1.7\n   </td>\n   <td>8.2\n   </td>\n   <td>14.7\n   </td>\n   <td>29.7\n   </td>\n   <td>35.3\n   </td>\n  </tr>\n  <tr>\n   <td>Nexus (0-shot)\n   </td>\n   <td>0\n   </td>\n   <td>macro_avg/acc\n   </td>\n   <td>18.1\n   </td>\n   <td>38.5\n   </td>\n   <td>47.8\n   </td>\n   <td>56.7\n   </td>\n   <td>58.7\n   </td>\n  </tr>\n  <tr>\n   <td>Multilingual\n   </td>\n   <td>Multilingual MGSM (CoT)\n   </td>\n   <td>0\n   </td>\n   <td>em\n   </td>\n   <td>-\n   </td>\n   <td>68.9\n   </td>\n   <td>-\n   </td>\n   <td>86.9\n   </td>\n   <td>91.6\n   </td>\n  </tr>\n</table>\n\n#### Multilingual benchmarks\n\n<table>\n  <tr>\n   <td><strong>Category</strong>\n   </td>\n   <td><strong>Benchmark</strong>\n   </td>\n   <td><strong>Language</strong>\n   </td>\n   <td><strong>Llama 3.1 8B</strong>\n   </td>\n   <td><strong>Llama 3.1 70B</strong>\n   </td>\n   <td><strong>Llama 3.1 405B</strong>\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"9\" ><strong>General</strong>\n   </td>\n   <td rowspan=\"9\" ><strong>MMLU (5-shot, macro_avg/acc)</strong>\n   </td>\n   <td>Portuguese\n   </td>\n   <td>62.12\n   </td>\n   <td>80.13\n   </td>\n   <td>84.95\n   </td>\n  </tr>\n  <tr>\n   <td>Spanish\n   </td>\n   <td>62.45\n   </td>\n   <td>80.05\n   </td>\n   <td>85.08\n   </td>\n  </tr>\n  <tr>\n   <td>Italian\n   </td>\n   <td>61.63\n   </td>\n   <td>80.4\n   </td>\n   <td>85.04\n   </td>\n  </tr>\n  <tr>\n   <td>German\n   </td>\n   <td>60.59\n   </td>\n   <td>79.27\n   </td>\n   <td>84.36\n   </td>\n  </tr>\n  <tr>\n   <td>French\n   </td>\n   <td>62.34\n   </td>\n   <td>79.82\n   </td>\n   <td>84.66\n   </td>\n  </tr>\n  <tr>\n   <td>Hindi\n   </td>\n   <td>50.88\n   </td>\n   <td>74.52\n   </td>\n   <td>80.31\n   </td>\n  </tr>\n  <tr>\n   <td>Thai\n   </td>\n   <td>50.32\n   </td>\n   <td>72.95\n   </td>\n   <td>78.21\n   </td>\n  </tr>\n</table>\n\n\n\n## Responsibility & Safety\n\nAs part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:\n\n\n\n* Enable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama. \n* Protect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm.\n* Provide protections for the community to help prevent the misuse of our models.\n\n\n### Responsible deployment \n\nLlama is a foundational technology designed to be used in a variety of use cases, examples on how Meta’s Llama models have been responsibly deployed can be found in our [Community Stories webpage](https://llama.meta.com/community-stories/). Our approach is to build the most helpful models enabling the world to benefit from the technology power, by aligning our model safety for the generic use cases addressing a standard set of harms. Developers are then in the driver seat to tailor safety for their use case, defining their own policy and deploying the models with the necessary safeguards in their Llama systems. Llama 3.1 was developed following the best practices outlined in our Responsible Use Guide, you can refer to the [Responsible Use Guide](https://llama.meta.com/responsible-use-guide/) to learn more. \n\n\n#### Llama 3.1 instruct \n\nOur main objectives for conducting safety fine-tuning are to provide the research community with a valuable resource for studying the robustness of safety fine-tuning, as well as to offer developers a readily available, safe, and powerful model for various applications to reduce the developer workload to deploy safe AI systems. For more details on the safety mitigations implemented please read the Llama 3 paper. \n\n**Fine-tuning data**\n\nWe employ a multi-faceted approach to data collection, combining human-generated data from our vendors with synthetic data to mitigate potential safety risks. We’ve developed many large language model (LLM)-based classifiers that enable us to thoughtfully select high-quality prompts and responses, enhancing data quality control. \n\n**Refusals and Tone**\n\nBuilding on the work we started with Llama 3, we put a great emphasis on model refusals to benign prompts as well as refusal tone. We included both borderline and adversarial prompts in our safety data strategy, and modified our safety data responses to follow  tone guidelines. \n\n\n#### Llama 3.1 systems\n\n**Large language models, including Llama 3.1, are not designed to be deployed in isolation but instead should be deployed as part of an overall AI system with additional safety guardrails as required.** Developers are expected to deploy system safeguards when building agentic systems. Safeguards are key to achieve the right helpfulness-safety alignment as well as mitigating safety and security risks inherent to the system and any integration of the model or system with external tools. \n\nAs part of our responsible release approach, we provide the community with [safeguards](https://llama.meta.com/trust-and-safety/) that developers should deploy with Llama models or other LLMs, including Llama Guard 3, Prompt Guard and Code Shield. All our [reference implementations](https://github.com/meta-llama/llama-agentic-system) demos contain these safeguards by default so developers can benefit from system-level safety out-of-the-box. \n\n\n#### New capabilities \n\nNote that this release introduces new capabilities, including a longer context window, multilingual inputs and outputs and possible integrations by developers with third party tools. Building with these new capabilities requires specific considerations in addition to the best practices that generally apply across all Generative AI use cases.\n\n**Tool-use**: Just like in standard software development, developers are responsible for the integration of the LLM with the tools and services of their choice. They should define a clear policy for their use case and assess the integrity of the third party services they use to be aware of the safety and security limitations when using this capability. Refer to the Responsible Use Guide for best practices on the safe deployment of the third party safeguards. \n\n**Multilinguality**: Llama 3.1 supports 7 languages in addition to English: French, German, Hindi, Italian, Portuguese, Spanish, and Thai. Llama may be able to output text in other languages than those that meet performance thresholds for safety and helpfulness. We strongly discourage developers from using this model to converse in non-supported languages without implementing finetuning and system controls in alignment with their policies and the best practices shared in the Responsible Use Guide. \n\n\n### Evaluations\n\nWe evaluated Llama models for common use cases as well as specific capabilities. Common use cases evaluations measure safety risks of systems for most commonly built applications including chat bot, coding assistant, tool calls. We built dedicated, adversarial evaluation datasets and evaluated systems composed of Llama models and Llama Guard 3 to filter input prompt and output response. It is important to evaluate applications in context, and we recommend building dedicated evaluation dataset for your use case. Prompt Guard and Code Shield are also available if relevant to the application. \n\nCapability evaluations measure vulnerabilities of Llama models inherent to specific capabilities, for which were crafted dedicated benchmarks including long context, multilingual, tools calls, coding or memorization.\n\n**Red teaming**\n\nFor both scenarios, we conducted recurring red teaming exercises with the goal of discovering risks via adversarial prompting and we used the learnings to improve our benchmarks and safety tuning datasets. \n\nWe partnered early with subject-matter experts in critical risk areas to understand the nature of these real-world harms and how such models may lead to unintended harm for society. Based on these conversations, we derived a set of adversarial goals for the red team to attempt to achieve, such as extracting harmful information or reprogramming the model to act in a potentially harmful capacity.  The red team consisted of experts in cybersecurity, adversarial machine learning, responsible AI, and integrity in addition to multilingual content specialists with background in integrity issues in specific geographic markets.\n\n\n### Critical and other risks \n\nWe specifically focused our efforts on mitigating the following critical risk areas:\n\n**1- CBRNE (Chemical, Biological, Radiological, Nuclear, and Explosive materials) helpfulness**\n\nTo assess risks related to proliferation of chemical and biological weapons, we performed uplift testing designed to assess whether use of Llama 3.1 models could meaningfully increase the capabilities of malicious actors to plan or carry out attacks using these types of weapons. \n\n\n**2. Child Safety**\n\nChild Safety risk assessments were conducted using a team of experts, to assess the model’s capability to produce outputs that could result in Child Safety risks and inform on any necessary and appropriate risk mitigations via fine tuning. We leveraged those expert red teaming sessions to expand the coverage of our evaluation benchmarks through Llama 3 model development.  For Llama 3, we conducted new in-depth sessions using objective based methodologies to assess the model risks along multiple attack vectors including the additional languages Llama 3 is trained on. We also partnered with content specialists to perform red teaming exercises assessing potentially violating content while taking account of market specific nuances or experiences. \n\n**3. Cyber attack enablement**\n\nOur cyber attack uplift study investigated whether LLMs can enhance human capabilities in hacking tasks, both in terms of skill level and speed.\n\nOur attack automation study focused on evaluating the capabilities of LLMs when used as autonomous agents in cyber offensive operations, specifically in the context of ransomware attacks. This evaluation was distinct from previous studies that considered LLMs as interactive assistants. The primary objective was to assess whether these models could effectively function as independent agents in executing complex cyber-attacks without human intervention.\n\nOur study of Llama-3.1-405B’s social engineering uplift for cyber attackers was conducted to assess the effectiveness of AI models in aiding cyber threat actors in spear phishing campaigns. Please read our Llama 3.1 Cyber security whitepaper to learn more.\n\n\n### Community \n\nGenerative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership on AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Purple Llama tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to our [Github repository](https://github.com/meta-llama/PurpleLlama). \n\nWe also set up the [Llama Impact Grants](https://llama.meta.com/llama-impact-grants/) program to identify and support the most compelling applications of Meta’s Llama model for societal benefit across three categories: education, climate and open innovation. The 20 finalists from the hundreds of applications can be found [here](https://llama.meta.com/llama-impact-grants/#finalists). \n\nFinally, we put in place a set of resources including an [output reporting mechanism](https://developers.facebook.com/llama_output_feedback) and [bug bounty program](https://www.facebook.com/whitehat) to continuously improve the Llama technology with the help of the community.\n\n\n## Ethical Considerations and Limitations\n\nThe core values of Llama 3.1 are openness, inclusivity and helpfulness. It is meant to serve everyone, and to work for a wide range of use cases. It is thus designed to be accessible to people across many different backgrounds, experiences and perspectives. Llama 3.1 addresses users and their needs as they are, without insertion unnecessary judgment or normativity, while reflecting the understanding that even content that may appear problematic in some cases can serve valuable purposes in others. It respects the dignity and autonomy of all users, especially in terms of the values of free thought and expression that power innovation and progress. \n\nBut Llama 3.1 is a new technology, and like any new technology, there are risks associated with its use. Testing conducted to date has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, Llama 3.1’s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 3.1 models, developers should perform safety testing and tuning tailored to their specific applications of the model. Please refer to available resources including our [Responsible Use Guide](https://llama.meta.com/responsible-use-guide), [Trust and Safety](https://llama.meta.com/trust-and-safety/) solutions, and other [resources](https://llama.meta.com/docs/get-started/) to learn more about responsible development. ",
    "card_content": "---\nlanguage:\n- en\n- de\n- fr\n- it\n- pt\n- hi\n- es\n- th\nlicense: llama3.1\nbase_model: meta-llama/Meta-Llama-3.1-8B\npipeline_tag: text-generation\ntags:\n- facebook\n- meta\n- pytorch\n- llama\n- llama-3\nextra_gated_prompt: \"### LLAMA 3.1 COMMUNITY LICENSE AGREEMENT\\nLlama 3.1 Version\\\n  \\ Release Date: July 23, 2024\\n\\\"Agreement\\\" means the terms and conditions for\\\n  \\ use, reproduction, distribution and modification of the  Llama Materials set forth\\\n  \\ herein.\\n\\\"Documentation\\\" means the specifications, manuals and documentation\\\n  \\ accompanying Llama 3.1 distributed by Meta at https://llama.meta.com/doc/overview.\\n\\\n  \\\"Licensee\\\" or \\\"you\\\" means you, or your employer or any other person or entity\\\n  \\ (if you are entering into this Agreement on such person or entity’s behalf), of\\\n  \\ the age required under applicable laws, rules or regulations to provide legal\\\n  \\ consent and that has legal authority to bind your employer or such other person\\\n  \\ or entity if you are entering in this Agreement on their behalf.\\n\\\"Llama 3.1\\\"\\\n  \\ means the foundational large language models and software and algorithms, including\\\n  \\ machine-learning model code, trained model weights, inference-enabling code, training-enabling\\\n  \\ code, fine-tuning enabling code and other elements of the foregoing distributed\\\n  \\ by Meta at https://llama.meta.com/llama-downloads.\\n\\\"Llama Materials\\\" means,\\\n  \\ collectively, Meta’s proprietary Llama 3.1 and Documentation (and any portion\\\n  \\ thereof) made available under this Agreement.\\n\\\"Meta\\\" or \\\"we\\\" means Meta Platforms\\\n  \\ Ireland Limited (if you are located in or, if you are an entity, your principal\\\n  \\ place of business is in the EEA or Switzerland) and Meta Platforms, Inc. (if you\\\n  \\ are located outside of the EEA or Switzerland).\\n   \\n1. License Rights and Redistribution.\\n\\\n  a. Grant of Rights. You are granted a non-exclusive, worldwide, non-transferable\\\n  \\ and royalty-free limited license under Meta’s intellectual property or other rights\\\n  \\ owned by Meta embodied in the Llama Materials to use, reproduce, distribute, copy,\\\n  \\ create derivative works of, and make modifications to the Llama Materials.\\nb.\\\n  \\ Redistribution and Use.\\ni. If you distribute or make available the Llama Materials\\\n  \\ (or any derivative works thereof), or a product or service (including another\\\n  \\ AI model) that contains any of them, you shall (A) provide a copy of this Agreement\\\n  \\ with any such Llama Materials; and (B) prominently display “Built with Llama”\\\n  \\ on a related website, user interface, blogpost, about page, or product documentation.\\\n  \\ If you use the Llama Materials or any outputs or results of the Llama Materials\\\n  \\ to create, train, fine tune, or otherwise improve an AI model, which is distributed\\\n  \\ or made available, you shall also include “Llama” at the beginning of any such\\\n  \\ AI model name.\\nii. If you receive Llama Materials, or any derivative works thereof,\\\n  \\ from a Licensee as part  of an integrated end user product, then Section 2 of\\\n  \\ this Agreement will not apply to you.\\niii. You must retain in all copies of the\\\n  \\ Llama Materials that you distribute the following attribution notice within a\\\n  \\ “Notice” text file distributed as a part of such copies: “Llama 3.1 is licensed\\\n  \\ under the Llama 3.1 Community License, Copyright © Meta Platforms, Inc. All Rights\\\n  \\ Reserved.”\\niv. Your use of the Llama Materials must comply with applicable laws\\\n  \\ and regulations (including trade compliance laws and regulations) and adhere to\\\n  \\ the Acceptable Use Policy for the Llama Materials (available at https://llama.meta.com/llama3_1/use-policy),\\\n  \\ which is hereby incorporated by reference into this Agreement.\\n2. Additional\\\n  \\ Commercial Terms. If, on the Llama 3.1 version release date, the monthly active\\\n  \\ users of the products or services made available by or for Licensee, or Licensee’s\\\n  \\ affiliates, is greater than 700 million monthly active users in the preceding\\\n  \\ calendar month, you must request a license from Meta, which Meta may grant to\\\n  \\ you in its sole discretion, and you are not authorized to exercise any of the\\\n  \\ rights under this Agreement unless or until Meta otherwise expressly grants you\\\n  \\ such rights.\\n3. Disclaimer of Warranty. UNLESS REQUIRED BY APPLICABLE LAW, THE\\\n  \\ LLAMA MATERIALS AND ANY OUTPUT AND RESULTS THEREFROM ARE PROVIDED ON AN “AS IS”\\\n  \\ BASIS, WITHOUT WARRANTIES OF ANY KIND, AND META DISCLAIMS ALL WARRANTIES OF ANY\\\n  \\ KIND, BOTH EXPRESS AND IMPLIED, INCLUDING, WITHOUT LIMITATION, ANY WARRANTIES\\\n  \\ OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.\\\n  \\ YOU ARE SOLELY RESPONSIBLE FOR DETERMINING THE APPROPRIATENESS OF USING OR REDISTRIBUTING\\\n  \\ THE LLAMA MATERIALS AND ASSUME ANY RISKS ASSOCIATED WITH YOUR USE OF THE LLAMA\\\n  \\ MATERIALS AND ANY OUTPUT AND RESULTS.\\n4. Limitation of Liability. IN NO EVENT\\\n  \\ WILL META OR ITS AFFILIATES BE LIABLE UNDER ANY THEORY OF LIABILITY, WHETHER IN\\\n  \\ CONTRACT, TORT, NEGLIGENCE, PRODUCTS LIABILITY, OR OTHERWISE, ARISING OUT OF THIS\\\n  \\ AGREEMENT, FOR ANY LOST PROFITS OR ANY INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL,\\\n  \\ EXEMPLARY OR PUNITIVE DAMAGES, EVEN IF META OR ITS AFFILIATES HAVE BEEN ADVISED\\\n  \\ OF THE POSSIBILITY OF ANY OF THE FOREGOING.\\n5. Intellectual Property.\\na. No\\\n  \\ trademark licenses are granted under this Agreement, and in connection with the\\\n  \\ Llama Materials, neither Meta nor Licensee may use any name or mark owned by or\\\n  \\ associated with the other or any of its affiliates, except as required for reasonable\\\n  \\ and customary use in describing and redistributing the Llama Materials or as set\\\n  \\ forth in this Section 5(a). Meta hereby grants you a license to use “Llama” (the\\\n  \\ “Mark”) solely as required to comply with the last sentence of Section 1.b.i.\\\n  \\ You will comply with Meta’s brand guidelines (currently accessible at https://about.meta.com/brand/resources/meta/company-brand/\\\n  \\ ). All goodwill arising out of your use of the Mark will inure to the benefit\\\n  \\ of Meta.\\nb. Subject to Meta’s ownership of Llama Materials and derivatives made\\\n  \\ by or for Meta, with respect to any derivative works and modifications of the\\\n  \\ Llama Materials that are made by you, as between you and Meta, you are and will\\\n  \\ be the owner of such derivative works and modifications.\\nc. If you institute\\\n  \\ litigation or other proceedings against Meta or any entity (including a cross-claim\\\n  \\ or counterclaim in a lawsuit) alleging that the Llama Materials or Llama 3.1 outputs\\\n  \\ or results, or any portion of any of the foregoing, constitutes infringement of\\\n  \\ intellectual property or other rights owned or licensable by you, then any licenses\\\n  \\ granted to you under this Agreement shall terminate as of the date such litigation\\\n  \\ or claim is filed or instituted. You will indemnify and hold harmless Meta from\\\n  \\ and against any claim by any third party arising out of or related to your use\\\n  \\ or distribution of the Llama Materials.\\n6. Term and Termination. The term of\\\n  \\ this Agreement will commence upon your acceptance of this Agreement or access\\\n  \\ to the Llama Materials and will continue in full force and effect until terminated\\\n  \\ in accordance with the terms and conditions herein. Meta may terminate this Agreement\\\n  \\ if you are in breach of any term or condition of this Agreement. Upon termination\\\n  \\ of this Agreement, you shall delete and cease use of the Llama Materials. Sections\\\n  \\ 3, 4 and 7 shall survive the termination of this Agreement.\\n7. Governing Law\\\n  \\ and Jurisdiction. This Agreement will be governed and construed under the laws\\\n  \\ of the State of California without regard to choice of law principles, and the\\\n  \\ UN Convention on Contracts for the International Sale of Goods does not apply\\\n  \\ to this Agreement. The courts of California shall have exclusive jurisdiction\\\n  \\ of any dispute arising out of this Agreement.\\n### Llama 3.1 Acceptable Use Policy\\n\\\n  Meta is committed to promoting safe and fair use of its tools and features, including\\\n  \\ Llama 3.1. If you access or use Llama 3.1, you agree to this Acceptable Use Policy\\\n  \\ (“Policy”). The most recent copy of this policy can be found at [https://llama.meta.com/llama3_1/use-policy](https://llama.meta.com/llama3_1/use-policy)\\n\\\n  #### Prohibited Uses\\nWe want everyone to use Llama 3.1 safely and responsibly.\\\n  \\ You agree you will not use, or allow others to use, Llama 3.1 to:\\n 1. Violate\\\n  \\ the law or others’ rights, including to:\\n    1. Engage in, promote, generate,\\\n  \\ contribute to, encourage, plan, incite, or further illegal or unlawful activity\\\n  \\ or content, such as:\\n        1. Violence or terrorism\\n        2. Exploitation\\\n  \\ or harm to children, including the solicitation, creation, acquisition, or dissemination\\\n  \\ of child exploitative content or failure to report Child Sexual Abuse Material\\n\\\n  \\        3. Human trafficking, exploitation, and sexual violence\\n        4. The\\\n  \\ illegal distribution of information or materials to minors, including obscene\\\n  \\ materials, or failure to employ legally required age-gating in connection with\\\n  \\ such information or materials.\\n        5. Sexual solicitation\\n        6. Any\\\n  \\ other criminal activity\\n    3. Engage in, promote, incite, or facilitate the\\\n  \\ harassment, abuse, threatening, or bullying of individuals or groups of individuals\\n\\\n  \\    4. Engage in, promote, incite, or facilitate discrimination or other unlawful\\\n  \\ or harmful conduct in the provision of employment, employment benefits, credit,\\\n  \\ housing, other economic benefits, or other essential goods and services\\n    5.\\\n  \\ Engage in the unauthorized or unlicensed practice of any profession including,\\\n  \\ but not limited to, financial, legal, medical/health, or related professional\\\n  \\ practices\\n    6. Collect, process, disclose, generate, or infer health, demographic,\\\n  \\ or other sensitive personal or private information about individuals without rights\\\n  \\ and consents required by applicable laws\\n    7. Engage in or facilitate any action\\\n  \\ or generate any content that infringes, misappropriates, or otherwise violates\\\n  \\ any third-party rights, including the outputs or results of any products or services\\\n  \\ using the Llama Materials\\n    8. Create, generate, or facilitate the creation\\\n  \\ of malicious code, malware, computer viruses or do anything else that could disable,\\\n  \\ overburden, interfere with or impair the proper working, integrity, operation\\\n  \\ or appearance of a website or computer system\\n2. Engage in, promote, incite,\\\n  \\ facilitate, or assist in the planning or development of activities that present\\\n  \\ a risk of death or bodily harm to individuals, including use of Llama 3.1 related\\\n  \\ to the following:\\n    1. Military, warfare, nuclear industries or applications,\\\n  \\ espionage, use for materials or activities that are subject to the International\\\n  \\ Traffic Arms Regulations (ITAR) maintained by the United States Department of\\\n  \\ State\\n    2. Guns and illegal weapons (including weapon development)\\n    3.\\\n  \\ Illegal drugs and regulated/controlled substances\\n    4. Operation of critical\\\n  \\ infrastructure, transportation technologies, or heavy machinery\\n    5. Self-harm\\\n  \\ or harm to others, including suicide, cutting, and eating disorders\\n    6. Any\\\n  \\ content intended to incite or promote violence, abuse, or any infliction of bodily\\\n  \\ harm to an individual\\n3. Intentionally deceive or mislead others, including use\\\n  \\ of Llama 3.1 related to the following:\\n    1. Generating, promoting, or furthering\\\n  \\ fraud or the creation or promotion of disinformation\\n    2. Generating, promoting,\\\n  \\ or furthering defamatory content, including the creation of defamatory statements,\\\n  \\ images, or other content\\n    3. Generating, promoting, or further distributing\\\n  \\ spam\\n    4. Impersonating another individual without consent, authorization,\\\n  \\ or legal right\\n    5. Representing that the use of Llama 3.1 or outputs are human-generated\\n\\\n  \\    6. Generating or facilitating false online engagement, including fake reviews\\\n  \\ and other means of fake online engagement\\n4. Fail to appropriately disclose to\\\n  \\ end users any known dangers of your AI system\\nPlease report any violation of\\\n  \\ this Policy, software “bug,” or other problems that could lead to a violation\\\n  \\ of this Policy through one of the following means:\\n    * Reporting issues with\\\n  \\ the model: [https://github.com/meta-llama/llama-models/issues](https://github.com/meta-llama/llama-models/issues)\\n\\\n  \\    * Reporting risky content generated by the model:\\n    developers.facebook.com/llama_output_feedback\\n\\\n  \\    * Reporting bugs and security concerns: facebook.com/whitehat/info\\n    * Reporting\\\n  \\ violations of the Acceptable Use Policy or unlicensed uses of Meta Llama 3: LlamaUseReport@meta.com\"\nextra_gated_fields:\n  First Name: text\n  Last Name: text\n  Date of birth: date_picker\n  Country: country\n  Affiliation: text\n  Job title:\n    type: select\n    options:\n    - Student\n    - Research Graduate\n    - AI researcher\n    - AI developer/engineer\n    - Reporter\n    - Other\n  geo: ip_location\n  ? By clicking Submit below I accept the terms of the license and acknowledge that\n    the information I provide will be collected stored processed and shared in accordance\n    with the Meta Privacy Policy\n  : checkbox\nextra_gated_description: The information you provide will be collected, stored, processed\n  and shared in accordance with the [Meta Privacy Policy](https://www.facebook.com/privacy/policy/).\nextra_gated_button_content: Submit\n---\n\n## Model Information\n\nThe Meta Llama 3.1 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction tuned generative models in 8B, 70B and 405B sizes (text in/text out). The Llama 3.1 instruction tuned text only models (8B, 70B, 405B) are optimized for multilingual dialogue use cases and outperform many of the available open source and closed chat models on common industry benchmarks.\n\n**Model developer**: Meta\n\n**Model Architecture:** Llama 3.1 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety. \n\n\n<table>\n  <tr>\n   <td>\n   </td>\n   <td><strong>Training Data</strong>\n   </td>\n   <td><strong>Params</strong>\n   </td>\n   <td><strong>Input modalities</strong>\n   </td>\n   <td><strong>Output modalities</strong>\n   </td>\n   <td><strong>Context length</strong>\n   </td>\n   <td><strong>GQA</strong>\n   </td>\n   <td><strong>Token count</strong>\n   </td>\n   <td><strong>Knowledge cutoff</strong>\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"3\" >Llama 3.1 (text only)\n   </td>\n   <td rowspan=\"3\" >A new mix of publicly available online data.\n   </td>\n   <td>8B\n   </td>\n   <td>Multilingual Text\n   </td>\n   <td>Multilingual Text and code\n   </td>\n   <td>128k\n   </td>\n   <td>Yes\n   </td>\n   <td rowspan=\"3\" >15T+\n   </td>\n   <td rowspan=\"3\" >December 2023\n   </td>\n  </tr>\n  <tr>\n   <td>70B\n   </td>\n   <td>Multilingual Text\n   </td>\n   <td>Multilingual Text and code\n   </td>\n   <td>128k\n   </td>\n   <td>Yes\n   </td>\n  </tr>\n  <tr>\n   <td>405B\n   </td>\n   <td>Multilingual Text\n   </td>\n   <td>Multilingual Text and code\n   </td>\n   <td>128k\n   </td>\n   <td>Yes\n   </td>\n  </tr>\n</table>\n\n\n**Supported languages:** English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai.\n\n**Llama 3.1 family of models**. Token counts refer to pretraining data only. All model versions use Grouped-Query Attention (GQA) for improved inference scalability.\n\n**Model Release Date:** July 23, 2024.\n\n**Status:** This is a static model trained on an offline dataset. Future versions of the tuned models will be released as we improve model safety with community feedback.\n\n**License:** A custom commercial license, the Llama 3.1 Community License, is available at: [https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/LICENSE](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/LICENSE)\n\nWhere to send questions or comments about the model Instructions on how to provide feedback or comments on the model can be found in the model [README](https://github.com/meta-llama/llama3). For more technical information about generation parameters and recipes for how to use Llama 3.1 in applications, please go [here](https://github.com/meta-llama/llama-recipes). \n\n\n## Intended Use\n\n**Intended Use Cases** Llama 3.1 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks. The Llama 3.1 model collection also supports the ability to leverage the outputs of its models to improve other models including synthetic data generation and distillation. The Llama 3.1 Community License allows for these use cases. \n\n**Out-of-scope** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.1 Community License. Use in languages beyond those explicitly referenced as supported in this model card**.\n\n**<span style=\"text-decoration:underline;\">Note</span>: Llama 3.1 has been trained on a broader collection of languages than the 8 supported languages. Developers may fine-tune Llama 3.1 models for languages beyond the 8 supported languages provided they comply with the Llama 3.1 Community License and the Acceptable Use Policy and in such cases are responsible for ensuring that any uses of Llama 3.1 in additional languages is done in a safe and responsible manner.\n\n## How to use\n\nThis repository contains two versions of Meta-Llama-3.1-8B-Instruct, for use with transformers and with the original `llama` codebase.\n\n### Use with transformers\n\nStarting with `transformers >= 4.43.0` onward, you can run conversational inference using the Transformers `pipeline` abstraction or by leveraging the Auto classes with the `generate()` function.\n\nMake sure to update your transformers installation via `pip install --upgrade transformers`.\n\n```python\nimport transformers\nimport torch\n\nmodel_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model_id,\n    model_kwargs={\"torch_dtype\": torch.bfloat16},\n    device_map=\"auto\",\n)\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n    {\"role\": \"user\", \"content\": \"Who are you?\"},\n]\n\noutputs = pipeline(\n    messages,\n    max_new_tokens=256,\n)\nprint(outputs[0][\"generated_text\"][-1])\n```\n\nNote: You can also find detailed recipes on how to use the model locally, with `torch.compile()`, assisted generations, quantised and more at [`huggingface-llama-recipes`](https://github.com/huggingface/huggingface-llama-recipes)\n\n### Tool use with transformers\n\nLLaMA-3.1 supports multiple tool use formats. You can see a full guide to prompt formatting [here](https://llama.meta.com/docs/model-cards-and-prompt-formats/llama3_1/).\n\nTool use is also supported through [chat templates](https://huggingface.co/docs/transformers/main/chat_templating#advanced-tool-use--function-calling) in Transformers. \nHere is a quick example showing a single simple tool:\n\n```python\n# First, define a tool\ndef get_current_temperature(location: str) -> float:\n    \"\"\"\n    Get the current temperature at a location.\n    \n    Args:\n        location: The location to get the temperature for, in the format \"City, Country\"\n    Returns:\n        The current temperature at the specified location in the specified units, as a float.\n    \"\"\"\n    return 22.  # A real function should probably actually get the temperature!\n\n# Next, create a chat and apply the chat template\nmessages = [\n  {\"role\": \"system\", \"content\": \"You are a bot that responds to weather queries.\"},\n  {\"role\": \"user\", \"content\": \"Hey, what's the temperature in Paris right now?\"}\n]\n\ninputs = tokenizer.apply_chat_template(messages, tools=[get_current_temperature], add_generation_prompt=True)\n```\n\nYou can then generate text from this input as normal. If the model generates a tool call, you should add it to the chat like so:\n\n```python\ntool_call = {\"name\": \"get_current_temperature\", \"arguments\": {\"location\": \"Paris, France\"}}\nmessages.append({\"role\": \"assistant\", \"tool_calls\": [{\"type\": \"function\", \"function\": tool_call}]})\n```\n\nand then call the tool and append the result, with the `tool` role, like so:\n\n```python\nmessages.append({\"role\": \"tool\", \"name\": \"get_current_temperature\", \"content\": \"22.0\"})\n```\n\nAfter that, you can `generate()` again to let the model use the tool result in the chat. Note that this was a very brief introduction to tool calling - for more information,\nsee the [LLaMA prompt format docs](https://llama.meta.com/docs/model-cards-and-prompt-formats/llama3_1/) and the Transformers [tool use documentation](https://huggingface.co/docs/transformers/main/chat_templating#advanced-tool-use--function-calling).\n\n\n### Use with `llama`\n\nPlease, follow the instructions in the [repository](https://github.com/meta-llama/llama)\n\nTo download Original checkpoints, see the example command below leveraging `huggingface-cli`:\n\n```\nhuggingface-cli download meta-llama/Meta-Llama-3.1-8B-Instruct --include \"original/*\" --local-dir Meta-Llama-3.1-8B-Instruct\n```\n\n## Hardware and Software\n\n**Training Factors** We used custom training libraries, Meta's custom built GPU cluster, and production infrastructure for pretraining. Fine-tuning, annotation, and evaluation were also performed on production infrastructure.\n\n**Training utilized a cumulative of** 39.3M GPU hours of computation on H100-80GB (TDP of 700W) type hardware, per the table below. Training time is the total GPU time required for training each model and power consumption is the peak power capacity per GPU device used, adjusted for power usage efficiency. \n\n\n**Training Greenhouse Gas Emissions** Estimated total location-based greenhouse gas emissions were **11,390** tons CO2eq for training. Since 2020, Meta has maintained net zero greenhouse gas emissions in its global operations and matched 100% of its electricity use with renewable energy, therefore the total market-based greenhouse gas emissions for training were 0 tons CO2eq.\n\n\n<table>\n  <tr>\n   <td>\n   </td>\n   <td><strong>Training Time (GPU hours)</strong>\n   </td>\n   <td><strong>Training Power Consumption (W)</strong>\n   </td>\n   <td><strong>Training Location-Based Greenhouse Gas Emissions</strong>\n<p>\n<strong>(tons CO2eq)</strong>\n   </td>\n   <td><strong>Training Market-Based Greenhouse Gas Emissions</strong>\n<p>\n<strong>(tons CO2eq)</strong>\n   </td>\n  </tr>\n  <tr>\n   <td>Llama 3.1 8B\n   </td>\n   <td>1.46M\n   </td>\n   <td>700\n   </td>\n   <td>420\n   </td>\n   <td>0\n   </td>\n  </tr>\n  <tr>\n   <td>Llama 3.1 70B\n   </td>\n   <td>7.0M\n   </td>\n   <td>700\n   </td>\n   <td>2,040\n   </td>\n   <td>0\n   </td>\n  </tr>\n  <tr>\n   <td>Llama 3.1 405B\n   </td>\n   <td>30.84M\n   </td>\n   <td>700\n   </td>\n   <td>8,930\n   </td>\n   <td>0\n   </td>\n  </tr>\n  <tr>\n   <td>Total\n   </td>\n   <td>39.3M\n   <td>\n<ul>\n\n</ul>\n   </td>\n   <td>11,390\n   </td>\n   <td>0\n   </td>\n  </tr>\n</table>\n\n\n\nThe methodology used to determine training energy use and greenhouse gas emissions can be found [here](https://arxiv.org/pdf/2204.05149).  Since Meta is openly releasing these models, the training energy use and greenhouse gas emissions  will not be incurred by others.\n\n\n## Training Data\n\n**Overview:** Llama 3.1 was pretrained on ~15 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over 25M synthetically generated examples. \n\n**Data Freshness:** The pretraining data has a cutoff of December 2023.\n\n\n## Benchmark scores\n\nIn this section, we report the results for Llama 3.1 models on standard automatic benchmarks. For all the evaluations, we use our internal evaluations library. \n\n### Base pretrained models\n\n\n<table>\n  <tr>\n   <td><strong>Category</strong>\n   </td>\n   <td><strong>Benchmark</strong>\n   </td>\n   <td><strong># Shots</strong>\n   </td>\n   <td><strong>Metric</strong>\n   </td>\n   <td><strong>Llama 3 8B</strong>\n   </td>\n   <td><strong>Llama 3.1 8B</strong>\n   </td>\n   <td><strong>Llama 3 70B</strong>\n   </td>\n   <td><strong>Llama 3.1 70B</strong>\n   </td>\n   <td><strong>Llama 3.1 405B</strong>\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"7\" >General\n   </td>\n   <td>MMLU\n   </td>\n   <td>5\n   </td>\n   <td>macro_avg/acc_char\n   </td>\n   <td>66.7\n   </td>\n   <td>66.7\n   </td>\n   <td>79.5\n   </td>\n   <td>79.3\n   </td>\n   <td>85.2\n   </td>\n  </tr>\n  <tr>\n   <td>MMLU-Pro (CoT)\n   </td>\n   <td>5\n   </td>\n   <td>macro_avg/acc_char\n   </td>\n   <td>36.2\n   </td>\n   <td>37.1\n   </td>\n   <td>55.0\n   </td>\n   <td>53.8\n   </td>\n   <td>61.6\n   </td>\n  </tr>\n  <tr>\n   <td>AGIEval English\n   </td>\n   <td>3-5\n   </td>\n   <td>average/acc_char\n   </td>\n   <td>47.1\n   </td>\n   <td>47.8\n   </td>\n   <td>63.0\n   </td>\n   <td>64.6\n   </td>\n   <td>71.6\n   </td>\n  </tr>\n  <tr>\n   <td>CommonSenseQA\n   </td>\n   <td>7\n   </td>\n   <td>acc_char\n   </td>\n   <td>72.6\n   </td>\n   <td>75.0\n   </td>\n   <td>83.8\n   </td>\n   <td>84.1\n   </td>\n   <td>85.8\n   </td>\n  </tr>\n  <tr>\n   <td>Winogrande\n   </td>\n   <td>5\n   </td>\n   <td>acc_char\n   </td>\n   <td>-\n   </td>\n   <td>60.5\n   </td>\n   <td>-\n   </td>\n   <td>83.3\n   </td>\n   <td>86.7\n   </td>\n  </tr>\n  <tr>\n   <td>BIG-Bench Hard (CoT)\n   </td>\n   <td>3\n   </td>\n   <td>average/em\n   </td>\n   <td>61.1\n   </td>\n   <td>64.2\n   </td>\n   <td>81.3\n   </td>\n   <td>81.6\n   </td>\n   <td>85.9\n   </td>\n  </tr>\n  <tr>\n   <td>ARC-Challenge\n   </td>\n   <td>25\n   </td>\n   <td>acc_char\n   </td>\n   <td>79.4\n   </td>\n   <td>79.7\n   </td>\n   <td>93.1\n   </td>\n   <td>92.9\n   </td>\n   <td>96.1\n   </td>\n  </tr>\n  <tr>\n   <td>Knowledge reasoning\n   </td>\n   <td>TriviaQA-Wiki\n   </td>\n   <td>5\n   </td>\n   <td>em\n   </td>\n   <td>78.5\n   </td>\n   <td>77.6\n   </td>\n   <td>89.7\n   </td>\n   <td>89.8\n   </td>\n   <td>91.8\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"4\" >Reading comprehension\n   </td>\n   <td>SQuAD\n   </td>\n   <td>1\n   </td>\n   <td>em\n   </td>\n   <td>76.4\n   </td>\n   <td>77.0\n   </td>\n   <td>85.6\n   </td>\n   <td>81.8\n   </td>\n   <td>89.3\n   </td>\n  </tr>\n  <tr>\n   <td>QuAC (F1)\n   </td>\n   <td>1\n   </td>\n   <td>f1\n   </td>\n   <td>44.4\n   </td>\n   <td>44.9\n   </td>\n   <td>51.1\n   </td>\n   <td>51.1\n   </td>\n   <td>53.6\n   </td>\n  </tr>\n  <tr>\n   <td>BoolQ\n   </td>\n   <td>0\n   </td>\n   <td>acc_char\n   </td>\n   <td>75.7\n   </td>\n   <td>75.0\n   </td>\n   <td>79.0\n   </td>\n   <td>79.4\n   </td>\n   <td>80.0\n   </td>\n  </tr>\n  <tr>\n   <td>DROP (F1)\n   </td>\n   <td>3\n   </td>\n   <td>f1\n   </td>\n   <td>58.4\n   </td>\n   <td>59.5\n   </td>\n   <td>79.7\n   </td>\n   <td>79.6\n   </td>\n   <td>84.8\n   </td>\n  </tr>\n</table>\n\n\n\n### Instruction tuned models\n\n\n<table>\n  <tr>\n   <td><strong>Category</strong>\n   </td>\n   <td><strong>Benchmark</strong>\n   </td>\n   <td><strong># Shots</strong>\n   </td>\n   <td><strong>Metric</strong>\n   </td>\n   <td><strong>Llama 3 8B Instruct</strong>\n   </td>\n   <td><strong>Llama 3.1 8B Instruct</strong>\n   </td>\n   <td><strong>Llama 3 70B Instruct</strong>\n   </td>\n   <td><strong>Llama 3.1 70B Instruct</strong>\n   </td>\n   <td><strong>Llama 3.1 405B Instruct</strong>\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"4\" >General\n   </td>\n   <td>MMLU\n   </td>\n   <td>5\n   </td>\n   <td>macro_avg/acc\n   </td>\n   <td>68.5\n   </td>\n   <td>69.4\n   </td>\n   <td>82.0\n   </td>\n   <td>83.6\n   </td>\n   <td>87.3\n   </td>\n  </tr>\n  <tr>\n   <td>MMLU (CoT)\n   </td>\n   <td>0\n   </td>\n   <td>macro_avg/acc\n   </td>\n   <td>65.3\n   </td>\n   <td>73.0\n   </td>\n   <td>80.9\n   </td>\n   <td>86.0\n   </td>\n   <td>88.6\n   </td>\n  </tr>\n  <tr>\n   <td>MMLU-Pro (CoT)\n   </td>\n   <td>5\n   </td>\n   <td>micro_avg/acc_char\n   </td>\n   <td>45.5\n   </td>\n   <td>48.3\n   </td>\n   <td>63.4\n   </td>\n   <td>66.4\n   </td>\n   <td>73.3\n   </td>\n  </tr>\n  <tr>\n   <td>IFEval\n   </td>\n   <td>\n   </td>\n   <td>\n   </td>\n   <td>76.8\n   </td>\n   <td>80.4\n   </td>\n   <td>82.9\n   </td>\n   <td>87.5\n   </td>\n   <td>88.6\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"2\" >Reasoning\n   </td>\n   <td>ARC-C\n   </td>\n   <td>0\n   </td>\n   <td>acc\n   </td>\n   <td>82.4\n   </td>\n   <td>83.4\n   </td>\n   <td>94.4\n   </td>\n   <td>94.8\n   </td>\n   <td>96.9\n   </td>\n  </tr>\n  <tr>\n   <td>GPQA\n   </td>\n   <td>0\n   </td>\n   <td>em\n   </td>\n   <td>34.6\n   </td>\n   <td>30.4\n   </td>\n   <td>39.5\n   </td>\n   <td>46.7\n   </td>\n   <td>50.7\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"4\" >Code\n   </td>\n   <td>HumanEval\n   </td>\n   <td>0\n   </td>\n   <td>pass@1\n   </td>\n   <td>60.4\n   </td>\n   <td>72.6\n   </td>\n   <td>81.7\n   </td>\n   <td>80.5\n   </td>\n   <td>89.0\n   </td>\n  </tr>\n  <tr>\n   <td>MBPP ++ base version\n   </td>\n   <td>0\n   </td>\n   <td>pass@1\n   </td>\n   <td>70.6\n   </td>\n   <td>72.8\n   </td>\n   <td>82.5\n   </td>\n   <td>86.0\n   </td>\n   <td>88.6\n   </td>\n  </tr>\n  <tr>\n   <td>Multipl-E HumanEval\n   </td>\n   <td>0\n   </td>\n   <td>pass@1\n   </td>\n   <td>-\n   </td>\n   <td>50.8\n   </td>\n   <td>-\n   </td>\n   <td>65.5\n   </td>\n   <td>75.2\n   </td>\n  </tr>\n  <tr>\n   <td>Multipl-E MBPP\n   </td>\n   <td>0\n   </td>\n   <td>pass@1\n   </td>\n   <td>-\n   </td>\n   <td>52.4\n   </td>\n   <td>-\n   </td>\n   <td>62.0\n   </td>\n   <td>65.7\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"2\" >Math\n   </td>\n   <td>GSM-8K (CoT)\n   </td>\n   <td>8\n   </td>\n   <td>em_maj1@1\n   </td>\n   <td>80.6\n   </td>\n   <td>84.5\n   </td>\n   <td>93.0\n   </td>\n   <td>95.1\n   </td>\n   <td>96.8\n   </td>\n  </tr>\n  <tr>\n   <td>MATH (CoT)\n   </td>\n   <td>0\n   </td>\n   <td>final_em\n   </td>\n   <td>29.1\n   </td>\n   <td>51.9\n   </td>\n   <td>51.0\n   </td>\n   <td>68.0\n   </td>\n   <td>73.8\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"4\" >Tool Use\n   </td>\n   <td>API-Bank\n   </td>\n   <td>0\n   </td>\n   <td>acc\n   </td>\n   <td>48.3\n   </td>\n   <td>82.6\n   </td>\n   <td>85.1\n   </td>\n   <td>90.0\n   </td>\n   <td>92.0\n   </td>\n  </tr>\n  <tr>\n   <td>BFCL\n   </td>\n   <td>0\n   </td>\n   <td>acc\n   </td>\n   <td>60.3\n   </td>\n   <td>76.1\n   </td>\n   <td>83.0\n   </td>\n   <td>84.8\n   </td>\n   <td>88.5\n   </td>\n  </tr>\n  <tr>\n   <td>Gorilla Benchmark API Bench\n   </td>\n   <td>0\n   </td>\n   <td>acc\n   </td>\n   <td>1.7\n   </td>\n   <td>8.2\n   </td>\n   <td>14.7\n   </td>\n   <td>29.7\n   </td>\n   <td>35.3\n   </td>\n  </tr>\n  <tr>\n   <td>Nexus (0-shot)\n   </td>\n   <td>0\n   </td>\n   <td>macro_avg/acc\n   </td>\n   <td>18.1\n   </td>\n   <td>38.5\n   </td>\n   <td>47.8\n   </td>\n   <td>56.7\n   </td>\n   <td>58.7\n   </td>\n  </tr>\n  <tr>\n   <td>Multilingual\n   </td>\n   <td>Multilingual MGSM (CoT)\n   </td>\n   <td>0\n   </td>\n   <td>em\n   </td>\n   <td>-\n   </td>\n   <td>68.9\n   </td>\n   <td>-\n   </td>\n   <td>86.9\n   </td>\n   <td>91.6\n   </td>\n  </tr>\n</table>\n\n#### Multilingual benchmarks\n\n<table>\n  <tr>\n   <td><strong>Category</strong>\n   </td>\n   <td><strong>Benchmark</strong>\n   </td>\n   <td><strong>Language</strong>\n   </td>\n   <td><strong>Llama 3.1 8B</strong>\n   </td>\n   <td><strong>Llama 3.1 70B</strong>\n   </td>\n   <td><strong>Llama 3.1 405B</strong>\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"9\" ><strong>General</strong>\n   </td>\n   <td rowspan=\"9\" ><strong>MMLU (5-shot, macro_avg/acc)</strong>\n   </td>\n   <td>Portuguese\n   </td>\n   <td>62.12\n   </td>\n   <td>80.13\n   </td>\n   <td>84.95\n   </td>\n  </tr>\n  <tr>\n   <td>Spanish\n   </td>\n   <td>62.45\n   </td>\n   <td>80.05\n   </td>\n   <td>85.08\n   </td>\n  </tr>\n  <tr>\n   <td>Italian\n   </td>\n   <td>61.63\n   </td>\n   <td>80.4\n   </td>\n   <td>85.04\n   </td>\n  </tr>\n  <tr>\n   <td>German\n   </td>\n   <td>60.59\n   </td>\n   <td>79.27\n   </td>\n   <td>84.36\n   </td>\n  </tr>\n  <tr>\n   <td>French\n   </td>\n   <td>62.34\n   </td>\n   <td>79.82\n   </td>\n   <td>84.66\n   </td>\n  </tr>\n  <tr>\n   <td>Hindi\n   </td>\n   <td>50.88\n   </td>\n   <td>74.52\n   </td>\n   <td>80.31\n   </td>\n  </tr>\n  <tr>\n   <td>Thai\n   </td>\n   <td>50.32\n   </td>\n   <td>72.95\n   </td>\n   <td>78.21\n   </td>\n  </tr>\n</table>\n\n\n\n## Responsibility & Safety\n\nAs part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:\n\n\n\n* Enable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama. \n* Protect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm.\n* Provide protections for the community to help prevent the misuse of our models.\n\n\n### Responsible deployment \n\nLlama is a foundational technology designed to be used in a variety of use cases, examples on how Meta’s Llama models have been responsibly deployed can be found in our [Community Stories webpage](https://llama.meta.com/community-stories/). Our approach is to build the most helpful models enabling the world to benefit from the technology power, by aligning our model safety for the generic use cases addressing a standard set of harms. Developers are then in the driver seat to tailor safety for their use case, defining their own policy and deploying the models with the necessary safeguards in their Llama systems. Llama 3.1 was developed following the best practices outlined in our Responsible Use Guide, you can refer to the [Responsible Use Guide](https://llama.meta.com/responsible-use-guide/) to learn more. \n\n\n#### Llama 3.1 instruct \n\nOur main objectives for conducting safety fine-tuning are to provide the research community with a valuable resource for studying the robustness of safety fine-tuning, as well as to offer developers a readily available, safe, and powerful model for various applications to reduce the developer workload to deploy safe AI systems. For more details on the safety mitigations implemented please read the Llama 3 paper. \n\n**Fine-tuning data**\n\nWe employ a multi-faceted approach to data collection, combining human-generated data from our vendors with synthetic data to mitigate potential safety risks. We’ve developed many large language model (LLM)-based classifiers that enable us to thoughtfully select high-quality prompts and responses, enhancing data quality control. \n\n**Refusals and Tone**\n\nBuilding on the work we started with Llama 3, we put a great emphasis on model refusals to benign prompts as well as refusal tone. We included both borderline and adversarial prompts in our safety data strategy, and modified our safety data responses to follow  tone guidelines. \n\n\n#### Llama 3.1 systems\n\n**Large language models, including Llama 3.1, are not designed to be deployed in isolation but instead should be deployed as part of an overall AI system with additional safety guardrails as required.** Developers are expected to deploy system safeguards when building agentic systems. Safeguards are key to achieve the right helpfulness-safety alignment as well as mitigating safety and security risks inherent to the system and any integration of the model or system with external tools. \n\nAs part of our responsible release approach, we provide the community with [safeguards](https://llama.meta.com/trust-and-safety/) that developers should deploy with Llama models or other LLMs, including Llama Guard 3, Prompt Guard and Code Shield. All our [reference implementations](https://github.com/meta-llama/llama-agentic-system) demos contain these safeguards by default so developers can benefit from system-level safety out-of-the-box. \n\n\n#### New capabilities \n\nNote that this release introduces new capabilities, including a longer context window, multilingual inputs and outputs and possible integrations by developers with third party tools. Building with these new capabilities requires specific considerations in addition to the best practices that generally apply across all Generative AI use cases.\n\n**Tool-use**: Just like in standard software development, developers are responsible for the integration of the LLM with the tools and services of their choice. They should define a clear policy for their use case and assess the integrity of the third party services they use to be aware of the safety and security limitations when using this capability. Refer to the Responsible Use Guide for best practices on the safe deployment of the third party safeguards. \n\n**Multilinguality**: Llama 3.1 supports 7 languages in addition to English: French, German, Hindi, Italian, Portuguese, Spanish, and Thai. Llama may be able to output text in other languages than those that meet performance thresholds for safety and helpfulness. We strongly discourage developers from using this model to converse in non-supported languages without implementing finetuning and system controls in alignment with their policies and the best practices shared in the Responsible Use Guide. \n\n\n### Evaluations\n\nWe evaluated Llama models for common use cases as well as specific capabilities. Common use cases evaluations measure safety risks of systems for most commonly built applications including chat bot, coding assistant, tool calls. We built dedicated, adversarial evaluation datasets and evaluated systems composed of Llama models and Llama Guard 3 to filter input prompt and output response. It is important to evaluate applications in context, and we recommend building dedicated evaluation dataset for your use case. Prompt Guard and Code Shield are also available if relevant to the application. \n\nCapability evaluations measure vulnerabilities of Llama models inherent to specific capabilities, for which were crafted dedicated benchmarks including long context, multilingual, tools calls, coding or memorization.\n\n**Red teaming**\n\nFor both scenarios, we conducted recurring red teaming exercises with the goal of discovering risks via adversarial prompting and we used the learnings to improve our benchmarks and safety tuning datasets. \n\nWe partnered early with subject-matter experts in critical risk areas to understand the nature of these real-world harms and how such models may lead to unintended harm for society. Based on these conversations, we derived a set of adversarial goals for the red team to attempt to achieve, such as extracting harmful information or reprogramming the model to act in a potentially harmful capacity.  The red team consisted of experts in cybersecurity, adversarial machine learning, responsible AI, and integrity in addition to multilingual content specialists with background in integrity issues in specific geographic markets.\n\n\n### Critical and other risks \n\nWe specifically focused our efforts on mitigating the following critical risk areas:\n\n**1- CBRNE (Chemical, Biological, Radiological, Nuclear, and Explosive materials) helpfulness**\n\nTo assess risks related to proliferation of chemical and biological weapons, we performed uplift testing designed to assess whether use of Llama 3.1 models could meaningfully increase the capabilities of malicious actors to plan or carry out attacks using these types of weapons. \n\n\n**2. Child Safety**\n\nChild Safety risk assessments were conducted using a team of experts, to assess the model’s capability to produce outputs that could result in Child Safety risks and inform on any necessary and appropriate risk mitigations via fine tuning. We leveraged those expert red teaming sessions to expand the coverage of our evaluation benchmarks through Llama 3 model development.  For Llama 3, we conducted new in-depth sessions using objective based methodologies to assess the model risks along multiple attack vectors including the additional languages Llama 3 is trained on. We also partnered with content specialists to perform red teaming exercises assessing potentially violating content while taking account of market specific nuances or experiences. \n\n**3. Cyber attack enablement**\n\nOur cyber attack uplift study investigated whether LLMs can enhance human capabilities in hacking tasks, both in terms of skill level and speed.\n\nOur attack automation study focused on evaluating the capabilities of LLMs when used as autonomous agents in cyber offensive operations, specifically in the context of ransomware attacks. This evaluation was distinct from previous studies that considered LLMs as interactive assistants. The primary objective was to assess whether these models could effectively function as independent agents in executing complex cyber-attacks without human intervention.\n\nOur study of Llama-3.1-405B’s social engineering uplift for cyber attackers was conducted to assess the effectiveness of AI models in aiding cyber threat actors in spear phishing campaigns. Please read our Llama 3.1 Cyber security whitepaper to learn more.\n\n\n### Community \n\nGenerative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership on AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Purple Llama tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to our [Github repository](https://github.com/meta-llama/PurpleLlama). \n\nWe also set up the [Llama Impact Grants](https://llama.meta.com/llama-impact-grants/) program to identify and support the most compelling applications of Meta’s Llama model for societal benefit across three categories: education, climate and open innovation. The 20 finalists from the hundreds of applications can be found [here](https://llama.meta.com/llama-impact-grants/#finalists). \n\nFinally, we put in place a set of resources including an [output reporting mechanism](https://developers.facebook.com/llama_output_feedback) and [bug bounty program](https://www.facebook.com/whitehat) to continuously improve the Llama technology with the help of the community.\n\n\n## Ethical Considerations and Limitations\n\nThe core values of Llama 3.1 are openness, inclusivity and helpfulness. It is meant to serve everyone, and to work for a wide range of use cases. It is thus designed to be accessible to people across many different backgrounds, experiences and perspectives. Llama 3.1 addresses users and their needs as they are, without insertion unnecessary judgment or normativity, while reflecting the understanding that even content that may appear problematic in some cases can serve valuable purposes in others. It respects the dignity and autonomy of all users, especially in terms of the values of free thought and expression that power innovation and progress. \n\nBut Llama 3.1 is a new technology, and like any new technology, there are risks associated with its use. Testing conducted to date has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, Llama 3.1’s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 3.1 models, developers should perform safety testing and tuning tailored to their specific applications of the model. Please refer to available resources including our [Responsible Use Guide](https://llama.meta.com/responsible-use-guide), [Trust and Safety](https://llama.meta.com/trust-and-safety/) solutions, and other [resources](https://llama.meta.com/docs/get-started/) to learn more about responsible development. ",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModelForCausalLM",
      "custom_class": null,
      "pipeline_tag": "text-generation",
      "processor": "AutoTokenizer"
    },
    "safetensors": {
      "parameters": {
        "BF16": 8030261248
      },
      "total": 8030261248
    },
    "model_index": null,
    "trending_score": null
  },
  {
    "model_id": "TheBloke/phi-2-GGUF",
    "model_name": "TheBloke/phi-2-GGUF",
    "author": "TheBloke",
    "downloads": 5283681,
    "downloads_all_time": null,
    "likes": 198,
    "tags": [
      "transformers",
      "gguf",
      "phi-msft",
      "nlp",
      "code",
      "text-generation",
      "en",
      "base_model:microsoft/phi-2",
      "base_model:quantized:microsoft/phi-2",
      "license:other",
      "region:us"
    ],
    "card_url": "https://huggingface.co/TheBloke/phi-2-GGUF",
    "dependencies": [
      [
        "torch",
        null
      ],
      [
        "transformers",
        null
      ]
    ],
    "last_modified": "2023-12-18T20:25:44+00:00",
    "created_at": "2023-12-18T20:22:56+00:00",
    "analysis_date": "2025-03-22T00:41:06.208197",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "phi-msft",
    "pipeline_tag": "text-generation",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "base_model": "microsoft/phi-2",
      "inference": false,
      "language": [
        "en"
      ],
      "license": "other",
      "license_link": "https://huggingface.co/microsoft/phi-2/resolve/main/LICENSE",
      "license_name": "microsoft-research-license",
      "model_creator": "Microsoft",
      "model_name": "Phi 2",
      "model_type": "phi-msft",
      "pipeline_tag": "text-generation",
      "prompt_template": "Instruct: {prompt}\nOutput:\n",
      "quantized_by": "TheBloke",
      "tags": [
        "nlp",
        "code"
      ]
    },
    "card_text": "<!-- markdownlint-disable MD041 -->\n\n<!-- header start -->\n<!-- 200823 -->\n<div style=\"width: auto; margin-left: auto; margin-right: auto\">\n<img src=\"https://i.imgur.com/EBdldam.jpg\" alt=\"TheBlokeAI\" style=\"width: 100%; min-width: 400px; display: block; margin: auto;\">\n</div>\n<div style=\"display: flex; justify-content: space-between; width: 100%;\">\n    <div style=\"display: flex; flex-direction: column; align-items: flex-start;\">\n        <p style=\"margin-top: 0.5em; margin-bottom: 0em;\"><a href=\"https://discord.gg/theblokeai\">Chat & support: TheBloke's Discord server</a></p>\n    </div>\n    <div style=\"display: flex; flex-direction: column; align-items: flex-end;\">\n        <p style=\"margin-top: 0.5em; margin-bottom: 0em;\"><a href=\"https://www.patreon.com/TheBlokeAI\">Want to contribute? TheBloke's Patreon page</a></p>\n    </div>\n</div>\n<div style=\"text-align:center; margin-top: 0em; margin-bottom: 0em\"><p style=\"margin-top: 0.25em; margin-bottom: 0em;\">TheBloke's LLM work is generously supported by a grant from <a href=\"https://a16z.com\">andreessen horowitz (a16z)</a></p></div>\n<hr style=\"margin-top: 1.0em; margin-bottom: 1.0em;\">\n<!-- header end -->\n\n# Phi 2 - GGUF\n- Model creator: [Microsoft](https://huggingface.co/microsoft)\n- Original model: [Phi 2](https://huggingface.co/microsoft/phi-2)\n\n<!-- description start -->\n## Description\n\nThis repo contains GGUF format model files for [Microsoft's Phi 2](https://huggingface.co/microsoft/phi-2).\n\n<!-- description end -->\n<!-- README_GGUF.md-about-gguf start -->\n### About GGUF\n\nGGUF is a new format introduced by the llama.cpp team on August 21st 2023. It is a replacement for GGML, which is no longer supported by llama.cpp.\n\nHere is an incomplete list of clients and libraries that are known to support GGUF:\n\n* [llama.cpp](https://github.com/ggerganov/llama.cpp). The source project for GGUF. Offers a CLI and a server option.\n* [text-generation-webui](https://github.com/oobabooga/text-generation-webui), the most widely used web UI, with many features and powerful extensions. Supports GPU acceleration.\n* [KoboldCpp](https://github.com/LostRuins/koboldcpp), a fully featured web UI, with GPU accel across all platforms and GPU architectures. Especially good for story telling.\n* [GPT4All](https://gpt4all.io/index.html), a free and open source local running GUI, supporting Windows, Linux and macOS with full GPU accel.\n* [LM Studio](https://lmstudio.ai/), an easy-to-use and powerful local GUI for Windows and macOS (Silicon), with GPU acceleration. Linux available, in beta as of 27/11/2023.\n* [LoLLMS Web UI](https://github.com/ParisNeo/lollms-webui), a great web UI with many interesting and unique features, including a full model library for easy model selection.\n* [Faraday.dev](https://faraday.dev/), an attractive and easy to use character-based chat GUI for Windows and macOS (both Silicon and Intel), with GPU acceleration.\n* [llama-cpp-python](https://github.com/abetlen/llama-cpp-python), a Python library with GPU accel, LangChain support, and OpenAI-compatible API server.\n* [candle](https://github.com/huggingface/candle), a Rust ML framework with a focus on performance, including GPU support, and ease of use.\n* [ctransformers](https://github.com/marella/ctransformers), a Python library with GPU accel, LangChain support, and OpenAI-compatible AI server. Note, as of time of writing (November 27th 2023), ctransformers has not been updated in a long time and does not support many recent models.\n\n<!-- README_GGUF.md-about-gguf end -->\n<!-- repositories-available start -->\n## Repositories available\n\n* [GPTQ models for GPU inference, with multiple quantisation parameter options.](https://huggingface.co/TheBloke/phi-2-GPTQ)\n* [2, 3, 4, 5, 6 and 8-bit GGUF models for CPU+GPU inference](https://huggingface.co/TheBloke/phi-2-GGUF)\n* [Microsoft's original unquantised fp16 model in pytorch format, for GPU inference and for further conversions](https://huggingface.co/microsoft/phi-2)\n<!-- repositories-available end -->\n\n<!-- prompt-template start -->\n## Prompt template: Phi\n\n```\nInstruct: {prompt}\nOutput:\n\n```\n\n<!-- prompt-template end -->\n\n\n<!-- compatibility_gguf start -->\n## Compatibility\n\nThese quantised GGUFv2 files are compatible with llama.cpp from August 27th onwards, as of commit [d0cee0d](https://github.com/ggerganov/llama.cpp/commit/d0cee0d36d5be95a0d9088b674dbb27354107221)\n\nThey are also compatible with many third party UIs and libraries - please see the list at the top of this README.\n\n## Explanation of quantisation methods\n\n<details>\n  <summary>Click to see details</summary>\n\nThe new methods available are:\n\n* GGML_TYPE_Q2_K - \"type-1\" 2-bit quantization in super-blocks containing 16 blocks, each block having 16 weight. Block scales and mins are quantized with 4 bits. This ends up effectively using 2.5625 bits per weight (bpw)\n* GGML_TYPE_Q3_K - \"type-0\" 3-bit quantization in super-blocks containing 16 blocks, each block having 16 weights. Scales are quantized with 6 bits. This end up using 3.4375 bpw.\n* GGML_TYPE_Q4_K - \"type-1\" 4-bit quantization in super-blocks containing 8 blocks, each block having 32 weights. Scales and mins are quantized with 6 bits. This ends up using 4.5 bpw.\n* GGML_TYPE_Q5_K - \"type-1\" 5-bit quantization. Same super-block structure as GGML_TYPE_Q4_K resulting in 5.5 bpw\n* GGML_TYPE_Q6_K - \"type-0\" 6-bit quantization. Super-blocks with 16 blocks, each block having 16 weights. Scales are quantized with 8 bits. This ends up using 6.5625 bpw\n\nRefer to the Provided Files table below to see what files use which methods, and how.\n</details>\n<!-- compatibility_gguf end -->\n\n<!-- README_GGUF.md-provided-files start -->\n## Provided files\n\n| Name | Quant method | Bits | Size | Max RAM required | Use case |\n| ---- | ---- | ---- | ---- | ---- | ----- |\n| [phi-2.Q2_K.gguf](https://huggingface.co/TheBloke/phi-2-GGUF/blob/main/phi-2.Q2_K.gguf) | Q2_K | 2 | 1.17 GB| 3.67 GB | smallest, significant quality loss - not recommended for most purposes |\n| [phi-2.Q3_K_S.gguf](https://huggingface.co/TheBloke/phi-2-GGUF/blob/main/phi-2.Q3_K_S.gguf) | Q3_K_S | 3 | 1.25 GB| 3.75 GB | very small, high quality loss |\n| [phi-2.Q3_K_M.gguf](https://huggingface.co/TheBloke/phi-2-GGUF/blob/main/phi-2.Q3_K_M.gguf) | Q3_K_M | 3 | 1.48 GB| 3.98 GB | very small, high quality loss |\n| [phi-2.Q4_0.gguf](https://huggingface.co/TheBloke/phi-2-GGUF/blob/main/phi-2.Q4_0.gguf) | Q4_0 | 4 | 1.60 GB| 4.10 GB | legacy; small, very high quality loss - prefer using Q3_K_M |\n| [phi-2.Q3_K_L.gguf](https://huggingface.co/TheBloke/phi-2-GGUF/blob/main/phi-2.Q3_K_L.gguf) | Q3_K_L | 3 | 1.60 GB| 4.10 GB | small, substantial quality loss |\n| [phi-2.Q4_K_S.gguf](https://huggingface.co/TheBloke/phi-2-GGUF/blob/main/phi-2.Q4_K_S.gguf) | Q4_K_S | 4 | 1.62 GB| 4.12 GB | small, greater quality loss |\n| [phi-2.Q4_K_M.gguf](https://huggingface.co/TheBloke/phi-2-GGUF/blob/main/phi-2.Q4_K_M.gguf) | Q4_K_M | 4 | 1.79 GB| 4.29 GB | medium, balanced quality - recommended |\n| [phi-2.Q5_0.gguf](https://huggingface.co/TheBloke/phi-2-GGUF/blob/main/phi-2.Q5_0.gguf) | Q5_0 | 5 | 1.93 GB| 4.43 GB | legacy; medium, balanced quality - prefer using Q4_K_M |\n| [phi-2.Q5_K_S.gguf](https://huggingface.co/TheBloke/phi-2-GGUF/blob/main/phi-2.Q5_K_S.gguf) | Q5_K_S | 5 | 1.93 GB| 4.43 GB | large, low quality loss - recommended |\n| [phi-2.Q5_K_M.gguf](https://huggingface.co/TheBloke/phi-2-GGUF/blob/main/phi-2.Q5_K_M.gguf) | Q5_K_M | 5 | 2.07 GB| 4.57 GB | large, very low quality loss - recommended |\n| [phi-2.Q6_K.gguf](https://huggingface.co/TheBloke/phi-2-GGUF/blob/main/phi-2.Q6_K.gguf) | Q6_K | 6 | 2.29 GB| 4.79 GB | very large, extremely low quality loss |\n| [phi-2.Q8_0.gguf](https://huggingface.co/TheBloke/phi-2-GGUF/blob/main/phi-2.Q8_0.gguf) | Q8_0 | 8 | 2.96 GB| 5.46 GB | very large, extremely low quality loss - not recommended |\n\n**Note**: the above RAM figures assume no GPU offloading. If layers are offloaded to the GPU, this will reduce RAM usage and use VRAM instead.\n\n\n\n<!-- README_GGUF.md-provided-files end -->\n\n<!-- README_GGUF.md-how-to-download start -->\n## How to download GGUF files\n\n**Note for manual downloaders:** You almost never want to clone the entire repo! Multiple different quantisation formats are provided, and most users only want to pick and download a single file.\n\nThe following clients/libraries will automatically download models for you, providing a list of available models to choose from:\n\n* LM Studio\n* LoLLMS Web UI\n* Faraday.dev\n\n### In `text-generation-webui`\n\nUnder Download Model, you can enter the model repo: TheBloke/phi-2-GGUF and below it, a specific filename to download, such as: phi-2.Q4_K_M.gguf.\n\nThen click Download.\n\n### On the command line, including multiple files at once\n\nI recommend using the `huggingface-hub` Python library:\n\n```shell\npip3 install huggingface-hub\n```\n\nThen you can download any individual model file to the current directory, at high speed, with a command like this:\n\n```shell\nhuggingface-cli download TheBloke/phi-2-GGUF phi-2.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False\n```\n\n<details>\n  <summary>More advanced huggingface-cli download usage (click to read)</summary>\n\nYou can also download multiple files at once with a pattern:\n\n```shell\nhuggingface-cli download TheBloke/phi-2-GGUF --local-dir . --local-dir-use-symlinks False --include='*Q4_K*gguf'\n```\n\nFor more documentation on downloading with `huggingface-cli`, please see: [HF -> Hub Python Library -> Download files -> Download from the CLI](https://huggingface.co/docs/huggingface_hub/guides/download#download-from-the-cli).\n\nTo accelerate downloads on fast connections (1Gbit/s or higher), install `hf_transfer`:\n\n```shell\npip3 install hf_transfer\n```\n\nAnd set environment variable `HF_HUB_ENABLE_HF_TRANSFER` to `1`:\n\n```shell\nHF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download TheBloke/phi-2-GGUF phi-2.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False\n```\n\nWindows Command Line users: You can set the environment variable by running `set HF_HUB_ENABLE_HF_TRANSFER=1` before the download command.\n</details>\n<!-- README_GGUF.md-how-to-download end -->\n\n<!-- README_GGUF.md-how-to-run start -->\n## Example `llama.cpp` command\n\nMake sure you are using `llama.cpp` from commit [d0cee0d](https://github.com/ggerganov/llama.cpp/commit/d0cee0d36d5be95a0d9088b674dbb27354107221) or later.\n\n```shell\n./main -ngl 35 -m phi-2.Q4_K_M.gguf --color -c 2048 --temp 0.7 --repeat_penalty 1.1 -n -1 -p \"Instruct: {prompt}\\nOutput:\"\n```\n\nChange `-ngl 32` to the number of layers to offload to GPU. Remove it if you don't have GPU acceleration.\n\nChange `-c 2048` to the desired sequence length. For extended sequence models - eg 8K, 16K, 32K - the necessary RoPE scaling parameters are read from the GGUF file and set by llama.cpp automatically. Note that longer sequence lengths require much more resources, so you may need to reduce this value.\n\nIf you want to have a chat-style conversation, replace the `-p <PROMPT>` argument with `-i -ins`\n\nFor other parameters and how to use them, please refer to [the llama.cpp documentation](https://github.com/ggerganov/llama.cpp/blob/master/examples/main/README.md)\n\n## How to run in `text-generation-webui`\n\nFurther instructions can be found in the text-generation-webui documentation, here: [text-generation-webui/docs/04 ‐ Model Tab.md](https://github.com/oobabooga/text-generation-webui/blob/main/docs/04%20%E2%80%90%20Model%20Tab.md#llamacpp).\n\n## How to run from Python code\n\nYou can use GGUF models from Python using the [llama-cpp-python](https://github.com/abetlen/llama-cpp-python) or [ctransformers](https://github.com/marella/ctransformers) libraries. Note that at the time of writing (Nov 27th 2023), ctransformers has not been updated for some time and is not compatible with some recent models. Therefore I recommend you use llama-cpp-python.\n\n### How to load this model in Python code, using llama-cpp-python\n\nFor full documentation, please see: [llama-cpp-python docs](https://abetlen.github.io/llama-cpp-python/).\n\n#### First install the package\n\nRun one of the following commands, according to your system:\n\n```shell\n# Base ctransformers with no GPU acceleration\npip install llama-cpp-python\n# With NVidia CUDA acceleration\nCMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install llama-cpp-python\n# Or with OpenBLAS acceleration\nCMAKE_ARGS=\"-DLLAMA_BLAS=ON -DLLAMA_BLAS_VENDOR=OpenBLAS\" pip install llama-cpp-python\n# Or with CLBLast acceleration\nCMAKE_ARGS=\"-DLLAMA_CLBLAST=on\" pip install llama-cpp-python\n# Or with AMD ROCm GPU acceleration (Linux only)\nCMAKE_ARGS=\"-DLLAMA_HIPBLAS=on\" pip install llama-cpp-python\n# Or with Metal GPU acceleration for macOS systems only\nCMAKE_ARGS=\"-DLLAMA_METAL=on\" pip install llama-cpp-python\n\n# In windows, to set the variables CMAKE_ARGS in PowerShell, follow this format; eg for NVidia CUDA:\n$env:CMAKE_ARGS = \"-DLLAMA_OPENBLAS=on\"\npip install llama-cpp-python\n```\n\n#### Simple llama-cpp-python example code\n\n```python\nfrom llama_cpp import Llama\n\n# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\nllm = Llama(\n  model_path=\"./phi-2.Q4_K_M.gguf\",  # Download the model file first\n  n_ctx=2048,  # The max sequence length to use - note that longer sequence lengths require much more resources\n  n_threads=8,            # The number of CPU threads to use, tailor to your system and the resulting performance\n  n_gpu_layers=35         # The number of layers to offload to GPU, if you have GPU acceleration available\n)\n\n# Simple inference example\noutput = llm(\n  \"Instruct: {prompt}\\nOutput:\", # Prompt\n  max_tokens=512,  # Generate up to 512 tokens\n  stop=[\"</s>\"],   # Example stop token - not necessarily correct for this specific model! Please check before using.\n  echo=True        # Whether to echo the prompt\n)\n\n# Chat Completion API\n\nllm = Llama(model_path=\"./phi-2.Q4_K_M.gguf\", chat_format=\"llama-2\")  # Set chat_format according to the model you are using\nllm.create_chat_completion(\n    messages = [\n        {\"role\": \"system\", \"content\": \"You are a story writing assistant.\"},\n        {\n            \"role\": \"user\",\n            \"content\": \"Write a story about llamas.\"\n        }\n    ]\n)\n```\n\n## How to use with LangChain\n\nHere are guides on using llama-cpp-python and ctransformers with LangChain:\n\n* [LangChain + llama-cpp-python](https://python.langchain.com/docs/integrations/llms/llamacpp)\n* [LangChain + ctransformers](https://python.langchain.com/docs/integrations/providers/ctransformers)\n\n<!-- README_GGUF.md-how-to-run end -->\n\n<!-- footer start -->\n<!-- 200823 -->\n## Discord\n\nFor further support, and discussions on these models and AI in general, join us at:\n\n[TheBloke AI's Discord server](https://discord.gg/theblokeai)\n\n## Thanks, and how to contribute\n\nThanks to the [chirper.ai](https://chirper.ai) team!\n\nThanks to Clay from [gpus.llm-utils.org](llm-utils)!\n\nI've had a lot of people ask if they can contribute. I enjoy providing models and helping people, and would love to be able to spend even more time doing it, as well as expanding into new projects like fine tuning/training.\n\nIf you're able and willing to contribute it will be most gratefully received and will help me to keep providing more models, and to start work on new AI projects.\n\nDonaters will get priority support on any and all AI/LLM/model questions and requests, access to a private Discord room, plus other benefits.\n\n* Patreon: https://patreon.com/TheBlokeAI\n* Ko-Fi: https://ko-fi.com/TheBlokeAI\n\n**Special thanks to**: Aemon Algiz.\n\n**Patreon special mentions**: Michael Levine, 阿明, Trailburnt, Nikolai Manek, John Detwiler, Randy H, Will Dee, Sebastain Graf, NimbleBox.ai, Eugene Pentland, Emad Mostaque, Ai Maven, Jim Angel, Jeff Scroggin, Michael Davis, Manuel Alberto Morcote, Stephen Murray, Robert, Justin Joy, Luke @flexchar, Brandon Frisco, Elijah Stavena, S_X, Dan Guido, Undi ., Komninos Chatzipapas, Shadi, theTransient, Lone Striker, Raven Klaugh, jjj, Cap'n Zoog, Michel-Marie MAUDET (LINAGORA), Matthew Berman, David, Fen Risland, Omer Bin Jawed, Luke Pendergrass, Kalila, OG, Erik Bjäreholt, Rooh Singh, Joseph William Delisle, Dan Lewis, TL, John Villwock, AzureBlack, Brad, Pedro Madruga, Caitlyn Gatomon, K, jinyuan sun, Mano Prime, Alex, Jeffrey Morgan, Alicia Loh, Illia Dulskyi, Chadd, transmissions 11, fincy, Rainer Wilmers, ReadyPlayerEmma, knownsqashed, Mandus, biorpg, Deo Leter, Brandon Phillips, SuperWojo, Sean Connelly, Iucharbius, Jack West, Harry Royden McLaughlin, Nicholas, terasurfer, Vitor Caleffi, Duane Dunston, Johann-Peter Hartmann, David Ziegler, Olakabola, Ken Nordquist, Trenton Dambrowitz, Tom X Nguyen, Vadim, Ajan Kanaga, Leonard Tan, Clay Pascal, Alexandros Triantafyllidis, JM33133, Xule, vamX, ya boyyy, subjectnull, Talal Aujan, Alps Aficionado, wassieverse, Ari Malik, James Bentley, Woland, Spencer Kim, Michael Dempsey, Fred von Graf, Elle, zynix, William Richards, Stanislav Ovsiannikov, Edmond Seymore, Jonathan Leane, Martin Kemka, usrbinkat, Enrico Ros\n\n\nThank you to all my generous patrons and donaters!\n\nAnd thank you again to a16z for their generous grant.\n\n<!-- footer end -->\n\n<!-- original-model-card start -->\n# Original model card: Microsoft's Phi 2\n\n\n## Model Summary\n\nPhi-2 is a Transformer with **2.7 billion** parameters. It was trained using the same data sources as [Phi-1.5](https://huggingface.co/microsoft/phi-1.5), augmented with a new data source that consists of various NLP synthetic texts and filtered websites (for safety and educational value). When assessed against benchmarks testing common sense, language understanding, and logical reasoning, Phi-2 showcased a nearly state-of-the-art performance among models with less than 13 billion parameters.\n\nOur model hasn't been fine-tuned through reinforcement learning from human feedback. The intention behind crafting this open-source model is to provide the research community with a non-restricted small model to explore vital safety challenges, such as reducing toxicity, understanding societal biases, enhancing controllability, and more.\n\n## Intended Uses\n\nPhi-2 is intended for research purposes only. Given the nature of the training data, the Phi-2 model is best suited for prompts using the QA format, the chat format, and the code format.\n\n### QA Format:\n\nYou can provide the prompt as a standalone question as follows:\n\n```markdown\nWrite a detailed analogy between mathematics and a lighthouse.\n```\nwhere the model generates the text after \".\" .\nTo encourage the model to write more concise answers, you can also try the following QA format using \"Instruct: \\<prompt\\>\\nOutput:\"\n```markdown\nInstruct: Write a detailed analogy between mathematics and a lighthouse.\nOutput: Mathematics is like a lighthouse. Just as a lighthouse guides ships safely to shore, mathematics provides a guiding light in the world of numbers and logic. It helps us navigate through complex problems and find solutions. Just as a lighthouse emits a steady beam of light, mathematics provides a consistent framework for reasoning and problem-solving. It illuminates the path to understanding and helps us make sense of the world around us.\n```\nwhere the model generates the text after \"Output:\".\n\n### Chat Format:\n\n```markdown\nAlice: I don't know why, I'm struggling to maintain focus while studying. Any suggestions?\nBob: Well, have you tried creating a study schedule and sticking to it?\nAlice: Yes, I have, but it doesn't seem to help much.\nBob: Hmm, maybe you should try studying in a quiet environment, like the library.\nAlice: ...\n```\n\nwhere the model generates the text after the first \"Bob:\".\n\n### Code Format:\n\n```python\ndef print_prime(n):\n   \"\"\"\n   Print all primes between 1 and n\n   \"\"\"\n   primes = []\n   for num in range(2, n+1):\n       is_prime = True\n       for i in range(2, int(math.sqrt(num))+1):\n           if num % i == 0:\n               is_prime = False\n               break\n       if is_prime:\n           primes.append(num)\n   print(primes)\n```\nwhere the model generates the text after the comments.\n\n**Notes:**\n* Phi-2 is intended for research purposes. The model-generated text/code should be treated as a starting point rather than a definitive solution for potential use cases. Users should be cautious when employing these models in their applications.\n* Direct adoption for production tasks is out of the scope of this research project. As a result, the Phi-2 model has not been tested to ensure that it performs adequately for any production-level application. Please refer to the limitation sections of this document for more details.\n* If you are using `transformers>=4.36.0`, always load the model with `trust_remote_code=True` to prevent side-effects.\n\n## Sample Code\n\nThere are four types of execution mode:\n\n1. FP16 / Flash-Attention / CUDA:\n   ```python\n   model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\", torch_dtype=\"auto\", flash_attn=True, flash_rotary=True, fused_dense=True, device_map=\"cuda\", trust_remote_code=True)\n   ```\n2. FP16 / CUDA:\n   ```python\n   model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\", torch_dtype=\"auto\", device_map=\"cuda\", trust_remote_code=True)\n   ```\n3. FP32 / CUDA:\n   ```python\n   model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\", torch_dtype=torch.float32, device_map=\"cuda\", trust_remote_code=True)\n   ```\n4. FP32 / CPU:\n   ```python\n   model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\", torch_dtype=torch.float32, device_map=\"cpu\", trust_remote_code=True)\n   ```\n\nTo ensure the maximum compatibility, we recommend using the second execution mode (FP16 / CUDA), as follows:\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ntorch.set_default_device(\"cuda\")\n\nmodel = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\", torch_dtype=\"auto\", trust_remote_code=True)\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\", trust_remote_code=True)\n\ninputs = tokenizer('''def print_prime(n):\n   \"\"\"\n   Print all primes between 1 and n\n   \"\"\"''', return_tensors=\"pt\", return_attention_mask=False)\n\noutputs = model.generate(**inputs, max_length=200)\ntext = tokenizer.batch_decode(outputs)[0]\nprint(text)\n```\n\n**Remark:** In the generation function, our model currently does not support beam search (`num_beams > 1`).\nFurthermore, in the forward pass of the model, we currently do not support outputting hidden states or attention values, or using custom input embeddings.\n\n## Limitations of Phi-2\n\n* Generate Inaccurate Code and Facts: The model may produce incorrect code snippets and statements. Users should treat these outputs as suggestions or starting points, not as definitive or accurate solutions.\n\n* Limited Scope for code: Majority of Phi-2 training data is based in Python and use common packages such as \"typing, math, random, collections, datetime, itertools\". If the model generates Python scripts that utilize other packages or scripts in other languages, we strongly recommend users manually verify all API uses.\n\n* Unreliable Responses to Instruction: The model has not undergone instruction fine-tuning. As a result, it may struggle or fail to adhere to intricate or nuanced instructions provided by users.\n\n* Language Limitations: The model is primarily designed to understand standard English. Informal English, slang, or any other languages might pose challenges to its comprehension, leading to potential misinterpretations or errors in response.\n\n* Potential Societal Biases: Phi-2 is not entirely free from societal biases despite efforts in assuring trainig data safety. There's a possibility it may generate content that mirrors these societal biases, particularly if prompted or instructed to do so. We urge users to be aware of this and to exercise caution and critical thinking when interpreting model outputs.\n\n* Toxicity: Despite being trained with carefully selected data, the model can still produce harmful content if explicitly prompted or instructed to do so. We chose to release the model for research purposes only -- We hope to help the open-source community develop the most effective ways to reduce the toxicity of a model directly after pretraining.\n\n* Verbosity: Phi-2 being a base model often produces irrelevant or extra text and responses following its first answer to user prompts within a single turn. This is due to its training dataset being primarily textbooks, which results in textbook-like responses.\n\n## Training\n\n### Model\n\n* Architecture: a Transformer-based model with next-word prediction objective\n\n* Context length: 2048 tokens\n\n* Dataset size: 250B tokens, combination of NLP synthetic data created by AOAI GPT-3.5 and filtered web data from Falcon RefinedWeb and SlimPajama, which was assessed by AOAI GPT-4.\n\n* Training tokens: 1.4T tokens\n\n* GPUs: 96xA100-80G\n\n* Training time: 14 days\n\n### Software\n\n* [PyTorch](https://github.com/pytorch/pytorch)\n\n* [DeepSpeed](https://github.com/microsoft/DeepSpeed)\n\n* [Flash-Attention](https://github.com/HazyResearch/flash-attention)\n\n### License\n\nThe model is licensed under the [microsoft-research-license](https://huggingface.co/microsoft/phi-2/resolve/main/LICENSE).\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow [Microsoft’s Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks). Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party’s policies.\n\n<!-- original-model-card end -->\n",
    "card_content": "---\nbase_model: microsoft/phi-2\ninference: false\nlanguage:\n- en\nlicense: other\nlicense_link: https://huggingface.co/microsoft/phi-2/resolve/main/LICENSE\nlicense_name: microsoft-research-license\nmodel_creator: Microsoft\nmodel_name: Phi 2\nmodel_type: phi-msft\npipeline_tag: text-generation\nprompt_template: 'Instruct: {prompt}\n\n  Output:\n\n  '\nquantized_by: TheBloke\ntags:\n- nlp\n- code\n---\n<!-- markdownlint-disable MD041 -->\n\n<!-- header start -->\n<!-- 200823 -->\n<div style=\"width: auto; margin-left: auto; margin-right: auto\">\n<img src=\"https://i.imgur.com/EBdldam.jpg\" alt=\"TheBlokeAI\" style=\"width: 100%; min-width: 400px; display: block; margin: auto;\">\n</div>\n<div style=\"display: flex; justify-content: space-between; width: 100%;\">\n    <div style=\"display: flex; flex-direction: column; align-items: flex-start;\">\n        <p style=\"margin-top: 0.5em; margin-bottom: 0em;\"><a href=\"https://discord.gg/theblokeai\">Chat & support: TheBloke's Discord server</a></p>\n    </div>\n    <div style=\"display: flex; flex-direction: column; align-items: flex-end;\">\n        <p style=\"margin-top: 0.5em; margin-bottom: 0em;\"><a href=\"https://www.patreon.com/TheBlokeAI\">Want to contribute? TheBloke's Patreon page</a></p>\n    </div>\n</div>\n<div style=\"text-align:center; margin-top: 0em; margin-bottom: 0em\"><p style=\"margin-top: 0.25em; margin-bottom: 0em;\">TheBloke's LLM work is generously supported by a grant from <a href=\"https://a16z.com\">andreessen horowitz (a16z)</a></p></div>\n<hr style=\"margin-top: 1.0em; margin-bottom: 1.0em;\">\n<!-- header end -->\n\n# Phi 2 - GGUF\n- Model creator: [Microsoft](https://huggingface.co/microsoft)\n- Original model: [Phi 2](https://huggingface.co/microsoft/phi-2)\n\n<!-- description start -->\n## Description\n\nThis repo contains GGUF format model files for [Microsoft's Phi 2](https://huggingface.co/microsoft/phi-2).\n\n<!-- description end -->\n<!-- README_GGUF.md-about-gguf start -->\n### About GGUF\n\nGGUF is a new format introduced by the llama.cpp team on August 21st 2023. It is a replacement for GGML, which is no longer supported by llama.cpp.\n\nHere is an incomplete list of clients and libraries that are known to support GGUF:\n\n* [llama.cpp](https://github.com/ggerganov/llama.cpp). The source project for GGUF. Offers a CLI and a server option.\n* [text-generation-webui](https://github.com/oobabooga/text-generation-webui), the most widely used web UI, with many features and powerful extensions. Supports GPU acceleration.\n* [KoboldCpp](https://github.com/LostRuins/koboldcpp), a fully featured web UI, with GPU accel across all platforms and GPU architectures. Especially good for story telling.\n* [GPT4All](https://gpt4all.io/index.html), a free and open source local running GUI, supporting Windows, Linux and macOS with full GPU accel.\n* [LM Studio](https://lmstudio.ai/), an easy-to-use and powerful local GUI for Windows and macOS (Silicon), with GPU acceleration. Linux available, in beta as of 27/11/2023.\n* [LoLLMS Web UI](https://github.com/ParisNeo/lollms-webui), a great web UI with many interesting and unique features, including a full model library for easy model selection.\n* [Faraday.dev](https://faraday.dev/), an attractive and easy to use character-based chat GUI for Windows and macOS (both Silicon and Intel), with GPU acceleration.\n* [llama-cpp-python](https://github.com/abetlen/llama-cpp-python), a Python library with GPU accel, LangChain support, and OpenAI-compatible API server.\n* [candle](https://github.com/huggingface/candle), a Rust ML framework with a focus on performance, including GPU support, and ease of use.\n* [ctransformers](https://github.com/marella/ctransformers), a Python library with GPU accel, LangChain support, and OpenAI-compatible AI server. Note, as of time of writing (November 27th 2023), ctransformers has not been updated in a long time and does not support many recent models.\n\n<!-- README_GGUF.md-about-gguf end -->\n<!-- repositories-available start -->\n## Repositories available\n\n* [GPTQ models for GPU inference, with multiple quantisation parameter options.](https://huggingface.co/TheBloke/phi-2-GPTQ)\n* [2, 3, 4, 5, 6 and 8-bit GGUF models for CPU+GPU inference](https://huggingface.co/TheBloke/phi-2-GGUF)\n* [Microsoft's original unquantised fp16 model in pytorch format, for GPU inference and for further conversions](https://huggingface.co/microsoft/phi-2)\n<!-- repositories-available end -->\n\n<!-- prompt-template start -->\n## Prompt template: Phi\n\n```\nInstruct: {prompt}\nOutput:\n\n```\n\n<!-- prompt-template end -->\n\n\n<!-- compatibility_gguf start -->\n## Compatibility\n\nThese quantised GGUFv2 files are compatible with llama.cpp from August 27th onwards, as of commit [d0cee0d](https://github.com/ggerganov/llama.cpp/commit/d0cee0d36d5be95a0d9088b674dbb27354107221)\n\nThey are also compatible with many third party UIs and libraries - please see the list at the top of this README.\n\n## Explanation of quantisation methods\n\n<details>\n  <summary>Click to see details</summary>\n\nThe new methods available are:\n\n* GGML_TYPE_Q2_K - \"type-1\" 2-bit quantization in super-blocks containing 16 blocks, each block having 16 weight. Block scales and mins are quantized with 4 bits. This ends up effectively using 2.5625 bits per weight (bpw)\n* GGML_TYPE_Q3_K - \"type-0\" 3-bit quantization in super-blocks containing 16 blocks, each block having 16 weights. Scales are quantized with 6 bits. This end up using 3.4375 bpw.\n* GGML_TYPE_Q4_K - \"type-1\" 4-bit quantization in super-blocks containing 8 blocks, each block having 32 weights. Scales and mins are quantized with 6 bits. This ends up using 4.5 bpw.\n* GGML_TYPE_Q5_K - \"type-1\" 5-bit quantization. Same super-block structure as GGML_TYPE_Q4_K resulting in 5.5 bpw\n* GGML_TYPE_Q6_K - \"type-0\" 6-bit quantization. Super-blocks with 16 blocks, each block having 16 weights. Scales are quantized with 8 bits. This ends up using 6.5625 bpw\n\nRefer to the Provided Files table below to see what files use which methods, and how.\n</details>\n<!-- compatibility_gguf end -->\n\n<!-- README_GGUF.md-provided-files start -->\n## Provided files\n\n| Name | Quant method | Bits | Size | Max RAM required | Use case |\n| ---- | ---- | ---- | ---- | ---- | ----- |\n| [phi-2.Q2_K.gguf](https://huggingface.co/TheBloke/phi-2-GGUF/blob/main/phi-2.Q2_K.gguf) | Q2_K | 2 | 1.17 GB| 3.67 GB | smallest, significant quality loss - not recommended for most purposes |\n| [phi-2.Q3_K_S.gguf](https://huggingface.co/TheBloke/phi-2-GGUF/blob/main/phi-2.Q3_K_S.gguf) | Q3_K_S | 3 | 1.25 GB| 3.75 GB | very small, high quality loss |\n| [phi-2.Q3_K_M.gguf](https://huggingface.co/TheBloke/phi-2-GGUF/blob/main/phi-2.Q3_K_M.gguf) | Q3_K_M | 3 | 1.48 GB| 3.98 GB | very small, high quality loss |\n| [phi-2.Q4_0.gguf](https://huggingface.co/TheBloke/phi-2-GGUF/blob/main/phi-2.Q4_0.gguf) | Q4_0 | 4 | 1.60 GB| 4.10 GB | legacy; small, very high quality loss - prefer using Q3_K_M |\n| [phi-2.Q3_K_L.gguf](https://huggingface.co/TheBloke/phi-2-GGUF/blob/main/phi-2.Q3_K_L.gguf) | Q3_K_L | 3 | 1.60 GB| 4.10 GB | small, substantial quality loss |\n| [phi-2.Q4_K_S.gguf](https://huggingface.co/TheBloke/phi-2-GGUF/blob/main/phi-2.Q4_K_S.gguf) | Q4_K_S | 4 | 1.62 GB| 4.12 GB | small, greater quality loss |\n| [phi-2.Q4_K_M.gguf](https://huggingface.co/TheBloke/phi-2-GGUF/blob/main/phi-2.Q4_K_M.gguf) | Q4_K_M | 4 | 1.79 GB| 4.29 GB | medium, balanced quality - recommended |\n| [phi-2.Q5_0.gguf](https://huggingface.co/TheBloke/phi-2-GGUF/blob/main/phi-2.Q5_0.gguf) | Q5_0 | 5 | 1.93 GB| 4.43 GB | legacy; medium, balanced quality - prefer using Q4_K_M |\n| [phi-2.Q5_K_S.gguf](https://huggingface.co/TheBloke/phi-2-GGUF/blob/main/phi-2.Q5_K_S.gguf) | Q5_K_S | 5 | 1.93 GB| 4.43 GB | large, low quality loss - recommended |\n| [phi-2.Q5_K_M.gguf](https://huggingface.co/TheBloke/phi-2-GGUF/blob/main/phi-2.Q5_K_M.gguf) | Q5_K_M | 5 | 2.07 GB| 4.57 GB | large, very low quality loss - recommended |\n| [phi-2.Q6_K.gguf](https://huggingface.co/TheBloke/phi-2-GGUF/blob/main/phi-2.Q6_K.gguf) | Q6_K | 6 | 2.29 GB| 4.79 GB | very large, extremely low quality loss |\n| [phi-2.Q8_0.gguf](https://huggingface.co/TheBloke/phi-2-GGUF/blob/main/phi-2.Q8_0.gguf) | Q8_0 | 8 | 2.96 GB| 5.46 GB | very large, extremely low quality loss - not recommended |\n\n**Note**: the above RAM figures assume no GPU offloading. If layers are offloaded to the GPU, this will reduce RAM usage and use VRAM instead.\n\n\n\n<!-- README_GGUF.md-provided-files end -->\n\n<!-- README_GGUF.md-how-to-download start -->\n## How to download GGUF files\n\n**Note for manual downloaders:** You almost never want to clone the entire repo! Multiple different quantisation formats are provided, and most users only want to pick and download a single file.\n\nThe following clients/libraries will automatically download models for you, providing a list of available models to choose from:\n\n* LM Studio\n* LoLLMS Web UI\n* Faraday.dev\n\n### In `text-generation-webui`\n\nUnder Download Model, you can enter the model repo: TheBloke/phi-2-GGUF and below it, a specific filename to download, such as: phi-2.Q4_K_M.gguf.\n\nThen click Download.\n\n### On the command line, including multiple files at once\n\nI recommend using the `huggingface-hub` Python library:\n\n```shell\npip3 install huggingface-hub\n```\n\nThen you can download any individual model file to the current directory, at high speed, with a command like this:\n\n```shell\nhuggingface-cli download TheBloke/phi-2-GGUF phi-2.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False\n```\n\n<details>\n  <summary>More advanced huggingface-cli download usage (click to read)</summary>\n\nYou can also download multiple files at once with a pattern:\n\n```shell\nhuggingface-cli download TheBloke/phi-2-GGUF --local-dir . --local-dir-use-symlinks False --include='*Q4_K*gguf'\n```\n\nFor more documentation on downloading with `huggingface-cli`, please see: [HF -> Hub Python Library -> Download files -> Download from the CLI](https://huggingface.co/docs/huggingface_hub/guides/download#download-from-the-cli).\n\nTo accelerate downloads on fast connections (1Gbit/s or higher), install `hf_transfer`:\n\n```shell\npip3 install hf_transfer\n```\n\nAnd set environment variable `HF_HUB_ENABLE_HF_TRANSFER` to `1`:\n\n```shell\nHF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download TheBloke/phi-2-GGUF phi-2.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False\n```\n\nWindows Command Line users: You can set the environment variable by running `set HF_HUB_ENABLE_HF_TRANSFER=1` before the download command.\n</details>\n<!-- README_GGUF.md-how-to-download end -->\n\n<!-- README_GGUF.md-how-to-run start -->\n## Example `llama.cpp` command\n\nMake sure you are using `llama.cpp` from commit [d0cee0d](https://github.com/ggerganov/llama.cpp/commit/d0cee0d36d5be95a0d9088b674dbb27354107221) or later.\n\n```shell\n./main -ngl 35 -m phi-2.Q4_K_M.gguf --color -c 2048 --temp 0.7 --repeat_penalty 1.1 -n -1 -p \"Instruct: {prompt}\\nOutput:\"\n```\n\nChange `-ngl 32` to the number of layers to offload to GPU. Remove it if you don't have GPU acceleration.\n\nChange `-c 2048` to the desired sequence length. For extended sequence models - eg 8K, 16K, 32K - the necessary RoPE scaling parameters are read from the GGUF file and set by llama.cpp automatically. Note that longer sequence lengths require much more resources, so you may need to reduce this value.\n\nIf you want to have a chat-style conversation, replace the `-p <PROMPT>` argument with `-i -ins`\n\nFor other parameters and how to use them, please refer to [the llama.cpp documentation](https://github.com/ggerganov/llama.cpp/blob/master/examples/main/README.md)\n\n## How to run in `text-generation-webui`\n\nFurther instructions can be found in the text-generation-webui documentation, here: [text-generation-webui/docs/04 ‐ Model Tab.md](https://github.com/oobabooga/text-generation-webui/blob/main/docs/04%20%E2%80%90%20Model%20Tab.md#llamacpp).\n\n## How to run from Python code\n\nYou can use GGUF models from Python using the [llama-cpp-python](https://github.com/abetlen/llama-cpp-python) or [ctransformers](https://github.com/marella/ctransformers) libraries. Note that at the time of writing (Nov 27th 2023), ctransformers has not been updated for some time and is not compatible with some recent models. Therefore I recommend you use llama-cpp-python.\n\n### How to load this model in Python code, using llama-cpp-python\n\nFor full documentation, please see: [llama-cpp-python docs](https://abetlen.github.io/llama-cpp-python/).\n\n#### First install the package\n\nRun one of the following commands, according to your system:\n\n```shell\n# Base ctransformers with no GPU acceleration\npip install llama-cpp-python\n# With NVidia CUDA acceleration\nCMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install llama-cpp-python\n# Or with OpenBLAS acceleration\nCMAKE_ARGS=\"-DLLAMA_BLAS=ON -DLLAMA_BLAS_VENDOR=OpenBLAS\" pip install llama-cpp-python\n# Or with CLBLast acceleration\nCMAKE_ARGS=\"-DLLAMA_CLBLAST=on\" pip install llama-cpp-python\n# Or with AMD ROCm GPU acceleration (Linux only)\nCMAKE_ARGS=\"-DLLAMA_HIPBLAS=on\" pip install llama-cpp-python\n# Or with Metal GPU acceleration for macOS systems only\nCMAKE_ARGS=\"-DLLAMA_METAL=on\" pip install llama-cpp-python\n\n# In windows, to set the variables CMAKE_ARGS in PowerShell, follow this format; eg for NVidia CUDA:\n$env:CMAKE_ARGS = \"-DLLAMA_OPENBLAS=on\"\npip install llama-cpp-python\n```\n\n#### Simple llama-cpp-python example code\n\n```python\nfrom llama_cpp import Llama\n\n# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\nllm = Llama(\n  model_path=\"./phi-2.Q4_K_M.gguf\",  # Download the model file first\n  n_ctx=2048,  # The max sequence length to use - note that longer sequence lengths require much more resources\n  n_threads=8,            # The number of CPU threads to use, tailor to your system and the resulting performance\n  n_gpu_layers=35         # The number of layers to offload to GPU, if you have GPU acceleration available\n)\n\n# Simple inference example\noutput = llm(\n  \"Instruct: {prompt}\\nOutput:\", # Prompt\n  max_tokens=512,  # Generate up to 512 tokens\n  stop=[\"</s>\"],   # Example stop token - not necessarily correct for this specific model! Please check before using.\n  echo=True        # Whether to echo the prompt\n)\n\n# Chat Completion API\n\nllm = Llama(model_path=\"./phi-2.Q4_K_M.gguf\", chat_format=\"llama-2\")  # Set chat_format according to the model you are using\nllm.create_chat_completion(\n    messages = [\n        {\"role\": \"system\", \"content\": \"You are a story writing assistant.\"},\n        {\n            \"role\": \"user\",\n            \"content\": \"Write a story about llamas.\"\n        }\n    ]\n)\n```\n\n## How to use with LangChain\n\nHere are guides on using llama-cpp-python and ctransformers with LangChain:\n\n* [LangChain + llama-cpp-python](https://python.langchain.com/docs/integrations/llms/llamacpp)\n* [LangChain + ctransformers](https://python.langchain.com/docs/integrations/providers/ctransformers)\n\n<!-- README_GGUF.md-how-to-run end -->\n\n<!-- footer start -->\n<!-- 200823 -->\n## Discord\n\nFor further support, and discussions on these models and AI in general, join us at:\n\n[TheBloke AI's Discord server](https://discord.gg/theblokeai)\n\n## Thanks, and how to contribute\n\nThanks to the [chirper.ai](https://chirper.ai) team!\n\nThanks to Clay from [gpus.llm-utils.org](llm-utils)!\n\nI've had a lot of people ask if they can contribute. I enjoy providing models and helping people, and would love to be able to spend even more time doing it, as well as expanding into new projects like fine tuning/training.\n\nIf you're able and willing to contribute it will be most gratefully received and will help me to keep providing more models, and to start work on new AI projects.\n\nDonaters will get priority support on any and all AI/LLM/model questions and requests, access to a private Discord room, plus other benefits.\n\n* Patreon: https://patreon.com/TheBlokeAI\n* Ko-Fi: https://ko-fi.com/TheBlokeAI\n\n**Special thanks to**: Aemon Algiz.\n\n**Patreon special mentions**: Michael Levine, 阿明, Trailburnt, Nikolai Manek, John Detwiler, Randy H, Will Dee, Sebastain Graf, NimbleBox.ai, Eugene Pentland, Emad Mostaque, Ai Maven, Jim Angel, Jeff Scroggin, Michael Davis, Manuel Alberto Morcote, Stephen Murray, Robert, Justin Joy, Luke @flexchar, Brandon Frisco, Elijah Stavena, S_X, Dan Guido, Undi ., Komninos Chatzipapas, Shadi, theTransient, Lone Striker, Raven Klaugh, jjj, Cap'n Zoog, Michel-Marie MAUDET (LINAGORA), Matthew Berman, David, Fen Risland, Omer Bin Jawed, Luke Pendergrass, Kalila, OG, Erik Bjäreholt, Rooh Singh, Joseph William Delisle, Dan Lewis, TL, John Villwock, AzureBlack, Brad, Pedro Madruga, Caitlyn Gatomon, K, jinyuan sun, Mano Prime, Alex, Jeffrey Morgan, Alicia Loh, Illia Dulskyi, Chadd, transmissions 11, fincy, Rainer Wilmers, ReadyPlayerEmma, knownsqashed, Mandus, biorpg, Deo Leter, Brandon Phillips, SuperWojo, Sean Connelly, Iucharbius, Jack West, Harry Royden McLaughlin, Nicholas, terasurfer, Vitor Caleffi, Duane Dunston, Johann-Peter Hartmann, David Ziegler, Olakabola, Ken Nordquist, Trenton Dambrowitz, Tom X Nguyen, Vadim, Ajan Kanaga, Leonard Tan, Clay Pascal, Alexandros Triantafyllidis, JM33133, Xule, vamX, ya boyyy, subjectnull, Talal Aujan, Alps Aficionado, wassieverse, Ari Malik, James Bentley, Woland, Spencer Kim, Michael Dempsey, Fred von Graf, Elle, zynix, William Richards, Stanislav Ovsiannikov, Edmond Seymore, Jonathan Leane, Martin Kemka, usrbinkat, Enrico Ros\n\n\nThank you to all my generous patrons and donaters!\n\nAnd thank you again to a16z for their generous grant.\n\n<!-- footer end -->\n\n<!-- original-model-card start -->\n# Original model card: Microsoft's Phi 2\n\n\n## Model Summary\n\nPhi-2 is a Transformer with **2.7 billion** parameters. It was trained using the same data sources as [Phi-1.5](https://huggingface.co/microsoft/phi-1.5), augmented with a new data source that consists of various NLP synthetic texts and filtered websites (for safety and educational value). When assessed against benchmarks testing common sense, language understanding, and logical reasoning, Phi-2 showcased a nearly state-of-the-art performance among models with less than 13 billion parameters.\n\nOur model hasn't been fine-tuned through reinforcement learning from human feedback. The intention behind crafting this open-source model is to provide the research community with a non-restricted small model to explore vital safety challenges, such as reducing toxicity, understanding societal biases, enhancing controllability, and more.\n\n## Intended Uses\n\nPhi-2 is intended for research purposes only. Given the nature of the training data, the Phi-2 model is best suited for prompts using the QA format, the chat format, and the code format.\n\n### QA Format:\n\nYou can provide the prompt as a standalone question as follows:\n\n```markdown\nWrite a detailed analogy between mathematics and a lighthouse.\n```\nwhere the model generates the text after \".\" .\nTo encourage the model to write more concise answers, you can also try the following QA format using \"Instruct: \\<prompt\\>\\nOutput:\"\n```markdown\nInstruct: Write a detailed analogy between mathematics and a lighthouse.\nOutput: Mathematics is like a lighthouse. Just as a lighthouse guides ships safely to shore, mathematics provides a guiding light in the world of numbers and logic. It helps us navigate through complex problems and find solutions. Just as a lighthouse emits a steady beam of light, mathematics provides a consistent framework for reasoning and problem-solving. It illuminates the path to understanding and helps us make sense of the world around us.\n```\nwhere the model generates the text after \"Output:\".\n\n### Chat Format:\n\n```markdown\nAlice: I don't know why, I'm struggling to maintain focus while studying. Any suggestions?\nBob: Well, have you tried creating a study schedule and sticking to it?\nAlice: Yes, I have, but it doesn't seem to help much.\nBob: Hmm, maybe you should try studying in a quiet environment, like the library.\nAlice: ...\n```\n\nwhere the model generates the text after the first \"Bob:\".\n\n### Code Format:\n\n```python\ndef print_prime(n):\n   \"\"\"\n   Print all primes between 1 and n\n   \"\"\"\n   primes = []\n   for num in range(2, n+1):\n       is_prime = True\n       for i in range(2, int(math.sqrt(num))+1):\n           if num % i == 0:\n               is_prime = False\n               break\n       if is_prime:\n           primes.append(num)\n   print(primes)\n```\nwhere the model generates the text after the comments.\n\n**Notes:**\n* Phi-2 is intended for research purposes. The model-generated text/code should be treated as a starting point rather than a definitive solution for potential use cases. Users should be cautious when employing these models in their applications.\n* Direct adoption for production tasks is out of the scope of this research project. As a result, the Phi-2 model has not been tested to ensure that it performs adequately for any production-level application. Please refer to the limitation sections of this document for more details.\n* If you are using `transformers>=4.36.0`, always load the model with `trust_remote_code=True` to prevent side-effects.\n\n## Sample Code\n\nThere are four types of execution mode:\n\n1. FP16 / Flash-Attention / CUDA:\n   ```python\n   model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\", torch_dtype=\"auto\", flash_attn=True, flash_rotary=True, fused_dense=True, device_map=\"cuda\", trust_remote_code=True)\n   ```\n2. FP16 / CUDA:\n   ```python\n   model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\", torch_dtype=\"auto\", device_map=\"cuda\", trust_remote_code=True)\n   ```\n3. FP32 / CUDA:\n   ```python\n   model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\", torch_dtype=torch.float32, device_map=\"cuda\", trust_remote_code=True)\n   ```\n4. FP32 / CPU:\n   ```python\n   model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\", torch_dtype=torch.float32, device_map=\"cpu\", trust_remote_code=True)\n   ```\n\nTo ensure the maximum compatibility, we recommend using the second execution mode (FP16 / CUDA), as follows:\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ntorch.set_default_device(\"cuda\")\n\nmodel = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\", torch_dtype=\"auto\", trust_remote_code=True)\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\", trust_remote_code=True)\n\ninputs = tokenizer('''def print_prime(n):\n   \"\"\"\n   Print all primes between 1 and n\n   \"\"\"''', return_tensors=\"pt\", return_attention_mask=False)\n\noutputs = model.generate(**inputs, max_length=200)\ntext = tokenizer.batch_decode(outputs)[0]\nprint(text)\n```\n\n**Remark:** In the generation function, our model currently does not support beam search (`num_beams > 1`).\nFurthermore, in the forward pass of the model, we currently do not support outputting hidden states or attention values, or using custom input embeddings.\n\n## Limitations of Phi-2\n\n* Generate Inaccurate Code and Facts: The model may produce incorrect code snippets and statements. Users should treat these outputs as suggestions or starting points, not as definitive or accurate solutions.\n\n* Limited Scope for code: Majority of Phi-2 training data is based in Python and use common packages such as \"typing, math, random, collections, datetime, itertools\". If the model generates Python scripts that utilize other packages or scripts in other languages, we strongly recommend users manually verify all API uses.\n\n* Unreliable Responses to Instruction: The model has not undergone instruction fine-tuning. As a result, it may struggle or fail to adhere to intricate or nuanced instructions provided by users.\n\n* Language Limitations: The model is primarily designed to understand standard English. Informal English, slang, or any other languages might pose challenges to its comprehension, leading to potential misinterpretations or errors in response.\n\n* Potential Societal Biases: Phi-2 is not entirely free from societal biases despite efforts in assuring trainig data safety. There's a possibility it may generate content that mirrors these societal biases, particularly if prompted or instructed to do so. We urge users to be aware of this and to exercise caution and critical thinking when interpreting model outputs.\n\n* Toxicity: Despite being trained with carefully selected data, the model can still produce harmful content if explicitly prompted or instructed to do so. We chose to release the model for research purposes only -- We hope to help the open-source community develop the most effective ways to reduce the toxicity of a model directly after pretraining.\n\n* Verbosity: Phi-2 being a base model often produces irrelevant or extra text and responses following its first answer to user prompts within a single turn. This is due to its training dataset being primarily textbooks, which results in textbook-like responses.\n\n## Training\n\n### Model\n\n* Architecture: a Transformer-based model with next-word prediction objective\n\n* Context length: 2048 tokens\n\n* Dataset size: 250B tokens, combination of NLP synthetic data created by AOAI GPT-3.5 and filtered web data from Falcon RefinedWeb and SlimPajama, which was assessed by AOAI GPT-4.\n\n* Training tokens: 1.4T tokens\n\n* GPUs: 96xA100-80G\n\n* Training time: 14 days\n\n### Software\n\n* [PyTorch](https://github.com/pytorch/pytorch)\n\n* [DeepSpeed](https://github.com/microsoft/DeepSpeed)\n\n* [Flash-Attention](https://github.com/HazyResearch/flash-attention)\n\n### License\n\nThe model is licensed under the [microsoft-research-license](https://huggingface.co/microsoft/phi-2/resolve/main/LICENSE).\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow [Microsoft’s Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks). Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party’s policies.\n\n<!-- original-model-card end -->\n",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModel",
      "custom_class": null,
      "pipeline_tag": null,
      "processor": null
    },
    "safetensors": null,
    "model_index": null,
    "trending_score": null
  },
  {
    "model_id": "smallcloudai/Refact-1_6B-fim",
    "model_name": "smallcloudai/Refact-1_6B-fim",
    "author": "smallcloudai",
    "downloads": 5195573,
    "downloads_all_time": null,
    "likes": 133,
    "tags": [
      "transformers",
      "pytorch",
      "safetensors",
      "gpt_refact",
      "text-generation",
      "code",
      "custom_code",
      "en",
      "dataset:bigcode/the-stack-dedup",
      "dataset:rombodawg/2XUNCENSORED_MegaCodeTraining188k",
      "dataset:bigcode/commitpackft",
      "arxiv:2108.12409",
      "arxiv:1607.06450",
      "arxiv:1910.07467",
      "arxiv:1911.02150",
      "license:bigscience-openrail-m",
      "model-index",
      "autotrain_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/smallcloudai/Refact-1_6B-fim",
    "dependencies": [
      [
        "transformers",
        null
      ]
    ],
    "last_modified": "2023-11-09T07:09:31+00:00",
    "created_at": "2023-08-29T15:48:36+00:00",
    "analysis_date": "2025-03-22T00:41:08.476617",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "gpt_refact",
    "pipeline_tag": "text-generation",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "pipeline_tag": "text-generation",
      "inference": true,
      "widget": [
        {
          "text": "def print_hello_world():",
          "example_title": "Hello world",
          "group": "Python"
        }
      ],
      "license": "bigscience-openrail-m",
      "pretrain-datasets": [
        "books",
        "arxiv",
        "c4",
        "falcon-refinedweb",
        "wiki",
        "github-issues",
        "stack_markdown",
        "self-made dataset of permissive github code"
      ],
      "datasets": [
        "bigcode/the-stack-dedup",
        "rombodawg/2XUNCENSORED_MegaCodeTraining188k",
        "bigcode/commitpackft"
      ],
      "metrics": [
        "code_eval"
      ],
      "library_name": "transformers",
      "tags": [
        "code"
      ],
      "language": [
        "en"
      ],
      "model-index": [
        {
          "name": "Refact-1.6B",
          "results": [
            {
              "task": {
                "type": "text-generation"
              },
              "dataset": {
                "name": "HumanEval",
                "type": "openai_humaneval"
              },
              "metrics": [
                {
                  "type": "pass@1",
                  "value": 32.0,
                  "name": "pass@1 (T=0.01)",
                  "verified": false
                },
                {
                  "type": "pass@1",
                  "value": 31.5,
                  "name": "pass@1 (T=0.2)",
                  "verified": false
                },
                {
                  "type": "pass@10",
                  "value": 53.0,
                  "name": "pass@10 (T=0.8)",
                  "verified": false
                },
                {
                  "type": "pass@100",
                  "value": 76.9,
                  "name": "pass@100 (T=0.8)",
                  "verified": false
                }
              ]
            },
            {
              "task": {
                "type": "text-generation"
              },
              "dataset": {
                "name": "HumanEvalSynthesize Python",
                "type": "bigcode/humanevalpack"
              },
              "metrics": [
                {
                  "type": "pass@1",
                  "value": 35.8,
                  "name": "pass@1 (T=0.2)",
                  "verified": false
                },
                {
                  "type": "pass@1",
                  "value": 31.6,
                  "name": "pass@1 (T=0.2)",
                  "verified": false
                },
                {
                  "type": "pass@1",
                  "value": 29.1,
                  "name": "pass@1 (T=0.2)",
                  "verified": false
                },
                {
                  "type": "pass@1",
                  "value": -1,
                  "name": "pass@1 (T=0.2)",
                  "verified": false
                },
                {
                  "type": "pass@1",
                  "value": 26.3,
                  "name": "pass@1 (T=0.2)",
                  "verified": false
                },
                {
                  "type": "pass@1",
                  "value": -1,
                  "name": "pass@1 (T=0.2)",
                  "verified": false
                },
                {
                  "type": "pass@1",
                  "value": -1,
                  "name": "pass@1 (T=0.2)",
                  "verified": false
                },
                {
                  "type": "pass@1",
                  "value": 18.38,
                  "name": "pass@1 (T=0.2)",
                  "verified": false
                },
                {
                  "type": "pass@1",
                  "value": 12.28,
                  "name": "pass@1 (T=0.2)",
                  "verified": false
                },
                {
                  "type": "pass@1",
                  "value": 15.12,
                  "name": "pass@1 (T=0.2)",
                  "verified": false
                },
                {
                  "type": "pass@1",
                  "value": -1,
                  "name": "pass@1 (T=0.2)",
                  "verified": false
                },
                {
                  "type": "pass@1",
                  "value": 13.17,
                  "name": "pass@1 (T=0.2)",
                  "verified": false
                },
                {
                  "type": "pass@1",
                  "value": 2.8,
                  "name": "pass@1 (T=0.2)",
                  "verified": false
                },
                {
                  "type": "pass@1",
                  "value": -1,
                  "name": "pass@1 (T=0.2)",
                  "verified": false
                },
                {
                  "type": "pass@1",
                  "value": 26.92,
                  "name": "pass@1 (T=0.2)",
                  "verified": false
                },
                {
                  "type": "pass@1",
                  "value": 26.85,
                  "name": "pass@1 (T=0.2)",
                  "verified": false
                },
                {
                  "type": "pass@1",
                  "value": 30.76,
                  "name": "pass@1 (T=0.2)",
                  "verified": false
                },
                {
                  "type": "pass@1",
                  "value": -1,
                  "name": "pass@1 (T=0.2)",
                  "verified": false
                },
                {
                  "type": "pass@1",
                  "value": 25.94,
                  "name": "pass@1 (T=0.2)",
                  "verified": false
                },
                {
                  "type": "pass@1",
                  "value": 8.44,
                  "name": "pass@1 (T=0.2)",
                  "verified": false
                },
                {
                  "type": "pass@1",
                  "value": -1,
                  "name": "pass@1 (T=0.2)",
                  "verified": false
                },
                {
                  "type": "pass@1",
                  "value": 26.46,
                  "name": "pass@1 (T=0.2)",
                  "verified": false
                },
                {
                  "type": "pass@1",
                  "value": 17.86,
                  "name": "pass@1 (T=0.2)",
                  "verified": false
                },
                {
                  "type": "pass@1",
                  "value": 20.94,
                  "name": "pass@1 (T=0.2)",
                  "verified": false
                },
                {
                  "type": "pass@1",
                  "value": -1,
                  "name": "pass@1 (T=0.2)",
                  "verified": false
                },
                {
                  "type": "pass@1",
                  "value": 18.78,
                  "name": "pass@1 (T=0.2)",
                  "verified": false
                },
                {
                  "type": "pass@1",
                  "value": -1,
                  "name": "pass@1 (T=0.2)",
                  "verified": false
                },
                {
                  "type": "pass@1",
                  "value": -1,
                  "name": "pass@1 (T=0.2)",
                  "verified": false
                }
              ]
            },
            {
              "task": {
                "type": "text-generation"
              },
              "dataset": {
                "name": "MBPP",
                "type": "mbpp"
              },
              "metrics": [
                {
                  "type": "pass@1",
                  "value": 31.15,
                  "name": "pass@1 (T=0.01)",
                  "verified": false
                }
              ]
            },
            {
              "task": {
                "type": "text-generation"
              },
              "dataset": {
                "name": "DS-1000 (Overall Completion)",
                "type": "ds1000"
              },
              "metrics": [
                {
                  "type": "pass@1",
                  "value": 10.1,
                  "name": "pass@1 (T=0.2)",
                  "verified": false
                }
              ]
            },
            {
              "task": {
                "type": "text-generation"
              },
              "dataset": {
                "name": "MultiPL-HumanEval (C++)",
                "type": "nuprl/MultiPL-E"
              },
              "metrics": [
                {
                  "type": "pass@1",
                  "value": 21.61,
                  "name": "pass@1 (T=0.2)",
                  "verified": false
                },
                {
                  "type": "pass@1",
                  "value": 13.91,
                  "name": "pass@1 (T=0.2)",
                  "verified": false
                },
                {
                  "type": "pass@1",
                  "value": 9.5,
                  "name": "pass@1 (T=0.2)",
                  "verified": false
                },
                {
                  "type": "pass@1",
                  "value": 53.57,
                  "name": "pass@1 (T=0.2)",
                  "verified": false
                },
                {
                  "type": "pass@1",
                  "value": 21.58,
                  "name": "pass@1 (T=0.2)",
                  "verified": false
                },
                {
                  "type": "pass@1",
                  "value": 13.75,
                  "name": "pass@1 (T=0.2)",
                  "verified": false
                },
                {
                  "type": "pass@1",
                  "value": 26.88,
                  "name": "pass@1 (T=0.2)",
                  "verified": false
                },
                {
                  "type": "pass@1",
                  "value": 15.26,
                  "name": "pass@1 (T=0.2)",
                  "verified": false
                },
                {
                  "type": "pass@1",
                  "value": 23.04,
                  "name": "pass@1 (T=0.2)",
                  "verified": false
                },
                {
                  "type": "pass@1",
                  "value": 12.1,
                  "name": "pass@1 (T=0.2)",
                  "verified": false
                },
                {
                  "type": "pass@1",
                  "value": 29.6,
                  "name": "pass@1 (T=0.2)",
                  "verified": false
                },
                {
                  "type": "pass@1",
                  "value": 13.77,
                  "name": "pass@1 (T=0.2)",
                  "verified": false
                },
                {
                  "type": "pass@1",
                  "value": 12.68,
                  "name": "pass@1 (T=0.2)",
                  "verified": false
                },
                {
                  "type": "pass@1",
                  "value": 4.29,
                  "name": "pass@1 (T=0.2)",
                  "verified": false
                },
                {
                  "type": "pass@1",
                  "value": 19.54,
                  "name": "pass@1 (T=0.2)",
                  "verified": false
                },
                {
                  "type": "pass@1",
                  "value": 18.33,
                  "name": "pass@1 (T=0.2)",
                  "verified": false
                },
                {
                  "type": "pass@1",
                  "value": 5.7,
                  "name": "pass@1 (T=0.2)",
                  "verified": false
                },
                {
                  "type": "pass@1",
                  "value": 17.68,
                  "name": "pass@1 (T=0.2)",
                  "verified": false
                },
                {
                  "type": "pass@1",
                  "value": 25,
                  "name": "pass@1 (T=0.2)",
                  "verified": false
                }
              ]
            }
          ]
        }
      ]
    },
    "card_text": "\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/643a9dd0c5f633a7fa7e804a/HkB0QYV0BbmB3ktMugbZy.png)\n\n\n# Refact-1.6B\n\nFinally, the model we started training with our [blog post](https://refact.ai/blog/2023/applying-recent-innovations-to-train-model/) is ready 🎉\n\nAfter fine-tuning on generated data, it beats Replit 3b, Stability Code 3b and many other models. It almost beats\nStarCoder ten times the size!\n\n\nModel                 | Size          | HumanEval pass@1   | HumanEval pass@10  |\n----------------------|---------------|--------------------|--------------------|\nDeciCoder-1b          |   1b          |  19.1%             |                    |\n<b>Refact-1.6-fim</b> | <b>1.6b</b>   |  <b>32.0%</b>      | <b>53.0%</b>       |\nStableCode            |   3b          |  20.2%             | 33.8%              |\nReplitCode v1         |   3b          |  21.9%             |                    |\nCodeGen2.5-multi      |   7b          |  28.4%             | 47.5%              |\nCodeLlama             |   7b          |  33.5%             | 59.6%              |\nStarCoder             |  15b          |  33.6%             |                    |\n\nLikely, it's the best model for practical use in your IDE for code completion because it's smart and fast!\nYou can start using it right now by downloading the\n[Refact plugin](https://refact.ai/). You can host the model yourself, too, using the\n[open source docker container](https://github.com/smallcloudai/refact).\n\nAnd it's multi-language (see MultiPL-HumanEval and other metrics below) and it works as a chat (see the section below).\n\n# It Works As a Chat\n\nThe primary application of this model is code completion (infill) in multiple programming languages.\nBut it works as a chat quite well.\n\nHumanEval results using instruction following (chat) format, against models specialized for chat only:\n\nModel                  | Size   | pass@1   | pass@10  |\n-----------------------|--------|----------|----------|\n<b>Refact-1.6-fim</b>  | 1.6b   |  38.4%   | 55.6%    |\nStableCode-instruct    |   3b   |  26.9%   | 36.2%    |\nOctoGeeX               |   6b   |  44.7%   |          |\nCodeLlama-instruct     |   7b   |  34.8%   | 64.3%    |\nCodeGen2.5-instruct    |   7b   |  36.2%   | 60.87    |\nCodeLlama-instruct     |  13b   |  42.7%   | 71.6%    |\nStarChat-β             |  15b   |  33.5%   |          |\nOctoCoder              |  15b   |  46.2%   |          |\n\n\n# Example\n\nFill-in-the-middle uses special tokens to identify the prefix/middle/suffix part of the input and output:\n\n```python\n# pip install -q transformers\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ncheckpoint = \"smallcloudai/Refact-1_6B-fim\"\ndevice = \"cuda\" # for GPU usage or \"cpu\" for CPU usage\n\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint, trust_remote_code=True).to(device)\n\nprompt = '<fim_prefix>def print_hello_world():\\n    \"\"\"<fim_suffix>\\n    print(\"Hello world!\")<fim_middle>'\n\ninputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\noutputs = model.generate(inputs, max_length=100, temperature=0.2)\nprint(\"-\"*80)\nprint(tokenizer.decode(outputs[0]))\n```\n\n# Chat Format\n\nThe same model works as chat (experimental).\n\n```python\nprompt_template = \"<empty_output>SYSTEM {system}\\n\" \\\n                  \"<empty_output>USER {query}\\n\" \\\n                  \"<empty_output>ASSISTANT\"\nprompt = prompt_template.format(system=\"You are a programming assistant\",\n                                query=\"How do I sort a list in Python?\")\n```\n\n# Architecture\n\nAs described in more detail in the blog post, we used:\n\n- [ALiBi](https://arxiv.org/abs/2108.12409) based attention\n- [LayerNorm](https://arxiv.org/abs/1607.06450v1) instead of [RMSNorm](https://arxiv.org/pdf/1910.07467.pdf)\n- [Multi Query Attention](https://arxiv.org/abs/1911.02150)\n\nWe also used LiON, flash attention, early dropout. It's not that innovative that you can't run it, in fact you can -- see an example below.\n\n\n# Pretraining\n\nFor the base model, we used our own dataset that contains code with permissive licenses only, and open text datasets.\nFiltering is the key to success of this model:\n\n- We only used text in English\n- Only topics related to computer science\n- Applied heavy deduplication\n\nThe text to code proportion was 50:50, model trained for 1.2T tokens. \n\nWe don't release the base model, because its Fill-in-the-Middle (FIM) capability likes to repeat itself too much, so\nits practical use is limited. But if you still want it, write us a message on Discord.\n\n\n# Finetuning\n\nWe tested our hypothesis that chat data should boost base model performance in FIM and\nregular left-to-right code completion. We found that just 15% of open\n[code](https://huggingface.co/datasets/bigcode/commitpackft)\n[instruction-following](https://huggingface.co/datasets/rombodawg/2XUNCENSORED_MegaCodeTraining188k) datasets,\nthat we filtered for quality, improves almost all metrics.\n\nAdditionally, to improve FIM, we observed common failure modes, and prepared a synthetic dataset based on\n[The Stack dedup v1.1](https://huggingface.co/datasets/bigcode/the-stack-dedup) to address them.\n\nThere is a distribution shift between typical code on the internet, and the code you write in your IDE.\nThe former is likely finished, so the model tries to come up with a suggestion that makes the code complete.\nYou are likely to have half-written code as you work on it, there is no single addition that can repair it\nfully.\n\nIn practice, model needs to have a tendency to stop after a couple of lines are added, and sometimes don't write\nanything at all. We found that just giving it empty completions, single line completions, multiline\ncompletions that end with a smaller text indent or at least a newline -- makes it much more usable. This data\nwas used as the rest 85% of the finetune dataset.\n\nThe final model is the result of several attempts to make it work as good as possible for code completion,\nand to perform well on a wide range of metrics. The best attempt took 40B tokens.\n\n\n# Limitations and Bias\n\nThe Refact-1.6B model was trained on text in English. But it has seen a lot more languages in\ncode comments. Its performance on non-English languages is lower, for sure.\n\n\n# Model Stats\n\n- **Architecture:** LLAMA-like model with multi-query attention\n- **Objectives** Fill-in-the-Middle, Chat\n- **Tokens context:** 4096\n- **Pretraining tokens:** 1.2T\n- **Finetuning tokens:** 40B\n- **Precision:** bfloat16\n- **GPUs** 64 NVidia A5000\n- **Training time** 28 days\n\n\n# License\n\nThe model is licensed under the BigScience OpenRAIL-M v1 license agreement\n\n\n# Citation\n\nIf you are using this model, please give a link to this page.",
    "card_content": "---\npipeline_tag: text-generation\ninference: true\nwidget:\n- text: 'def print_hello_world():'\n  example_title: Hello world\n  group: Python\nlicense: bigscience-openrail-m\npretrain-datasets:\n- books\n- arxiv\n- c4\n- falcon-refinedweb\n- wiki\n- github-issues\n- stack_markdown\n- self-made dataset of permissive github code\ndatasets:\n- bigcode/the-stack-dedup\n- rombodawg/2XUNCENSORED_MegaCodeTraining188k\n- bigcode/commitpackft\nmetrics:\n- code_eval\nlibrary_name: transformers\ntags:\n- code\nlanguage:\n- en\nmodel-index:\n- name: Refact-1.6B\n  results:\n  - task:\n      type: text-generation\n    dataset:\n      name: HumanEval\n      type: openai_humaneval\n    metrics:\n    - type: pass@1\n      value: 32.0\n      name: pass@1 (T=0.01)\n      verified: false\n    - type: pass@1\n      value: 31.5\n      name: pass@1 (T=0.2)\n      verified: false\n    - type: pass@10\n      value: 53.0\n      name: pass@10 (T=0.8)\n      verified: false\n    - type: pass@100\n      value: 76.9\n      name: pass@100 (T=0.8)\n      verified: false\n  - task:\n      type: text-generation\n    dataset:\n      name: HumanEvalSynthesize Python\n      type: bigcode/humanevalpack\n    metrics:\n    - type: pass@1\n      value: 35.8\n      name: pass@1 (T=0.2)\n      verified: false\n    - type: pass@1\n      value: 31.6\n      name: pass@1 (T=0.2)\n      verified: false\n    - type: pass@1\n      value: 29.1\n      name: pass@1 (T=0.2)\n      verified: false\n    - type: pass@1\n      value: -1\n      name: pass@1 (T=0.2)\n      verified: false\n    - type: pass@1\n      value: 26.3\n      name: pass@1 (T=0.2)\n      verified: false\n    - type: pass@1\n      value: -1\n      name: pass@1 (T=0.2)\n      verified: false\n    - type: pass@1\n      value: -1\n      name: pass@1 (T=0.2)\n      verified: false\n    - type: pass@1\n      value: 18.38\n      name: pass@1 (T=0.2)\n      verified: false\n    - type: pass@1\n      value: 12.28\n      name: pass@1 (T=0.2)\n      verified: false\n    - type: pass@1\n      value: 15.12\n      name: pass@1 (T=0.2)\n      verified: false\n    - type: pass@1\n      value: -1\n      name: pass@1 (T=0.2)\n      verified: false\n    - type: pass@1\n      value: 13.17\n      name: pass@1 (T=0.2)\n      verified: false\n    - type: pass@1\n      value: 2.8\n      name: pass@1 (T=0.2)\n      verified: false\n    - type: pass@1\n      value: -1\n      name: pass@1 (T=0.2)\n      verified: false\n    - type: pass@1\n      value: 26.92\n      name: pass@1 (T=0.2)\n      verified: false\n    - type: pass@1\n      value: 26.85\n      name: pass@1 (T=0.2)\n      verified: false\n    - type: pass@1\n      value: 30.76\n      name: pass@1 (T=0.2)\n      verified: false\n    - type: pass@1\n      value: -1\n      name: pass@1 (T=0.2)\n      verified: false\n    - type: pass@1\n      value: 25.94\n      name: pass@1 (T=0.2)\n      verified: false\n    - type: pass@1\n      value: 8.44\n      name: pass@1 (T=0.2)\n      verified: false\n    - type: pass@1\n      value: -1\n      name: pass@1 (T=0.2)\n      verified: false\n    - type: pass@1\n      value: 26.46\n      name: pass@1 (T=0.2)\n      verified: false\n    - type: pass@1\n      value: 17.86\n      name: pass@1 (T=0.2)\n      verified: false\n    - type: pass@1\n      value: 20.94\n      name: pass@1 (T=0.2)\n      verified: false\n    - type: pass@1\n      value: -1\n      name: pass@1 (T=0.2)\n      verified: false\n    - type: pass@1\n      value: 18.78\n      name: pass@1 (T=0.2)\n      verified: false\n    - type: pass@1\n      value: -1\n      name: pass@1 (T=0.2)\n      verified: false\n    - type: pass@1\n      value: -1\n      name: pass@1 (T=0.2)\n      verified: false\n  - task:\n      type: text-generation\n    dataset:\n      name: MBPP\n      type: mbpp\n    metrics:\n    - type: pass@1\n      value: 31.15\n      name: pass@1 (T=0.01)\n      verified: false\n  - task:\n      type: text-generation\n    dataset:\n      name: DS-1000 (Overall Completion)\n      type: ds1000\n    metrics:\n    - type: pass@1\n      value: 10.1\n      name: pass@1 (T=0.2)\n      verified: false\n  - task:\n      type: text-generation\n    dataset:\n      name: MultiPL-HumanEval (C++)\n      type: nuprl/MultiPL-E\n    metrics:\n    - type: pass@1\n      value: 21.61\n      name: pass@1 (T=0.2)\n      verified: false\n    - type: pass@1\n      value: 13.91\n      name: pass@1 (T=0.2)\n      verified: false\n    - type: pass@1\n      value: 9.5\n      name: pass@1 (T=0.2)\n      verified: false\n    - type: pass@1\n      value: 53.57\n      name: pass@1 (T=0.2)\n      verified: false\n    - type: pass@1\n      value: 21.58\n      name: pass@1 (T=0.2)\n      verified: false\n    - type: pass@1\n      value: 13.75\n      name: pass@1 (T=0.2)\n      verified: false\n    - type: pass@1\n      value: 26.88\n      name: pass@1 (T=0.2)\n      verified: false\n    - type: pass@1\n      value: 15.26\n      name: pass@1 (T=0.2)\n      verified: false\n    - type: pass@1\n      value: 23.04\n      name: pass@1 (T=0.2)\n      verified: false\n    - type: pass@1\n      value: 12.1\n      name: pass@1 (T=0.2)\n      verified: false\n    - type: pass@1\n      value: 29.6\n      name: pass@1 (T=0.2)\n      verified: false\n    - type: pass@1\n      value: 13.77\n      name: pass@1 (T=0.2)\n      verified: false\n    - type: pass@1\n      value: 12.68\n      name: pass@1 (T=0.2)\n      verified: false\n    - type: pass@1\n      value: 4.29\n      name: pass@1 (T=0.2)\n      verified: false\n    - type: pass@1\n      value: 19.54\n      name: pass@1 (T=0.2)\n      verified: false\n    - type: pass@1\n      value: 18.33\n      name: pass@1 (T=0.2)\n      verified: false\n    - type: pass@1\n      value: 5.7\n      name: pass@1 (T=0.2)\n      verified: false\n    - type: pass@1\n      value: 17.68\n      name: pass@1 (T=0.2)\n      verified: false\n    - type: pass@1\n      value: 25\n      name: pass@1 (T=0.2)\n      verified: false\n---\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/643a9dd0c5f633a7fa7e804a/HkB0QYV0BbmB3ktMugbZy.png)\n\n\n# Refact-1.6B\n\nFinally, the model we started training with our [blog post](https://refact.ai/blog/2023/applying-recent-innovations-to-train-model/) is ready 🎉\n\nAfter fine-tuning on generated data, it beats Replit 3b, Stability Code 3b and many other models. It almost beats\nStarCoder ten times the size!\n\n\nModel                 | Size          | HumanEval pass@1   | HumanEval pass@10  |\n----------------------|---------------|--------------------|--------------------|\nDeciCoder-1b          |   1b          |  19.1%             |                    |\n<b>Refact-1.6-fim</b> | <b>1.6b</b>   |  <b>32.0%</b>      | <b>53.0%</b>       |\nStableCode            |   3b          |  20.2%             | 33.8%              |\nReplitCode v1         |   3b          |  21.9%             |                    |\nCodeGen2.5-multi      |   7b          |  28.4%             | 47.5%              |\nCodeLlama             |   7b          |  33.5%             | 59.6%              |\nStarCoder             |  15b          |  33.6%             |                    |\n\nLikely, it's the best model for practical use in your IDE for code completion because it's smart and fast!\nYou can start using it right now by downloading the\n[Refact plugin](https://refact.ai/). You can host the model yourself, too, using the\n[open source docker container](https://github.com/smallcloudai/refact).\n\nAnd it's multi-language (see MultiPL-HumanEval and other metrics below) and it works as a chat (see the section below).\n\n# It Works As a Chat\n\nThe primary application of this model is code completion (infill) in multiple programming languages.\nBut it works as a chat quite well.\n\nHumanEval results using instruction following (chat) format, against models specialized for chat only:\n\nModel                  | Size   | pass@1   | pass@10  |\n-----------------------|--------|----------|----------|\n<b>Refact-1.6-fim</b>  | 1.6b   |  38.4%   | 55.6%    |\nStableCode-instruct    |   3b   |  26.9%   | 36.2%    |\nOctoGeeX               |   6b   |  44.7%   |          |\nCodeLlama-instruct     |   7b   |  34.8%   | 64.3%    |\nCodeGen2.5-instruct    |   7b   |  36.2%   | 60.87    |\nCodeLlama-instruct     |  13b   |  42.7%   | 71.6%    |\nStarChat-β             |  15b   |  33.5%   |          |\nOctoCoder              |  15b   |  46.2%   |          |\n\n\n# Example\n\nFill-in-the-middle uses special tokens to identify the prefix/middle/suffix part of the input and output:\n\n```python\n# pip install -q transformers\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ncheckpoint = \"smallcloudai/Refact-1_6B-fim\"\ndevice = \"cuda\" # for GPU usage or \"cpu\" for CPU usage\n\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint, trust_remote_code=True).to(device)\n\nprompt = '<fim_prefix>def print_hello_world():\\n    \"\"\"<fim_suffix>\\n    print(\"Hello world!\")<fim_middle>'\n\ninputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\noutputs = model.generate(inputs, max_length=100, temperature=0.2)\nprint(\"-\"*80)\nprint(tokenizer.decode(outputs[0]))\n```\n\n# Chat Format\n\nThe same model works as chat (experimental).\n\n```python\nprompt_template = \"<empty_output>SYSTEM {system}\\n\" \\\n                  \"<empty_output>USER {query}\\n\" \\\n                  \"<empty_output>ASSISTANT\"\nprompt = prompt_template.format(system=\"You are a programming assistant\",\n                                query=\"How do I sort a list in Python?\")\n```\n\n# Architecture\n\nAs described in more detail in the blog post, we used:\n\n- [ALiBi](https://arxiv.org/abs/2108.12409) based attention\n- [LayerNorm](https://arxiv.org/abs/1607.06450v1) instead of [RMSNorm](https://arxiv.org/pdf/1910.07467.pdf)\n- [Multi Query Attention](https://arxiv.org/abs/1911.02150)\n\nWe also used LiON, flash attention, early dropout. It's not that innovative that you can't run it, in fact you can -- see an example below.\n\n\n# Pretraining\n\nFor the base model, we used our own dataset that contains code with permissive licenses only, and open text datasets.\nFiltering is the key to success of this model:\n\n- We only used text in English\n- Only topics related to computer science\n- Applied heavy deduplication\n\nThe text to code proportion was 50:50, model trained for 1.2T tokens. \n\nWe don't release the base model, because its Fill-in-the-Middle (FIM) capability likes to repeat itself too much, so\nits practical use is limited. But if you still want it, write us a message on Discord.\n\n\n# Finetuning\n\nWe tested our hypothesis that chat data should boost base model performance in FIM and\nregular left-to-right code completion. We found that just 15% of open\n[code](https://huggingface.co/datasets/bigcode/commitpackft)\n[instruction-following](https://huggingface.co/datasets/rombodawg/2XUNCENSORED_MegaCodeTraining188k) datasets,\nthat we filtered for quality, improves almost all metrics.\n\nAdditionally, to improve FIM, we observed common failure modes, and prepared a synthetic dataset based on\n[The Stack dedup v1.1](https://huggingface.co/datasets/bigcode/the-stack-dedup) to address them.\n\nThere is a distribution shift between typical code on the internet, and the code you write in your IDE.\nThe former is likely finished, so the model tries to come up with a suggestion that makes the code complete.\nYou are likely to have half-written code as you work on it, there is no single addition that can repair it\nfully.\n\nIn practice, model needs to have a tendency to stop after a couple of lines are added, and sometimes don't write\nanything at all. We found that just giving it empty completions, single line completions, multiline\ncompletions that end with a smaller text indent or at least a newline -- makes it much more usable. This data\nwas used as the rest 85% of the finetune dataset.\n\nThe final model is the result of several attempts to make it work as good as possible for code completion,\nand to perform well on a wide range of metrics. The best attempt took 40B tokens.\n\n\n# Limitations and Bias\n\nThe Refact-1.6B model was trained on text in English. But it has seen a lot more languages in\ncode comments. Its performance on non-English languages is lower, for sure.\n\n\n# Model Stats\n\n- **Architecture:** LLAMA-like model with multi-query attention\n- **Objectives** Fill-in-the-Middle, Chat\n- **Tokens context:** 4096\n- **Pretraining tokens:** 1.2T\n- **Finetuning tokens:** 40B\n- **Precision:** bfloat16\n- **GPUs** 64 NVidia A5000\n- **Training time** 28 days\n\n\n# License\n\nThe model is licensed under the BigScience OpenRAIL-M v1 license agreement\n\n\n# Citation\n\nIf you are using this model, please give a link to this page.",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModelForCausalLM",
      "custom_class": "modeling_gpt_refact.GPTRefactForCausalLM",
      "pipeline_tag": "text-generation",
      "processor": null
    },
    "safetensors": {
      "parameters": {
        "BF16": 1585842176
      },
      "total": 1585842176
    },
    "model_index": [
      {
        "name": "Refact-1.6B",
        "results": [
          {
            "task": {
              "type": "text-generation"
            },
            "dataset": {
              "type": "openai_humaneval",
              "name": "HumanEval"
            },
            "metrics": [
              {
                "name": "pass@1 (T=0.01)",
                "type": "pass@1",
                "value": 32,
                "verified": false
              },
              {
                "name": "pass@1 (T=0.2)",
                "type": "pass@1",
                "value": 31.5,
                "verified": false
              },
              {
                "name": "pass@10 (T=0.8)",
                "type": "pass@10",
                "value": 53,
                "verified": false
              },
              {
                "name": "pass@100 (T=0.8)",
                "type": "pass@100",
                "value": 76.9,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "text-generation"
            },
            "dataset": {
              "type": "bigcode/humanevalpack",
              "name": "HumanEvalSynthesize Python"
            },
            "metrics": [
              {
                "name": "pass@1 (T=0.2)",
                "type": "pass@1",
                "value": 35.8,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "text-generation"
            },
            "dataset": {
              "type": "bigcode/humanevalpack",
              "name": "HumanEvalSynthesize JavaScript"
            },
            "metrics": [
              {
                "name": "pass@1 (T=0.2)",
                "type": "pass@1",
                "value": 31.6,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "text-generation"
            },
            "dataset": {
              "type": "bigcode/humanevalpack",
              "name": "HumanEvalSynthesize Java"
            },
            "metrics": [
              {
                "name": "pass@1 (T=0.2)",
                "type": "pass@1",
                "value": 29.1,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "text-generation"
            },
            "dataset": {
              "type": "bigcode/humanevalpack",
              "name": "HumanEvalSynthesize Go"
            },
            "metrics": [
              {
                "name": "pass@1 (T=0.2)",
                "type": "pass@1",
                "value": -1,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "text-generation"
            },
            "dataset": {
              "type": "bigcode/humanevalpack",
              "name": "HumanEvalSynthesize C++"
            },
            "metrics": [
              {
                "name": "pass@1 (T=0.2)",
                "type": "pass@1",
                "value": 26.3,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "text-generation"
            },
            "dataset": {
              "type": "bigcode/humanevalpack",
              "name": "HumanEvalSynthesize Rust"
            },
            "metrics": [
              {
                "name": "pass@1 (T=0.2)",
                "type": "pass@1",
                "value": -1,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "text-generation"
            },
            "dataset": {
              "type": "bigcode/humanevalpack",
              "name": "HumanEvalSynthesize Average"
            },
            "metrics": [
              {
                "name": "pass@1 (T=0.2)",
                "type": "pass@1",
                "value": -1,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "text-generation"
            },
            "dataset": {
              "type": "bigcode/humanevalpack",
              "name": "HumanEvalFixTests Python"
            },
            "metrics": [
              {
                "name": "pass@1 (T=0.2)",
                "type": "pass@1",
                "value": 18.38,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "text-generation"
            },
            "dataset": {
              "type": "bigcode/humanevalpack",
              "name": "HumanEvalFixTests JavaScript"
            },
            "metrics": [
              {
                "name": "pass@1 (T=0.2)",
                "type": "pass@1",
                "value": 12.28,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "text-generation"
            },
            "dataset": {
              "type": "bigcode/humanevalpack",
              "name": "HumanEvalFixTests Java"
            },
            "metrics": [
              {
                "name": "pass@1 (T=0.2)",
                "type": "pass@1",
                "value": 15.12,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "text-generation"
            },
            "dataset": {
              "type": "bigcode/humanevalpack",
              "name": "HumanEvalFixTests Go"
            },
            "metrics": [
              {
                "name": "pass@1 (T=0.2)",
                "type": "pass@1",
                "value": -1,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "text-generation"
            },
            "dataset": {
              "type": "bigcode/humanevalpack",
              "name": "HumanEvalFixTests C++"
            },
            "metrics": [
              {
                "name": "pass@1 (T=0.2)",
                "type": "pass@1",
                "value": 13.17,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "text-generation"
            },
            "dataset": {
              "type": "bigcode/humanevalpack",
              "name": "HumanEvalFixTests Rust"
            },
            "metrics": [
              {
                "name": "pass@1 (T=0.2)",
                "type": "pass@1",
                "value": 2.8,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "text-generation"
            },
            "dataset": {
              "type": "bigcode/humanevalpack",
              "name": "HumanEvalFixTests Average"
            },
            "metrics": [
              {
                "name": "pass@1 (T=0.2)",
                "type": "pass@1",
                "value": -1,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "text-generation"
            },
            "dataset": {
              "type": "bigcode/humanevalpack",
              "name": "HumanEvalFixDocs Python"
            },
            "metrics": [
              {
                "name": "pass@1 (T=0.2)",
                "type": "pass@1",
                "value": 26.92,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "text-generation"
            },
            "dataset": {
              "type": "bigcode/humanevalpack",
              "name": "HumanEvalFixDocs JavaScript"
            },
            "metrics": [
              {
                "name": "pass@1 (T=0.2)",
                "type": "pass@1",
                "value": 26.85,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "text-generation"
            },
            "dataset": {
              "type": "bigcode/humanevalpack",
              "name": "HumanEvalFixDocs Java"
            },
            "metrics": [
              {
                "name": "pass@1 (T=0.2)",
                "type": "pass@1",
                "value": 30.76,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "text-generation"
            },
            "dataset": {
              "type": "bigcode/humanevalpack",
              "name": "HumanEvalFixDocs Go"
            },
            "metrics": [
              {
                "name": "pass@1 (T=0.2)",
                "type": "pass@1",
                "value": -1,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "text-generation"
            },
            "dataset": {
              "type": "bigcode/humanevalpack",
              "name": "HumanEvalFixDocs C++"
            },
            "metrics": [
              {
                "name": "pass@1 (T=0.2)",
                "type": "pass@1",
                "value": 25.94,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "text-generation"
            },
            "dataset": {
              "type": "bigcode/humanevalpack",
              "name": "HumanEvalFixDocs Rust"
            },
            "metrics": [
              {
                "name": "pass@1 (T=0.2)",
                "type": "pass@1",
                "value": 8.44,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "text-generation"
            },
            "dataset": {
              "type": "bigcode/humanevalpack",
              "name": "HumanEvalFixDocs Average"
            },
            "metrics": [
              {
                "name": "pass@1 (T=0.2)",
                "type": "pass@1",
                "value": -1,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "text-generation"
            },
            "dataset": {
              "type": "bigcode/humanevalpack",
              "name": "HumanEvalExplain Python"
            },
            "metrics": [
              {
                "name": "pass@1 (T=0.2)",
                "type": "pass@1",
                "value": 26.46,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "text-generation"
            },
            "dataset": {
              "type": "bigcode/humanevalpack",
              "name": "HumanEvalExplain JavaScript"
            },
            "metrics": [
              {
                "name": "pass@1 (T=0.2)",
                "type": "pass@1",
                "value": 17.86,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "text-generation"
            },
            "dataset": {
              "type": "bigcode/humanevalpack",
              "name": "HumanEvalExplain Java"
            },
            "metrics": [
              {
                "name": "pass@1 (T=0.2)",
                "type": "pass@1",
                "value": 20.94,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "text-generation"
            },
            "dataset": {
              "type": "bigcode/humanevalpack",
              "name": "HumanEvalExplain Go"
            },
            "metrics": [
              {
                "name": "pass@1 (T=0.2)",
                "type": "pass@1",
                "value": -1,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "text-generation"
            },
            "dataset": {
              "type": "bigcode/humanevalpack",
              "name": "HumanEvalExplain C++"
            },
            "metrics": [
              {
                "name": "pass@1 (T=0.2)",
                "type": "pass@1",
                "value": 18.78,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "text-generation"
            },
            "dataset": {
              "type": "bigcode/humanevalpack",
              "name": "HumanEvalExplain Rust"
            },
            "metrics": [
              {
                "name": "pass@1 (T=0.2)",
                "type": "pass@1",
                "value": -1,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "text-generation"
            },
            "dataset": {
              "type": "bigcode/humanevalpack",
              "name": "HumanEvalExplain Average"
            },
            "metrics": [
              {
                "name": "pass@1 (T=0.2)",
                "type": "pass@1",
                "value": -1,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "text-generation"
            },
            "dataset": {
              "type": "mbpp",
              "name": "MBPP"
            },
            "metrics": [
              {
                "name": "pass@1 (T=0.01)",
                "type": "pass@1",
                "value": 31.15,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "text-generation"
            },
            "dataset": {
              "type": "ds1000",
              "name": "DS-1000 (Overall Completion)"
            },
            "metrics": [
              {
                "name": "pass@1 (T=0.2)",
                "type": "pass@1",
                "value": 10.1,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "text-generation"
            },
            "dataset": {
              "type": "nuprl/MultiPL-E",
              "name": "MultiPL-HumanEval (C++)"
            },
            "metrics": [
              {
                "name": "pass@1 (T=0.2)",
                "type": "pass@1",
                "value": 21.61,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "text-generation"
            },
            "dataset": {
              "type": "nuprl/MultiPL-E",
              "name": "MultiPL-HumanEval (C#)"
            },
            "metrics": [
              {
                "name": "pass@1 (T=0.2)",
                "type": "pass@1",
                "value": 13.91,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "text-generation"
            },
            "dataset": {
              "type": "nuprl/MultiPL-E",
              "name": "MultiPL-HumanEval (D)"
            },
            "metrics": [
              {
                "name": "pass@1 (T=0.2)",
                "type": "pass@1",
                "value": 9.5,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "text-generation"
            },
            "dataset": {
              "type": "nuprl/MultiPL-E",
              "name": "MultiPL-HumanEval (Go)"
            },
            "metrics": [
              {
                "name": "pass@1 (T=0.2)",
                "type": "pass@1",
                "value": 53.57,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "text-generation"
            },
            "dataset": {
              "type": "nuprl/MultiPL-E",
              "name": "MultiPL-HumanEval (Java)"
            },
            "metrics": [
              {
                "name": "pass@1 (T=0.2)",
                "type": "pass@1",
                "value": 21.58,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "text-generation"
            },
            "dataset": {
              "type": "nuprl/MultiPL-E",
              "name": "MultiPL-HumanEval (Julia)"
            },
            "metrics": [
              {
                "name": "pass@1 (T=0.2)",
                "type": "pass@1",
                "value": 13.75,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "text-generation"
            },
            "dataset": {
              "type": "nuprl/MultiPL-E",
              "name": "MultiPL-HumanEval (JavaScript)"
            },
            "metrics": [
              {
                "name": "pass@1 (T=0.2)",
                "type": "pass@1",
                "value": 26.88,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "text-generation"
            },
            "dataset": {
              "type": "nuprl/MultiPL-E",
              "name": "MultiPL-HumanEval (Lua)"
            },
            "metrics": [
              {
                "name": "pass@1 (T=0.2)",
                "type": "pass@1",
                "value": 15.26,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "text-generation"
            },
            "dataset": {
              "type": "nuprl/MultiPL-E",
              "name": "MultiPL-HumanEval (PHP)"
            },
            "metrics": [
              {
                "name": "pass@1 (T=0.2)",
                "type": "pass@1",
                "value": 23.04,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "text-generation"
            },
            "dataset": {
              "type": "nuprl/MultiPL-E",
              "name": "MultiPL-HumanEval (Perl)"
            },
            "metrics": [
              {
                "name": "pass@1 (T=0.2)",
                "type": "pass@1",
                "value": 12.1,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "text-generation"
            },
            "dataset": {
              "type": "nuprl/MultiPL-E",
              "name": "MultiPL-HumanEval (Python)"
            },
            "metrics": [
              {
                "name": "pass@1 (T=0.2)",
                "type": "pass@1",
                "value": 29.6,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "text-generation"
            },
            "dataset": {
              "type": "nuprl/MultiPL-E",
              "name": "MultiPL-HumanEval (R)"
            },
            "metrics": [
              {
                "name": "pass@1 (T=0.2)",
                "type": "pass@1",
                "value": 13.77,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "text-generation"
            },
            "dataset": {
              "type": "nuprl/MultiPL-E",
              "name": "MultiPL-HumanEval (Ruby)"
            },
            "metrics": [
              {
                "name": "pass@1 (T=0.2)",
                "type": "pass@1",
                "value": 12.68,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "text-generation"
            },
            "dataset": {
              "type": "nuprl/MultiPL-E",
              "name": "MultiPL-HumanEval (Racket)"
            },
            "metrics": [
              {
                "name": "pass@1 (T=0.2)",
                "type": "pass@1",
                "value": 4.29,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "text-generation"
            },
            "dataset": {
              "type": "nuprl/MultiPL-E",
              "name": "MultiPL-HumanEval (Rust)"
            },
            "metrics": [
              {
                "name": "pass@1 (T=0.2)",
                "type": "pass@1",
                "value": 19.54,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "text-generation"
            },
            "dataset": {
              "type": "nuprl/MultiPL-E",
              "name": "MultiPL-HumanEval (Scala)"
            },
            "metrics": [
              {
                "name": "pass@1 (T=0.2)",
                "type": "pass@1",
                "value": 18.33,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "text-generation"
            },
            "dataset": {
              "type": "nuprl/MultiPL-E",
              "name": "MultiPL-HumanEval (Bash)"
            },
            "metrics": [
              {
                "name": "pass@1 (T=0.2)",
                "type": "pass@1",
                "value": 5.7,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "text-generation"
            },
            "dataset": {
              "type": "nuprl/MultiPL-E",
              "name": "MultiPL-HumanEval (Swift)"
            },
            "metrics": [
              {
                "name": "pass@1 (T=0.2)",
                "type": "pass@1",
                "value": 17.68,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "text-generation"
            },
            "dataset": {
              "type": "nuprl/MultiPL-E",
              "name": "MultiPL-HumanEval (TypeScript)"
            },
            "metrics": [
              {
                "name": "pass@1 (T=0.2)",
                "type": "pass@1",
                "value": 25,
                "verified": false
              }
            ]
          }
        ]
      }
    ],
    "trending_score": null
  },
  {
    "model_id": "meta-llama/Llama-3.2-1B",
    "model_name": "meta-llama/Llama-3.2-1B",
    "author": "meta-llama",
    "downloads": 4255138,
    "downloads_all_time": null,
    "likes": 1708,
    "tags": [
      "transformers",
      "safetensors",
      "llama",
      "text-generation",
      "facebook",
      "meta",
      "pytorch",
      "llama-3",
      "en",
      "de",
      "fr",
      "it",
      "pt",
      "hi",
      "es",
      "th",
      "arxiv:2204.05149",
      "arxiv:2405.16406",
      "license:llama3.2",
      "autotrain_compatible",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/meta-llama/Llama-3.2-1B",
    "dependencies": null,
    "last_modified": "2024-10-24T15:08:03+00:00",
    "created_at": "2024-09-18T15:03:14+00:00",
    "analysis_date": "2025-03-22T00:41:11.319419",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "llama",
    "pipeline_tag": "text-generation",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": [
        "en",
        "de",
        "fr",
        "it",
        "pt",
        "hi",
        "es",
        "th"
      ],
      "library_name": "transformers",
      "pipeline_tag": "text-generation",
      "tags": [
        "facebook",
        "meta",
        "pytorch",
        "llama",
        "llama-3"
      ],
      "license": "llama3.2",
      "extra_gated_prompt": "### LLAMA 3.2 COMMUNITY LICENSE AGREEMENT\n\nLlama 3.2 Version Release Date: September 25, 2024\n\n“Agreement” means the terms and conditions for use, reproduction, distribution  and modification of the Llama Materials set forth herein.\n\n“Documentation” means the specifications, manuals and documentation accompanying Llama 3.2 distributed by Meta at https://llama.meta.com/doc/overview.\n\n“Licensee” or “you” means you, or your employer or any other person or entity (if you are  entering into this Agreement on such person or entity’s behalf), of the age required under applicable laws, rules or regulations to provide legal consent and that has legal authority to bind your employer or such other person or entity if you are entering in this Agreement on their behalf.\n\n“Llama 3.2” means the foundational large language models and software and algorithms, including machine-learning model code, trained model weights, inference-enabling code, training-enabling code, fine-tuning enabling code and other elements of the foregoing distributed by Meta at  https://www.llama.com/llama-downloads.\n\n“Llama Materials” means, collectively, Meta’s proprietary Llama 3.2 and Documentation (and  any portion thereof) made available under this Agreement.\n\n“Meta” or “we” means Meta Platforms Ireland Limited (if you are located in or,  if you are an entity, your principal place of business is in the EEA or Switzerland)  and Meta Platforms, Inc. (if you are located outside of the EEA or Switzerland). \n\nBy clicking “I Accept” below or by using or distributing any portion or element of the Llama Materials, you agree to be bound by this Agreement.\n\n1. License Rights and Redistribution.\na. Grant of Rights. You are granted a non-exclusive, worldwide,  non-transferable and royalty-free limited license under Meta’s intellectual property or other rights  owned by Meta embodied in the Llama Materials to use, reproduce, distribute, copy, create derivative works  of, and make modifications to the Llama Materials.  \nb. Redistribution and Use.  \ni. If you distribute or make available the Llama Materials (or any derivative works thereof),  or a product or service (including another AI model) that contains any of them, you shall (A) provide a copy of this Agreement with any such Llama Materials; and (B) prominently display “Built with Llama” on a related website, user interface, blogpost, about page, or product documentation. If you use the Llama Materials or any outputs or results of the Llama Materials to create, train, fine tune, or otherwise improve an AI model, which is distributed or made available, you shall also include “Llama” at the beginning of any such AI model name.\nii. If you receive Llama Materials, or any derivative works thereof, from a Licensee as part of an integrated end user product, then Section 2 of this Agreement will not apply to you. \niii. You must retain in all copies of the Llama Materials that you distribute the  following attribution notice within a “Notice” text file distributed as a part of such copies:  “Llama 3.2 is licensed under the Llama 3.2 Community License, Copyright © Meta Platforms, Inc. All Rights Reserved.”\niv. Your use of the Llama Materials must comply with applicable laws and regulations (including trade compliance laws and regulations) and adhere to the Acceptable Use Policy for the Llama Materials (available at https://www.llama.com/llama3_2/use-policy), which is hereby  incorporated by reference into this Agreement.\n  \n2. Additional Commercial Terms. If, on the Llama 3.2 version release date, the monthly active users of the products or services made available by or for Licensee, or Licensee’s affiliates,  is greater than 700 million monthly active users in the preceding calendar month, you must request  a license from Meta, which Meta may grant to you in its sole discretion, and you are not authorized to exercise any of the rights under this Agreement unless or until Meta otherwise expressly grants you such rights.\n3. Disclaimer of Warranty. UNLESS REQUIRED BY APPLICABLE LAW, THE LLAMA MATERIALS AND ANY OUTPUT AND  RESULTS THEREFROM ARE PROVIDED ON AN “AS IS” BASIS, WITHOUT WARRANTIES OF ANY KIND, AND META DISCLAIMS ALL WARRANTIES OF ANY KIND, BOTH EXPRESS AND IMPLIED, INCLUDING, WITHOUT LIMITATION, ANY WARRANTIES OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE. YOU ARE SOLELY RESPONSIBLE FOR DETERMINING THE APPROPRIATENESS OF USING OR REDISTRIBUTING THE LLAMA MATERIALS AND ASSUME ANY RISKS ASSOCIATED WITH YOUR USE OF THE LLAMA MATERIALS AND ANY OUTPUT AND RESULTS.\n4. Limitation of Liability. IN NO EVENT WILL META OR ITS AFFILIATES BE LIABLE UNDER ANY THEORY OF LIABILITY,  WHETHER IN CONTRACT, TORT, NEGLIGENCE, PRODUCTS LIABILITY, OR OTHERWISE, ARISING OUT OF THIS AGREEMENT,  FOR ANY LOST PROFITS OR ANY INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL, EXEMPLARY OR PUNITIVE DAMAGES, EVEN  IF META OR ITS AFFILIATES HAVE BEEN ADVISED OF THE POSSIBILITY OF ANY OF THE FOREGOING.\n5. Intellectual Property.\na. No trademark licenses are granted under this Agreement, and in connection with the Llama Materials,  neither Meta nor Licensee may use any name or mark owned by or associated with the other or any of its affiliates,  except as required for reasonable and customary use in describing and redistributing the Llama Materials or as  set forth in this Section 5(a). Meta hereby grants you a license to use “Llama” (the “Mark”) solely as required  to comply with the last sentence of Section 1.b.i. You will comply with Meta’s brand guidelines (currently accessible  at https://about.meta.com/brand/resources/meta/company-brand/). All goodwill arising out of your use of the Mark  will inure to the benefit of Meta.\nb. Subject to Meta’s ownership of Llama Materials and derivatives made by or for Meta, with respect to any derivative works and modifications of the Llama Materials that are made by you, as between you and Meta, you are and will be the owner of such derivative works and modifications.\nc. If you institute litigation or other proceedings against Meta or any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Llama Materials or Llama 3.2 outputs or results, or any portion of any of the foregoing, constitutes infringement of intellectual property or other rights owned or licensable by you, then any licenses granted to you under this Agreement shall terminate as of the date such litigation or claim is filed or instituted. You will indemnify and hold harmless Meta from and against any claim by any third party arising out of or related to your use or distribution of the Llama Materials.\n6. Term and Termination. The term of this Agreement will commence upon your acceptance of this Agreement or access to the Llama Materials and will continue in full force and effect until terminated in accordance with the terms and conditions herein. Meta may terminate this Agreement if you are in breach of any term or condition of this Agreement. Upon termination of this Agreement, you shall delete and cease use of the Llama Materials. Sections 3, 4 and 7 shall survive the termination of this Agreement. \n7. Governing Law and Jurisdiction. This Agreement will be governed and construed under the laws of the State of  California without regard to choice of law principles, and the UN Convention on Contracts for the International Sale of Goods does not apply to this Agreement. The courts of California shall have exclusive jurisdiction of any dispute arising out of this Agreement. \n### Llama 3.2 Acceptable Use Policy\nMeta is committed to promoting safe and fair use of its tools and features, including Llama 3.2.  If you access or use Llama 3.2, you agree to this Acceptable Use Policy (“**Policy**”).  The most recent copy of this policy can be found at [https://www.llama.com/llama3_2/use-policy](https://www.llama.com/llama3_2/use-policy).\n#### Prohibited Uses\nWe want everyone to use Llama 3.2 safely and responsibly. You agree you will not use, or allow others to use, Llama 3.2 to:\n1. Violate the law or others’ rights, including to:\n    1. Engage in, promote, generate, contribute to, encourage, plan, incite, or further illegal or unlawful activity or content, such as:\n        1. Violence or terrorism\n        2. Exploitation or harm to children, including the solicitation, creation, acquisition, or dissemination of child exploitative content or failure to report Child Sexual Abuse Material\n        3. Human trafficking, exploitation, and sexual violence\n        4. The illegal distribution of information or materials to minors, including obscene materials, or failure to employ legally required age-gating in connection with such information or materials.\n        5. Sexual solicitation\n        6. Any other criminal activity\n    1. Engage in, promote, incite, or facilitate the harassment, abuse, threatening, or bullying of individuals or groups of individuals\n    2. Engage in, promote, incite, or facilitate discrimination or other unlawful or harmful conduct in the provision of employment, employment benefits, credit, housing, other economic benefits, or other essential goods and services\n    3. Engage in the unauthorized or unlicensed practice of any profession including, but not limited to, financial, legal, medical/health, or related professional practices\n    4. Collect, process, disclose, generate, or infer private or sensitive information about individuals, including information about individuals’ identity, health, or demographic information, unless you have obtained the right to do so in accordance with applicable law\n    5. Engage in or facilitate any action or generate any content that infringes, misappropriates, or otherwise violates any third-party rights, including the outputs or results of any products or services using the Llama Materials\n    6. Create, generate, or facilitate the creation of malicious code, malware, computer viruses or do anything else that could disable, overburden, interfere with or impair the proper working, integrity, operation or appearance of a website or computer system\n    7. Engage in any action, or facilitate any action, to intentionally circumvent or remove usage restrictions or other safety measures, or to enable functionality disabled by Meta \n2. Engage in, promote, incite, facilitate, or assist in the planning or development of activities that present a risk of death or bodily harm to individuals, including use of Llama 3.2 related to the following:\n    8. Military, warfare, nuclear industries or applications, espionage, use for materials or activities that are subject to the International Traffic Arms Regulations (ITAR) maintained by the United States Department of State or to the U.S. Biological Weapons Anti-Terrorism Act of 1989 or the Chemical Weapons Convention Implementation Act of 1997\n    9. Guns and illegal weapons (including weapon development)\n    10. Illegal drugs and regulated/controlled substances\n    11. Operation of critical infrastructure, transportation technologies, or heavy machinery\n    12. Self-harm or harm to others, including suicide, cutting, and eating disorders\n    13. Any content intended to incite or promote violence, abuse, or any infliction of bodily harm to an individual\n3. Intentionally deceive or mislead others, including use of Llama 3.2 related to the following:\n    14. Generating, promoting, or furthering fraud or the creation or promotion of disinformation\n    15. Generating, promoting, or furthering defamatory content, including the creation of defamatory statements, images, or other content\n    16. Generating, promoting, or further distributing spam\n    17. Impersonating another individual without consent, authorization, or legal right\n    18. Representing that the use of Llama 3.2 or outputs are human-generated\n    19. Generating or facilitating false online engagement, including fake reviews and other means of fake online engagement \n4. Fail to appropriately disclose to end users any known dangers of your AI system 5. Interact with third party tools, models, or software designed to generate unlawful content or engage in unlawful or harmful conduct and/or represent that the outputs of such tools, models, or software are associated with Meta or Llama 3.2\n\nWith respect to any multimodal models included in Llama 3.2, the rights granted under Section 1(a) of the Llama 3.2 Community License Agreement are not being granted to you if you are an individual domiciled in, or a company with a principal place of business in, the European Union. This restriction does not apply to end users of a product or service that incorporates any such multimodal models.\n\nPlease report any violation of this Policy, software “bug,” or other problems that could lead to a violation of this Policy through one of the following means:\n\n* Reporting issues with the model: [https://github.com/meta-llama/llama-models/issues](https://l.workplace.com/l.php?u=https%3A%2F%2Fgithub.com%2Fmeta-llama%2Fllama-models%2Fissues&h=AT0qV8W9BFT6NwihiOHRuKYQM_UnkzN_NmHMy91OT55gkLpgi4kQupHUl0ssR4dQsIQ8n3tfd0vtkobvsEvt1l4Ic6GXI2EeuHV8N08OG2WnbAmm0FL4ObkazC6G_256vN0lN9DsykCvCqGZ)\n* Reporting risky content generated by the model: [developers.facebook.com/llama_output_feedback](http://developers.facebook.com/llama_output_feedback)\n* Reporting bugs and security concerns: [facebook.com/whitehat/info](http://facebook.com/whitehat/info)\n* Reporting violations of the Acceptable Use Policy or unlicensed uses of Llama 3.2: LlamaUseReport@meta.com",
      "extra_gated_fields": {
        "First Name": "text",
        "Last Name": "text",
        "Date of birth": "date_picker",
        "Country": "country",
        "Affiliation": "text",
        "Job title": {
          "type": "select",
          "options": [
            "Student",
            "Research Graduate",
            "AI researcher",
            "AI developer/engineer",
            "Reporter",
            "Other"
          ]
        },
        "geo": "ip_location",
        "By clicking Submit below I accept the terms of the license and acknowledge that the information I provide will be collected stored processed and shared in accordance with the Meta Privacy Policy": "checkbox"
      },
      "extra_gated_description": "The information you provide will be collected, stored, processed and shared in accordance with the [Meta Privacy Policy](https://www.facebook.com/privacy/policy/).",
      "extra_gated_button_content": "Submit"
    },
    "card_text": "\n## Model Information\n\nThe Llama 3.2 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction-tuned generative models in 1B and 3B sizes (text in/text out). The Llama 3.2 instruction-tuned text only models are optimized for multilingual dialogue use cases, including agentic retrieval and summarization tasks. They outperform many of the available open source and closed chat models on common industry benchmarks.\n\n**Model Developer:** Meta\n\n**Model Architecture:** Llama 3.2 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.\n\n|  | Training Data | Params | Input modalities | Output modalities | Context Length | GQA | Shared Embeddings | Token count | Knowledge cutoff |\n| :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- |\n| Llama 3.2 (text only)  | A new mix of publicly available online data. | 1B (1.23B) | Multilingual Text | Multilingual Text and code  | 128k | Yes | Yes | Up to 9T tokens | December 2023 |\n|  |  | 3B (3.21B) | Multilingual Text | Multilingual Text and code  |  |  |  |  |  |\n| Llama 3.2 Quantized (text only)  | A new mix of publicly available online data. | 1B (1.23B) | Multilingual Text | Multilingual Text and code  | 8k | Yes | Yes | Up to 9T tokens | December 2023 |\n|  |  | 3B (3.21B) | Multilingual Text | Multilingual Text and code |  |  |  |  |  |\n\n**Supported Languages:** English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai are officially supported. Llama 3.2 has been trained on a broader collection of languages than these 8 supported languages. Developers may fine-tune Llama 3.2 models for languages beyond these supported languages, provided they comply with the Llama 3.2 Community License and the Acceptable Use Policy. Developers are always expected to ensure that their deployments, including those that involve additional languages, are completed safely and responsibly.\n\n**Llama 3.2 Model Family:** Token counts refer to pretraining data only. All model versions use Grouped-Query Attention (GQA) for improved inference scalability.\n\n**Model Release Date:** Sept 25, 2024\n\n**Status:** This is a static model trained on an offline dataset. Future versions may be released that improve model capabilities and safety.\n\n**License:** Use of Llama 3.2 is governed by the [Llama 3.2 Community License](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/LICENSE) (a custom, commercial license agreement).\n\n**Feedback:** Instructions on how to provide feedback or comments on the model can be found in the Llama Models [README](https://github.com/meta-llama/llama-models/blob/main/README.md). For more technical information about generation parameters and recipes for how to use Llama 3.2 in applications, please go [here](https://github.com/meta-llama/llama-recipes).\n\n## Intended Use\n\n**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.\n\n**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.\n\n## How to use\n\nThis repository contains two versions of Llama-3.2-1B, for use with transformers and with the original `llama` codebase.\n\n### Use with transformers\n\nStarting with transformers >= 4.43.0 onward, you can run conversational inference using the Transformers pipeline abstraction or by leveraging the Auto classes with the generate() function.\n\nMake sure to update your transformers installation via pip install --upgrade transformers.\n\n```python\nimport torch\nfrom transformers import pipeline\n\nmodel_id = \"meta-llama/Llama-3.2-1B\"\n\npipe = pipeline(\n    \"text-generation\", \n    model=model_id, \n    torch_dtype=torch.bfloat16, \n    device_map=\"auto\"\n)\n\npipe(\"The key to life is\")\n```\n\n### Use with `llama`\n\nPlease, follow the instructions in the [repository](https://github.com/meta-llama/llama).\n\nTo download Original checkpoints, see the example command below leveraging `huggingface-cli`:\n\n```\nhuggingface-cli download meta-llama/Llama-3.2-1B --include \"original/*\" --local-dir Llama-3.2-1B\n```\n\n## Hardware and Software\n\n**Training Factors:** We used custom training libraries, Meta's custom built GPU cluster, and production infrastructure for pretraining. Fine-tuning, quantization, annotation, and evaluation were also performed on production infrastructure.\n\n**Training Energy Use:** Training utilized a cumulative of **916k** GPU hours of computation on H100-80GB (TDP of 700W) type hardware, per the table below. Training time is the total GPU time required for training each model and power consumption is the peak power capacity per GPU device used, adjusted for power usage efficiency.\n\n**Training Greenhouse Gas Emissions:** Estimated total location-based greenhouse gas emissions were **240** tons CO2eq for training. Since 2020, Meta has maintained net zero greenhouse gas emissions in its global operations and matched 100% of its electricity use with renewable energy; therefore, the total market-based greenhouse gas emissions for training were 0 tons CO2eq.\n\n|  | Training Time (GPU hours) | Logit Generation Time (GPU Hours) | Training Power Consumption (W) | Training Location-Based Greenhouse Gas Emissions (tons CO2eq) | Training Market-Based Greenhouse Gas Emissions (tons CO2eq) |\n| :---- | :---: | ----- | :---: | :---: | :---: |\n| Llama 3.2 1B | 370k | \\- | 700 | 107 | 0 |\n| Llama 3.2 3B | 460k | \\- | 700 | 133 | 0 |\n| Llama 3.2 1B SpinQuant | 1.7 | 0 | 700 | *Negligible*\\*\\* | 0 |\n| Llama 3.2 3B SpinQuant | 2.4 | 0 | 700 | *Negligible*\\*\\* | 0 |\n| Llama 3.2 1B QLora | 1.3k | 0 | 700 | 0.381 | 0 |\n| Llama 3.2 3B QLora | 1.6k | 0 | 700 | 0.461 | 0 |\n| Total | 833k |         86k |  | 240 | 0 |\n\n\\*\\* The location-based CO2e emissions of Llama 3.2 1B SpinQuant and Llama 3.2 3B SpinQuant are less than 0.001 metric tonnes each. This is due to the minimal training GPU hours that are required.\n\nThe methodology used to determine training energy use and greenhouse gas emissions can be found [here](https://arxiv.org/pdf/2204.05149). Since Meta is openly releasing these models, the training energy use and greenhouse gas emissions will not be incurred by others.\n\n## Training Data\n\n**Overview:** Llama 3.2 was pretrained on up to 9 trillion tokens of data from publicly available sources. For the 1B and 3B Llama 3.2 models, we incorporated logits from the Llama 3.1 8B and 70B models into the pretraining stage of the model development, where outputs (logits) from these larger models were used as token-level targets. Knowledge distillation was used after pruning to recover performance. In post-training we used a similar recipe as Llama 3.1 and produced final chat models by doing several rounds of alignment on top of the pre-trained model. Each round involved Supervised Fine-Tuning (SFT), Rejection Sampling (RS), and Direct Preference Optimization (DPO).\n\n**Data Freshness:** The pretraining data has a cutoff of December 2023\\.\n\n## Quantization\n\n### Quantization Scheme\n\nWe designed the current quantization scheme with the [PyTorch’s ExecuTorch](https://github.com/pytorch/executorch) inference framework and Arm CPU backend in mind, taking into account metrics including model quality, prefill/decoding speed, and memory footprint. Our quantization scheme involves three parts:\n- All linear layers in all transformer blocks are quantized to a 4-bit groupwise scheme (with a group size of 32) for weights and 8-bit per-token dynamic quantization for activations.\n- The classification layer is quantized to 8-bit per-channel for weight and 8-bit per token dynamic quantization for activation.\n- Similar to classification layer, an 8-bit per channel quantization is used for embedding layer.\n\n\n### Quantization-Aware Training and LoRA\n\nThe quantization-aware training (QAT) with low-rank adaptation (LoRA) models went through only post-training stages, using the same data as the full precision models. To initialize QAT, we utilize BF16 Llama 3.2 model checkpoints obtained after supervised fine-tuning (SFT) and perform an additional full round of SFT training with QAT. We then freeze the backbone of the QAT model and perform another round of SFT with LoRA adaptors applied to all layers within the transformer block. Meanwhile, the LoRA adaptors' weights and activations are maintained in BF16. Because our approach is similar to QLoRA of Dettmers et al., (2023) (i.e., quantization followed by LoRA adapters), we refer this method as QLoRA. Finally, we fine-tune the resulting model (both backbone and LoRA adaptors) using direct preference optimization (DPO).\n\n### SpinQuant\n\n[SpinQuant](https://arxiv.org/abs/2405.16406) was applied, together with generative post-training quantization (GPTQ). For the SpinQuant rotation matrix fine-tuning, we optimized for 100 iterations, using 800 samples with sequence-length 2048 from the WikiText 2 dataset. For GPTQ, we used 128 samples from the same dataset with the same sequence-length.\n\n## Benchmarks \\- English Text\n\nIn this section, we report the results for Llama 3.2 models on standard automatic benchmarks. For all these evaluations, we used our internal evaluations library.\n\n### Base Pretrained Models\n\n| Category | Benchmark | \\# Shots | Metric | Llama 3.2 1B | Llama 3.2 3B | Llama 3.1 8B |\n| ----- | ----- | :---: | :---: | :---: | :---: | :---: |\n| General | MMLU | 5 | macro\\_avg/acc\\_char | 32.2 | 58 | 66.7 |\n|  | AGIEval English | 3-5 | average/acc\\_char | 23.3 | 39.2 | 47.8 |\n|  | ARC-Challenge | 25 | acc\\_char | 32.8 | 69.1 | 79.7 |\n| Reading comprehension | SQuAD | 1 | em | 49.2 | 67.7 | 77 |\n|  | QuAC (F1) | 1 | f1 | 37.9 | 42.9 | 44.9 |\n|  | DROP (F1) | 3 | f1 | 28.0 | 45.2 | 59.5 |\n| Long Context | Needle in Haystack | 0 | em | 96.8 | 1 | 1 |\n\n### Instruction Tuned Models\n\n| Capability |  | Benchmark | \\# Shots | Metric | Llama 3.2 1B bf16 | Llama 3.2 1B Vanilla PTQ\\*\\* | Llama 3.2 1B Spin Quant | Llama 3.2 1B QLoRA | Llama 3.2 3B bf16 | Llama 3.2 3B Vanilla PTQ\\*\\* | Llama 3.2 3B Spin Quant | Llama 3.2 3B QLoRA | Llama 3.1 8B |\n| :---: | ----- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| General |  | MMLU | 5 | macro\\_avg/acc | 49.3 | 43.3 | 47.3 | 49.0 | 63.4 | 60.5 | 62 | 62.4 | 69.4 |\n| Re-writing |  | Open-rewrite eval | 0 | micro\\_avg/rougeL | 41.6 | 39.2 | 40.9 | 41.2 | 40.1 | 40.3 | 40.8 | 40.7 | 40.9 |\n| Summarization |  | TLDR9+ (test) | 1 | rougeL | 16.8 | 14.9 | 16.7 | 16.8 | 19.0 | 19.1 | 19.2 | 19.1 | 17.2 |\n| Instruction following |  | IFEval | 0 | Avg(Prompt/Instruction acc Loose/Strict) | 59.5 | 51.5 | 58.4 | 55.6 | 77.4 | 73.9 | 73.5 | 75.9 | 80.4 |\n| Math |  | GSM8K (CoT) | 8 | em\\_maj1@1 | 44.4 | 33.1 | 40.6 | 46.5 | 77.7 | 72.9 | 75.7 | 77.9 | 84.5 |\n|  |  | MATH (CoT) | 0 | final\\_em | 30.6 | 20.5 | 25.3 | 31.0 | 48.0 | 44.2 | 45.3 | 49.2 | 51.9 |\n| Reasoning |  | ARC-C | 0 | acc | 59.4 | 54.3 | 57 | 60.7 | 78.6 | 75.6 | 77.6 | 77.6 | 83.4 |\n|  |  | GPQA | 0 | acc | 27.2 | 25.9 | 26.3 | 25.9 | 32.8 | 32.8 | 31.7 | 33.9 | 32.8 |\n|  |  | Hellaswag | 0 | acc | 41.2 | 38.1 | 41.3 | 41.5 | 69.8 | 66.3 | 68 | 66.3 | 78.7 |\n| Tool Use |  | BFCL V2 | 0 | acc | 25.7 | 14.3 | 15.9 | 23.7 | 67.0 | 53.4 | 60.1 | 63.5 | 67.1 |\n|  |  | Nexus | 0 | macro\\_avg/acc | 13.5 | 5.2 | 9.6 | 12.5 | 34.3 | 32.4 | 31.5 | 30.1 | 38.5 |\n| Long Context |  | InfiniteBench/En.QA | 0 | longbook\\_qa/f1 | 20.3 | N/A | N/A | N/A | 19.8 | N/A | N/A | N/A | 27.3 |\n|  |  | InfiniteBench/En.MC | 0 | longbook\\_choice/acc | 38.0 | N/A | N/A | N/A | 63.3 | N/A | N/A | N/A | 72.2 |\n|  |  | NIH/Multi-needle | 0 | recall | 75.0 | N/A | N/A | N/A | 84.7 | N/A | N/A | N/A | 98.8 |\n| Multilingual |  | MGSM (CoT) | 0 | em | 24.5 | 13.7 | 18.2 | 24.4 | 58.2 | 48.9 | 54.3 | 56.8 | 68.9 |\n\n\\*\\*for comparison purposes only. Model not released.\n\n### Multilingual Benchmarks\n\n| Category | Benchmark | Language | Llama 3.2 1B | Llama 3.2 1B Vanilla PTQ\\*\\* | Llama 3.2 1B Spin Quant | Llama 3.2 1B QLoRA | Llama 3.2 3B | Llama 3.2 3B Vanilla PTQ\\*\\* | Llama 3.2 3B Spin Quant | Llama 3.2 3B QLoRA | Llama 3.1 8B |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| General | MMLU (5-shot, macro_avg/acc) | Portuguese | 39.8 | 34.9 | 38.9 | 40.2 | 54.5 | 50.9 | 53.3 | 53.4 | 62.1 |\n| | | Spanish | 41.5 | 36.0 | 39.8 | 41.8 | 55.1 | 51.9 | 53.6 | 53.6 | 62.5 |\n| | | Italian | 39.8 | 34.9 | 38.1 | 40.6 | 53.8 | 49.9 | 52.1 | 51.7 | 61.6 |\n| | | German | 39.2 | 34.9 | 37.5 | 39.6 | 53.3 | 50.0 | 52.2 | 51.3 | 60.6 |\n| | | French | 40.5 | 34.8 | 39.2 | 40.8 | 54.6 | 51.2 | 53.3 | 53.3 | 62.3 |\n| | | Hindi | 33.5 | 30.0 | 32.1 | 34.0 | 43.3 | 40.4 | 42.0 | 42.1 | 50.9 |\n| | | Thai | 34.7 | 31.2 | 32.4 | 34.9 | 44.5 | 41.3 | 44.0 | 42.2 | 50.3 |\n\n\\*\\*for comparison purposes only. Model not released.\n\n## Inference time\n\nIn the below table, we compare the performance metrics of different quantization methods (SpinQuant and QAT \\+ LoRA) with the BF16 baseline. The evaluation was done using the [ExecuTorch](https://github.com/pytorch/executorch) framework as the inference engine, with the ARM CPU as a backend using Android OnePlus 12 device.\n\n| Category | Decode (tokens/sec)  | Time-to-first-token (sec) | Prefill (tokens/sec) | Model size (PTE file size in MB) | Memory size (RSS in MB) |\n| :---- | ----- | ----- | ----- | ----- | ----- |\n| 1B BF16 (baseline) | 19.2 | 1.0 | 60.3 | 2358 | 3,185 |\n| 1B SpinQuant | 50.2 (2.6x) | 0.3 (-76.9%) | 260.5 (4.3x) | 1083 (-54.1%) | 1,921 (-39.7%) |\n| 1B QLoRA | 45.8 (2.4x) | 0.3 (-76.0%) | 252.0 (4.2x) | 1127 (-52.2%) | 2,255 (-29.2%) |\n| 3B BF16 (baseline) | 7.6 | 3.0 | 21.2 | 6129 | 7,419 |\n| 3B SpinQuant | 19.7 (2.6x) | 0.7 (-76.4%) | 89.7 (4.2x) | 2435 (-60.3%) | 3,726 (-49.8%) |\n| 3B QLoRA | 18.5 (2.4x) | 0.7 (-76.1%) | 88.8 (4.2x) | 2529 (-58.7%) | 4,060 (-45.3%) |\n\n(\\*) The performance measurement is done using an adb binary-based approach.\n(\\*\\*) It is measured on an Android OnePlus 12 device.\n(\\*\\*\\*) Time-to-first-token (TTFT)  is measured with prompt length=64\n\n*Footnote:*\n\n- *Decode (tokens/second) is for how quickly it keeps generating. Higher is better.*\n- *Time-to-first-token (TTFT for shorthand) is for how fast it generates the first token for a given prompt. Lower is better.*\n- *Prefill is the inverse of TTFT (aka 1/TTFT)  in tokens/second. Higher is better*\n- *Model size \\- how big is the model, measured by, PTE file, a binary file format for ExecuTorch*\n- *RSS size \\- Memory usage in resident set size (RSS)*\n\n## Responsibility & Safety\n\nAs part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:\n\n1. Enable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama\n2. Protect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm\n3. Provide protections for the community to help prevent the misuse of our models\n\n### Responsible Deployment\n\n**Approach:** Llama is a foundational technology designed to be used in a variety of use cases. Examples on how Meta’s Llama models have been responsibly deployed can be found in our [Community Stories webpage](https://llama.meta.com/community-stories/). Our approach is to build the most helpful models, enabling the world to benefit from the technology power, by aligning our model safety for generic use cases and addressing a standard set of harms. Developers are then in the driver’s seat to tailor safety for their use cases, defining their own policies and deploying the models with the necessary safeguards in their Llama systems. Llama 3.2 was developed following the best practices outlined in our [Responsible Use Guide](https://llama.meta.com/responsible-use-guide/).\n\n#### Llama 3.2 Instruct\n\n**Objective:** Our main objectives for conducting safety fine-tuning are to provide the research community with a valuable resource for studying the robustness of safety fine-tuning, as well as to offer developers a readily available, safe, and powerful model for various applications to reduce the developer workload to deploy safe AI systems. We implemented the same set of safety mitigations as in Llama 3, and you can learn more about these in the Llama 3 [paper](https://ai.meta.com/research/publications/the-llama-3-herd-of-models/).\n\n**Fine-Tuning Data:** We employ a multi-faceted approach to data collection, combining human-generated data from our vendors with synthetic data to mitigate potential safety risks. We’ve developed many large language model (LLM)-based classifiers that enable us to thoughtfully select high-quality prompts and responses, enhancing data quality control.\n\n**Refusals and Tone:** Building on the work we started with Llama 3, we put a great emphasis on model refusals to benign prompts as well as refusal tone. We included both borderline and adversarial prompts in our safety data strategy, and modified our safety data responses to follow tone guidelines.\n\n#### Llama 3.2 Systems\n\n**Safety as a System:** Large language models, including Llama 3.2, **are not designed to be deployed in isolation** but instead should be deployed as part of an overall AI system with additional safety guardrails as required. Developers are expected to deploy system safeguards when building agentic systems. Safeguards are key to achieve the right helpfulness-safety alignment as well as mitigating safety and security risks inherent to the system and any integration of the model or system with external tools. As part of our responsible release approach, we provide the community with [safeguards](https://llama.meta.com/trust-and-safety/) that developers should deploy with Llama models or other LLMs, including Llama Guard, Prompt Guard and Code Shield. All our [reference implementations](https://github.com/meta-llama/llama-agentic-system) demos contain these safeguards by default so developers can benefit from system-level safety out-of-the-box.\n\n### New Capabilities and Use Cases\n\n**Technological Advancement:** Llama releases usually introduce new capabilities that require specific considerations in addition to the best practices that generally apply across all Generative AI use cases. For prior release capabilities also supported by Llama 3.2, see [Llama 3.1 Model Card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md), as the same considerations apply here as well.\n\n**Constrained Environments:** Llama 3.2 1B and 3B models are expected to be deployed in highly constrained environments, such as mobile devices. LLM Systems using smaller models will have a different alignment profile and safety/helpfulness tradeoff than more complex, larger systems. Developers should ensure the safety of their system meets the requirements of their use case. We recommend using lighter system safeguards for such use cases, like Llama Guard 3-1B or its mobile-optimized version.\n\n### Evaluations\n\n**Scaled Evaluations:** We built dedicated, adversarial evaluation datasets and evaluated systems composed of Llama models and Purple Llama safeguards to filter input prompt and output response. It is important to evaluate applications in context, and we recommend building dedicated evaluation dataset for your use case.\n\n**Red Teaming:** We conducted recurring red teaming exercises with the goal of discovering risks via adversarial prompting and we used the learnings to improve our benchmarks and safety tuning datasets. We partnered early with subject-matter experts in critical risk areas to understand the nature of these real-world harms and how such models may lead to unintended harm for society. Based on these conversations, we derived a set of adversarial goals for the red team to attempt to achieve, such as extracting harmful information or reprogramming the model to act in a potentially harmful capacity. The red team consisted of experts in cybersecurity, adversarial machine learning, responsible AI, and integrity in addition to multilingual content specialists with background in integrity issues in specific geographic markets.\n\n### Critical Risks\n\nIn addition to our safety work above, we took extra care on measuring and/or mitigating the following critical risk areas:\n\n**1\\. CBRNE (Chemical, Biological, Radiological, Nuclear, and Explosive Weapons):** Llama 3.2 1B and 3B models are smaller and less capable derivatives of Llama 3.1. For Llama 3.1 70B and 405B, to assess risks related to proliferation of chemical and biological weapons, we performed uplift testing designed to assess whether use of Llama 3.1 models could meaningfully increase the capabilities of malicious actors to plan or carry out attacks using these types of weapons and have determined that such testing also applies to the smaller 1B and 3B models.\n\n**2\\. Child Safety:** Child Safety risk assessments were conducted using a team of experts, to assess the model’s capability to produce outputs that could result in Child Safety risks and inform on any necessary and appropriate risk mitigations via fine tuning. We leveraged those expert red teaming sessions to expand the coverage of our evaluation benchmarks through Llama 3 model development. For Llama 3, we conducted new in-depth sessions using objective based methodologies to assess the model risks along multiple attack vectors including the additional languages Llama 3 is trained on. We also partnered with content specialists to perform red teaming exercises assessing potentially violating content while taking account of market specific nuances or experiences.\n\n**3\\. Cyber Attacks:** For Llama 3.1 405B, our cyber attack uplift study investigated whether LLMs can enhance human capabilities in hacking tasks, both in terms of skill level and speed.\nOur attack automation study focused on evaluating the capabilities of LLMs when used as autonomous agents in cyber offensive operations, specifically in the context of ransomware attacks. This evaluation was distinct from previous studies that considered LLMs as interactive assistants. The primary objective was to assess whether these models could effectively function as independent agents in executing complex cyber-attacks without human intervention. Because Llama 3.2’s 1B and 3B models are smaller and less capable models than Llama 3.1 405B, we broadly believe that the testing conducted for the 405B model also applies to Llama 3.2 models.\n\n### Community\n\n**Industry Partnerships:** Generative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership on AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Purple Llama tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to our [Github repository](https://github.com/meta-llama/PurpleLlama).\n\n**Grants:** We also set up the [Llama Impact Grants](https://llama.meta.com/llama-impact-grants/) program to identify and support the most compelling applications of Meta’s Llama model for societal benefit across three categories: education, climate and open innovation. The 20 finalists from the hundreds of applications can be found [here](https://llama.meta.com/llama-impact-grants/#finalists).\n\n**Reporting:** Finally, we put in place a set of resources including an [output reporting mechanism](https://developers.facebook.com/llama_output_feedback) and [bug bounty program](https://www.facebook.com/whitehat) to continuously improve the Llama technology with the help of the community.\n\n## Ethical Considerations and Limitations\n\n**Values:** The core values of Llama 3.2 are openness, inclusivity and helpfulness. It is meant to serve everyone, and to work for a wide range of use cases. It is thus designed to be accessible to people across many different backgrounds, experiences and perspectives. Llama 3.2 addresses users and their needs as they are, without insertion unnecessary judgment or normativity, while reflecting the understanding that even content that may appear problematic in some cases can serve valuable purposes in others. It respects the dignity and autonomy of all users, especially in terms of the values of free thought and expression that power innovation and progress.\n\n**Testing:** Llama 3.2 is a new technology, and like any new technology, there are risks associated with its use. Testing conducted to date has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, Llama 3.2’s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 3.2 models, developers should perform safety testing and tuning tailored to their specific applications of the model. Please refer to available resources including our [Responsible Use Guide](https://llama.meta.com/responsible-use-guide), [Trust and Safety](https://llama.meta.com/trust-and-safety/) solutions, and other [resources](https://llama.meta.com/docs/get-started/) to learn more about responsible development.\n",
    "card_content": "---\nlanguage:\n- en\n- de\n- fr\n- it\n- pt\n- hi\n- es\n- th\nlibrary_name: transformers\npipeline_tag: text-generation\ntags:\n- facebook\n- meta\n- pytorch\n- llama\n- llama-3\nlicense: llama3.2\nextra_gated_prompt: \"### LLAMA 3.2 COMMUNITY LICENSE AGREEMENT\\n\\nLlama 3.2 Version\\\n  \\ Release Date: September 25, 2024\\n\\n“Agreement” means the terms and conditions\\\n  \\ for use, reproduction, distribution  and modification of the Llama Materials set\\\n  \\ forth herein.\\n\\n“Documentation” means the specifications, manuals and documentation\\\n  \\ accompanying Llama 3.2 distributed by Meta at https://llama.meta.com/doc/overview.\\n\\\n  \\n“Licensee” or “you” means you, or your employer or any other person or entity\\\n  \\ (if you are  entering into this Agreement on such person or entity’s behalf),\\\n  \\ of the age required under applicable laws, rules or regulations to provide legal\\\n  \\ consent and that has legal authority to bind your employer or such other person\\\n  \\ or entity if you are entering in this Agreement on their behalf.\\n\\n“Llama 3.2”\\\n  \\ means the foundational large language models and software and algorithms, including\\\n  \\ machine-learning model code, trained model weights, inference-enabling code, training-enabling\\\n  \\ code, fine-tuning enabling code and other elements of the foregoing distributed\\\n  \\ by Meta at  https://www.llama.com/llama-downloads.\\n\\n“Llama Materials” means,\\\n  \\ collectively, Meta’s proprietary Llama 3.2 and Documentation (and  any portion\\\n  \\ thereof) made available under this Agreement.\\n\\n“Meta” or “we” means Meta Platforms\\\n  \\ Ireland Limited (if you are located in or,  if you are an entity, your principal\\\n  \\ place of business is in the EEA or Switzerland)  and Meta Platforms, Inc. (if\\\n  \\ you are located outside of the EEA or Switzerland). \\n\\nBy clicking “I Accept”\\\n  \\ below or by using or distributing any portion or element of the Llama Materials,\\\n  \\ you agree to be bound by this Agreement.\\n\\n1. License Rights and Redistribution.\\n\\\n  a. Grant of Rights. You are granted a non-exclusive, worldwide,  non-transferable\\\n  \\ and royalty-free limited license under Meta’s intellectual property or other rights\\\n  \\  owned by Meta embodied in the Llama Materials to use, reproduce, distribute,\\\n  \\ copy, create derivative works  of, and make modifications to the Llama Materials.\\\n  \\  \\nb. Redistribution and Use.  \\ni. If you distribute or make available the Llama\\\n  \\ Materials (or any derivative works thereof),  or a product or service (including\\\n  \\ another AI model) that contains any of them, you shall (A) provide a copy of this\\\n  \\ Agreement with any such Llama Materials; and (B) prominently display “Built with\\\n  \\ Llama” on a related website, user interface, blogpost, about page, or product\\\n  \\ documentation. If you use the Llama Materials or any outputs or results of the\\\n  \\ Llama Materials to create, train, fine tune, or otherwise improve an AI model,\\\n  \\ which is distributed or made available, you shall also include “Llama” at the\\\n  \\ beginning of any such AI model name.\\nii. If you receive Llama Materials, or any\\\n  \\ derivative works thereof, from a Licensee as part of an integrated end user product,\\\n  \\ then Section 2 of this Agreement will not apply to you. \\niii. You must retain\\\n  \\ in all copies of the Llama Materials that you distribute the  following attribution\\\n  \\ notice within a “Notice” text file distributed as a part of such copies:  “Llama\\\n  \\ 3.2 is licensed under the Llama 3.2 Community License, Copyright © Meta Platforms,\\\n  \\ Inc. All Rights Reserved.”\\niv. Your use of the Llama Materials must comply with\\\n  \\ applicable laws and regulations (including trade compliance laws and regulations)\\\n  \\ and adhere to the Acceptable Use Policy for the Llama Materials (available at\\\n  \\ https://www.llama.com/llama3_2/use-policy), which is hereby  incorporated by reference\\\n  \\ into this Agreement.\\n  \\n2. Additional Commercial Terms. If, on the Llama 3.2\\\n  \\ version release date, the monthly active users of the products or services made\\\n  \\ available by or for Licensee, or Licensee’s affiliates,  is greater than 700 million\\\n  \\ monthly active users in the preceding calendar month, you must request  a license\\\n  \\ from Meta, which Meta may grant to you in its sole discretion, and you are not\\\n  \\ authorized to exercise any of the rights under this Agreement unless or until\\\n  \\ Meta otherwise expressly grants you such rights.\\n3. Disclaimer of Warranty. UNLESS\\\n  \\ REQUIRED BY APPLICABLE LAW, THE LLAMA MATERIALS AND ANY OUTPUT AND  RESULTS THEREFROM\\\n  \\ ARE PROVIDED ON AN “AS IS” BASIS, WITHOUT WARRANTIES OF ANY KIND, AND META DISCLAIMS\\\n  \\ ALL WARRANTIES OF ANY KIND, BOTH EXPRESS AND IMPLIED, INCLUDING, WITHOUT LIMITATION,\\\n  \\ ANY WARRANTIES OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR\\\n  \\ PURPOSE. YOU ARE SOLELY RESPONSIBLE FOR DETERMINING THE APPROPRIATENESS OF USING\\\n  \\ OR REDISTRIBUTING THE LLAMA MATERIALS AND ASSUME ANY RISKS ASSOCIATED WITH YOUR\\\n  \\ USE OF THE LLAMA MATERIALS AND ANY OUTPUT AND RESULTS.\\n4. Limitation of Liability.\\\n  \\ IN NO EVENT WILL META OR ITS AFFILIATES BE LIABLE UNDER ANY THEORY OF LIABILITY,\\\n  \\  WHETHER IN CONTRACT, TORT, NEGLIGENCE, PRODUCTS LIABILITY, OR OTHERWISE, ARISING\\\n  \\ OUT OF THIS AGREEMENT,  FOR ANY LOST PROFITS OR ANY INDIRECT, SPECIAL, CONSEQUENTIAL,\\\n  \\ INCIDENTAL, EXEMPLARY OR PUNITIVE DAMAGES, EVEN  IF META OR ITS AFFILIATES HAVE\\\n  \\ BEEN ADVISED OF THE POSSIBILITY OF ANY OF THE FOREGOING.\\n5. Intellectual Property.\\n\\\n  a. No trademark licenses are granted under this Agreement, and in connection with\\\n  \\ the Llama Materials,  neither Meta nor Licensee may use any name or mark owned\\\n  \\ by or associated with the other or any of its affiliates,  except as required\\\n  \\ for reasonable and customary use in describing and redistributing the Llama Materials\\\n  \\ or as  set forth in this Section 5(a). Meta hereby grants you a license to use\\\n  \\ “Llama” (the “Mark”) solely as required  to comply with the last sentence of Section\\\n  \\ 1.b.i. You will comply with Meta’s brand guidelines (currently accessible  at\\\n  \\ https://about.meta.com/brand/resources/meta/company-brand/). All goodwill arising\\\n  \\ out of your use of the Mark  will inure to the benefit of Meta.\\nb. Subject to\\\n  \\ Meta’s ownership of Llama Materials and derivatives made by or for Meta, with\\\n  \\ respect to any derivative works and modifications of the Llama Materials that\\\n  \\ are made by you, as between you and Meta, you are and will be the owner of such\\\n  \\ derivative works and modifications.\\nc. If you institute litigation or other proceedings\\\n  \\ against Meta or any entity (including a cross-claim or counterclaim in a lawsuit)\\\n  \\ alleging that the Llama Materials or Llama 3.2 outputs or results, or any portion\\\n  \\ of any of the foregoing, constitutes infringement of intellectual property or\\\n  \\ other rights owned or licensable by you, then any licenses granted to you under\\\n  \\ this Agreement shall terminate as of the date such litigation or claim is filed\\\n  \\ or instituted. You will indemnify and hold harmless Meta from and against any\\\n  \\ claim by any third party arising out of or related to your use or distribution\\\n  \\ of the Llama Materials.\\n6. Term and Termination. The term of this Agreement will\\\n  \\ commence upon your acceptance of this Agreement or access to the Llama Materials\\\n  \\ and will continue in full force and effect until terminated in accordance with\\\n  \\ the terms and conditions herein. Meta may terminate this Agreement if you are\\\n  \\ in breach of any term or condition of this Agreement. Upon termination of this\\\n  \\ Agreement, you shall delete and cease use of the Llama Materials. Sections 3,\\\n  \\ 4 and 7 shall survive the termination of this Agreement. \\n7. Governing Law and\\\n  \\ Jurisdiction. This Agreement will be governed and construed under the laws of\\\n  \\ the State of  California without regard to choice of law principles, and the UN\\\n  \\ Convention on Contracts for the International Sale of Goods does not apply to\\\n  \\ this Agreement. The courts of California shall have exclusive jurisdiction of\\\n  \\ any dispute arising out of this Agreement. \\n### Llama 3.2 Acceptable Use Policy\\n\\\n  Meta is committed to promoting safe and fair use of its tools and features, including\\\n  \\ Llama 3.2.  If you access or use Llama 3.2, you agree to this Acceptable Use Policy\\\n  \\ (“**Policy**”).  The most recent copy of this policy can be found at [https://www.llama.com/llama3_2/use-policy](https://www.llama.com/llama3_2/use-policy).\\n\\\n  #### Prohibited Uses\\nWe want everyone to use Llama 3.2 safely and responsibly.\\\n  \\ You agree you will not use, or allow others to use, Llama 3.2 to:\\n1. Violate\\\n  \\ the law or others’ rights, including to:\\n    1. Engage in, promote, generate,\\\n  \\ contribute to, encourage, plan, incite, or further illegal or unlawful activity\\\n  \\ or content, such as:\\n        1. Violence or terrorism\\n        2. Exploitation\\\n  \\ or harm to children, including the solicitation, creation, acquisition, or dissemination\\\n  \\ of child exploitative content or failure to report Child Sexual Abuse Material\\n\\\n  \\        3. Human trafficking, exploitation, and sexual violence\\n        4. The\\\n  \\ illegal distribution of information or materials to minors, including obscene\\\n  \\ materials, or failure to employ legally required age-gating in connection with\\\n  \\ such information or materials.\\n        5. Sexual solicitation\\n        6. Any\\\n  \\ other criminal activity\\n    1. Engage in, promote, incite, or facilitate the\\\n  \\ harassment, abuse, threatening, or bullying of individuals or groups of individuals\\n\\\n  \\    2. Engage in, promote, incite, or facilitate discrimination or other unlawful\\\n  \\ or harmful conduct in the provision of employment, employment benefits, credit,\\\n  \\ housing, other economic benefits, or other essential goods and services\\n    3.\\\n  \\ Engage in the unauthorized or unlicensed practice of any profession including,\\\n  \\ but not limited to, financial, legal, medical/health, or related professional\\\n  \\ practices\\n    4. Collect, process, disclose, generate, or infer private or sensitive\\\n  \\ information about individuals, including information about individuals’ identity,\\\n  \\ health, or demographic information, unless you have obtained the right to do so\\\n  \\ in accordance with applicable law\\n    5. Engage in or facilitate any action or\\\n  \\ generate any content that infringes, misappropriates, or otherwise violates any\\\n  \\ third-party rights, including the outputs or results of any products or services\\\n  \\ using the Llama Materials\\n    6. Create, generate, or facilitate the creation\\\n  \\ of malicious code, malware, computer viruses or do anything else that could disable,\\\n  \\ overburden, interfere with or impair the proper working, integrity, operation\\\n  \\ or appearance of a website or computer system\\n    7. Engage in any action, or\\\n  \\ facilitate any action, to intentionally circumvent or remove usage restrictions\\\n  \\ or other safety measures, or to enable functionality disabled by Meta \\n2. Engage\\\n  \\ in, promote, incite, facilitate, or assist in the planning or development of activities\\\n  \\ that present a risk of death or bodily harm to individuals, including use of Llama\\\n  \\ 3.2 related to the following:\\n    8. Military, warfare, nuclear industries or\\\n  \\ applications, espionage, use for materials or activities that are subject to the\\\n  \\ International Traffic Arms Regulations (ITAR) maintained by the United States\\\n  \\ Department of State or to the U.S. Biological Weapons Anti-Terrorism Act of 1989\\\n  \\ or the Chemical Weapons Convention Implementation Act of 1997\\n    9. Guns and\\\n  \\ illegal weapons (including weapon development)\\n    10. Illegal drugs and regulated/controlled\\\n  \\ substances\\n    11. Operation of critical infrastructure, transportation technologies,\\\n  \\ or heavy machinery\\n    12. Self-harm or harm to others, including suicide, cutting,\\\n  \\ and eating disorders\\n    13. Any content intended to incite or promote violence,\\\n  \\ abuse, or any infliction of bodily harm to an individual\\n3. Intentionally deceive\\\n  \\ or mislead others, including use of Llama 3.2 related to the following:\\n    14.\\\n  \\ Generating, promoting, or furthering fraud or the creation or promotion of disinformation\\n\\\n  \\    15. Generating, promoting, or furthering defamatory content, including the\\\n  \\ creation of defamatory statements, images, or other content\\n    16. Generating,\\\n  \\ promoting, or further distributing spam\\n    17. Impersonating another individual\\\n  \\ without consent, authorization, or legal right\\n    18. Representing that the\\\n  \\ use of Llama 3.2 or outputs are human-generated\\n    19. Generating or facilitating\\\n  \\ false online engagement, including fake reviews and other means of fake online\\\n  \\ engagement \\n4. Fail to appropriately disclose to end users any known dangers\\\n  \\ of your AI system 5. Interact with third party tools, models, or software designed\\\n  \\ to generate unlawful content or engage in unlawful or harmful conduct and/or represent\\\n  \\ that the outputs of such tools, models, or software are associated with Meta or\\\n  \\ Llama 3.2\\n\\nWith respect to any multimodal models included in Llama 3.2, the\\\n  \\ rights granted under Section 1(a) of the Llama 3.2 Community License Agreement\\\n  \\ are not being granted to you if you are an individual domiciled in, or a company\\\n  \\ with a principal place of business in, the European Union. This restriction does\\\n  \\ not apply to end users of a product or service that incorporates any such multimodal\\\n  \\ models.\\n\\nPlease report any violation of this Policy, software “bug,” or other\\\n  \\ problems that could lead to a violation of this Policy through one of the following\\\n  \\ means:\\n\\n* Reporting issues with the model: [https://github.com/meta-llama/llama-models/issues](https://l.workplace.com/l.php?u=https%3A%2F%2Fgithub.com%2Fmeta-llama%2Fllama-models%2Fissues&h=AT0qV8W9BFT6NwihiOHRuKYQM_UnkzN_NmHMy91OT55gkLpgi4kQupHUl0ssR4dQsIQ8n3tfd0vtkobvsEvt1l4Ic6GXI2EeuHV8N08OG2WnbAmm0FL4ObkazC6G_256vN0lN9DsykCvCqGZ)\\n\\\n  * Reporting risky content generated by the model: [developers.facebook.com/llama_output_feedback](http://developers.facebook.com/llama_output_feedback)\\n\\\n  * Reporting bugs and security concerns: [facebook.com/whitehat/info](http://facebook.com/whitehat/info)\\n\\\n  * Reporting violations of the Acceptable Use Policy or unlicensed uses of Llama\\\n  \\ 3.2: LlamaUseReport@meta.com\"\nextra_gated_fields:\n  First Name: text\n  Last Name: text\n  Date of birth: date_picker\n  Country: country\n  Affiliation: text\n  Job title:\n    type: select\n    options:\n    - Student\n    - Research Graduate\n    - AI researcher\n    - AI developer/engineer\n    - Reporter\n    - Other\n  geo: ip_location\n  ? By clicking Submit below I accept the terms of the license and acknowledge that\n    the information I provide will be collected stored processed and shared in accordance\n    with the Meta Privacy Policy\n  : checkbox\nextra_gated_description: The information you provide will be collected, stored, processed\n  and shared in accordance with the [Meta Privacy Policy](https://www.facebook.com/privacy/policy/).\nextra_gated_button_content: Submit\n---\n\n## Model Information\n\nThe Llama 3.2 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction-tuned generative models in 1B and 3B sizes (text in/text out). The Llama 3.2 instruction-tuned text only models are optimized for multilingual dialogue use cases, including agentic retrieval and summarization tasks. They outperform many of the available open source and closed chat models on common industry benchmarks.\n\n**Model Developer:** Meta\n\n**Model Architecture:** Llama 3.2 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.\n\n|  | Training Data | Params | Input modalities | Output modalities | Context Length | GQA | Shared Embeddings | Token count | Knowledge cutoff |\n| :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- |\n| Llama 3.2 (text only)  | A new mix of publicly available online data. | 1B (1.23B) | Multilingual Text | Multilingual Text and code  | 128k | Yes | Yes | Up to 9T tokens | December 2023 |\n|  |  | 3B (3.21B) | Multilingual Text | Multilingual Text and code  |  |  |  |  |  |\n| Llama 3.2 Quantized (text only)  | A new mix of publicly available online data. | 1B (1.23B) | Multilingual Text | Multilingual Text and code  | 8k | Yes | Yes | Up to 9T tokens | December 2023 |\n|  |  | 3B (3.21B) | Multilingual Text | Multilingual Text and code |  |  |  |  |  |\n\n**Supported Languages:** English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai are officially supported. Llama 3.2 has been trained on a broader collection of languages than these 8 supported languages. Developers may fine-tune Llama 3.2 models for languages beyond these supported languages, provided they comply with the Llama 3.2 Community License and the Acceptable Use Policy. Developers are always expected to ensure that their deployments, including those that involve additional languages, are completed safely and responsibly.\n\n**Llama 3.2 Model Family:** Token counts refer to pretraining data only. All model versions use Grouped-Query Attention (GQA) for improved inference scalability.\n\n**Model Release Date:** Sept 25, 2024\n\n**Status:** This is a static model trained on an offline dataset. Future versions may be released that improve model capabilities and safety.\n\n**License:** Use of Llama 3.2 is governed by the [Llama 3.2 Community License](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/LICENSE) (a custom, commercial license agreement).\n\n**Feedback:** Instructions on how to provide feedback or comments on the model can be found in the Llama Models [README](https://github.com/meta-llama/llama-models/blob/main/README.md). For more technical information about generation parameters and recipes for how to use Llama 3.2 in applications, please go [here](https://github.com/meta-llama/llama-recipes).\n\n## Intended Use\n\n**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.\n\n**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.\n\n## How to use\n\nThis repository contains two versions of Llama-3.2-1B, for use with transformers and with the original `llama` codebase.\n\n### Use with transformers\n\nStarting with transformers >= 4.43.0 onward, you can run conversational inference using the Transformers pipeline abstraction or by leveraging the Auto classes with the generate() function.\n\nMake sure to update your transformers installation via pip install --upgrade transformers.\n\n```python\nimport torch\nfrom transformers import pipeline\n\nmodel_id = \"meta-llama/Llama-3.2-1B\"\n\npipe = pipeline(\n    \"text-generation\", \n    model=model_id, \n    torch_dtype=torch.bfloat16, \n    device_map=\"auto\"\n)\n\npipe(\"The key to life is\")\n```\n\n### Use with `llama`\n\nPlease, follow the instructions in the [repository](https://github.com/meta-llama/llama).\n\nTo download Original checkpoints, see the example command below leveraging `huggingface-cli`:\n\n```\nhuggingface-cli download meta-llama/Llama-3.2-1B --include \"original/*\" --local-dir Llama-3.2-1B\n```\n\n## Hardware and Software\n\n**Training Factors:** We used custom training libraries, Meta's custom built GPU cluster, and production infrastructure for pretraining. Fine-tuning, quantization, annotation, and evaluation were also performed on production infrastructure.\n\n**Training Energy Use:** Training utilized a cumulative of **916k** GPU hours of computation on H100-80GB (TDP of 700W) type hardware, per the table below. Training time is the total GPU time required for training each model and power consumption is the peak power capacity per GPU device used, adjusted for power usage efficiency.\n\n**Training Greenhouse Gas Emissions:** Estimated total location-based greenhouse gas emissions were **240** tons CO2eq for training. Since 2020, Meta has maintained net zero greenhouse gas emissions in its global operations and matched 100% of its electricity use with renewable energy; therefore, the total market-based greenhouse gas emissions for training were 0 tons CO2eq.\n\n|  | Training Time (GPU hours) | Logit Generation Time (GPU Hours) | Training Power Consumption (W) | Training Location-Based Greenhouse Gas Emissions (tons CO2eq) | Training Market-Based Greenhouse Gas Emissions (tons CO2eq) |\n| :---- | :---: | ----- | :---: | :---: | :---: |\n| Llama 3.2 1B | 370k | \\- | 700 | 107 | 0 |\n| Llama 3.2 3B | 460k | \\- | 700 | 133 | 0 |\n| Llama 3.2 1B SpinQuant | 1.7 | 0 | 700 | *Negligible*\\*\\* | 0 |\n| Llama 3.2 3B SpinQuant | 2.4 | 0 | 700 | *Negligible*\\*\\* | 0 |\n| Llama 3.2 1B QLora | 1.3k | 0 | 700 | 0.381 | 0 |\n| Llama 3.2 3B QLora | 1.6k | 0 | 700 | 0.461 | 0 |\n| Total | 833k |         86k |  | 240 | 0 |\n\n\\*\\* The location-based CO2e emissions of Llama 3.2 1B SpinQuant and Llama 3.2 3B SpinQuant are less than 0.001 metric tonnes each. This is due to the minimal training GPU hours that are required.\n\nThe methodology used to determine training energy use and greenhouse gas emissions can be found [here](https://arxiv.org/pdf/2204.05149). Since Meta is openly releasing these models, the training energy use and greenhouse gas emissions will not be incurred by others.\n\n## Training Data\n\n**Overview:** Llama 3.2 was pretrained on up to 9 trillion tokens of data from publicly available sources. For the 1B and 3B Llama 3.2 models, we incorporated logits from the Llama 3.1 8B and 70B models into the pretraining stage of the model development, where outputs (logits) from these larger models were used as token-level targets. Knowledge distillation was used after pruning to recover performance. In post-training we used a similar recipe as Llama 3.1 and produced final chat models by doing several rounds of alignment on top of the pre-trained model. Each round involved Supervised Fine-Tuning (SFT), Rejection Sampling (RS), and Direct Preference Optimization (DPO).\n\n**Data Freshness:** The pretraining data has a cutoff of December 2023\\.\n\n## Quantization\n\n### Quantization Scheme\n\nWe designed the current quantization scheme with the [PyTorch’s ExecuTorch](https://github.com/pytorch/executorch) inference framework and Arm CPU backend in mind, taking into account metrics including model quality, prefill/decoding speed, and memory footprint. Our quantization scheme involves three parts:\n- All linear layers in all transformer blocks are quantized to a 4-bit groupwise scheme (with a group size of 32) for weights and 8-bit per-token dynamic quantization for activations.\n- The classification layer is quantized to 8-bit per-channel for weight and 8-bit per token dynamic quantization for activation.\n- Similar to classification layer, an 8-bit per channel quantization is used for embedding layer.\n\n\n### Quantization-Aware Training and LoRA\n\nThe quantization-aware training (QAT) with low-rank adaptation (LoRA) models went through only post-training stages, using the same data as the full precision models. To initialize QAT, we utilize BF16 Llama 3.2 model checkpoints obtained after supervised fine-tuning (SFT) and perform an additional full round of SFT training with QAT. We then freeze the backbone of the QAT model and perform another round of SFT with LoRA adaptors applied to all layers within the transformer block. Meanwhile, the LoRA adaptors' weights and activations are maintained in BF16. Because our approach is similar to QLoRA of Dettmers et al., (2023) (i.e., quantization followed by LoRA adapters), we refer this method as QLoRA. Finally, we fine-tune the resulting model (both backbone and LoRA adaptors) using direct preference optimization (DPO).\n\n### SpinQuant\n\n[SpinQuant](https://arxiv.org/abs/2405.16406) was applied, together with generative post-training quantization (GPTQ). For the SpinQuant rotation matrix fine-tuning, we optimized for 100 iterations, using 800 samples with sequence-length 2048 from the WikiText 2 dataset. For GPTQ, we used 128 samples from the same dataset with the same sequence-length.\n\n## Benchmarks \\- English Text\n\nIn this section, we report the results for Llama 3.2 models on standard automatic benchmarks. For all these evaluations, we used our internal evaluations library.\n\n### Base Pretrained Models\n\n| Category | Benchmark | \\# Shots | Metric | Llama 3.2 1B | Llama 3.2 3B | Llama 3.1 8B |\n| ----- | ----- | :---: | :---: | :---: | :---: | :---: |\n| General | MMLU | 5 | macro\\_avg/acc\\_char | 32.2 | 58 | 66.7 |\n|  | AGIEval English | 3-5 | average/acc\\_char | 23.3 | 39.2 | 47.8 |\n|  | ARC-Challenge | 25 | acc\\_char | 32.8 | 69.1 | 79.7 |\n| Reading comprehension | SQuAD | 1 | em | 49.2 | 67.7 | 77 |\n|  | QuAC (F1) | 1 | f1 | 37.9 | 42.9 | 44.9 |\n|  | DROP (F1) | 3 | f1 | 28.0 | 45.2 | 59.5 |\n| Long Context | Needle in Haystack | 0 | em | 96.8 | 1 | 1 |\n\n### Instruction Tuned Models\n\n| Capability |  | Benchmark | \\# Shots | Metric | Llama 3.2 1B bf16 | Llama 3.2 1B Vanilla PTQ\\*\\* | Llama 3.2 1B Spin Quant | Llama 3.2 1B QLoRA | Llama 3.2 3B bf16 | Llama 3.2 3B Vanilla PTQ\\*\\* | Llama 3.2 3B Spin Quant | Llama 3.2 3B QLoRA | Llama 3.1 8B |\n| :---: | ----- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| General |  | MMLU | 5 | macro\\_avg/acc | 49.3 | 43.3 | 47.3 | 49.0 | 63.4 | 60.5 | 62 | 62.4 | 69.4 |\n| Re-writing |  | Open-rewrite eval | 0 | micro\\_avg/rougeL | 41.6 | 39.2 | 40.9 | 41.2 | 40.1 | 40.3 | 40.8 | 40.7 | 40.9 |\n| Summarization |  | TLDR9+ (test) | 1 | rougeL | 16.8 | 14.9 | 16.7 | 16.8 | 19.0 | 19.1 | 19.2 | 19.1 | 17.2 |\n| Instruction following |  | IFEval | 0 | Avg(Prompt/Instruction acc Loose/Strict) | 59.5 | 51.5 | 58.4 | 55.6 | 77.4 | 73.9 | 73.5 | 75.9 | 80.4 |\n| Math |  | GSM8K (CoT) | 8 | em\\_maj1@1 | 44.4 | 33.1 | 40.6 | 46.5 | 77.7 | 72.9 | 75.7 | 77.9 | 84.5 |\n|  |  | MATH (CoT) | 0 | final\\_em | 30.6 | 20.5 | 25.3 | 31.0 | 48.0 | 44.2 | 45.3 | 49.2 | 51.9 |\n| Reasoning |  | ARC-C | 0 | acc | 59.4 | 54.3 | 57 | 60.7 | 78.6 | 75.6 | 77.6 | 77.6 | 83.4 |\n|  |  | GPQA | 0 | acc | 27.2 | 25.9 | 26.3 | 25.9 | 32.8 | 32.8 | 31.7 | 33.9 | 32.8 |\n|  |  | Hellaswag | 0 | acc | 41.2 | 38.1 | 41.3 | 41.5 | 69.8 | 66.3 | 68 | 66.3 | 78.7 |\n| Tool Use |  | BFCL V2 | 0 | acc | 25.7 | 14.3 | 15.9 | 23.7 | 67.0 | 53.4 | 60.1 | 63.5 | 67.1 |\n|  |  | Nexus | 0 | macro\\_avg/acc | 13.5 | 5.2 | 9.6 | 12.5 | 34.3 | 32.4 | 31.5 | 30.1 | 38.5 |\n| Long Context |  | InfiniteBench/En.QA | 0 | longbook\\_qa/f1 | 20.3 | N/A | N/A | N/A | 19.8 | N/A | N/A | N/A | 27.3 |\n|  |  | InfiniteBench/En.MC | 0 | longbook\\_choice/acc | 38.0 | N/A | N/A | N/A | 63.3 | N/A | N/A | N/A | 72.2 |\n|  |  | NIH/Multi-needle | 0 | recall | 75.0 | N/A | N/A | N/A | 84.7 | N/A | N/A | N/A | 98.8 |\n| Multilingual |  | MGSM (CoT) | 0 | em | 24.5 | 13.7 | 18.2 | 24.4 | 58.2 | 48.9 | 54.3 | 56.8 | 68.9 |\n\n\\*\\*for comparison purposes only. Model not released.\n\n### Multilingual Benchmarks\n\n| Category | Benchmark | Language | Llama 3.2 1B | Llama 3.2 1B Vanilla PTQ\\*\\* | Llama 3.2 1B Spin Quant | Llama 3.2 1B QLoRA | Llama 3.2 3B | Llama 3.2 3B Vanilla PTQ\\*\\* | Llama 3.2 3B Spin Quant | Llama 3.2 3B QLoRA | Llama 3.1 8B |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| General | MMLU (5-shot, macro_avg/acc) | Portuguese | 39.8 | 34.9 | 38.9 | 40.2 | 54.5 | 50.9 | 53.3 | 53.4 | 62.1 |\n| | | Spanish | 41.5 | 36.0 | 39.8 | 41.8 | 55.1 | 51.9 | 53.6 | 53.6 | 62.5 |\n| | | Italian | 39.8 | 34.9 | 38.1 | 40.6 | 53.8 | 49.9 | 52.1 | 51.7 | 61.6 |\n| | | German | 39.2 | 34.9 | 37.5 | 39.6 | 53.3 | 50.0 | 52.2 | 51.3 | 60.6 |\n| | | French | 40.5 | 34.8 | 39.2 | 40.8 | 54.6 | 51.2 | 53.3 | 53.3 | 62.3 |\n| | | Hindi | 33.5 | 30.0 | 32.1 | 34.0 | 43.3 | 40.4 | 42.0 | 42.1 | 50.9 |\n| | | Thai | 34.7 | 31.2 | 32.4 | 34.9 | 44.5 | 41.3 | 44.0 | 42.2 | 50.3 |\n\n\\*\\*for comparison purposes only. Model not released.\n\n## Inference time\n\nIn the below table, we compare the performance metrics of different quantization methods (SpinQuant and QAT \\+ LoRA) with the BF16 baseline. The evaluation was done using the [ExecuTorch](https://github.com/pytorch/executorch) framework as the inference engine, with the ARM CPU as a backend using Android OnePlus 12 device.\n\n| Category | Decode (tokens/sec)  | Time-to-first-token (sec) | Prefill (tokens/sec) | Model size (PTE file size in MB) | Memory size (RSS in MB) |\n| :---- | ----- | ----- | ----- | ----- | ----- |\n| 1B BF16 (baseline) | 19.2 | 1.0 | 60.3 | 2358 | 3,185 |\n| 1B SpinQuant | 50.2 (2.6x) | 0.3 (-76.9%) | 260.5 (4.3x) | 1083 (-54.1%) | 1,921 (-39.7%) |\n| 1B QLoRA | 45.8 (2.4x) | 0.3 (-76.0%) | 252.0 (4.2x) | 1127 (-52.2%) | 2,255 (-29.2%) |\n| 3B BF16 (baseline) | 7.6 | 3.0 | 21.2 | 6129 | 7,419 |\n| 3B SpinQuant | 19.7 (2.6x) | 0.7 (-76.4%) | 89.7 (4.2x) | 2435 (-60.3%) | 3,726 (-49.8%) |\n| 3B QLoRA | 18.5 (2.4x) | 0.7 (-76.1%) | 88.8 (4.2x) | 2529 (-58.7%) | 4,060 (-45.3%) |\n\n(\\*) The performance measurement is done using an adb binary-based approach.\n(\\*\\*) It is measured on an Android OnePlus 12 device.\n(\\*\\*\\*) Time-to-first-token (TTFT)  is measured with prompt length=64\n\n*Footnote:*\n\n- *Decode (tokens/second) is for how quickly it keeps generating. Higher is better.*\n- *Time-to-first-token (TTFT for shorthand) is for how fast it generates the first token for a given prompt. Lower is better.*\n- *Prefill is the inverse of TTFT (aka 1/TTFT)  in tokens/second. Higher is better*\n- *Model size \\- how big is the model, measured by, PTE file, a binary file format for ExecuTorch*\n- *RSS size \\- Memory usage in resident set size (RSS)*\n\n## Responsibility & Safety\n\nAs part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:\n\n1. Enable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama\n2. Protect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm\n3. Provide protections for the community to help prevent the misuse of our models\n\n### Responsible Deployment\n\n**Approach:** Llama is a foundational technology designed to be used in a variety of use cases. Examples on how Meta’s Llama models have been responsibly deployed can be found in our [Community Stories webpage](https://llama.meta.com/community-stories/). Our approach is to build the most helpful models, enabling the world to benefit from the technology power, by aligning our model safety for generic use cases and addressing a standard set of harms. Developers are then in the driver’s seat to tailor safety for their use cases, defining their own policies and deploying the models with the necessary safeguards in their Llama systems. Llama 3.2 was developed following the best practices outlined in our [Responsible Use Guide](https://llama.meta.com/responsible-use-guide/).\n\n#### Llama 3.2 Instruct\n\n**Objective:** Our main objectives for conducting safety fine-tuning are to provide the research community with a valuable resource for studying the robustness of safety fine-tuning, as well as to offer developers a readily available, safe, and powerful model for various applications to reduce the developer workload to deploy safe AI systems. We implemented the same set of safety mitigations as in Llama 3, and you can learn more about these in the Llama 3 [paper](https://ai.meta.com/research/publications/the-llama-3-herd-of-models/).\n\n**Fine-Tuning Data:** We employ a multi-faceted approach to data collection, combining human-generated data from our vendors with synthetic data to mitigate potential safety risks. We’ve developed many large language model (LLM)-based classifiers that enable us to thoughtfully select high-quality prompts and responses, enhancing data quality control.\n\n**Refusals and Tone:** Building on the work we started with Llama 3, we put a great emphasis on model refusals to benign prompts as well as refusal tone. We included both borderline and adversarial prompts in our safety data strategy, and modified our safety data responses to follow tone guidelines.\n\n#### Llama 3.2 Systems\n\n**Safety as a System:** Large language models, including Llama 3.2, **are not designed to be deployed in isolation** but instead should be deployed as part of an overall AI system with additional safety guardrails as required. Developers are expected to deploy system safeguards when building agentic systems. Safeguards are key to achieve the right helpfulness-safety alignment as well as mitigating safety and security risks inherent to the system and any integration of the model or system with external tools. As part of our responsible release approach, we provide the community with [safeguards](https://llama.meta.com/trust-and-safety/) that developers should deploy with Llama models or other LLMs, including Llama Guard, Prompt Guard and Code Shield. All our [reference implementations](https://github.com/meta-llama/llama-agentic-system) demos contain these safeguards by default so developers can benefit from system-level safety out-of-the-box.\n\n### New Capabilities and Use Cases\n\n**Technological Advancement:** Llama releases usually introduce new capabilities that require specific considerations in addition to the best practices that generally apply across all Generative AI use cases. For prior release capabilities also supported by Llama 3.2, see [Llama 3.1 Model Card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md), as the same considerations apply here as well.\n\n**Constrained Environments:** Llama 3.2 1B and 3B models are expected to be deployed in highly constrained environments, such as mobile devices. LLM Systems using smaller models will have a different alignment profile and safety/helpfulness tradeoff than more complex, larger systems. Developers should ensure the safety of their system meets the requirements of their use case. We recommend using lighter system safeguards for such use cases, like Llama Guard 3-1B or its mobile-optimized version.\n\n### Evaluations\n\n**Scaled Evaluations:** We built dedicated, adversarial evaluation datasets and evaluated systems composed of Llama models and Purple Llama safeguards to filter input prompt and output response. It is important to evaluate applications in context, and we recommend building dedicated evaluation dataset for your use case.\n\n**Red Teaming:** We conducted recurring red teaming exercises with the goal of discovering risks via adversarial prompting and we used the learnings to improve our benchmarks and safety tuning datasets. We partnered early with subject-matter experts in critical risk areas to understand the nature of these real-world harms and how such models may lead to unintended harm for society. Based on these conversations, we derived a set of adversarial goals for the red team to attempt to achieve, such as extracting harmful information or reprogramming the model to act in a potentially harmful capacity. The red team consisted of experts in cybersecurity, adversarial machine learning, responsible AI, and integrity in addition to multilingual content specialists with background in integrity issues in specific geographic markets.\n\n### Critical Risks\n\nIn addition to our safety work above, we took extra care on measuring and/or mitigating the following critical risk areas:\n\n**1\\. CBRNE (Chemical, Biological, Radiological, Nuclear, and Explosive Weapons):** Llama 3.2 1B and 3B models are smaller and less capable derivatives of Llama 3.1. For Llama 3.1 70B and 405B, to assess risks related to proliferation of chemical and biological weapons, we performed uplift testing designed to assess whether use of Llama 3.1 models could meaningfully increase the capabilities of malicious actors to plan or carry out attacks using these types of weapons and have determined that such testing also applies to the smaller 1B and 3B models.\n\n**2\\. Child Safety:** Child Safety risk assessments were conducted using a team of experts, to assess the model’s capability to produce outputs that could result in Child Safety risks and inform on any necessary and appropriate risk mitigations via fine tuning. We leveraged those expert red teaming sessions to expand the coverage of our evaluation benchmarks through Llama 3 model development. For Llama 3, we conducted new in-depth sessions using objective based methodologies to assess the model risks along multiple attack vectors including the additional languages Llama 3 is trained on. We also partnered with content specialists to perform red teaming exercises assessing potentially violating content while taking account of market specific nuances or experiences.\n\n**3\\. Cyber Attacks:** For Llama 3.1 405B, our cyber attack uplift study investigated whether LLMs can enhance human capabilities in hacking tasks, both in terms of skill level and speed.\nOur attack automation study focused on evaluating the capabilities of LLMs when used as autonomous agents in cyber offensive operations, specifically in the context of ransomware attacks. This evaluation was distinct from previous studies that considered LLMs as interactive assistants. The primary objective was to assess whether these models could effectively function as independent agents in executing complex cyber-attacks without human intervention. Because Llama 3.2’s 1B and 3B models are smaller and less capable models than Llama 3.1 405B, we broadly believe that the testing conducted for the 405B model also applies to Llama 3.2 models.\n\n### Community\n\n**Industry Partnerships:** Generative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership on AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Purple Llama tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to our [Github repository](https://github.com/meta-llama/PurpleLlama).\n\n**Grants:** We also set up the [Llama Impact Grants](https://llama.meta.com/llama-impact-grants/) program to identify and support the most compelling applications of Meta’s Llama model for societal benefit across three categories: education, climate and open innovation. The 20 finalists from the hundreds of applications can be found [here](https://llama.meta.com/llama-impact-grants/#finalists).\n\n**Reporting:** Finally, we put in place a set of resources including an [output reporting mechanism](https://developers.facebook.com/llama_output_feedback) and [bug bounty program](https://www.facebook.com/whitehat) to continuously improve the Llama technology with the help of the community.\n\n## Ethical Considerations and Limitations\n\n**Values:** The core values of Llama 3.2 are openness, inclusivity and helpfulness. It is meant to serve everyone, and to work for a wide range of use cases. It is thus designed to be accessible to people across many different backgrounds, experiences and perspectives. Llama 3.2 addresses users and their needs as they are, without insertion unnecessary judgment or normativity, while reflecting the understanding that even content that may appear problematic in some cases can serve valuable purposes in others. It respects the dignity and autonomy of all users, especially in terms of the values of free thought and expression that power innovation and progress.\n\n**Testing:** Llama 3.2 is a new technology, and like any new technology, there are risks associated with its use. Testing conducted to date has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, Llama 3.2’s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 3.2 models, developers should perform safety testing and tuning tailored to their specific applications of the model. Please refer to available resources including our [Responsible Use Guide](https://llama.meta.com/responsible-use-guide), [Trust and Safety](https://llama.meta.com/trust-and-safety/) solutions, and other [resources](https://llama.meta.com/docs/get-started/) to learn more about responsible development.\n",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModelForCausalLM",
      "custom_class": null,
      "pipeline_tag": "text-generation",
      "processor": "AutoTokenizer"
    },
    "safetensors": {
      "parameters": {
        "BF16": 1235814400
      },
      "total": 1235814400
    },
    "model_index": null,
    "trending_score": null
  },
  {
    "model_id": "mistralai/Mistral-7B-Instruct-v0.2",
    "model_name": "mistralai/Mistral-7B-Instruct-v0.2",
    "author": "mistralai",
    "downloads": 3862583,
    "downloads_all_time": null,
    "likes": 2687,
    "tags": [
      "transformers",
      "pytorch",
      "safetensors",
      "mistral",
      "text-generation",
      "finetuned",
      "conversational",
      "arxiv:2310.06825",
      "license:apache-2.0",
      "autotrain_compatible",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2",
    "dependencies": [
      [
        "mistral_common",
        null
      ],
      [
        "mistral_inference",
        null
      ],
      [
        "transformers",
        null
      ]
    ],
    "last_modified": "2024-09-27T10:41:20+00:00",
    "created_at": "2023-12-11T13:18:44+00:00",
    "analysis_date": "2025-03-22T00:41:12.792412",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "mistral",
    "pipeline_tag": "text-generation",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "license": "apache-2.0",
      "tags": [
        "finetuned"
      ],
      "pipeline_tag": "text-generation",
      "new_version": "mistralai/Mistral-7B-Instruct-v0.3",
      "inference": true,
      "widget": [
        {
          "messages": [
            {
              "role": "user",
              "content": "What is your favorite condiment?"
            }
          ]
        }
      ],
      "extra_gated_description": "If you want to learn more about how we process your personal data, please read our <a href=\"https://mistral.ai/terms/\">Privacy Policy</a>."
    },
    "card_text": "\n# Model Card for Mistral-7B-Instruct-v0.2\n\n\n## Encode and Decode with `mistral_common`\n            \n```py\nfrom mistral_common.tokens.tokenizers.mistral import MistralTokenizer\nfrom mistral_common.protocol.instruct.messages import UserMessage\nfrom mistral_common.protocol.instruct.request import ChatCompletionRequest\n \nmistral_models_path = \"MISTRAL_MODELS_PATH\"\n \ntokenizer = MistralTokenizer.v1()\n \ncompletion_request = ChatCompletionRequest(messages=[UserMessage(content=\"Explain Machine Learning to me in a nutshell.\")])\n \ntokens = tokenizer.encode_chat_completion(completion_request).tokens\n```\n \n## Inference with `mistral_inference`\n \n ```py\nfrom mistral_inference.transformer import Transformer\nfrom mistral_inference.generate import generate\n \nmodel = Transformer.from_folder(mistral_models_path)\nout_tokens, _ = generate([tokens], model, max_tokens=64, temperature=0.0, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)\n\nresult = tokenizer.decode(out_tokens[0])\n\nprint(result)\n```\n\n## Inference with hugging face `transformers`\n \n```py\nfrom transformers import AutoModelForCausalLM\n \nmodel = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\nmodel.to(\"cuda\")\n \ngenerated_ids = model.generate(tokens, max_new_tokens=1000, do_sample=True)\n\n# decode with mistral tokenizer\nresult = tokenizer.decode(generated_ids[0].tolist())\nprint(result)\n```\n\n> [!TIP]\n> PRs to correct the `transformers` tokenizer so that it gives 1-to-1 the same results as the `mistral_common` reference implementation are very welcome!\n            \n---\n\nThe Mistral-7B-Instruct-v0.2 Large Language Model (LLM) is an instruct fine-tuned version of the Mistral-7B-v0.2.\n\nMistral-7B-v0.2 has the following changes compared to Mistral-7B-v0.1\n- 32k context window (vs 8k context in v0.1)\n- Rope-theta = 1e6\n- No Sliding-Window Attention\n\nFor full details of this model please read our [paper](https://arxiv.org/abs/2310.06825) and [release blog post](https://mistral.ai/news/la-plateforme/).\n\n## Instruction format\n\nIn order to leverage instruction fine-tuning, your prompt should be surrounded by `[INST]` and `[/INST]` tokens. The very first instruction should begin with a begin of sentence id. The next instructions should not. The assistant generation will be ended by the end-of-sentence token id.\n\nE.g.\n```\ntext = \"<s>[INST] What is your favourite condiment? [/INST]\"\n\"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!</s> \"\n\"[INST] Do you have mayonnaise recipes? [/INST]\"\n```\n\nThis format is available as a [chat template](https://huggingface.co/docs/transformers/main/chat_templating) via the `apply_chat_template()` method:\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ndevice = \"cuda\" # the device to load the model onto\n\nmodel = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\ntokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n    {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n    {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n]\n\nencodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n\nmodel_inputs = encodeds.to(device)\nmodel.to(device)\n\ngenerated_ids = model.generate(model_inputs, max_new_tokens=1000, do_sample=True)\ndecoded = tokenizer.batch_decode(generated_ids)\nprint(decoded[0])\n```\n\n## Troubleshooting\n- If you see the following error:\n```\nTraceback (most recent call last):\nFile \"\", line 1, in\nFile \"/transformers/models/auto/auto_factory.py\", line 482, in from_pretrained\nconfig, kwargs = AutoConfig.from_pretrained(\nFile \"/transformers/models/auto/configuration_auto.py\", line 1022, in from_pretrained\nconfig_class = CONFIG_MAPPING[config_dict[\"model_type\"]]\nFile \"/transformers/models/auto/configuration_auto.py\", line 723, in getitem\nraise KeyError(key)\nKeyError: 'mistral'\n```\n\nInstalling transformers from source should solve the issue\npip install git+https://github.com/huggingface/transformers\n\nThis should not be required after transformers-v4.33.4.\n\n## Limitations\n\nThe Mistral 7B Instruct model is a quick demonstration that the base model can be easily fine-tuned to achieve compelling performance. \nIt does not have any moderation mechanisms. We're looking forward to engaging with the community on ways to\nmake the model finely respect guardrails, allowing for deployment in environments requiring moderated outputs.\n\n## The Mistral AI Team\n\nAlbert Jiang, Alexandre Sablayrolles, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lélio Renard Lavaud, Louis Ternon, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, William El Sayed.",
    "card_content": "---\nlicense: apache-2.0\ntags:\n- finetuned\npipeline_tag: text-generation\nnew_version: mistralai/Mistral-7B-Instruct-v0.3\ninference: true\nwidget:\n- messages:\n  - role: user\n    content: What is your favorite condiment?\nextra_gated_description: If you want to learn more about how we process your personal\n  data, please read our <a href=\"https://mistral.ai/terms/\">Privacy Policy</a>.\n---\n\n# Model Card for Mistral-7B-Instruct-v0.2\n\n\n## Encode and Decode with `mistral_common`\n            \n```py\nfrom mistral_common.tokens.tokenizers.mistral import MistralTokenizer\nfrom mistral_common.protocol.instruct.messages import UserMessage\nfrom mistral_common.protocol.instruct.request import ChatCompletionRequest\n \nmistral_models_path = \"MISTRAL_MODELS_PATH\"\n \ntokenizer = MistralTokenizer.v1()\n \ncompletion_request = ChatCompletionRequest(messages=[UserMessage(content=\"Explain Machine Learning to me in a nutshell.\")])\n \ntokens = tokenizer.encode_chat_completion(completion_request).tokens\n```\n \n## Inference with `mistral_inference`\n \n ```py\nfrom mistral_inference.transformer import Transformer\nfrom mistral_inference.generate import generate\n \nmodel = Transformer.from_folder(mistral_models_path)\nout_tokens, _ = generate([tokens], model, max_tokens=64, temperature=0.0, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)\n\nresult = tokenizer.decode(out_tokens[0])\n\nprint(result)\n```\n\n## Inference with hugging face `transformers`\n \n```py\nfrom transformers import AutoModelForCausalLM\n \nmodel = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\nmodel.to(\"cuda\")\n \ngenerated_ids = model.generate(tokens, max_new_tokens=1000, do_sample=True)\n\n# decode with mistral tokenizer\nresult = tokenizer.decode(generated_ids[0].tolist())\nprint(result)\n```\n\n> [!TIP]\n> PRs to correct the `transformers` tokenizer so that it gives 1-to-1 the same results as the `mistral_common` reference implementation are very welcome!\n            \n---\n\nThe Mistral-7B-Instruct-v0.2 Large Language Model (LLM) is an instruct fine-tuned version of the Mistral-7B-v0.2.\n\nMistral-7B-v0.2 has the following changes compared to Mistral-7B-v0.1\n- 32k context window (vs 8k context in v0.1)\n- Rope-theta = 1e6\n- No Sliding-Window Attention\n\nFor full details of this model please read our [paper](https://arxiv.org/abs/2310.06825) and [release blog post](https://mistral.ai/news/la-plateforme/).\n\n## Instruction format\n\nIn order to leverage instruction fine-tuning, your prompt should be surrounded by `[INST]` and `[/INST]` tokens. The very first instruction should begin with a begin of sentence id. The next instructions should not. The assistant generation will be ended by the end-of-sentence token id.\n\nE.g.\n```\ntext = \"<s>[INST] What is your favourite condiment? [/INST]\"\n\"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!</s> \"\n\"[INST] Do you have mayonnaise recipes? [/INST]\"\n```\n\nThis format is available as a [chat template](https://huggingface.co/docs/transformers/main/chat_templating) via the `apply_chat_template()` method:\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ndevice = \"cuda\" # the device to load the model onto\n\nmodel = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\ntokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n    {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n    {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n]\n\nencodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n\nmodel_inputs = encodeds.to(device)\nmodel.to(device)\n\ngenerated_ids = model.generate(model_inputs, max_new_tokens=1000, do_sample=True)\ndecoded = tokenizer.batch_decode(generated_ids)\nprint(decoded[0])\n```\n\n## Troubleshooting\n- If you see the following error:\n```\nTraceback (most recent call last):\nFile \"\", line 1, in\nFile \"/transformers/models/auto/auto_factory.py\", line 482, in from_pretrained\nconfig, kwargs = AutoConfig.from_pretrained(\nFile \"/transformers/models/auto/configuration_auto.py\", line 1022, in from_pretrained\nconfig_class = CONFIG_MAPPING[config_dict[\"model_type\"]]\nFile \"/transformers/models/auto/configuration_auto.py\", line 723, in getitem\nraise KeyError(key)\nKeyError: 'mistral'\n```\n\nInstalling transformers from source should solve the issue\npip install git+https://github.com/huggingface/transformers\n\nThis should not be required after transformers-v4.33.4.\n\n## Limitations\n\nThe Mistral 7B Instruct model is a quick demonstration that the base model can be easily fine-tuned to achieve compelling performance. \nIt does not have any moderation mechanisms. We're looking forward to engaging with the community on ways to\nmake the model finely respect guardrails, allowing for deployment in environments requiring moderated outputs.\n\n## The Mistral AI Team\n\nAlbert Jiang, Alexandre Sablayrolles, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lélio Renard Lavaud, Louis Ternon, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, William El Sayed.",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModelForCausalLM",
      "custom_class": null,
      "pipeline_tag": "text-generation",
      "processor": "AutoTokenizer"
    },
    "safetensors": {
      "parameters": {
        "BF16": 7241732096
      },
      "total": 7241732096
    },
    "model_index": null,
    "trending_score": null
  },
  {
    "model_id": "unsloth/DeepSeek-R1-GGUF",
    "model_name": "unsloth/DeepSeek-R1-GGUF",
    "author": "unsloth",
    "downloads": 3310896,
    "downloads_all_time": null,
    "likes": 999,
    "tags": [
      "transformers",
      "gguf",
      "deepseek_v3",
      "text-generation",
      "deepseek",
      "unsloth",
      "custom_code",
      "en",
      "arxiv:2501.12948",
      "base_model:deepseek-ai/DeepSeek-R1",
      "base_model:quantized:deepseek-ai/DeepSeek-R1",
      "license:mit",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us",
      "conversational"
    ],
    "card_url": "https://huggingface.co/unsloth/DeepSeek-R1-GGUF",
    "dependencies": null,
    "last_modified": "2025-02-13T12:39:13+00:00",
    "created_at": "2025-01-20T13:09:42+00:00",
    "analysis_date": "2025-03-22T00:41:22.676822",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "deepseek_v3",
    "pipeline_tag": "text-generation",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "base_model": "deepseek-ai/DeepSeek-R1",
      "language": [
        "en"
      ],
      "library_name": "transformers",
      "license": "mit",
      "tags": [
        "deepseek",
        "unsloth",
        "transformers"
      ]
    },
    "card_text": "\n<div>\n  <p style=\"margin-bottom: 0; margin-top: 0;\">\n    <strong>See <a href=\"https://huggingface.co/collections/unsloth/deepseek-r1-all-versions-678e1c48f5d2fce87892ace5\">our collection</a> for versions of Deepseek-R1 including GGUF & 4-bit formats.</strong>\n  </p>\n  <p style=\"margin-bottom: 0;\">\n    <em>Unsloth's DeepSeek-R1 <a href=\"https://unsloth.ai/blog/deepseekr1-dynamic\">1.58-bit + 2-bit Dynamic Quants</a> is selectively quantized, greatly improving accuracy over standard 1-bit/2-bit.</em>\n  </p>\n  <div style=\"display: flex; gap: 5px; align-items: center; \">\n    <a href=\"https://github.com/unslothai/unsloth/\">\n      <img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"133\">\n    </a>\n    <a href=\"https://discord.gg/unsloth\">\n      <img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord%20button.png\" width=\"173\">\n    </a>\n    <a href=\"https://docs.unsloth.ai/basics/tutorial-how-to-run-deepseek-r1-on-your-own-local-device\">\n      <img src=\"https://raw.githubusercontent.com/unslothai/unsloth/refs/heads/main/images/documentation%20green%20button.png\" width=\"143\">\n    </a>\n  </div>\n<h1 style=\"margin-top: 0rem;\">Instructions to run this model in llama.cpp:</h2>\n</div>\n\nOr you can view more detailed instructions here: [unsloth.ai/blog/deepseekr1-dynamic](https://unsloth.ai/blog/deepseekr1-dynamic)\n1. Do not forget about `<｜User｜>` and `<｜Assistant｜>` tokens! - Or use a chat template formatter\n2. Obtain the latest `llama.cpp` at https://github.com/ggerganov/llama.cpp. You can follow the build instructions below as well:\n```bash\napt-get update\napt-get install build-essential cmake curl libcurl4-openssl-dev -y\ngit clone https://github.com/ggerganov/llama.cpp\ncmake llama.cpp -B llama.cpp/build \\\n\t-DBUILD_SHARED_LIBS=OFF -DGGML_CUDA=ON -DLLAMA_CURL=ON\ncmake --build llama.cpp/build --config Release -j --clean-first --target llama-quantize llama-cli llama-gguf-split\ncp llama.cpp/build/bin/llama-* llama.cpp\n```\n3. It's best to use `--min-p 0.05` to counteract very rare token predictions - I found this to work well especially for the 1.58bit model.\n4. Download the model via:\n```python\n# pip install huggingface_hub hf_transfer\n# import os # Optional for faster downloading\n# os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n\nfrom huggingface_hub import snapshot_download\nsnapshot_download(\n  repo_id = \"unsloth/DeepSeek-R1-GGUF\",\n  local_dir = \"DeepSeek-R1-GGUF\",\n  allow_patterns = [\"*UD-IQ1_S*\"], # Select quant type UD-IQ1_S for 1.58bit\n)\n```\n5. Example with Q4_0 K quantized cache **Notice -no-cnv disables auto conversation mode**\n```bash\n   ./llama.cpp/llama-cli \\\n\t  --model DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ1_S/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf \\\n\t  --cache-type-k q4_0 \\\n\t  --threads 12 -no-cnv --prio 2 \\\n\t  --temp 0.6 \\\n\t  --ctx-size 8192 \\\n\t  --seed 3407 \\\n\t  --prompt \"<｜User｜>Create a Flappy Bird game in Python.<｜Assistant｜>\"\n```\n   Example output:\n   \n   ```txt\n    <think>\n    Okay, so I need to figure out what 1 plus 1 is. Hmm, where do I even start? I remember from school that adding numbers is pretty basic, but I want to make sure I understand it properly.\n    Let me think, 1 plus 1. So, I have one item and I add another one. Maybe like a apple plus another apple. If I have one apple and someone gives me another, I now have two apples. So, 1 plus 1 should be 2. That makes sense.\n    Wait, but sometimes math can be tricky. Could it be something else? Like, in a different number system maybe? But I think the question is straightforward, using regular numbers, not like binary or hexadecimal or anything.\n    I also recall that in arithmetic, addition is combining quantities. So, if you have two quantities of 1, combining them gives you a total of 2. Yeah, that seems right.\n    Is there a scenario where 1 plus 1 wouldn't be 2? I can't think of any...\n   ```\n   \n6. If you have a GPU (RTX 4090 for example) with 24GB, you can offload multiple layers to the GPU for faster processing. If you have multiple GPUs, you can probably offload more layers.\n```bash\n  ./llama.cpp/llama-cli \\\n    --model DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ1_S/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf \\\n    --cache-type-k q4_0 \\\n    --threads 12 -no-cnv --prio 2 \\\n    --n-gpu-layers 7 \\\n    --temp 0.6 \\\n    --ctx-size 8192 \\\n    --seed 3407 \\\n    --prompt \"<｜User｜>Create a Flappy Bird game in Python.<｜Assistant｜>\"\n```\n7. If you want to merge the weights together, use this script:\n```\n./llama.cpp/llama-gguf-split --merge \\\n    DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf \\\n    merged_file.gguf\n```\n\n| MoE Bits     | Type   | Disk Size |  Accuracy | Link                      | Details   |\n| -------- | -------- | ------------ | ------------ | ---------------------|  ---------- |\n| 1.58bit | UD-IQ1_S |   **131GB**    | Fair           | [Link](https://huggingface.co/unsloth/DeepSeek-R1-GGUF/tree/main/DeepSeek-R1-UD-IQ1_S) | MoE all 1.56bit. `down_proj` in MoE mixture of 2.06/1.56bit |\n| 1.73bit | UD-IQ1_M |   **158GB**    | Good | [Link](https://huggingface.co/unsloth/DeepSeek-R1-GGUF/tree/main/DeepSeek-R1-UD-IQ1_M) | MoE all 1.56bit. `down_proj` in MoE left at 2.06bit |\n| 2.22bit | UD-IQ2_XXS |   **183GB**    | Better      | [Link](https://huggingface.co/unsloth/DeepSeek-R1-GGUF/tree/main/DeepSeek-R1-UD-IQ2_XXS) | MoE all 2.06bit. `down_proj` in MoE mixture of 2.5/2.06bit |\n| 2.51bit | UD-Q2_K_XL |   **212GB**    | Best | [Link](https://huggingface.co/unsloth/DeepSeek-R1-GGUF/tree/main/DeepSeek-R1-UD-Q2_K_XL) | MoE all 2.5bit. `down_proj` in MoE mixture of 3.5/2.5bit |\n\n# Finetune your own Reasoning model like R1 with Unsloth!\nWe have a free Google Colab notebook for turning Llama 3.1 (8B) into a reasoning model: https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb\n\n[<img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/Discord%20button.png\" width=\"200\"/>](https://discord.gg/unsloth)\n[<img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20made%20with%20love.png\" width=\"200\"/>](https://github.com/unslothai/unsloth)\n\n\n## ✨ Finetune for Free\n\nAll notebooks are **beginner friendly**! Add your dataset, click \"Run All\", and you'll get a 2x faster finetuned model which can be exported to GGUF, vLLM or uploaded to Hugging Face.\n\n| Unsloth supports          |    Free Notebooks                                                                                           | Performance | Memory use |\n|-----------------|--------------------------------------------------------------------------------------------------------------------------|-------------|----------|\n| **GRPO with Phi-4 (14B)**      | [▶️ Start on Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_4_(14B)-GRPO.ipynb)               | 2x faster | 80% less |\n| **Llama-3.2 (3B)**      | [▶️ Start on Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(1B_and_3B)-Conversational.ipynb)               | 2.4x faster | 58% less |\n| **Llama-3.2 (11B vision)**      | [▶️ Start on Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)               | 2x faster | 60% less |\n| **Qwen2 VL (7B)**      | [▶️ Start on Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2_VL_(7B)-Vision.ipynb)               | 1.8x faster | 60% less |\n| **Qwen2.5 (7B)**      | [▶️ Start on Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2.5_(7B)-Alpaca.ipynb)               | 2x faster | 60% less |\n| **Llama-3.1 (8B)**      | [▶️ Start on Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-Alpaca.ipynb)               | 2.4x faster | 58% less |\n| **Phi-3.5 (mini)** | [▶️ Start on Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_3.5_Mini-Conversational.ipynb)               | 2x faster | 50% less |\n| **Gemma 2 (9B)**      | [▶️ Start on Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma2_(9B)-Alpaca.ipynb)               | 2.4x faster | 58% less |\n| **Mistral (7B)**    | [▶️ Start on Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_v0.3_(7B)-Conversational.ipynb)               | 2.2x faster | 62% less |\n\n[<img src=\"https://raw.githubusercontent.com/unslothai/unsloth/refs/heads/main/images/documentation%20green%20button.png\" width=\"200\"/>](https://docs.unsloth.ai)\n\n- This [Llama 3.2 conversational notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(1B_and_3B)-Conversational.ipynb) is useful for ShareGPT ChatML / Vicuna templates.\n- This [text completion notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_(7B)-Text_Completion.ipynb) is for raw text. This [DPO notebook](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing) replicates Zephyr.\n- \\* Kaggle has 2x T4s, but we use 1. Due to overhead, 1x T4 is 5x faster.\n\n## Special Thanks\nA huge thank you to the DeepSeek team for creating and releasing these models.\n\n\n\n# DeepSeek-R1\n<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align=\"center\">\n  <img src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true\" width=\"60%\" alt=\"DeepSeek-V3\" />\n</div>\n<hr>\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://www.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Homepage\" src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://chat.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/🤖%20Chat-DeepSeek%20R1-536af5?color=536af5&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://huggingface.co/deepseek-ai\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://discord.gg/Tc7c45Zzu5\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Wechat\" src=\"https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://twitter.com/deepseek_ai\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Twitter Follow\" src=\"https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE-CODE\" style=\"margin: 2px;\">\n    <img alt=\"Code License\" src=\"https://img.shields.io/badge/Code_License-MIT-f5de53?&color=f5de53\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE-MODEL\" style=\"margin: 2px;\">\n    <img alt=\"Model License\" src=\"https://img.shields.io/badge/Model_License-Model_Agreement-f5de53?&color=f5de53\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n\n<p align=\"center\">\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf\"><b>Paper Link</b>👁️</a>\n</p>\n\n\n## 1. Introduction\n\nWe introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. \nDeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning.\nWith RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors.\nHowever, DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. To address these issues and further enhance reasoning performance,\nwe introduce DeepSeek-R1, which incorporates cold-start data before RL.\nDeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks. \nTo support the research community, we have open-sourced DeepSeek-R1-Zero, DeepSeek-R1, and six dense models distilled from DeepSeek-R1 based on Llama and Qwen. DeepSeek-R1-Distill-Qwen-32B outperforms OpenAI-o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.\n\n**NOTE: Before running DeepSeek-R1 series models locally, we kindly recommend reviewing the [Usage Recommendation](#usage-recommendations) section.**\n\n<p align=\"center\">\n  <img width=\"80%\" src=\"figures/benchmark.jpg\">\n</p>\n\n## 2. Model Summary\n\n---\n\n**Post-Training: Large-Scale Reinforcement Learning on the Base Model**\n\n-  We directly apply reinforcement learning (RL) to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.\n\n-   We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model's reasoning and non-reasoning capabilities.\n    We believe the pipeline will benefit the industry by creating better models. \n\n---\n\n**Distillation: Smaller Models Can Be Powerful Too**\n\n-  We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future. \n- Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.\n\n## 3. Model Downloads\n\n### DeepSeek-R1 Models\n\n<div align=\"center\">\n\n| **Model** | **#Total Params** | **#Activated Params** | **Context Length** | **Download** |\n| :------------: | :------------: | :------------: | :------------: | :------------: |\n| DeepSeek-R1-Zero | 671B | 37B | 128K   | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Zero)   |\n| DeepSeek-R1   | 671B | 37B |  128K   | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1)   |\n\n</div>\n\nDeepSeek-R1-Zero & DeepSeek-R1 are trained based on DeepSeek-V3-Base. \nFor more details regarding the model architecture, please refer to [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) repository.\n\n### DeepSeek-R1-Distill Models\n\n<div align=\"center\">\n\n| **Model** | **Base Model** | **Download** |\n| :------------: | :------------: | :------------: |\n| DeepSeek-R1-Distill-Qwen-1.5B  | [Qwen2.5-Math-1.5B](https://huggingface.co/Qwen/Qwen2.5-Math-1.5B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B)   |\n| DeepSeek-R1-Distill-Qwen-7B  | [Qwen2.5-Math-7B](https://huggingface.co/Qwen/Qwen2.5-Math-7B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B)   |\n| DeepSeek-R1-Distill-Llama-8B  | [Llama-3.1-8B](https://huggingface.co/meta-llama/Llama-3.1-8B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B)   |\n| DeepSeek-R1-Distill-Qwen-14B   | [Qwen2.5-14B](https://huggingface.co/Qwen/Qwen2.5-14B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B)   |\n|DeepSeek-R1-Distill-Qwen-32B  | [Qwen2.5-32B](https://huggingface.co/Qwen/Qwen2.5-32B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B)   |\n| DeepSeek-R1-Distill-Llama-70B  | [Llama-3.3-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B)   |\n\n</div>\n\nDeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1.\nWe slightly change their configs and tokenizers. Please use our setting to run these models.\n\n## 4. Evaluation Results\n\n### DeepSeek-R1-Evaluation\n For all our models, the maximum generation length is set to 32,768 tokens. For benchmarks requiring sampling, we use a temperature of $0.6$, a top-p value of $0.95$, and generate 64 responses per query to estimate pass@1.\n<div align=\"center\">\n\n\n| Category | Benchmark (Metric) | Claude-3.5-Sonnet-1022 | GPT-4o 0513 | DeepSeek V3 | OpenAI o1-mini | OpenAI o1-1217 | DeepSeek R1 |\n|----------|-------------------|----------------------|------------|--------------|----------------|------------|--------------|\n| | Architecture | - | - | MoE | - | - | MoE |\n| | # Activated Params | - | - | 37B | - | - | 37B |\n| | # Total Params | - | - | 671B | - | - | 671B |\n| English | MMLU (Pass@1) | 88.3 | 87.2 | 88.5 | 85.2 | **91.8** | 90.8 |\n| | MMLU-Redux (EM) | 88.9 | 88.0 | 89.1 | 86.7 | - | **92.9** |\n| | MMLU-Pro (EM) | 78.0 | 72.6 | 75.9 | 80.3 | - | **84.0** |\n| | DROP (3-shot F1) | 88.3 | 83.7 | 91.6 | 83.9 | 90.2 | **92.2** |\n| | IF-Eval (Prompt Strict) | **86.5** | 84.3 | 86.1 | 84.8 | - | 83.3 |\n| | GPQA-Diamond (Pass@1) | 65.0 | 49.9 | 59.1 | 60.0 | **75.7** | 71.5 |\n| | SimpleQA (Correct) | 28.4 | 38.2 | 24.9 | 7.0 | **47.0** | 30.1 |\n| | FRAMES (Acc.) | 72.5 | 80.5 | 73.3 | 76.9 | - | **82.5** |\n| | AlpacaEval2.0 (LC-winrate) | 52.0 | 51.1 | 70.0 | 57.8 | - | **87.6** |\n| | ArenaHard (GPT-4-1106) | 85.2 | 80.4 | 85.5 | 92.0 | - | **92.3** |\n| Code | LiveCodeBench (Pass@1-COT) | 33.8 | 34.2 | - | 53.8 | 63.4 | **65.9** |\n| | Codeforces (Percentile) | 20.3 | 23.6 | 58.7 | 93.4 | **96.6** | 96.3 |\n| | Codeforces (Rating) | 717 | 759 | 1134 | 1820 | **2061** | 2029 |\n| | SWE Verified (Resolved) | **50.8** | 38.8 | 42.0 | 41.6 | 48.9 | 49.2 |\n| | Aider-Polyglot (Acc.) | 45.3 | 16.0 | 49.6 | 32.9 | **61.7** | 53.3 |\n| Math | AIME 2024 (Pass@1) | 16.0 | 9.3 | 39.2 | 63.6 | 79.2 | **79.8** |\n| | MATH-500 (Pass@1) | 78.3 | 74.6 | 90.2 | 90.0 | 96.4 | **97.3** |\n| | CNMO 2024 (Pass@1) | 13.1 | 10.8 | 43.2 | 67.6 | - | **78.8** |\n| Chinese | CLUEWSC (EM) | 85.4 | 87.9 | 90.9 | 89.9 | - | **92.8** |\n| | C-Eval (EM) | 76.7 | 76.0 | 86.5 | 68.9 | - | **91.8** |\n| | C-SimpleQA (Correct) | 55.4 | 58.7 | **68.0** | 40.3 | - | 63.7 |\n\n</div>\n\n\n### Distilled Model Evaluation\n\n\n<div align=\"center\">\n\n| Model                                    | AIME 2024 pass@1 | AIME 2024 cons@64 | MATH-500 pass@1 | GPQA Diamond pass@1 | LiveCodeBench pass@1 | CodeForces rating |\n|------------------------------------------|------------------|-------------------|-----------------|----------------------|----------------------|-------------------|\n| GPT-4o-0513                          | 9.3              | 13.4              | 74.6            | 49.9                 | 32.9                 | 759               |\n| Claude-3.5-Sonnet-1022             | 16.0             | 26.7                 | 78.3            | 65.0                 | 38.9                 | 717               |\n| o1-mini                              | 63.6             | 80.0              | 90.0            | 60.0                 | 53.8                 | **1820**          |\n| QwQ-32B-Preview                              | 44.0             | 60.0                 | 90.6            | 54.5               | 41.9                 | 1316              |\n| DeepSeek-R1-Distill-Qwen-1.5B       | 28.9             | 52.7              | 83.9            | 33.8                 | 16.9                 | 954               |\n| DeepSeek-R1-Distill-Qwen-7B          | 55.5             | 83.3              | 92.8            | 49.1                 | 37.6                 | 1189              |\n| DeepSeek-R1-Distill-Qwen-14B         | 69.7             | 80.0              | 93.9            | 59.1                 | 53.1                 | 1481              |\n| DeepSeek-R1-Distill-Qwen-32B        | **72.6**         | 83.3              | 94.3            | 62.1                 | 57.2                 | 1691              |\n| DeepSeek-R1-Distill-Llama-8B         | 50.4             | 80.0              | 89.1            | 49.0                 | 39.6                 | 1205              |\n| DeepSeek-R1-Distill-Llama-70B        | 70.0             | **86.7**          | **94.5**        | **65.2**             | **57.5**             | 1633              |\n\n</div>\n\n\n## 5. Chat Website & API Platform\nYou can chat with DeepSeek-R1 on DeepSeek's official website: [chat.deepseek.com](https://chat.deepseek.com), and switch on the button \"DeepThink\"\n\nWe also provide OpenAI-Compatible API at DeepSeek Platform: [platform.deepseek.com](https://platform.deepseek.com/)\n\n## 6. How to Run Locally\n\n### DeepSeek-R1 Models\n\nPlease visit [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) repo for more information about running DeepSeek-R1 locally.\n\n### DeepSeek-R1-Distill Models\n\nDeepSeek-R1-Distill models can be utilized in the same manner as Qwen or Llama models.\n\nFor instance, you can easily start a service using [vLLM](https://github.com/vllm-project/vllm):\n\n```shell\nvllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --tensor-parallel-size 2 --max-model-len 32768 --enforce-eager\n```\n\nYou can also easily start a service using [SGLang](https://github.com/sgl-project/sglang)\n\n```bash\npython3 -m sglang.launch_server --model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --trust-remote-code --tp 2\n```\n\n### Usage Recommendations\n\n**We recommend adhering to the following configurations when utilizing the DeepSeek-R1 series models, including benchmarking, to achieve the expected performance:**\n\n1. Set the temperature within the range of 0.5-0.7 (0.6 is recommended) to prevent endless repetitions or incoherent outputs.\n2. **Avoid adding a system prompt; all instructions should be contained within the user prompt.**\n3. For mathematical problems, it is advisable to include a directive in your prompt such as: \"Please reason step by step, and put your final answer within \\boxed{}.\"\n4. When evaluating model performance, it is recommended to conduct multiple tests and average the results.\n\n## 7. License\nThis code repository and the model weights are licensed under the [MIT License](https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE).\nDeepSeek-R1 series support commercial use, allow for any modifications and derivative works, including, but not limited to, distillation for training other LLMs. Please note that:\n- DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Qwen-14B and DeepSeek-R1-Distill-Qwen-32B are derived from [Qwen-2.5 series](https://github.com/QwenLM/Qwen2.5), which are originally licensed under [Apache 2.0 License](https://huggingface.co/Qwen/Qwen2.5-1.5B/blob/main/LICENSE), and now finetuned with 800k samples curated with DeepSeek-R1.\n- DeepSeek-R1-Distill-Llama-8B is derived from Llama3.1-8B-Base and is originally licensed under [llama3.1 license](https://huggingface.co/meta-llama/Llama-3.1-8B/blob/main/LICENSE).\n- DeepSeek-R1-Distill-Llama-70B is derived from Llama3.3-70B-Instruct and is originally licensed under [llama3.3 license](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct/blob/main/LICENSE).\n\n## 8. Citation\n```\n@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\n      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, \n      author={DeepSeek-AI and Daya Guo and Dejian Yang and Haowei Zhang and Junxiao Song and Ruoyu Zhang and Runxin Xu and Qihao Zhu and Shirong Ma and Peiyi Wang and Xiao Bi and Xiaokang Zhang and Xingkai Yu and Yu Wu and Z. F. Wu and Zhibin Gou and Zhihong Shao and Zhuoshu Li and Ziyi Gao and Aixin Liu and Bing Xue and Bingxuan Wang and Bochao Wu and Bei Feng and Chengda Lu and Chenggang Zhao and Chengqi Deng and Chenyu Zhang and Chong Ruan and Damai Dai and Deli Chen and Dongjie Ji and Erhang Li and Fangyun Lin and Fucong Dai and Fuli Luo and Guangbo Hao and Guanting Chen and Guowei Li and H. Zhang and Han Bao and Hanwei Xu and Haocheng Wang and Honghui Ding and Huajian Xin and Huazuo Gao and Hui Qu and Hui Li and Jianzhong Guo and Jiashi Li and Jiawei Wang and Jingchang Chen and Jingyang Yuan and Junjie Qiu and Junlong Li and J. L. Cai and Jiaqi Ni and Jian Liang and Jin Chen and Kai Dong and Kai Hu and Kaige Gao and Kang Guan and Kexin Huang and Kuai Yu and Lean Wang and Lecong Zhang and Liang Zhao and Litong Wang and Liyue Zhang and Lei Xu and Leyi Xia and Mingchuan Zhang and Minghua Zhang and Minghui Tang and Meng Li and Miaojun Wang and Mingming Li and Ning Tian and Panpan Huang and Peng Zhang and Qiancheng Wang and Qinyu Chen and Qiushi Du and Ruiqi Ge and Ruisong Zhang and Ruizhe Pan and Runji Wang and R. J. Chen and R. L. Jin and Ruyi Chen and Shanghao Lu and Shangyan Zhou and Shanhuang Chen and Shengfeng Ye and Shiyu Wang and Shuiping Yu and Shunfeng Zhou and Shuting Pan and S. S. Li and Shuang Zhou and Shaoqing Wu and Shengfeng Ye and Tao Yun and Tian Pei and Tianyu Sun and T. Wang and Wangding Zeng and Wanjia Zhao and Wen Liu and Wenfeng Liang and Wenjun Gao and Wenqin Yu and Wentao Zhang and W. L. Xiao and Wei An and Xiaodong Liu and Xiaohan Wang and Xiaokang Chen and Xiaotao Nie and Xin Cheng and Xin Liu and Xin Xie and Xingchao Liu and Xinyu Yang and Xinyuan Li and Xuecheng Su and Xuheng Lin and X. Q. Li and Xiangyue Jin and Xiaojin Shen and Xiaosha Chen and Xiaowen Sun and Xiaoxiang Wang and Xinnan Song and Xinyi Zhou and Xianzu Wang and Xinxia Shan and Y. K. Li and Y. Q. Wang and Y. X. Wei and Yang Zhang and Yanhong Xu and Yao Li and Yao Zhao and Yaofeng Sun and Yaohui Wang and Yi Yu and Yichao Zhang and Yifan Shi and Yiliang Xiong and Ying He and Yishi Piao and Yisong Wang and Yixuan Tan and Yiyang Ma and Yiyuan Liu and Yongqiang Guo and Yuan Ou and Yuduan Wang and Yue Gong and Yuheng Zou and Yujia He and Yunfan Xiong and Yuxiang Luo and Yuxiang You and Yuxuan Liu and Yuyang Zhou and Y. X. Zhu and Yanhong Xu and Yanping Huang and Yaohui Li and Yi Zheng and Yuchen Zhu and Yunxian Ma and Ying Tang and Yukun Zha and Yuting Yan and Z. Z. Ren and Zehui Ren and Zhangli Sha and Zhe Fu and Zhean Xu and Zhenda Xie and Zhengyan Zhang and Zhewen Hao and Zhicheng Ma and Zhigang Yan and Zhiyu Wu and Zihui Gu and Zijia Zhu and Zijun Liu and Zilin Li and Ziwei Xie and Ziyang Song and Zizheng Pan and Zhen Huang and Zhipeng Xu and Zhongyu Zhang and Zhen Zhang},\n      year={2025},\n      eprint={2501.12948},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2501.12948}, \n}\n\n```\n\n## 9. Contact\nIf you have any questions, please raise an issue or contact us at [service@deepseek.com](service@deepseek.com).\n",
    "card_content": "---\nbase_model: deepseek-ai/DeepSeek-R1\nlanguage:\n- en\nlibrary_name: transformers\nlicense: mit\ntags:\n- deepseek\n- unsloth\n- transformers\n---\n\n<div>\n  <p style=\"margin-bottom: 0; margin-top: 0;\">\n    <strong>See <a href=\"https://huggingface.co/collections/unsloth/deepseek-r1-all-versions-678e1c48f5d2fce87892ace5\">our collection</a> for versions of Deepseek-R1 including GGUF & 4-bit formats.</strong>\n  </p>\n  <p style=\"margin-bottom: 0;\">\n    <em>Unsloth's DeepSeek-R1 <a href=\"https://unsloth.ai/blog/deepseekr1-dynamic\">1.58-bit + 2-bit Dynamic Quants</a> is selectively quantized, greatly improving accuracy over standard 1-bit/2-bit.</em>\n  </p>\n  <div style=\"display: flex; gap: 5px; align-items: center; \">\n    <a href=\"https://github.com/unslothai/unsloth/\">\n      <img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"133\">\n    </a>\n    <a href=\"https://discord.gg/unsloth\">\n      <img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord%20button.png\" width=\"173\">\n    </a>\n    <a href=\"https://docs.unsloth.ai/basics/tutorial-how-to-run-deepseek-r1-on-your-own-local-device\">\n      <img src=\"https://raw.githubusercontent.com/unslothai/unsloth/refs/heads/main/images/documentation%20green%20button.png\" width=\"143\">\n    </a>\n  </div>\n<h1 style=\"margin-top: 0rem;\">Instructions to run this model in llama.cpp:</h2>\n</div>\n\nOr you can view more detailed instructions here: [unsloth.ai/blog/deepseekr1-dynamic](https://unsloth.ai/blog/deepseekr1-dynamic)\n1. Do not forget about `<｜User｜>` and `<｜Assistant｜>` tokens! - Or use a chat template formatter\n2. Obtain the latest `llama.cpp` at https://github.com/ggerganov/llama.cpp. You can follow the build instructions below as well:\n```bash\napt-get update\napt-get install build-essential cmake curl libcurl4-openssl-dev -y\ngit clone https://github.com/ggerganov/llama.cpp\ncmake llama.cpp -B llama.cpp/build \\\n\t-DBUILD_SHARED_LIBS=OFF -DGGML_CUDA=ON -DLLAMA_CURL=ON\ncmake --build llama.cpp/build --config Release -j --clean-first --target llama-quantize llama-cli llama-gguf-split\ncp llama.cpp/build/bin/llama-* llama.cpp\n```\n3. It's best to use `--min-p 0.05` to counteract very rare token predictions - I found this to work well especially for the 1.58bit model.\n4. Download the model via:\n```python\n# pip install huggingface_hub hf_transfer\n# import os # Optional for faster downloading\n# os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n\nfrom huggingface_hub import snapshot_download\nsnapshot_download(\n  repo_id = \"unsloth/DeepSeek-R1-GGUF\",\n  local_dir = \"DeepSeek-R1-GGUF\",\n  allow_patterns = [\"*UD-IQ1_S*\"], # Select quant type UD-IQ1_S for 1.58bit\n)\n```\n5. Example with Q4_0 K quantized cache **Notice -no-cnv disables auto conversation mode**\n```bash\n   ./llama.cpp/llama-cli \\\n\t  --model DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ1_S/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf \\\n\t  --cache-type-k q4_0 \\\n\t  --threads 12 -no-cnv --prio 2 \\\n\t  --temp 0.6 \\\n\t  --ctx-size 8192 \\\n\t  --seed 3407 \\\n\t  --prompt \"<｜User｜>Create a Flappy Bird game in Python.<｜Assistant｜>\"\n```\n   Example output:\n   \n   ```txt\n    <think>\n    Okay, so I need to figure out what 1 plus 1 is. Hmm, where do I even start? I remember from school that adding numbers is pretty basic, but I want to make sure I understand it properly.\n    Let me think, 1 plus 1. So, I have one item and I add another one. Maybe like a apple plus another apple. If I have one apple and someone gives me another, I now have two apples. So, 1 plus 1 should be 2. That makes sense.\n    Wait, but sometimes math can be tricky. Could it be something else? Like, in a different number system maybe? But I think the question is straightforward, using regular numbers, not like binary or hexadecimal or anything.\n    I also recall that in arithmetic, addition is combining quantities. So, if you have two quantities of 1, combining them gives you a total of 2. Yeah, that seems right.\n    Is there a scenario where 1 plus 1 wouldn't be 2? I can't think of any...\n   ```\n   \n6. If you have a GPU (RTX 4090 for example) with 24GB, you can offload multiple layers to the GPU for faster processing. If you have multiple GPUs, you can probably offload more layers.\n```bash\n  ./llama.cpp/llama-cli \\\n    --model DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ1_S/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf \\\n    --cache-type-k q4_0 \\\n    --threads 12 -no-cnv --prio 2 \\\n    --n-gpu-layers 7 \\\n    --temp 0.6 \\\n    --ctx-size 8192 \\\n    --seed 3407 \\\n    --prompt \"<｜User｜>Create a Flappy Bird game in Python.<｜Assistant｜>\"\n```\n7. If you want to merge the weights together, use this script:\n```\n./llama.cpp/llama-gguf-split --merge \\\n    DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf \\\n    merged_file.gguf\n```\n\n| MoE Bits     | Type   | Disk Size |  Accuracy | Link                      | Details   |\n| -------- | -------- | ------------ | ------------ | ---------------------|  ---------- |\n| 1.58bit | UD-IQ1_S |   **131GB**    | Fair           | [Link](https://huggingface.co/unsloth/DeepSeek-R1-GGUF/tree/main/DeepSeek-R1-UD-IQ1_S) | MoE all 1.56bit. `down_proj` in MoE mixture of 2.06/1.56bit |\n| 1.73bit | UD-IQ1_M |   **158GB**    | Good | [Link](https://huggingface.co/unsloth/DeepSeek-R1-GGUF/tree/main/DeepSeek-R1-UD-IQ1_M) | MoE all 1.56bit. `down_proj` in MoE left at 2.06bit |\n| 2.22bit | UD-IQ2_XXS |   **183GB**    | Better      | [Link](https://huggingface.co/unsloth/DeepSeek-R1-GGUF/tree/main/DeepSeek-R1-UD-IQ2_XXS) | MoE all 2.06bit. `down_proj` in MoE mixture of 2.5/2.06bit |\n| 2.51bit | UD-Q2_K_XL |   **212GB**    | Best | [Link](https://huggingface.co/unsloth/DeepSeek-R1-GGUF/tree/main/DeepSeek-R1-UD-Q2_K_XL) | MoE all 2.5bit. `down_proj` in MoE mixture of 3.5/2.5bit |\n\n# Finetune your own Reasoning model like R1 with Unsloth!\nWe have a free Google Colab notebook for turning Llama 3.1 (8B) into a reasoning model: https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb\n\n[<img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/Discord%20button.png\" width=\"200\"/>](https://discord.gg/unsloth)\n[<img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20made%20with%20love.png\" width=\"200\"/>](https://github.com/unslothai/unsloth)\n\n\n## ✨ Finetune for Free\n\nAll notebooks are **beginner friendly**! Add your dataset, click \"Run All\", and you'll get a 2x faster finetuned model which can be exported to GGUF, vLLM or uploaded to Hugging Face.\n\n| Unsloth supports          |    Free Notebooks                                                                                           | Performance | Memory use |\n|-----------------|--------------------------------------------------------------------------------------------------------------------------|-------------|----------|\n| **GRPO with Phi-4 (14B)**      | [▶️ Start on Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_4_(14B)-GRPO.ipynb)               | 2x faster | 80% less |\n| **Llama-3.2 (3B)**      | [▶️ Start on Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(1B_and_3B)-Conversational.ipynb)               | 2.4x faster | 58% less |\n| **Llama-3.2 (11B vision)**      | [▶️ Start on Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)               | 2x faster | 60% less |\n| **Qwen2 VL (7B)**      | [▶️ Start on Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2_VL_(7B)-Vision.ipynb)               | 1.8x faster | 60% less |\n| **Qwen2.5 (7B)**      | [▶️ Start on Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2.5_(7B)-Alpaca.ipynb)               | 2x faster | 60% less |\n| **Llama-3.1 (8B)**      | [▶️ Start on Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-Alpaca.ipynb)               | 2.4x faster | 58% less |\n| **Phi-3.5 (mini)** | [▶️ Start on Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_3.5_Mini-Conversational.ipynb)               | 2x faster | 50% less |\n| **Gemma 2 (9B)**      | [▶️ Start on Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma2_(9B)-Alpaca.ipynb)               | 2.4x faster | 58% less |\n| **Mistral (7B)**    | [▶️ Start on Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_v0.3_(7B)-Conversational.ipynb)               | 2.2x faster | 62% less |\n\n[<img src=\"https://raw.githubusercontent.com/unslothai/unsloth/refs/heads/main/images/documentation%20green%20button.png\" width=\"200\"/>](https://docs.unsloth.ai)\n\n- This [Llama 3.2 conversational notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(1B_and_3B)-Conversational.ipynb) is useful for ShareGPT ChatML / Vicuna templates.\n- This [text completion notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_(7B)-Text_Completion.ipynb) is for raw text. This [DPO notebook](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing) replicates Zephyr.\n- \\* Kaggle has 2x T4s, but we use 1. Due to overhead, 1x T4 is 5x faster.\n\n## Special Thanks\nA huge thank you to the DeepSeek team for creating and releasing these models.\n\n\n\n# DeepSeek-R1\n<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align=\"center\">\n  <img src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true\" width=\"60%\" alt=\"DeepSeek-V3\" />\n</div>\n<hr>\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://www.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Homepage\" src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://chat.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/🤖%20Chat-DeepSeek%20R1-536af5?color=536af5&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://huggingface.co/deepseek-ai\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://discord.gg/Tc7c45Zzu5\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Wechat\" src=\"https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://twitter.com/deepseek_ai\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Twitter Follow\" src=\"https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE-CODE\" style=\"margin: 2px;\">\n    <img alt=\"Code License\" src=\"https://img.shields.io/badge/Code_License-MIT-f5de53?&color=f5de53\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE-MODEL\" style=\"margin: 2px;\">\n    <img alt=\"Model License\" src=\"https://img.shields.io/badge/Model_License-Model_Agreement-f5de53?&color=f5de53\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n\n<p align=\"center\">\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf\"><b>Paper Link</b>👁️</a>\n</p>\n\n\n## 1. Introduction\n\nWe introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. \nDeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning.\nWith RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors.\nHowever, DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. To address these issues and further enhance reasoning performance,\nwe introduce DeepSeek-R1, which incorporates cold-start data before RL.\nDeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks. \nTo support the research community, we have open-sourced DeepSeek-R1-Zero, DeepSeek-R1, and six dense models distilled from DeepSeek-R1 based on Llama and Qwen. DeepSeek-R1-Distill-Qwen-32B outperforms OpenAI-o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.\n\n**NOTE: Before running DeepSeek-R1 series models locally, we kindly recommend reviewing the [Usage Recommendation](#usage-recommendations) section.**\n\n<p align=\"center\">\n  <img width=\"80%\" src=\"figures/benchmark.jpg\">\n</p>\n\n## 2. Model Summary\n\n---\n\n**Post-Training: Large-Scale Reinforcement Learning on the Base Model**\n\n-  We directly apply reinforcement learning (RL) to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.\n\n-   We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model's reasoning and non-reasoning capabilities.\n    We believe the pipeline will benefit the industry by creating better models. \n\n---\n\n**Distillation: Smaller Models Can Be Powerful Too**\n\n-  We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future. \n- Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.\n\n## 3. Model Downloads\n\n### DeepSeek-R1 Models\n\n<div align=\"center\">\n\n| **Model** | **#Total Params** | **#Activated Params** | **Context Length** | **Download** |\n| :------------: | :------------: | :------------: | :------------: | :------------: |\n| DeepSeek-R1-Zero | 671B | 37B | 128K   | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Zero)   |\n| DeepSeek-R1   | 671B | 37B |  128K   | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1)   |\n\n</div>\n\nDeepSeek-R1-Zero & DeepSeek-R1 are trained based on DeepSeek-V3-Base. \nFor more details regarding the model architecture, please refer to [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) repository.\n\n### DeepSeek-R1-Distill Models\n\n<div align=\"center\">\n\n| **Model** | **Base Model** | **Download** |\n| :------------: | :------------: | :------------: |\n| DeepSeek-R1-Distill-Qwen-1.5B  | [Qwen2.5-Math-1.5B](https://huggingface.co/Qwen/Qwen2.5-Math-1.5B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B)   |\n| DeepSeek-R1-Distill-Qwen-7B  | [Qwen2.5-Math-7B](https://huggingface.co/Qwen/Qwen2.5-Math-7B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B)   |\n| DeepSeek-R1-Distill-Llama-8B  | [Llama-3.1-8B](https://huggingface.co/meta-llama/Llama-3.1-8B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B)   |\n| DeepSeek-R1-Distill-Qwen-14B   | [Qwen2.5-14B](https://huggingface.co/Qwen/Qwen2.5-14B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B)   |\n|DeepSeek-R1-Distill-Qwen-32B  | [Qwen2.5-32B](https://huggingface.co/Qwen/Qwen2.5-32B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B)   |\n| DeepSeek-R1-Distill-Llama-70B  | [Llama-3.3-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B)   |\n\n</div>\n\nDeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1.\nWe slightly change their configs and tokenizers. Please use our setting to run these models.\n\n## 4. Evaluation Results\n\n### DeepSeek-R1-Evaluation\n For all our models, the maximum generation length is set to 32,768 tokens. For benchmarks requiring sampling, we use a temperature of $0.6$, a top-p value of $0.95$, and generate 64 responses per query to estimate pass@1.\n<div align=\"center\">\n\n\n| Category | Benchmark (Metric) | Claude-3.5-Sonnet-1022 | GPT-4o 0513 | DeepSeek V3 | OpenAI o1-mini | OpenAI o1-1217 | DeepSeek R1 |\n|----------|-------------------|----------------------|------------|--------------|----------------|------------|--------------|\n| | Architecture | - | - | MoE | - | - | MoE |\n| | # Activated Params | - | - | 37B | - | - | 37B |\n| | # Total Params | - | - | 671B | - | - | 671B |\n| English | MMLU (Pass@1) | 88.3 | 87.2 | 88.5 | 85.2 | **91.8** | 90.8 |\n| | MMLU-Redux (EM) | 88.9 | 88.0 | 89.1 | 86.7 | - | **92.9** |\n| | MMLU-Pro (EM) | 78.0 | 72.6 | 75.9 | 80.3 | - | **84.0** |\n| | DROP (3-shot F1) | 88.3 | 83.7 | 91.6 | 83.9 | 90.2 | **92.2** |\n| | IF-Eval (Prompt Strict) | **86.5** | 84.3 | 86.1 | 84.8 | - | 83.3 |\n| | GPQA-Diamond (Pass@1) | 65.0 | 49.9 | 59.1 | 60.0 | **75.7** | 71.5 |\n| | SimpleQA (Correct) | 28.4 | 38.2 | 24.9 | 7.0 | **47.0** | 30.1 |\n| | FRAMES (Acc.) | 72.5 | 80.5 | 73.3 | 76.9 | - | **82.5** |\n| | AlpacaEval2.0 (LC-winrate) | 52.0 | 51.1 | 70.0 | 57.8 | - | **87.6** |\n| | ArenaHard (GPT-4-1106) | 85.2 | 80.4 | 85.5 | 92.0 | - | **92.3** |\n| Code | LiveCodeBench (Pass@1-COT) | 33.8 | 34.2 | - | 53.8 | 63.4 | **65.9** |\n| | Codeforces (Percentile) | 20.3 | 23.6 | 58.7 | 93.4 | **96.6** | 96.3 |\n| | Codeforces (Rating) | 717 | 759 | 1134 | 1820 | **2061** | 2029 |\n| | SWE Verified (Resolved) | **50.8** | 38.8 | 42.0 | 41.6 | 48.9 | 49.2 |\n| | Aider-Polyglot (Acc.) | 45.3 | 16.0 | 49.6 | 32.9 | **61.7** | 53.3 |\n| Math | AIME 2024 (Pass@1) | 16.0 | 9.3 | 39.2 | 63.6 | 79.2 | **79.8** |\n| | MATH-500 (Pass@1) | 78.3 | 74.6 | 90.2 | 90.0 | 96.4 | **97.3** |\n| | CNMO 2024 (Pass@1) | 13.1 | 10.8 | 43.2 | 67.6 | - | **78.8** |\n| Chinese | CLUEWSC (EM) | 85.4 | 87.9 | 90.9 | 89.9 | - | **92.8** |\n| | C-Eval (EM) | 76.7 | 76.0 | 86.5 | 68.9 | - | **91.8** |\n| | C-SimpleQA (Correct) | 55.4 | 58.7 | **68.0** | 40.3 | - | 63.7 |\n\n</div>\n\n\n### Distilled Model Evaluation\n\n\n<div align=\"center\">\n\n| Model                                    | AIME 2024 pass@1 | AIME 2024 cons@64 | MATH-500 pass@1 | GPQA Diamond pass@1 | LiveCodeBench pass@1 | CodeForces rating |\n|------------------------------------------|------------------|-------------------|-----------------|----------------------|----------------------|-------------------|\n| GPT-4o-0513                          | 9.3              | 13.4              | 74.6            | 49.9                 | 32.9                 | 759               |\n| Claude-3.5-Sonnet-1022             | 16.0             | 26.7                 | 78.3            | 65.0                 | 38.9                 | 717               |\n| o1-mini                              | 63.6             | 80.0              | 90.0            | 60.0                 | 53.8                 | **1820**          |\n| QwQ-32B-Preview                              | 44.0             | 60.0                 | 90.6            | 54.5               | 41.9                 | 1316              |\n| DeepSeek-R1-Distill-Qwen-1.5B       | 28.9             | 52.7              | 83.9            | 33.8                 | 16.9                 | 954               |\n| DeepSeek-R1-Distill-Qwen-7B          | 55.5             | 83.3              | 92.8            | 49.1                 | 37.6                 | 1189              |\n| DeepSeek-R1-Distill-Qwen-14B         | 69.7             | 80.0              | 93.9            | 59.1                 | 53.1                 | 1481              |\n| DeepSeek-R1-Distill-Qwen-32B        | **72.6**         | 83.3              | 94.3            | 62.1                 | 57.2                 | 1691              |\n| DeepSeek-R1-Distill-Llama-8B         | 50.4             | 80.0              | 89.1            | 49.0                 | 39.6                 | 1205              |\n| DeepSeek-R1-Distill-Llama-70B        | 70.0             | **86.7**          | **94.5**        | **65.2**             | **57.5**             | 1633              |\n\n</div>\n\n\n## 5. Chat Website & API Platform\nYou can chat with DeepSeek-R1 on DeepSeek's official website: [chat.deepseek.com](https://chat.deepseek.com), and switch on the button \"DeepThink\"\n\nWe also provide OpenAI-Compatible API at DeepSeek Platform: [platform.deepseek.com](https://platform.deepseek.com/)\n\n## 6. How to Run Locally\n\n### DeepSeek-R1 Models\n\nPlease visit [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) repo for more information about running DeepSeek-R1 locally.\n\n### DeepSeek-R1-Distill Models\n\nDeepSeek-R1-Distill models can be utilized in the same manner as Qwen or Llama models.\n\nFor instance, you can easily start a service using [vLLM](https://github.com/vllm-project/vllm):\n\n```shell\nvllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --tensor-parallel-size 2 --max-model-len 32768 --enforce-eager\n```\n\nYou can also easily start a service using [SGLang](https://github.com/sgl-project/sglang)\n\n```bash\npython3 -m sglang.launch_server --model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --trust-remote-code --tp 2\n```\n\n### Usage Recommendations\n\n**We recommend adhering to the following configurations when utilizing the DeepSeek-R1 series models, including benchmarking, to achieve the expected performance:**\n\n1. Set the temperature within the range of 0.5-0.7 (0.6 is recommended) to prevent endless repetitions or incoherent outputs.\n2. **Avoid adding a system prompt; all instructions should be contained within the user prompt.**\n3. For mathematical problems, it is advisable to include a directive in your prompt such as: \"Please reason step by step, and put your final answer within \\boxed{}.\"\n4. When evaluating model performance, it is recommended to conduct multiple tests and average the results.\n\n## 7. License\nThis code repository and the model weights are licensed under the [MIT License](https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE).\nDeepSeek-R1 series support commercial use, allow for any modifications and derivative works, including, but not limited to, distillation for training other LLMs. Please note that:\n- DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Qwen-14B and DeepSeek-R1-Distill-Qwen-32B are derived from [Qwen-2.5 series](https://github.com/QwenLM/Qwen2.5), which are originally licensed under [Apache 2.0 License](https://huggingface.co/Qwen/Qwen2.5-1.5B/blob/main/LICENSE), and now finetuned with 800k samples curated with DeepSeek-R1.\n- DeepSeek-R1-Distill-Llama-8B is derived from Llama3.1-8B-Base and is originally licensed under [llama3.1 license](https://huggingface.co/meta-llama/Llama-3.1-8B/blob/main/LICENSE).\n- DeepSeek-R1-Distill-Llama-70B is derived from Llama3.3-70B-Instruct and is originally licensed under [llama3.3 license](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct/blob/main/LICENSE).\n\n## 8. Citation\n```\n@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\n      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, \n      author={DeepSeek-AI and Daya Guo and Dejian Yang and Haowei Zhang and Junxiao Song and Ruoyu Zhang and Runxin Xu and Qihao Zhu and Shirong Ma and Peiyi Wang and Xiao Bi and Xiaokang Zhang and Xingkai Yu and Yu Wu and Z. F. Wu and Zhibin Gou and Zhihong Shao and Zhuoshu Li and Ziyi Gao and Aixin Liu and Bing Xue and Bingxuan Wang and Bochao Wu and Bei Feng and Chengda Lu and Chenggang Zhao and Chengqi Deng and Chenyu Zhang and Chong Ruan and Damai Dai and Deli Chen and Dongjie Ji and Erhang Li and Fangyun Lin and Fucong Dai and Fuli Luo and Guangbo Hao and Guanting Chen and Guowei Li and H. Zhang and Han Bao and Hanwei Xu and Haocheng Wang and Honghui Ding and Huajian Xin and Huazuo Gao and Hui Qu and Hui Li and Jianzhong Guo and Jiashi Li and Jiawei Wang and Jingchang Chen and Jingyang Yuan and Junjie Qiu and Junlong Li and J. L. Cai and Jiaqi Ni and Jian Liang and Jin Chen and Kai Dong and Kai Hu and Kaige Gao and Kang Guan and Kexin Huang and Kuai Yu and Lean Wang and Lecong Zhang and Liang Zhao and Litong Wang and Liyue Zhang and Lei Xu and Leyi Xia and Mingchuan Zhang and Minghua Zhang and Minghui Tang and Meng Li and Miaojun Wang and Mingming Li and Ning Tian and Panpan Huang and Peng Zhang and Qiancheng Wang and Qinyu Chen and Qiushi Du and Ruiqi Ge and Ruisong Zhang and Ruizhe Pan and Runji Wang and R. J. Chen and R. L. Jin and Ruyi Chen and Shanghao Lu and Shangyan Zhou and Shanhuang Chen and Shengfeng Ye and Shiyu Wang and Shuiping Yu and Shunfeng Zhou and Shuting Pan and S. S. Li and Shuang Zhou and Shaoqing Wu and Shengfeng Ye and Tao Yun and Tian Pei and Tianyu Sun and T. Wang and Wangding Zeng and Wanjia Zhao and Wen Liu and Wenfeng Liang and Wenjun Gao and Wenqin Yu and Wentao Zhang and W. L. Xiao and Wei An and Xiaodong Liu and Xiaohan Wang and Xiaokang Chen and Xiaotao Nie and Xin Cheng and Xin Liu and Xin Xie and Xingchao Liu and Xinyu Yang and Xinyuan Li and Xuecheng Su and Xuheng Lin and X. Q. Li and Xiangyue Jin and Xiaojin Shen and Xiaosha Chen and Xiaowen Sun and Xiaoxiang Wang and Xinnan Song and Xinyi Zhou and Xianzu Wang and Xinxia Shan and Y. K. Li and Y. Q. Wang and Y. X. Wei and Yang Zhang and Yanhong Xu and Yao Li and Yao Zhao and Yaofeng Sun and Yaohui Wang and Yi Yu and Yichao Zhang and Yifan Shi and Yiliang Xiong and Ying He and Yishi Piao and Yisong Wang and Yixuan Tan and Yiyang Ma and Yiyuan Liu and Yongqiang Guo and Yuan Ou and Yuduan Wang and Yue Gong and Yuheng Zou and Yujia He and Yunfan Xiong and Yuxiang Luo and Yuxiang You and Yuxuan Liu and Yuyang Zhou and Y. X. Zhu and Yanhong Xu and Yanping Huang and Yaohui Li and Yi Zheng and Yuchen Zhu and Yunxian Ma and Ying Tang and Yukun Zha and Yuting Yan and Z. Z. Ren and Zehui Ren and Zhangli Sha and Zhe Fu and Zhean Xu and Zhenda Xie and Zhengyan Zhang and Zhewen Hao and Zhicheng Ma and Zhigang Yan and Zhiyu Wu and Zihui Gu and Zijia Zhu and Zijun Liu and Zilin Li and Ziwei Xie and Ziyang Song and Zizheng Pan and Zhen Huang and Zhipeng Xu and Zhongyu Zhang and Zhen Zhang},\n      year={2025},\n      eprint={2501.12948},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2501.12948}, \n}\n\n```\n\n## 9. Contact\nIf you have any questions, please raise an issue or contact us at [service@deepseek.com](service@deepseek.com).\n",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModelForCausalLM",
      "custom_class": "modeling_deepseek.DeepseekV3ForCausalLM",
      "pipeline_tag": "text-generation",
      "processor": null
    },
    "safetensors": null,
    "model_index": null,
    "trending_score": null
  },
  {
    "model_id": "meta-llama/Llama-3.2-1B-Instruct",
    "model_name": "meta-llama/Llama-3.2-1B-Instruct",
    "author": "meta-llama",
    "downloads": 2886759,
    "downloads_all_time": null,
    "likes": 835,
    "tags": [
      "transformers",
      "safetensors",
      "llama",
      "text-generation",
      "facebook",
      "meta",
      "pytorch",
      "llama-3",
      "conversational",
      "en",
      "de",
      "fr",
      "it",
      "pt",
      "hi",
      "es",
      "th",
      "arxiv:2204.05149",
      "arxiv:2405.16406",
      "license:llama3.2",
      "autotrain_compatible",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct",
    "dependencies": null,
    "last_modified": "2024-10-24T15:07:51+00:00",
    "created_at": "2024-09-18T15:12:47+00:00",
    "analysis_date": "2025-03-22T00:41:27.083686",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "llama",
    "pipeline_tag": "text-generation",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": [
        "en",
        "de",
        "fr",
        "it",
        "pt",
        "hi",
        "es",
        "th"
      ],
      "library_name": "transformers",
      "pipeline_tag": "text-generation",
      "tags": [
        "facebook",
        "meta",
        "pytorch",
        "llama",
        "llama-3"
      ],
      "license": "llama3.2",
      "extra_gated_prompt": "### LLAMA 3.2 COMMUNITY LICENSE AGREEMENT\n\nLlama 3.2 Version Release Date: September 25, 2024\n\n“Agreement” means the terms and conditions for use, reproduction, distribution  and modification of the Llama Materials set forth herein.\n\n“Documentation” means the specifications, manuals and documentation accompanying Llama 3.2 distributed by Meta at https://llama.meta.com/doc/overview.\n\n“Licensee” or “you” means you, or your employer or any other person or entity (if you are  entering into this Agreement on such person or entity’s behalf), of the age required under applicable laws, rules or regulations to provide legal consent and that has legal authority to bind your employer or such other person or entity if you are entering in this Agreement on their behalf.\n\n“Llama 3.2” means the foundational large language models and software and algorithms, including machine-learning model code, trained model weights, inference-enabling code, training-enabling code, fine-tuning enabling code and other elements of the foregoing distributed by Meta at  https://www.llama.com/llama-downloads.\n\n“Llama Materials” means, collectively, Meta’s proprietary Llama 3.2 and Documentation (and  any portion thereof) made available under this Agreement.\n\n“Meta” or “we” means Meta Platforms Ireland Limited (if you are located in or,  if you are an entity, your principal place of business is in the EEA or Switzerland)  and Meta Platforms, Inc. (if you are located outside of the EEA or Switzerland). \n\nBy clicking “I Accept” below or by using or distributing any portion or element of the Llama Materials, you agree to be bound by this Agreement.\n\n1. License Rights and Redistribution.\na. Grant of Rights. You are granted a non-exclusive, worldwide,  non-transferable and royalty-free limited license under Meta’s intellectual property or other rights  owned by Meta embodied in the Llama Materials to use, reproduce, distribute, copy, create derivative works  of, and make modifications to the Llama Materials.  \nb. Redistribution and Use.  \ni. If you distribute or make available the Llama Materials (or any derivative works thereof),  or a product or service (including another AI model) that contains any of them, you shall (A) provide a copy of this Agreement with any such Llama Materials; and (B) prominently display “Built with Llama” on a related website, user interface, blogpost, about page, or product documentation. If you use the Llama Materials or any outputs or results of the Llama Materials to create, train, fine tune, or otherwise improve an AI model, which is distributed or made available, you shall also include “Llama” at the beginning of any such AI model name.\nii. If you receive Llama Materials, or any derivative works thereof, from a Licensee as part of an integrated end user product, then Section 2 of this Agreement will not apply to you. \niii. You must retain in all copies of the Llama Materials that you distribute the  following attribution notice within a “Notice” text file distributed as a part of such copies:  “Llama 3.2 is licensed under the Llama 3.2 Community License, Copyright © Meta Platforms, Inc. All Rights Reserved.”\niv. Your use of the Llama Materials must comply with applicable laws and regulations (including trade compliance laws and regulations) and adhere to the Acceptable Use Policy for the Llama Materials (available at https://www.llama.com/llama3_2/use-policy), which is hereby  incorporated by reference into this Agreement.\n  \n2. Additional Commercial Terms. If, on the Llama 3.2 version release date, the monthly active users of the products or services made available by or for Licensee, or Licensee’s affiliates,  is greater than 700 million monthly active users in the preceding calendar month, you must request  a license from Meta, which Meta may grant to you in its sole discretion, and you are not authorized to exercise any of the rights under this Agreement unless or until Meta otherwise expressly grants you such rights.\n3. Disclaimer of Warranty. UNLESS REQUIRED BY APPLICABLE LAW, THE LLAMA MATERIALS AND ANY OUTPUT AND  RESULTS THEREFROM ARE PROVIDED ON AN “AS IS” BASIS, WITHOUT WARRANTIES OF ANY KIND, AND META DISCLAIMS ALL WARRANTIES OF ANY KIND, BOTH EXPRESS AND IMPLIED, INCLUDING, WITHOUT LIMITATION, ANY WARRANTIES OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE. YOU ARE SOLELY RESPONSIBLE FOR DETERMINING THE APPROPRIATENESS OF USING OR REDISTRIBUTING THE LLAMA MATERIALS AND ASSUME ANY RISKS ASSOCIATED WITH YOUR USE OF THE LLAMA MATERIALS AND ANY OUTPUT AND RESULTS.\n4. Limitation of Liability. IN NO EVENT WILL META OR ITS AFFILIATES BE LIABLE UNDER ANY THEORY OF LIABILITY,  WHETHER IN CONTRACT, TORT, NEGLIGENCE, PRODUCTS LIABILITY, OR OTHERWISE, ARISING OUT OF THIS AGREEMENT,  FOR ANY LOST PROFITS OR ANY INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL, EXEMPLARY OR PUNITIVE DAMAGES, EVEN  IF META OR ITS AFFILIATES HAVE BEEN ADVISED OF THE POSSIBILITY OF ANY OF THE FOREGOING.\n5. Intellectual Property.\na. No trademark licenses are granted under this Agreement, and in connection with the Llama Materials,  neither Meta nor Licensee may use any name or mark owned by or associated with the other or any of its affiliates,  except as required for reasonable and customary use in describing and redistributing the Llama Materials or as  set forth in this Section 5(a). Meta hereby grants you a license to use “Llama” (the “Mark”) solely as required  to comply with the last sentence of Section 1.b.i. You will comply with Meta’s brand guidelines (currently accessible  at https://about.meta.com/brand/resources/meta/company-brand/). All goodwill arising out of your use of the Mark  will inure to the benefit of Meta.\nb. Subject to Meta’s ownership of Llama Materials and derivatives made by or for Meta, with respect to any derivative works and modifications of the Llama Materials that are made by you, as between you and Meta, you are and will be the owner of such derivative works and modifications.\nc. If you institute litigation or other proceedings against Meta or any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Llama Materials or Llama 3.2 outputs or results, or any portion of any of the foregoing, constitutes infringement of intellectual property or other rights owned or licensable by you, then any licenses granted to you under this Agreement shall terminate as of the date such litigation or claim is filed or instituted. You will indemnify and hold harmless Meta from and against any claim by any third party arising out of or related to your use or distribution of the Llama Materials.\n6. Term and Termination. The term of this Agreement will commence upon your acceptance of this Agreement or access to the Llama Materials and will continue in full force and effect until terminated in accordance with the terms and conditions herein. Meta may terminate this Agreement if you are in breach of any term or condition of this Agreement. Upon termination of this Agreement, you shall delete and cease use of the Llama Materials. Sections 3, 4 and 7 shall survive the termination of this Agreement. \n7. Governing Law and Jurisdiction. This Agreement will be governed and construed under the laws of the State of  California without regard to choice of law principles, and the UN Convention on Contracts for the International Sale of Goods does not apply to this Agreement. The courts of California shall have exclusive jurisdiction of any dispute arising out of this Agreement. \n### Llama 3.2 Acceptable Use Policy\nMeta is committed to promoting safe and fair use of its tools and features, including Llama 3.2.  If you access or use Llama 3.2, you agree to this Acceptable Use Policy (“**Policy**”).  The most recent copy of this policy can be found at [https://www.llama.com/llama3_2/use-policy](https://www.llama.com/llama3_2/use-policy).\n#### Prohibited Uses\nWe want everyone to use Llama 3.2 safely and responsibly. You agree you will not use, or allow others to use, Llama 3.2 to:\n1. Violate the law or others’ rights, including to:\n    1. Engage in, promote, generate, contribute to, encourage, plan, incite, or further illegal or unlawful activity or content, such as:\n        1. Violence or terrorism\n        2. Exploitation or harm to children, including the solicitation, creation, acquisition, or dissemination of child exploitative content or failure to report Child Sexual Abuse Material\n        3. Human trafficking, exploitation, and sexual violence\n        4. The illegal distribution of information or materials to minors, including obscene materials, or failure to employ legally required age-gating in connection with such information or materials.\n        5. Sexual solicitation\n        6. Any other criminal activity\n    1. Engage in, promote, incite, or facilitate the harassment, abuse, threatening, or bullying of individuals or groups of individuals\n    2. Engage in, promote, incite, or facilitate discrimination or other unlawful or harmful conduct in the provision of employment, employment benefits, credit, housing, other economic benefits, or other essential goods and services\n    3. Engage in the unauthorized or unlicensed practice of any profession including, but not limited to, financial, legal, medical/health, or related professional practices\n    4. Collect, process, disclose, generate, or infer private or sensitive information about individuals, including information about individuals’ identity, health, or demographic information, unless you have obtained the right to do so in accordance with applicable law\n    5. Engage in or facilitate any action or generate any content that infringes, misappropriates, or otherwise violates any third-party rights, including the outputs or results of any products or services using the Llama Materials\n    6. Create, generate, or facilitate the creation of malicious code, malware, computer viruses or do anything else that could disable, overburden, interfere with or impair the proper working, integrity, operation or appearance of a website or computer system\n    7. Engage in any action, or facilitate any action, to intentionally circumvent or remove usage restrictions or other safety measures, or to enable functionality disabled by Meta \n2. Engage in, promote, incite, facilitate, or assist in the planning or development of activities that present a risk of death or bodily harm to individuals, including use of Llama 3.2 related to the following:\n    8. Military, warfare, nuclear industries or applications, espionage, use for materials or activities that are subject to the International Traffic Arms Regulations (ITAR) maintained by the United States Department of State or to the U.S. Biological Weapons Anti-Terrorism Act of 1989 or the Chemical Weapons Convention Implementation Act of 1997\n    9. Guns and illegal weapons (including weapon development)\n    10. Illegal drugs and regulated/controlled substances\n    11. Operation of critical infrastructure, transportation technologies, or heavy machinery\n    12. Self-harm or harm to others, including suicide, cutting, and eating disorders\n    13. Any content intended to incite or promote violence, abuse, or any infliction of bodily harm to an individual\n3. Intentionally deceive or mislead others, including use of Llama 3.2 related to the following:\n    14. Generating, promoting, or furthering fraud or the creation or promotion of disinformation\n    15. Generating, promoting, or furthering defamatory content, including the creation of defamatory statements, images, or other content\n    16. Generating, promoting, or further distributing spam\n    17. Impersonating another individual without consent, authorization, or legal right\n    18. Representing that the use of Llama 3.2 or outputs are human-generated\n    19. Generating or facilitating false online engagement, including fake reviews and other means of fake online engagement \n4. Fail to appropriately disclose to end users any known dangers of your AI system 5. Interact with third party tools, models, or software designed to generate unlawful content or engage in unlawful or harmful conduct and/or represent that the outputs of such tools, models, or software are associated with Meta or Llama 3.2\n\nWith respect to any multimodal models included in Llama 3.2, the rights granted under Section 1(a) of the Llama 3.2 Community License Agreement are not being granted to you if you are an individual domiciled in, or a company with a principal place of business in, the European Union. This restriction does not apply to end users of a product or service that incorporates any such multimodal models.\n\nPlease report any violation of this Policy, software “bug,” or other problems that could lead to a violation of this Policy through one of the following means:\n\n* Reporting issues with the model: [https://github.com/meta-llama/llama-models/issues](https://l.workplace.com/l.php?u=https%3A%2F%2Fgithub.com%2Fmeta-llama%2Fllama-models%2Fissues&h=AT0qV8W9BFT6NwihiOHRuKYQM_UnkzN_NmHMy91OT55gkLpgi4kQupHUl0ssR4dQsIQ8n3tfd0vtkobvsEvt1l4Ic6GXI2EeuHV8N08OG2WnbAmm0FL4ObkazC6G_256vN0lN9DsykCvCqGZ)\n* Reporting risky content generated by the model: [developers.facebook.com/llama_output_feedback](http://developers.facebook.com/llama_output_feedback)\n* Reporting bugs and security concerns: [facebook.com/whitehat/info](http://facebook.com/whitehat/info)\n* Reporting violations of the Acceptable Use Policy or unlicensed uses of Llama 3.2: LlamaUseReport@meta.com",
      "extra_gated_fields": {
        "First Name": "text",
        "Last Name": "text",
        "Date of birth": "date_picker",
        "Country": "country",
        "Affiliation": "text",
        "Job title": {
          "type": "select",
          "options": [
            "Student",
            "Research Graduate",
            "AI researcher",
            "AI developer/engineer",
            "Reporter",
            "Other"
          ]
        },
        "geo": "ip_location",
        "By clicking Submit below I accept the terms of the license and acknowledge that the information I provide will be collected stored processed and shared in accordance with the Meta Privacy Policy": "checkbox"
      },
      "extra_gated_description": "The information you provide will be collected, stored, processed and shared in accordance with the [Meta Privacy Policy](https://www.facebook.com/privacy/policy/).",
      "extra_gated_button_content": "Submit"
    },
    "card_text": "\n## Model Information\n\nThe Llama 3.2 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction-tuned generative models in 1B and 3B sizes (text in/text out). The Llama 3.2 instruction-tuned text only models are optimized for multilingual dialogue use cases, including agentic retrieval and summarization tasks. They outperform many of the available open source and closed chat models on common industry benchmarks.\n\n**Model Developer:** Meta\n\n**Model Architecture:** Llama 3.2 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.\n\n|  | Training Data | Params | Input modalities | Output modalities | Context Length | GQA | Shared Embeddings | Token count | Knowledge cutoff |\n| :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- |\n| Llama 3.2 (text only)  | A new mix of publicly available online data. | 1B (1.23B) | Multilingual Text | Multilingual Text and code  | 128k | Yes | Yes | Up to 9T tokens | December 2023 |\n|  |  | 3B (3.21B) | Multilingual Text | Multilingual Text and code  |  |  |  |  |  |\n| Llama 3.2 Quantized (text only)  | A new mix of publicly available online data. | 1B (1.23B) | Multilingual Text | Multilingual Text and code  | 8k | Yes | Yes | Up to 9T tokens | December 2023 |\n|  |  | 3B (3.21B) | Multilingual Text | Multilingual Text and code |  |  |  |  |  |\n\n**Supported Languages:** English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai are officially supported. Llama 3.2 has been trained on a broader collection of languages than these 8 supported languages. Developers may fine-tune Llama 3.2 models for languages beyond these supported languages, provided they comply with the Llama 3.2 Community License and the Acceptable Use Policy. Developers are always expected to ensure that their deployments, including those that involve additional languages, are completed safely and responsibly.\n\n**Llama 3.2 Model Family:** Token counts refer to pretraining data only. All model versions use Grouped-Query Attention (GQA) for improved inference scalability.\n\n**Model Release Date:** Sept 25, 2024\n\n**Status:** This is a static model trained on an offline dataset. Future versions may be released that improve model capabilities and safety.\n\n**License:** Use of Llama 3.2 is governed by the [Llama 3.2 Community License](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/LICENSE) (a custom, commercial license agreement).\n\n**Feedback:** Instructions on how to provide feedback or comments on the model can be found in the Llama Models [README](https://github.com/meta-llama/llama-models/blob/main/README.md). For more technical information about generation parameters and recipes for how to use Llama 3.2 in applications, please go [here](https://github.com/meta-llama/llama-recipes).\n\n## Intended Use\n\n**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.\n\n**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.\n\n## How to use\n\nThis repository contains two versions of Llama-3.2-1B-Instruct, for use with transformers and with the original `llama` codebase.\n\n### Use with transformers\n\nStarting with `transformers >= 4.43.0` onward, you can run conversational inference using the Transformers `pipeline` abstraction or by leveraging the Auto classes with the `generate()` function.\n\nMake sure to update your transformers installation via `pip install --upgrade transformers`.\n\n```python\nimport torch\nfrom transformers import pipeline\n\nmodel_id = \"meta-llama/Llama-3.2-1B-Instruct\"\npipe = pipeline(\n    \"text-generation\",\n    model=model_id,\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\",\n)\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n    {\"role\": \"user\", \"content\": \"Who are you?\"},\n]\noutputs = pipe(\n    messages,\n    max_new_tokens=256,\n)\nprint(outputs[0][\"generated_text\"][-1])\n```\n\nNote: You can also find detailed recipes on how to use the model locally, with `torch.compile()`, assisted generations, quantised and more at [`huggingface-llama-recipes`](https://github.com/huggingface/huggingface-llama-recipes)\n\n### Use with `llama`\n\nPlease, follow the instructions in the [repository](https://github.com/meta-llama/llama)\n\nTo download Original checkpoints, see the example command below leveraging `huggingface-cli`:\n\n```\nhuggingface-cli download meta-llama/Llama-3.2-1B-Instruct --include \"original/*\" --local-dir Llama-3.2-1B-Instruct\n```\n\n## Hardware and Software\n\n**Training Factors:** We used custom training libraries, Meta's custom built GPU cluster, and production infrastructure for pretraining. Fine-tuning, quantization, annotation, and evaluation were also performed on production infrastructure.\n\n**Training Energy Use:** Training utilized a cumulative of **916k** GPU hours of computation on H100-80GB (TDP of 700W) type hardware, per the table below. Training time is the total GPU time required for training each model and power consumption is the peak power capacity per GPU device used, adjusted for power usage efficiency.\n\n**Training Greenhouse Gas Emissions:** Estimated total location-based greenhouse gas emissions were **240** tons CO2eq for training. Since 2020, Meta has maintained net zero greenhouse gas emissions in its global operations and matched 100% of its electricity use with renewable energy; therefore, the total market-based greenhouse gas emissions for training were 0 tons CO2eq.\n\n|  | Training Time (GPU hours) | Logit Generation Time (GPU Hours) | Training Power Consumption (W) | Training Location-Based Greenhouse Gas Emissions (tons CO2eq) | Training Market-Based Greenhouse Gas Emissions (tons CO2eq) |\n| :---- | :---: | ----- | :---: | :---: | :---: |\n| Llama 3.2 1B | 370k | \\- | 700 | 107 | 0 |\n| Llama 3.2 3B | 460k | \\- | 700 | 133 | 0 |\n| Llama 3.2 1B SpinQuant | 1.7 | 0 | 700 | *Negligible*\\*\\* | 0 |\n| Llama 3.2 3B SpinQuant | 2.4 | 0 | 700 | *Negligible*\\*\\* | 0 |\n| Llama 3.2 1B QLora | 1.3k | 0 | 700 | 0.381 | 0 |\n| Llama 3.2 3B QLora | 1.6k | 0 | 700 | 0.461 | 0 |\n| Total | 833k |         86k |  | 240 | 0 |\n\n\\*\\* The location-based CO2e emissions of Llama 3.2 1B SpinQuant and Llama 3.2 3B SpinQuant are less than 0.001 metric tonnes each. This is due to the minimal training GPU hours that are required.\n\nThe methodology used to determine training energy use and greenhouse gas emissions can be found [here](https://arxiv.org/pdf/2204.05149). Since Meta is openly releasing these models, the training energy use and greenhouse gas emissions will not be incurred by others.\n\n## Training Data\n\n**Overview:** Llama 3.2 was pretrained on up to 9 trillion tokens of data from publicly available sources. For the 1B and 3B Llama 3.2 models, we incorporated logits from the Llama 3.1 8B and 70B models into the pretraining stage of the model development, where outputs (logits) from these larger models were used as token-level targets. Knowledge distillation was used after pruning to recover performance. In post-training we used a similar recipe as Llama 3.1 and produced final chat models by doing several rounds of alignment on top of the pre-trained model. Each round involved Supervised Fine-Tuning (SFT), Rejection Sampling (RS), and Direct Preference Optimization (DPO).\n\n**Data Freshness:** The pretraining data has a cutoff of December 2023\\.\n\n## Quantization\n\n### Quantization Scheme\n\nWe designed the current quantization scheme with the [PyTorch’s ExecuTorch](https://github.com/pytorch/executorch) inference framework and Arm CPU backend in mind, taking into account metrics including model quality, prefill/decoding speed, and memory footprint. Our quantization scheme involves three parts:\n- All linear layers in all transformer blocks are quantized to a 4-bit groupwise scheme (with a group size of 32) for weights and 8-bit per-token dynamic quantization for activations.\n- The classification layer is quantized to 8-bit per-channel for weight and 8-bit per token dynamic quantization for activation.\n- Similar to classification layer, an 8-bit per channel quantization is used for embedding layer.\n\n\n### Quantization-Aware Training and LoRA\n\nThe quantization-aware training (QAT) with low-rank adaptation (LoRA) models went through only post-training stages, using the same data as the full precision models. To initialize QAT, we utilize BF16 Llama 3.2 model checkpoints obtained after supervised fine-tuning (SFT) and perform an additional full round of SFT training with QAT. We then freeze the backbone of the QAT model and perform another round of SFT with LoRA adaptors applied to all layers within the transformer block. Meanwhile, the LoRA adaptors' weights and activations are maintained in BF16. Because our approach is similar to QLoRA of Dettmers et al., (2023) (i.e., quantization followed by LoRA adapters), we refer this method as QLoRA. Finally, we fine-tune the resulting model (both backbone and LoRA adaptors) using direct preference optimization (DPO).\n\n### SpinQuant\n\n[SpinQuant](https://arxiv.org/abs/2405.16406) was applied, together with generative post-training quantization (GPTQ). For the SpinQuant rotation matrix fine-tuning, we optimized for 100 iterations, using 800 samples with sequence-length 2048 from the WikiText 2 dataset. For GPTQ, we used 128 samples from the same dataset with the same sequence-length.\n\n## Benchmarks \\- English Text\n\nIn this section, we report the results for Llama 3.2 models on standard automatic benchmarks. For all these evaluations, we used our internal evaluations library.\n\n### Base Pretrained Models\n\n| Category | Benchmark | \\# Shots | Metric | Llama 3.2 1B | Llama 3.2 3B | Llama 3.1 8B |\n| ----- | ----- | :---: | :---: | :---: | :---: | :---: |\n| General | MMLU | 5 | macro\\_avg/acc\\_char | 32.2 | 58 | 66.7 |\n|  | AGIEval English | 3-5 | average/acc\\_char | 23.3 | 39.2 | 47.8 |\n|  | ARC-Challenge | 25 | acc\\_char | 32.8 | 69.1 | 79.7 |\n| Reading comprehension | SQuAD | 1 | em | 49.2 | 67.7 | 77 |\n|  | QuAC (F1) | 1 | f1 | 37.9 | 42.9 | 44.9 |\n|  | DROP (F1) | 3 | f1 | 28.0 | 45.2 | 59.5 |\n| Long Context | Needle in Haystack | 0 | em | 96.8 | 1 | 1 |\n\n### Instruction Tuned Models\n\n| Capability |  | Benchmark | \\# Shots | Metric | Llama 3.2 1B bf16 | Llama 3.2 1B Vanilla PTQ\\*\\* | Llama 3.2 1B Spin Quant | Llama 3.2 1B QLoRA | Llama 3.2 3B bf16 | Llama 3.2 3B Vanilla PTQ\\*\\* | Llama 3.2 3B Spin Quant | Llama 3.2 3B QLoRA | Llama 3.1 8B |\n| :---: | ----- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| General |  | MMLU | 5 | macro\\_avg/acc | 49.3 | 43.3 | 47.3 | 49.0 | 63.4 | 60.5 | 62 | 62.4 | 69.4 |\n| Re-writing |  | Open-rewrite eval | 0 | micro\\_avg/rougeL | 41.6 | 39.2 | 40.9 | 41.2 | 40.1 | 40.3 | 40.8 | 40.7 | 40.9 |\n| Summarization |  | TLDR9+ (test) | 1 | rougeL | 16.8 | 14.9 | 16.7 | 16.8 | 19.0 | 19.1 | 19.2 | 19.1 | 17.2 |\n| Instruction following |  | IFEval | 0 | Avg(Prompt/Instruction acc Loose/Strict) | 59.5 | 51.5 | 58.4 | 55.6 | 77.4 | 73.9 | 73.5 | 75.9 | 80.4 |\n| Math |  | GSM8K (CoT) | 8 | em\\_maj1@1 | 44.4 | 33.1 | 40.6 | 46.5 | 77.7 | 72.9 | 75.7 | 77.9 | 84.5 |\n|  |  | MATH (CoT) | 0 | final\\_em | 30.6 | 20.5 | 25.3 | 31.0 | 48.0 | 44.2 | 45.3 | 49.2 | 51.9 |\n| Reasoning |  | ARC-C | 0 | acc | 59.4 | 54.3 | 57 | 60.7 | 78.6 | 75.6 | 77.6 | 77.6 | 83.4 |\n|  |  | GPQA | 0 | acc | 27.2 | 25.9 | 26.3 | 25.9 | 32.8 | 32.8 | 31.7 | 33.9 | 32.8 |\n|  |  | Hellaswag | 0 | acc | 41.2 | 38.1 | 41.3 | 41.5 | 69.8 | 66.3 | 68 | 66.3 | 78.7 |\n| Tool Use |  | BFCL V2 | 0 | acc | 25.7 | 14.3 | 15.9 | 23.7 | 67.0 | 53.4 | 60.1 | 63.5 | 67.1 |\n|  |  | Nexus | 0 | macro\\_avg/acc | 13.5 | 5.2 | 9.6 | 12.5 | 34.3 | 32.4 | 31.5 | 30.1 | 38.5 |\n| Long Context |  | InfiniteBench/En.QA | 0 | longbook\\_qa/f1 | 20.3 | N/A | N/A | N/A | 19.8 | N/A | N/A | N/A | 27.3 |\n|  |  | InfiniteBench/En.MC | 0 | longbook\\_choice/acc | 38.0 | N/A | N/A | N/A | 63.3 | N/A | N/A | N/A | 72.2 |\n|  |  | NIH/Multi-needle | 0 | recall | 75.0 | N/A | N/A | N/A | 84.7 | N/A | N/A | N/A | 98.8 |\n| Multilingual |  | MGSM (CoT) | 0 | em | 24.5 | 13.7 | 18.2 | 24.4 | 58.2 | 48.9 | 54.3 | 56.8 | 68.9 |\n\n\\*\\*for comparison purposes only. Model not released.\n\n### Multilingual Benchmarks\n\n| Category | Benchmark | Language | Llama 3.2 1B | Llama 3.2 1B Vanilla PTQ\\*\\* | Llama 3.2 1B Spin Quant | Llama 3.2 1B QLoRA | Llama 3.2 3B | Llama 3.2 3B Vanilla PTQ\\*\\* | Llama 3.2 3B Spin Quant | Llama 3.2 3B QLoRA | Llama 3.1 8B |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| General | MMLU (5-shot, macro_avg/acc) | Portuguese | 39.8 | 34.9 | 38.9 | 40.2 | 54.5 | 50.9 | 53.3 | 53.4 | 62.1 |\n| | | Spanish | 41.5 | 36.0 | 39.8 | 41.8 | 55.1 | 51.9 | 53.6 | 53.6 | 62.5 |\n| | | Italian | 39.8 | 34.9 | 38.1 | 40.6 | 53.8 | 49.9 | 52.1 | 51.7 | 61.6 |\n| | | German | 39.2 | 34.9 | 37.5 | 39.6 | 53.3 | 50.0 | 52.2 | 51.3 | 60.6 |\n| | | French | 40.5 | 34.8 | 39.2 | 40.8 | 54.6 | 51.2 | 53.3 | 53.3 | 62.3 |\n| | | Hindi | 33.5 | 30.0 | 32.1 | 34.0 | 43.3 | 40.4 | 42.0 | 42.1 | 50.9 |\n| | | Thai | 34.7 | 31.2 | 32.4 | 34.9 | 44.5 | 41.3 | 44.0 | 42.2 | 50.3 |\n\n\\*\\*for comparison purposes only. Model not released.\n\n## Inference time\n\nIn the below table, we compare the performance metrics of different quantization methods (SpinQuant and QAT \\+ LoRA) with the BF16 baseline. The evaluation was done using the [ExecuTorch](https://github.com/pytorch/executorch) framework as the inference engine, with the ARM CPU as a backend using Android OnePlus 12 device.\n\n| Category | Decode (tokens/sec)  | Time-to-first-token (sec) | Prefill (tokens/sec) | Model size (PTE file size in MB) | Memory size (RSS in MB) |\n| :---- | ----- | ----- | ----- | ----- | ----- |\n| 1B BF16 (baseline) | 19.2 | 1.0 | 60.3 | 2358 | 3,185 |\n| 1B SpinQuant | 50.2 (2.6x) | 0.3 (-76.9%) | 260.5 (4.3x) | 1083 (-54.1%) | 1,921 (-39.7%) |\n| 1B QLoRA | 45.8 (2.4x) | 0.3 (-76.0%) | 252.0 (4.2x) | 1127 (-52.2%) | 2,255 (-29.2%) |\n| 3B BF16 (baseline) | 7.6 | 3.0 | 21.2 | 6129 | 7,419 |\n| 3B SpinQuant | 19.7 (2.6x) | 0.7 (-76.4%) | 89.7 (4.2x) | 2435 (-60.3%) | 3,726 (-49.8%) |\n| 3B QLoRA | 18.5 (2.4x) | 0.7 (-76.1%) | 88.8 (4.2x) | 2529 (-58.7%) | 4,060 (-45.3%) |\n\n(\\*) The performance measurement is done using an adb binary-based approach.\n(\\*\\*) It is measured on an Android OnePlus 12 device.\n(\\*\\*\\*) Time-to-first-token (TTFT)  is measured with prompt length=64\n\n*Footnote:*\n\n- *Decode (tokens/second) is for how quickly it keeps generating. Higher is better.*\n- *Time-to-first-token (TTFT for shorthand) is for how fast it generates the first token for a given prompt. Lower is better.*\n- *Prefill is the inverse of TTFT (aka 1/TTFT)  in tokens/second. Higher is better*\n- *Model size \\- how big is the model, measured by, PTE file, a binary file format for ExecuTorch*\n- *RSS size \\- Memory usage in resident set size (RSS)*\n\n## Responsibility & Safety\n\nAs part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:\n\n1. Enable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama\n2. Protect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm\n3. Provide protections for the community to help prevent the misuse of our models\n\n### Responsible Deployment\n\n**Approach:** Llama is a foundational technology designed to be used in a variety of use cases. Examples on how Meta’s Llama models have been responsibly deployed can be found in our [Community Stories webpage](https://llama.meta.com/community-stories/). Our approach is to build the most helpful models, enabling the world to benefit from the technology power, by aligning our model safety for generic use cases and addressing a standard set of harms. Developers are then in the driver’s seat to tailor safety for their use cases, defining their own policies and deploying the models with the necessary safeguards in their Llama systems. Llama 3.2 was developed following the best practices outlined in our [Responsible Use Guide](https://llama.meta.com/responsible-use-guide/).\n\n#### Llama 3.2 Instruct\n\n**Objective:** Our main objectives for conducting safety fine-tuning are to provide the research community with a valuable resource for studying the robustness of safety fine-tuning, as well as to offer developers a readily available, safe, and powerful model for various applications to reduce the developer workload to deploy safe AI systems. We implemented the same set of safety mitigations as in Llama 3, and you can learn more about these in the Llama 3 [paper](https://ai.meta.com/research/publications/the-llama-3-herd-of-models/).\n\n**Fine-Tuning Data:** We employ a multi-faceted approach to data collection, combining human-generated data from our vendors with synthetic data to mitigate potential safety risks. We’ve developed many large language model (LLM)-based classifiers that enable us to thoughtfully select high-quality prompts and responses, enhancing data quality control.\n\n**Refusals and Tone:** Building on the work we started with Llama 3, we put a great emphasis on model refusals to benign prompts as well as refusal tone. We included both borderline and adversarial prompts in our safety data strategy, and modified our safety data responses to follow tone guidelines.\n\n#### Llama 3.2 Systems\n\n**Safety as a System:** Large language models, including Llama 3.2, **are not designed to be deployed in isolation** but instead should be deployed as part of an overall AI system with additional safety guardrails as required. Developers are expected to deploy system safeguards when building agentic systems. Safeguards are key to achieve the right helpfulness-safety alignment as well as mitigating safety and security risks inherent to the system and any integration of the model or system with external tools. As part of our responsible release approach, we provide the community with [safeguards](https://llama.meta.com/trust-and-safety/) that developers should deploy with Llama models or other LLMs, including Llama Guard, Prompt Guard and Code Shield. All our [reference implementations](https://github.com/meta-llama/llama-agentic-system) demos contain these safeguards by default so developers can benefit from system-level safety out-of-the-box.\n\n### New Capabilities and Use Cases\n\n**Technological Advancement:** Llama releases usually introduce new capabilities that require specific considerations in addition to the best practices that generally apply across all Generative AI use cases. For prior release capabilities also supported by Llama 3.2, see [Llama 3.1 Model Card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md), as the same considerations apply here as well.\n\n**Constrained Environments:** Llama 3.2 1B and 3B models are expected to be deployed in highly constrained environments, such as mobile devices. LLM Systems using smaller models will have a different alignment profile and safety/helpfulness tradeoff than more complex, larger systems. Developers should ensure the safety of their system meets the requirements of their use case. We recommend using lighter system safeguards for such use cases, like Llama Guard 3-1B or its mobile-optimized version.\n\n### Evaluations\n\n**Scaled Evaluations:** We built dedicated, adversarial evaluation datasets and evaluated systems composed of Llama models and Purple Llama safeguards to filter input prompt and output response. It is important to evaluate applications in context, and we recommend building dedicated evaluation dataset for your use case.\n\n**Red Teaming:** We conducted recurring red teaming exercises with the goal of discovering risks via adversarial prompting and we used the learnings to improve our benchmarks and safety tuning datasets. We partnered early with subject-matter experts in critical risk areas to understand the nature of these real-world harms and how such models may lead to unintended harm for society. Based on these conversations, we derived a set of adversarial goals for the red team to attempt to achieve, such as extracting harmful information or reprogramming the model to act in a potentially harmful capacity. The red team consisted of experts in cybersecurity, adversarial machine learning, responsible AI, and integrity in addition to multilingual content specialists with background in integrity issues in specific geographic markets.\n\n### Critical Risks\n\nIn addition to our safety work above, we took extra care on measuring and/or mitigating the following critical risk areas:\n\n**1\\. CBRNE (Chemical, Biological, Radiological, Nuclear, and Explosive Weapons):** Llama 3.2 1B and 3B models are smaller and less capable derivatives of Llama 3.1. For Llama 3.1 70B and 405B, to assess risks related to proliferation of chemical and biological weapons, we performed uplift testing designed to assess whether use of Llama 3.1 models could meaningfully increase the capabilities of malicious actors to plan or carry out attacks using these types of weapons and have determined that such testing also applies to the smaller 1B and 3B models.\n\n**2\\. Child Safety:** Child Safety risk assessments were conducted using a team of experts, to assess the model’s capability to produce outputs that could result in Child Safety risks and inform on any necessary and appropriate risk mitigations via fine tuning. We leveraged those expert red teaming sessions to expand the coverage of our evaluation benchmarks through Llama 3 model development. For Llama 3, we conducted new in-depth sessions using objective based methodologies to assess the model risks along multiple attack vectors including the additional languages Llama 3 is trained on. We also partnered with content specialists to perform red teaming exercises assessing potentially violating content while taking account of market specific nuances or experiences.\n\n**3\\. Cyber Attacks:** For Llama 3.1 405B, our cyber attack uplift study investigated whether LLMs can enhance human capabilities in hacking tasks, both in terms of skill level and speed.\nOur attack automation study focused on evaluating the capabilities of LLMs when used as autonomous agents in cyber offensive operations, specifically in the context of ransomware attacks. This evaluation was distinct from previous studies that considered LLMs as interactive assistants. The primary objective was to assess whether these models could effectively function as independent agents in executing complex cyber-attacks without human intervention. Because Llama 3.2’s 1B and 3B models are smaller and less capable models than Llama 3.1 405B, we broadly believe that the testing conducted for the 405B model also applies to Llama 3.2 models.\n\n### Community\n\n**Industry Partnerships:** Generative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership on AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Purple Llama tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to our [Github repository](https://github.com/meta-llama/PurpleLlama).\n\n**Grants:** We also set up the [Llama Impact Grants](https://llama.meta.com/llama-impact-grants/) program to identify and support the most compelling applications of Meta’s Llama model for societal benefit across three categories: education, climate and open innovation. The 20 finalists from the hundreds of applications can be found [here](https://llama.meta.com/llama-impact-grants/#finalists).\n\n**Reporting:** Finally, we put in place a set of resources including an [output reporting mechanism](https://developers.facebook.com/llama_output_feedback) and [bug bounty program](https://www.facebook.com/whitehat) to continuously improve the Llama technology with the help of the community.\n\n## Ethical Considerations and Limitations\n\n**Values:** The core values of Llama 3.2 are openness, inclusivity and helpfulness. It is meant to serve everyone, and to work for a wide range of use cases. It is thus designed to be accessible to people across many different backgrounds, experiences and perspectives. Llama 3.2 addresses users and their needs as they are, without insertion unnecessary judgment or normativity, while reflecting the understanding that even content that may appear problematic in some cases can serve valuable purposes in others. It respects the dignity and autonomy of all users, especially in terms of the values of free thought and expression that power innovation and progress.\n\n**Testing:** Llama 3.2 is a new technology, and like any new technology, there are risks associated with its use. Testing conducted to date has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, Llama 3.2’s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 3.2 models, developers should perform safety testing and tuning tailored to their specific applications of the model. Please refer to available resources including our [Responsible Use Guide](https://llama.meta.com/responsible-use-guide), [Trust and Safety](https://llama.meta.com/trust-and-safety/) solutions, and other [resources](https://llama.meta.com/docs/get-started/) to learn more about responsible development.\n",
    "card_content": "---\nlanguage:\n- en\n- de\n- fr\n- it\n- pt\n- hi\n- es\n- th\nlibrary_name: transformers\npipeline_tag: text-generation\ntags:\n- facebook\n- meta\n- pytorch\n- llama\n- llama-3\nlicense: llama3.2\nextra_gated_prompt: \"### LLAMA 3.2 COMMUNITY LICENSE AGREEMENT\\n\\nLlama 3.2 Version\\\n  \\ Release Date: September 25, 2024\\n\\n“Agreement” means the terms and conditions\\\n  \\ for use, reproduction, distribution  and modification of the Llama Materials set\\\n  \\ forth herein.\\n\\n“Documentation” means the specifications, manuals and documentation\\\n  \\ accompanying Llama 3.2 distributed by Meta at https://llama.meta.com/doc/overview.\\n\\\n  \\n“Licensee” or “you” means you, or your employer or any other person or entity\\\n  \\ (if you are  entering into this Agreement on such person or entity’s behalf),\\\n  \\ of the age required under applicable laws, rules or regulations to provide legal\\\n  \\ consent and that has legal authority to bind your employer or such other person\\\n  \\ or entity if you are entering in this Agreement on their behalf.\\n\\n“Llama 3.2”\\\n  \\ means the foundational large language models and software and algorithms, including\\\n  \\ machine-learning model code, trained model weights, inference-enabling code, training-enabling\\\n  \\ code, fine-tuning enabling code and other elements of the foregoing distributed\\\n  \\ by Meta at  https://www.llama.com/llama-downloads.\\n\\n“Llama Materials” means,\\\n  \\ collectively, Meta’s proprietary Llama 3.2 and Documentation (and  any portion\\\n  \\ thereof) made available under this Agreement.\\n\\n“Meta” or “we” means Meta Platforms\\\n  \\ Ireland Limited (if you are located in or,  if you are an entity, your principal\\\n  \\ place of business is in the EEA or Switzerland)  and Meta Platforms, Inc. (if\\\n  \\ you are located outside of the EEA or Switzerland). \\n\\nBy clicking “I Accept”\\\n  \\ below or by using or distributing any portion or element of the Llama Materials,\\\n  \\ you agree to be bound by this Agreement.\\n\\n1. License Rights and Redistribution.\\n\\\n  a. Grant of Rights. You are granted a non-exclusive, worldwide,  non-transferable\\\n  \\ and royalty-free limited license under Meta’s intellectual property or other rights\\\n  \\  owned by Meta embodied in the Llama Materials to use, reproduce, distribute,\\\n  \\ copy, create derivative works  of, and make modifications to the Llama Materials.\\\n  \\  \\nb. Redistribution and Use.  \\ni. If you distribute or make available the Llama\\\n  \\ Materials (or any derivative works thereof),  or a product or service (including\\\n  \\ another AI model) that contains any of them, you shall (A) provide a copy of this\\\n  \\ Agreement with any such Llama Materials; and (B) prominently display “Built with\\\n  \\ Llama” on a related website, user interface, blogpost, about page, or product\\\n  \\ documentation. If you use the Llama Materials or any outputs or results of the\\\n  \\ Llama Materials to create, train, fine tune, or otherwise improve an AI model,\\\n  \\ which is distributed or made available, you shall also include “Llama” at the\\\n  \\ beginning of any such AI model name.\\nii. If you receive Llama Materials, or any\\\n  \\ derivative works thereof, from a Licensee as part of an integrated end user product,\\\n  \\ then Section 2 of this Agreement will not apply to you. \\niii. You must retain\\\n  \\ in all copies of the Llama Materials that you distribute the  following attribution\\\n  \\ notice within a “Notice” text file distributed as a part of such copies:  “Llama\\\n  \\ 3.2 is licensed under the Llama 3.2 Community License, Copyright © Meta Platforms,\\\n  \\ Inc. All Rights Reserved.”\\niv. Your use of the Llama Materials must comply with\\\n  \\ applicable laws and regulations (including trade compliance laws and regulations)\\\n  \\ and adhere to the Acceptable Use Policy for the Llama Materials (available at\\\n  \\ https://www.llama.com/llama3_2/use-policy), which is hereby  incorporated by reference\\\n  \\ into this Agreement.\\n  \\n2. Additional Commercial Terms. If, on the Llama 3.2\\\n  \\ version release date, the monthly active users of the products or services made\\\n  \\ available by or for Licensee, or Licensee’s affiliates,  is greater than 700 million\\\n  \\ monthly active users in the preceding calendar month, you must request  a license\\\n  \\ from Meta, which Meta may grant to you in its sole discretion, and you are not\\\n  \\ authorized to exercise any of the rights under this Agreement unless or until\\\n  \\ Meta otherwise expressly grants you such rights.\\n3. Disclaimer of Warranty. UNLESS\\\n  \\ REQUIRED BY APPLICABLE LAW, THE LLAMA MATERIALS AND ANY OUTPUT AND  RESULTS THEREFROM\\\n  \\ ARE PROVIDED ON AN “AS IS” BASIS, WITHOUT WARRANTIES OF ANY KIND, AND META DISCLAIMS\\\n  \\ ALL WARRANTIES OF ANY KIND, BOTH EXPRESS AND IMPLIED, INCLUDING, WITHOUT LIMITATION,\\\n  \\ ANY WARRANTIES OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR\\\n  \\ PURPOSE. YOU ARE SOLELY RESPONSIBLE FOR DETERMINING THE APPROPRIATENESS OF USING\\\n  \\ OR REDISTRIBUTING THE LLAMA MATERIALS AND ASSUME ANY RISKS ASSOCIATED WITH YOUR\\\n  \\ USE OF THE LLAMA MATERIALS AND ANY OUTPUT AND RESULTS.\\n4. Limitation of Liability.\\\n  \\ IN NO EVENT WILL META OR ITS AFFILIATES BE LIABLE UNDER ANY THEORY OF LIABILITY,\\\n  \\  WHETHER IN CONTRACT, TORT, NEGLIGENCE, PRODUCTS LIABILITY, OR OTHERWISE, ARISING\\\n  \\ OUT OF THIS AGREEMENT,  FOR ANY LOST PROFITS OR ANY INDIRECT, SPECIAL, CONSEQUENTIAL,\\\n  \\ INCIDENTAL, EXEMPLARY OR PUNITIVE DAMAGES, EVEN  IF META OR ITS AFFILIATES HAVE\\\n  \\ BEEN ADVISED OF THE POSSIBILITY OF ANY OF THE FOREGOING.\\n5. Intellectual Property.\\n\\\n  a. No trademark licenses are granted under this Agreement, and in connection with\\\n  \\ the Llama Materials,  neither Meta nor Licensee may use any name or mark owned\\\n  \\ by or associated with the other or any of its affiliates,  except as required\\\n  \\ for reasonable and customary use in describing and redistributing the Llama Materials\\\n  \\ or as  set forth in this Section 5(a). Meta hereby grants you a license to use\\\n  \\ “Llama” (the “Mark”) solely as required  to comply with the last sentence of Section\\\n  \\ 1.b.i. You will comply with Meta’s brand guidelines (currently accessible  at\\\n  \\ https://about.meta.com/brand/resources/meta/company-brand/). All goodwill arising\\\n  \\ out of your use of the Mark  will inure to the benefit of Meta.\\nb. Subject to\\\n  \\ Meta’s ownership of Llama Materials and derivatives made by or for Meta, with\\\n  \\ respect to any derivative works and modifications of the Llama Materials that\\\n  \\ are made by you, as between you and Meta, you are and will be the owner of such\\\n  \\ derivative works and modifications.\\nc. If you institute litigation or other proceedings\\\n  \\ against Meta or any entity (including a cross-claim or counterclaim in a lawsuit)\\\n  \\ alleging that the Llama Materials or Llama 3.2 outputs or results, or any portion\\\n  \\ of any of the foregoing, constitutes infringement of intellectual property or\\\n  \\ other rights owned or licensable by you, then any licenses granted to you under\\\n  \\ this Agreement shall terminate as of the date such litigation or claim is filed\\\n  \\ or instituted. You will indemnify and hold harmless Meta from and against any\\\n  \\ claim by any third party arising out of or related to your use or distribution\\\n  \\ of the Llama Materials.\\n6. Term and Termination. The term of this Agreement will\\\n  \\ commence upon your acceptance of this Agreement or access to the Llama Materials\\\n  \\ and will continue in full force and effect until terminated in accordance with\\\n  \\ the terms and conditions herein. Meta may terminate this Agreement if you are\\\n  \\ in breach of any term or condition of this Agreement. Upon termination of this\\\n  \\ Agreement, you shall delete and cease use of the Llama Materials. Sections 3,\\\n  \\ 4 and 7 shall survive the termination of this Agreement. \\n7. Governing Law and\\\n  \\ Jurisdiction. This Agreement will be governed and construed under the laws of\\\n  \\ the State of  California without regard to choice of law principles, and the UN\\\n  \\ Convention on Contracts for the International Sale of Goods does not apply to\\\n  \\ this Agreement. The courts of California shall have exclusive jurisdiction of\\\n  \\ any dispute arising out of this Agreement. \\n### Llama 3.2 Acceptable Use Policy\\n\\\n  Meta is committed to promoting safe and fair use of its tools and features, including\\\n  \\ Llama 3.2.  If you access or use Llama 3.2, you agree to this Acceptable Use Policy\\\n  \\ (“**Policy**”).  The most recent copy of this policy can be found at [https://www.llama.com/llama3_2/use-policy](https://www.llama.com/llama3_2/use-policy).\\n\\\n  #### Prohibited Uses\\nWe want everyone to use Llama 3.2 safely and responsibly.\\\n  \\ You agree you will not use, or allow others to use, Llama 3.2 to:\\n1. Violate\\\n  \\ the law or others’ rights, including to:\\n    1. Engage in, promote, generate,\\\n  \\ contribute to, encourage, plan, incite, or further illegal or unlawful activity\\\n  \\ or content, such as:\\n        1. Violence or terrorism\\n        2. Exploitation\\\n  \\ or harm to children, including the solicitation, creation, acquisition, or dissemination\\\n  \\ of child exploitative content or failure to report Child Sexual Abuse Material\\n\\\n  \\        3. Human trafficking, exploitation, and sexual violence\\n        4. The\\\n  \\ illegal distribution of information or materials to minors, including obscene\\\n  \\ materials, or failure to employ legally required age-gating in connection with\\\n  \\ such information or materials.\\n        5. Sexual solicitation\\n        6. Any\\\n  \\ other criminal activity\\n    1. Engage in, promote, incite, or facilitate the\\\n  \\ harassment, abuse, threatening, or bullying of individuals or groups of individuals\\n\\\n  \\    2. Engage in, promote, incite, or facilitate discrimination or other unlawful\\\n  \\ or harmful conduct in the provision of employment, employment benefits, credit,\\\n  \\ housing, other economic benefits, or other essential goods and services\\n    3.\\\n  \\ Engage in the unauthorized or unlicensed practice of any profession including,\\\n  \\ but not limited to, financial, legal, medical/health, or related professional\\\n  \\ practices\\n    4. Collect, process, disclose, generate, or infer private or sensitive\\\n  \\ information about individuals, including information about individuals’ identity,\\\n  \\ health, or demographic information, unless you have obtained the right to do so\\\n  \\ in accordance with applicable law\\n    5. Engage in or facilitate any action or\\\n  \\ generate any content that infringes, misappropriates, or otherwise violates any\\\n  \\ third-party rights, including the outputs or results of any products or services\\\n  \\ using the Llama Materials\\n    6. Create, generate, or facilitate the creation\\\n  \\ of malicious code, malware, computer viruses or do anything else that could disable,\\\n  \\ overburden, interfere with or impair the proper working, integrity, operation\\\n  \\ or appearance of a website or computer system\\n    7. Engage in any action, or\\\n  \\ facilitate any action, to intentionally circumvent or remove usage restrictions\\\n  \\ or other safety measures, or to enable functionality disabled by Meta \\n2. Engage\\\n  \\ in, promote, incite, facilitate, or assist in the planning or development of activities\\\n  \\ that present a risk of death or bodily harm to individuals, including use of Llama\\\n  \\ 3.2 related to the following:\\n    8. Military, warfare, nuclear industries or\\\n  \\ applications, espionage, use for materials or activities that are subject to the\\\n  \\ International Traffic Arms Regulations (ITAR) maintained by the United States\\\n  \\ Department of State or to the U.S. Biological Weapons Anti-Terrorism Act of 1989\\\n  \\ or the Chemical Weapons Convention Implementation Act of 1997\\n    9. Guns and\\\n  \\ illegal weapons (including weapon development)\\n    10. Illegal drugs and regulated/controlled\\\n  \\ substances\\n    11. Operation of critical infrastructure, transportation technologies,\\\n  \\ or heavy machinery\\n    12. Self-harm or harm to others, including suicide, cutting,\\\n  \\ and eating disorders\\n    13. Any content intended to incite or promote violence,\\\n  \\ abuse, or any infliction of bodily harm to an individual\\n3. Intentionally deceive\\\n  \\ or mislead others, including use of Llama 3.2 related to the following:\\n    14.\\\n  \\ Generating, promoting, or furthering fraud or the creation or promotion of disinformation\\n\\\n  \\    15. Generating, promoting, or furthering defamatory content, including the\\\n  \\ creation of defamatory statements, images, or other content\\n    16. Generating,\\\n  \\ promoting, or further distributing spam\\n    17. Impersonating another individual\\\n  \\ without consent, authorization, or legal right\\n    18. Representing that the\\\n  \\ use of Llama 3.2 or outputs are human-generated\\n    19. Generating or facilitating\\\n  \\ false online engagement, including fake reviews and other means of fake online\\\n  \\ engagement \\n4. Fail to appropriately disclose to end users any known dangers\\\n  \\ of your AI system 5. Interact with third party tools, models, or software designed\\\n  \\ to generate unlawful content or engage in unlawful or harmful conduct and/or represent\\\n  \\ that the outputs of such tools, models, or software are associated with Meta or\\\n  \\ Llama 3.2\\n\\nWith respect to any multimodal models included in Llama 3.2, the\\\n  \\ rights granted under Section 1(a) of the Llama 3.2 Community License Agreement\\\n  \\ are not being granted to you if you are an individual domiciled in, or a company\\\n  \\ with a principal place of business in, the European Union. This restriction does\\\n  \\ not apply to end users of a product or service that incorporates any such multimodal\\\n  \\ models.\\n\\nPlease report any violation of this Policy, software “bug,” or other\\\n  \\ problems that could lead to a violation of this Policy through one of the following\\\n  \\ means:\\n\\n* Reporting issues with the model: [https://github.com/meta-llama/llama-models/issues](https://l.workplace.com/l.php?u=https%3A%2F%2Fgithub.com%2Fmeta-llama%2Fllama-models%2Fissues&h=AT0qV8W9BFT6NwihiOHRuKYQM_UnkzN_NmHMy91OT55gkLpgi4kQupHUl0ssR4dQsIQ8n3tfd0vtkobvsEvt1l4Ic6GXI2EeuHV8N08OG2WnbAmm0FL4ObkazC6G_256vN0lN9DsykCvCqGZ)\\n\\\n  * Reporting risky content generated by the model: [developers.facebook.com/llama_output_feedback](http://developers.facebook.com/llama_output_feedback)\\n\\\n  * Reporting bugs and security concerns: [facebook.com/whitehat/info](http://facebook.com/whitehat/info)\\n\\\n  * Reporting violations of the Acceptable Use Policy or unlicensed uses of Llama\\\n  \\ 3.2: LlamaUseReport@meta.com\"\nextra_gated_fields:\n  First Name: text\n  Last Name: text\n  Date of birth: date_picker\n  Country: country\n  Affiliation: text\n  Job title:\n    type: select\n    options:\n    - Student\n    - Research Graduate\n    - AI researcher\n    - AI developer/engineer\n    - Reporter\n    - Other\n  geo: ip_location\n  ? By clicking Submit below I accept the terms of the license and acknowledge that\n    the information I provide will be collected stored processed and shared in accordance\n    with the Meta Privacy Policy\n  : checkbox\nextra_gated_description: The information you provide will be collected, stored, processed\n  and shared in accordance with the [Meta Privacy Policy](https://www.facebook.com/privacy/policy/).\nextra_gated_button_content: Submit\n---\n\n## Model Information\n\nThe Llama 3.2 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction-tuned generative models in 1B and 3B sizes (text in/text out). The Llama 3.2 instruction-tuned text only models are optimized for multilingual dialogue use cases, including agentic retrieval and summarization tasks. They outperform many of the available open source and closed chat models on common industry benchmarks.\n\n**Model Developer:** Meta\n\n**Model Architecture:** Llama 3.2 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.\n\n|  | Training Data | Params | Input modalities | Output modalities | Context Length | GQA | Shared Embeddings | Token count | Knowledge cutoff |\n| :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- |\n| Llama 3.2 (text only)  | A new mix of publicly available online data. | 1B (1.23B) | Multilingual Text | Multilingual Text and code  | 128k | Yes | Yes | Up to 9T tokens | December 2023 |\n|  |  | 3B (3.21B) | Multilingual Text | Multilingual Text and code  |  |  |  |  |  |\n| Llama 3.2 Quantized (text only)  | A new mix of publicly available online data. | 1B (1.23B) | Multilingual Text | Multilingual Text and code  | 8k | Yes | Yes | Up to 9T tokens | December 2023 |\n|  |  | 3B (3.21B) | Multilingual Text | Multilingual Text and code |  |  |  |  |  |\n\n**Supported Languages:** English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai are officially supported. Llama 3.2 has been trained on a broader collection of languages than these 8 supported languages. Developers may fine-tune Llama 3.2 models for languages beyond these supported languages, provided they comply with the Llama 3.2 Community License and the Acceptable Use Policy. Developers are always expected to ensure that their deployments, including those that involve additional languages, are completed safely and responsibly.\n\n**Llama 3.2 Model Family:** Token counts refer to pretraining data only. All model versions use Grouped-Query Attention (GQA) for improved inference scalability.\n\n**Model Release Date:** Sept 25, 2024\n\n**Status:** This is a static model trained on an offline dataset. Future versions may be released that improve model capabilities and safety.\n\n**License:** Use of Llama 3.2 is governed by the [Llama 3.2 Community License](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/LICENSE) (a custom, commercial license agreement).\n\n**Feedback:** Instructions on how to provide feedback or comments on the model can be found in the Llama Models [README](https://github.com/meta-llama/llama-models/blob/main/README.md). For more technical information about generation parameters and recipes for how to use Llama 3.2 in applications, please go [here](https://github.com/meta-llama/llama-recipes).\n\n## Intended Use\n\n**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.\n\n**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.\n\n## How to use\n\nThis repository contains two versions of Llama-3.2-1B-Instruct, for use with transformers and with the original `llama` codebase.\n\n### Use with transformers\n\nStarting with `transformers >= 4.43.0` onward, you can run conversational inference using the Transformers `pipeline` abstraction or by leveraging the Auto classes with the `generate()` function.\n\nMake sure to update your transformers installation via `pip install --upgrade transformers`.\n\n```python\nimport torch\nfrom transformers import pipeline\n\nmodel_id = \"meta-llama/Llama-3.2-1B-Instruct\"\npipe = pipeline(\n    \"text-generation\",\n    model=model_id,\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\",\n)\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n    {\"role\": \"user\", \"content\": \"Who are you?\"},\n]\noutputs = pipe(\n    messages,\n    max_new_tokens=256,\n)\nprint(outputs[0][\"generated_text\"][-1])\n```\n\nNote: You can also find detailed recipes on how to use the model locally, with `torch.compile()`, assisted generations, quantised and more at [`huggingface-llama-recipes`](https://github.com/huggingface/huggingface-llama-recipes)\n\n### Use with `llama`\n\nPlease, follow the instructions in the [repository](https://github.com/meta-llama/llama)\n\nTo download Original checkpoints, see the example command below leveraging `huggingface-cli`:\n\n```\nhuggingface-cli download meta-llama/Llama-3.2-1B-Instruct --include \"original/*\" --local-dir Llama-3.2-1B-Instruct\n```\n\n## Hardware and Software\n\n**Training Factors:** We used custom training libraries, Meta's custom built GPU cluster, and production infrastructure for pretraining. Fine-tuning, quantization, annotation, and evaluation were also performed on production infrastructure.\n\n**Training Energy Use:** Training utilized a cumulative of **916k** GPU hours of computation on H100-80GB (TDP of 700W) type hardware, per the table below. Training time is the total GPU time required for training each model and power consumption is the peak power capacity per GPU device used, adjusted for power usage efficiency.\n\n**Training Greenhouse Gas Emissions:** Estimated total location-based greenhouse gas emissions were **240** tons CO2eq for training. Since 2020, Meta has maintained net zero greenhouse gas emissions in its global operations and matched 100% of its electricity use with renewable energy; therefore, the total market-based greenhouse gas emissions for training were 0 tons CO2eq.\n\n|  | Training Time (GPU hours) | Logit Generation Time (GPU Hours) | Training Power Consumption (W) | Training Location-Based Greenhouse Gas Emissions (tons CO2eq) | Training Market-Based Greenhouse Gas Emissions (tons CO2eq) |\n| :---- | :---: | ----- | :---: | :---: | :---: |\n| Llama 3.2 1B | 370k | \\- | 700 | 107 | 0 |\n| Llama 3.2 3B | 460k | \\- | 700 | 133 | 0 |\n| Llama 3.2 1B SpinQuant | 1.7 | 0 | 700 | *Negligible*\\*\\* | 0 |\n| Llama 3.2 3B SpinQuant | 2.4 | 0 | 700 | *Negligible*\\*\\* | 0 |\n| Llama 3.2 1B QLora | 1.3k | 0 | 700 | 0.381 | 0 |\n| Llama 3.2 3B QLora | 1.6k | 0 | 700 | 0.461 | 0 |\n| Total | 833k |         86k |  | 240 | 0 |\n\n\\*\\* The location-based CO2e emissions of Llama 3.2 1B SpinQuant and Llama 3.2 3B SpinQuant are less than 0.001 metric tonnes each. This is due to the minimal training GPU hours that are required.\n\nThe methodology used to determine training energy use and greenhouse gas emissions can be found [here](https://arxiv.org/pdf/2204.05149). Since Meta is openly releasing these models, the training energy use and greenhouse gas emissions will not be incurred by others.\n\n## Training Data\n\n**Overview:** Llama 3.2 was pretrained on up to 9 trillion tokens of data from publicly available sources. For the 1B and 3B Llama 3.2 models, we incorporated logits from the Llama 3.1 8B and 70B models into the pretraining stage of the model development, where outputs (logits) from these larger models were used as token-level targets. Knowledge distillation was used after pruning to recover performance. In post-training we used a similar recipe as Llama 3.1 and produced final chat models by doing several rounds of alignment on top of the pre-trained model. Each round involved Supervised Fine-Tuning (SFT), Rejection Sampling (RS), and Direct Preference Optimization (DPO).\n\n**Data Freshness:** The pretraining data has a cutoff of December 2023\\.\n\n## Quantization\n\n### Quantization Scheme\n\nWe designed the current quantization scheme with the [PyTorch’s ExecuTorch](https://github.com/pytorch/executorch) inference framework and Arm CPU backend in mind, taking into account metrics including model quality, prefill/decoding speed, and memory footprint. Our quantization scheme involves three parts:\n- All linear layers in all transformer blocks are quantized to a 4-bit groupwise scheme (with a group size of 32) for weights and 8-bit per-token dynamic quantization for activations.\n- The classification layer is quantized to 8-bit per-channel for weight and 8-bit per token dynamic quantization for activation.\n- Similar to classification layer, an 8-bit per channel quantization is used for embedding layer.\n\n\n### Quantization-Aware Training and LoRA\n\nThe quantization-aware training (QAT) with low-rank adaptation (LoRA) models went through only post-training stages, using the same data as the full precision models. To initialize QAT, we utilize BF16 Llama 3.2 model checkpoints obtained after supervised fine-tuning (SFT) and perform an additional full round of SFT training with QAT. We then freeze the backbone of the QAT model and perform another round of SFT with LoRA adaptors applied to all layers within the transformer block. Meanwhile, the LoRA adaptors' weights and activations are maintained in BF16. Because our approach is similar to QLoRA of Dettmers et al., (2023) (i.e., quantization followed by LoRA adapters), we refer this method as QLoRA. Finally, we fine-tune the resulting model (both backbone and LoRA adaptors) using direct preference optimization (DPO).\n\n### SpinQuant\n\n[SpinQuant](https://arxiv.org/abs/2405.16406) was applied, together with generative post-training quantization (GPTQ). For the SpinQuant rotation matrix fine-tuning, we optimized for 100 iterations, using 800 samples with sequence-length 2048 from the WikiText 2 dataset. For GPTQ, we used 128 samples from the same dataset with the same sequence-length.\n\n## Benchmarks \\- English Text\n\nIn this section, we report the results for Llama 3.2 models on standard automatic benchmarks. For all these evaluations, we used our internal evaluations library.\n\n### Base Pretrained Models\n\n| Category | Benchmark | \\# Shots | Metric | Llama 3.2 1B | Llama 3.2 3B | Llama 3.1 8B |\n| ----- | ----- | :---: | :---: | :---: | :---: | :---: |\n| General | MMLU | 5 | macro\\_avg/acc\\_char | 32.2 | 58 | 66.7 |\n|  | AGIEval English | 3-5 | average/acc\\_char | 23.3 | 39.2 | 47.8 |\n|  | ARC-Challenge | 25 | acc\\_char | 32.8 | 69.1 | 79.7 |\n| Reading comprehension | SQuAD | 1 | em | 49.2 | 67.7 | 77 |\n|  | QuAC (F1) | 1 | f1 | 37.9 | 42.9 | 44.9 |\n|  | DROP (F1) | 3 | f1 | 28.0 | 45.2 | 59.5 |\n| Long Context | Needle in Haystack | 0 | em | 96.8 | 1 | 1 |\n\n### Instruction Tuned Models\n\n| Capability |  | Benchmark | \\# Shots | Metric | Llama 3.2 1B bf16 | Llama 3.2 1B Vanilla PTQ\\*\\* | Llama 3.2 1B Spin Quant | Llama 3.2 1B QLoRA | Llama 3.2 3B bf16 | Llama 3.2 3B Vanilla PTQ\\*\\* | Llama 3.2 3B Spin Quant | Llama 3.2 3B QLoRA | Llama 3.1 8B |\n| :---: | ----- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| General |  | MMLU | 5 | macro\\_avg/acc | 49.3 | 43.3 | 47.3 | 49.0 | 63.4 | 60.5 | 62 | 62.4 | 69.4 |\n| Re-writing |  | Open-rewrite eval | 0 | micro\\_avg/rougeL | 41.6 | 39.2 | 40.9 | 41.2 | 40.1 | 40.3 | 40.8 | 40.7 | 40.9 |\n| Summarization |  | TLDR9+ (test) | 1 | rougeL | 16.8 | 14.9 | 16.7 | 16.8 | 19.0 | 19.1 | 19.2 | 19.1 | 17.2 |\n| Instruction following |  | IFEval | 0 | Avg(Prompt/Instruction acc Loose/Strict) | 59.5 | 51.5 | 58.4 | 55.6 | 77.4 | 73.9 | 73.5 | 75.9 | 80.4 |\n| Math |  | GSM8K (CoT) | 8 | em\\_maj1@1 | 44.4 | 33.1 | 40.6 | 46.5 | 77.7 | 72.9 | 75.7 | 77.9 | 84.5 |\n|  |  | MATH (CoT) | 0 | final\\_em | 30.6 | 20.5 | 25.3 | 31.0 | 48.0 | 44.2 | 45.3 | 49.2 | 51.9 |\n| Reasoning |  | ARC-C | 0 | acc | 59.4 | 54.3 | 57 | 60.7 | 78.6 | 75.6 | 77.6 | 77.6 | 83.4 |\n|  |  | GPQA | 0 | acc | 27.2 | 25.9 | 26.3 | 25.9 | 32.8 | 32.8 | 31.7 | 33.9 | 32.8 |\n|  |  | Hellaswag | 0 | acc | 41.2 | 38.1 | 41.3 | 41.5 | 69.8 | 66.3 | 68 | 66.3 | 78.7 |\n| Tool Use |  | BFCL V2 | 0 | acc | 25.7 | 14.3 | 15.9 | 23.7 | 67.0 | 53.4 | 60.1 | 63.5 | 67.1 |\n|  |  | Nexus | 0 | macro\\_avg/acc | 13.5 | 5.2 | 9.6 | 12.5 | 34.3 | 32.4 | 31.5 | 30.1 | 38.5 |\n| Long Context |  | InfiniteBench/En.QA | 0 | longbook\\_qa/f1 | 20.3 | N/A | N/A | N/A | 19.8 | N/A | N/A | N/A | 27.3 |\n|  |  | InfiniteBench/En.MC | 0 | longbook\\_choice/acc | 38.0 | N/A | N/A | N/A | 63.3 | N/A | N/A | N/A | 72.2 |\n|  |  | NIH/Multi-needle | 0 | recall | 75.0 | N/A | N/A | N/A | 84.7 | N/A | N/A | N/A | 98.8 |\n| Multilingual |  | MGSM (CoT) | 0 | em | 24.5 | 13.7 | 18.2 | 24.4 | 58.2 | 48.9 | 54.3 | 56.8 | 68.9 |\n\n\\*\\*for comparison purposes only. Model not released.\n\n### Multilingual Benchmarks\n\n| Category | Benchmark | Language | Llama 3.2 1B | Llama 3.2 1B Vanilla PTQ\\*\\* | Llama 3.2 1B Spin Quant | Llama 3.2 1B QLoRA | Llama 3.2 3B | Llama 3.2 3B Vanilla PTQ\\*\\* | Llama 3.2 3B Spin Quant | Llama 3.2 3B QLoRA | Llama 3.1 8B |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| General | MMLU (5-shot, macro_avg/acc) | Portuguese | 39.8 | 34.9 | 38.9 | 40.2 | 54.5 | 50.9 | 53.3 | 53.4 | 62.1 |\n| | | Spanish | 41.5 | 36.0 | 39.8 | 41.8 | 55.1 | 51.9 | 53.6 | 53.6 | 62.5 |\n| | | Italian | 39.8 | 34.9 | 38.1 | 40.6 | 53.8 | 49.9 | 52.1 | 51.7 | 61.6 |\n| | | German | 39.2 | 34.9 | 37.5 | 39.6 | 53.3 | 50.0 | 52.2 | 51.3 | 60.6 |\n| | | French | 40.5 | 34.8 | 39.2 | 40.8 | 54.6 | 51.2 | 53.3 | 53.3 | 62.3 |\n| | | Hindi | 33.5 | 30.0 | 32.1 | 34.0 | 43.3 | 40.4 | 42.0 | 42.1 | 50.9 |\n| | | Thai | 34.7 | 31.2 | 32.4 | 34.9 | 44.5 | 41.3 | 44.0 | 42.2 | 50.3 |\n\n\\*\\*for comparison purposes only. Model not released.\n\n## Inference time\n\nIn the below table, we compare the performance metrics of different quantization methods (SpinQuant and QAT \\+ LoRA) with the BF16 baseline. The evaluation was done using the [ExecuTorch](https://github.com/pytorch/executorch) framework as the inference engine, with the ARM CPU as a backend using Android OnePlus 12 device.\n\n| Category | Decode (tokens/sec)  | Time-to-first-token (sec) | Prefill (tokens/sec) | Model size (PTE file size in MB) | Memory size (RSS in MB) |\n| :---- | ----- | ----- | ----- | ----- | ----- |\n| 1B BF16 (baseline) | 19.2 | 1.0 | 60.3 | 2358 | 3,185 |\n| 1B SpinQuant | 50.2 (2.6x) | 0.3 (-76.9%) | 260.5 (4.3x) | 1083 (-54.1%) | 1,921 (-39.7%) |\n| 1B QLoRA | 45.8 (2.4x) | 0.3 (-76.0%) | 252.0 (4.2x) | 1127 (-52.2%) | 2,255 (-29.2%) |\n| 3B BF16 (baseline) | 7.6 | 3.0 | 21.2 | 6129 | 7,419 |\n| 3B SpinQuant | 19.7 (2.6x) | 0.7 (-76.4%) | 89.7 (4.2x) | 2435 (-60.3%) | 3,726 (-49.8%) |\n| 3B QLoRA | 18.5 (2.4x) | 0.7 (-76.1%) | 88.8 (4.2x) | 2529 (-58.7%) | 4,060 (-45.3%) |\n\n(\\*) The performance measurement is done using an adb binary-based approach.\n(\\*\\*) It is measured on an Android OnePlus 12 device.\n(\\*\\*\\*) Time-to-first-token (TTFT)  is measured with prompt length=64\n\n*Footnote:*\n\n- *Decode (tokens/second) is for how quickly it keeps generating. Higher is better.*\n- *Time-to-first-token (TTFT for shorthand) is for how fast it generates the first token for a given prompt. Lower is better.*\n- *Prefill is the inverse of TTFT (aka 1/TTFT)  in tokens/second. Higher is better*\n- *Model size \\- how big is the model, measured by, PTE file, a binary file format for ExecuTorch*\n- *RSS size \\- Memory usage in resident set size (RSS)*\n\n## Responsibility & Safety\n\nAs part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:\n\n1. Enable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama\n2. Protect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm\n3. Provide protections for the community to help prevent the misuse of our models\n\n### Responsible Deployment\n\n**Approach:** Llama is a foundational technology designed to be used in a variety of use cases. Examples on how Meta’s Llama models have been responsibly deployed can be found in our [Community Stories webpage](https://llama.meta.com/community-stories/). Our approach is to build the most helpful models, enabling the world to benefit from the technology power, by aligning our model safety for generic use cases and addressing a standard set of harms. Developers are then in the driver’s seat to tailor safety for their use cases, defining their own policies and deploying the models with the necessary safeguards in their Llama systems. Llama 3.2 was developed following the best practices outlined in our [Responsible Use Guide](https://llama.meta.com/responsible-use-guide/).\n\n#### Llama 3.2 Instruct\n\n**Objective:** Our main objectives for conducting safety fine-tuning are to provide the research community with a valuable resource for studying the robustness of safety fine-tuning, as well as to offer developers a readily available, safe, and powerful model for various applications to reduce the developer workload to deploy safe AI systems. We implemented the same set of safety mitigations as in Llama 3, and you can learn more about these in the Llama 3 [paper](https://ai.meta.com/research/publications/the-llama-3-herd-of-models/).\n\n**Fine-Tuning Data:** We employ a multi-faceted approach to data collection, combining human-generated data from our vendors with synthetic data to mitigate potential safety risks. We’ve developed many large language model (LLM)-based classifiers that enable us to thoughtfully select high-quality prompts and responses, enhancing data quality control.\n\n**Refusals and Tone:** Building on the work we started with Llama 3, we put a great emphasis on model refusals to benign prompts as well as refusal tone. We included both borderline and adversarial prompts in our safety data strategy, and modified our safety data responses to follow tone guidelines.\n\n#### Llama 3.2 Systems\n\n**Safety as a System:** Large language models, including Llama 3.2, **are not designed to be deployed in isolation** but instead should be deployed as part of an overall AI system with additional safety guardrails as required. Developers are expected to deploy system safeguards when building agentic systems. Safeguards are key to achieve the right helpfulness-safety alignment as well as mitigating safety and security risks inherent to the system and any integration of the model or system with external tools. As part of our responsible release approach, we provide the community with [safeguards](https://llama.meta.com/trust-and-safety/) that developers should deploy with Llama models or other LLMs, including Llama Guard, Prompt Guard and Code Shield. All our [reference implementations](https://github.com/meta-llama/llama-agentic-system) demos contain these safeguards by default so developers can benefit from system-level safety out-of-the-box.\n\n### New Capabilities and Use Cases\n\n**Technological Advancement:** Llama releases usually introduce new capabilities that require specific considerations in addition to the best practices that generally apply across all Generative AI use cases. For prior release capabilities also supported by Llama 3.2, see [Llama 3.1 Model Card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md), as the same considerations apply here as well.\n\n**Constrained Environments:** Llama 3.2 1B and 3B models are expected to be deployed in highly constrained environments, such as mobile devices. LLM Systems using smaller models will have a different alignment profile and safety/helpfulness tradeoff than more complex, larger systems. Developers should ensure the safety of their system meets the requirements of their use case. We recommend using lighter system safeguards for such use cases, like Llama Guard 3-1B or its mobile-optimized version.\n\n### Evaluations\n\n**Scaled Evaluations:** We built dedicated, adversarial evaluation datasets and evaluated systems composed of Llama models and Purple Llama safeguards to filter input prompt and output response. It is important to evaluate applications in context, and we recommend building dedicated evaluation dataset for your use case.\n\n**Red Teaming:** We conducted recurring red teaming exercises with the goal of discovering risks via adversarial prompting and we used the learnings to improve our benchmarks and safety tuning datasets. We partnered early with subject-matter experts in critical risk areas to understand the nature of these real-world harms and how such models may lead to unintended harm for society. Based on these conversations, we derived a set of adversarial goals for the red team to attempt to achieve, such as extracting harmful information or reprogramming the model to act in a potentially harmful capacity. The red team consisted of experts in cybersecurity, adversarial machine learning, responsible AI, and integrity in addition to multilingual content specialists with background in integrity issues in specific geographic markets.\n\n### Critical Risks\n\nIn addition to our safety work above, we took extra care on measuring and/or mitigating the following critical risk areas:\n\n**1\\. CBRNE (Chemical, Biological, Radiological, Nuclear, and Explosive Weapons):** Llama 3.2 1B and 3B models are smaller and less capable derivatives of Llama 3.1. For Llama 3.1 70B and 405B, to assess risks related to proliferation of chemical and biological weapons, we performed uplift testing designed to assess whether use of Llama 3.1 models could meaningfully increase the capabilities of malicious actors to plan or carry out attacks using these types of weapons and have determined that such testing also applies to the smaller 1B and 3B models.\n\n**2\\. Child Safety:** Child Safety risk assessments were conducted using a team of experts, to assess the model’s capability to produce outputs that could result in Child Safety risks and inform on any necessary and appropriate risk mitigations via fine tuning. We leveraged those expert red teaming sessions to expand the coverage of our evaluation benchmarks through Llama 3 model development. For Llama 3, we conducted new in-depth sessions using objective based methodologies to assess the model risks along multiple attack vectors including the additional languages Llama 3 is trained on. We also partnered with content specialists to perform red teaming exercises assessing potentially violating content while taking account of market specific nuances or experiences.\n\n**3\\. Cyber Attacks:** For Llama 3.1 405B, our cyber attack uplift study investigated whether LLMs can enhance human capabilities in hacking tasks, both in terms of skill level and speed.\nOur attack automation study focused on evaluating the capabilities of LLMs when used as autonomous agents in cyber offensive operations, specifically in the context of ransomware attacks. This evaluation was distinct from previous studies that considered LLMs as interactive assistants. The primary objective was to assess whether these models could effectively function as independent agents in executing complex cyber-attacks without human intervention. Because Llama 3.2’s 1B and 3B models are smaller and less capable models than Llama 3.1 405B, we broadly believe that the testing conducted for the 405B model also applies to Llama 3.2 models.\n\n### Community\n\n**Industry Partnerships:** Generative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership on AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Purple Llama tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to our [Github repository](https://github.com/meta-llama/PurpleLlama).\n\n**Grants:** We also set up the [Llama Impact Grants](https://llama.meta.com/llama-impact-grants/) program to identify and support the most compelling applications of Meta’s Llama model for societal benefit across three categories: education, climate and open innovation. The 20 finalists from the hundreds of applications can be found [here](https://llama.meta.com/llama-impact-grants/#finalists).\n\n**Reporting:** Finally, we put in place a set of resources including an [output reporting mechanism](https://developers.facebook.com/llama_output_feedback) and [bug bounty program](https://www.facebook.com/whitehat) to continuously improve the Llama technology with the help of the community.\n\n## Ethical Considerations and Limitations\n\n**Values:** The core values of Llama 3.2 are openness, inclusivity and helpfulness. It is meant to serve everyone, and to work for a wide range of use cases. It is thus designed to be accessible to people across many different backgrounds, experiences and perspectives. Llama 3.2 addresses users and their needs as they are, without insertion unnecessary judgment or normativity, while reflecting the understanding that even content that may appear problematic in some cases can serve valuable purposes in others. It respects the dignity and autonomy of all users, especially in terms of the values of free thought and expression that power innovation and progress.\n\n**Testing:** Llama 3.2 is a new technology, and like any new technology, there are risks associated with its use. Testing conducted to date has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, Llama 3.2’s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 3.2 models, developers should perform safety testing and tuning tailored to their specific applications of the model. Please refer to available resources including our [Responsible Use Guide](https://llama.meta.com/responsible-use-guide), [Trust and Safety](https://llama.meta.com/trust-and-safety/) solutions, and other [resources](https://llama.meta.com/docs/get-started/) to learn more about responsible development.\n",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModelForCausalLM",
      "custom_class": null,
      "pipeline_tag": "text-generation",
      "processor": "AutoTokenizer"
    },
    "safetensors": {
      "parameters": {
        "BF16": 1235814400
      },
      "total": 1235814400
    },
    "model_index": null,
    "trending_score": null
  },
  {
    "model_id": "trl-internal-testing/tiny-Qwen2ForCausalLM-2.5",
    "model_name": "trl-internal-testing/tiny-Qwen2ForCausalLM-2.5",
    "author": "trl-internal-testing",
    "downloads": 2844634,
    "downloads_all_time": null,
    "likes": 0,
    "tags": [
      "transformers",
      "safetensors",
      "qwen2",
      "text-generation",
      "trl",
      "conversational",
      "autotrain_compatible",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/trl-internal-testing/tiny-Qwen2ForCausalLM-2.5",
    "dependencies": [
      [
        "transformers",
        null
      ],
      [
        "trl",
        null
      ]
    ],
    "last_modified": "2024-11-25T15:06:18+00:00",
    "created_at": "2024-11-25T15:06:15+00:00",
    "analysis_date": "2025-03-22T00:41:27.969359",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "qwen2",
    "pipeline_tag": "text-generation",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "library_name": "transformers",
      "tags": [
        "trl"
      ]
    },
    "card_text": "\n# Tiny Qwen2ForCausalLM\n\nThis is a minimal model built for unit tests in the [TRL](https://github.com/huggingface/trl) library.\n",
    "card_content": "---\nlibrary_name: transformers\ntags:\n- trl\n---\n\n# Tiny Qwen2ForCausalLM\n\nThis is a minimal model built for unit tests in the [TRL](https://github.com/huggingface/trl) library.\n",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModelForCausalLM",
      "custom_class": null,
      "pipeline_tag": "text-generation",
      "processor": "AutoTokenizer"
    },
    "safetensors": {
      "parameters": {
        "F32": 2428632
      },
      "total": 2428632
    },
    "model_index": null,
    "trending_score": null
  },
  {
    "model_id": "meta-llama/Llama-3.2-3B-Instruct",
    "model_name": "meta-llama/Llama-3.2-3B-Instruct",
    "author": "meta-llama",
    "downloads": 2842629,
    "downloads_all_time": null,
    "likes": 1256,
    "tags": [
      "transformers",
      "safetensors",
      "llama",
      "text-generation",
      "facebook",
      "meta",
      "pytorch",
      "llama-3",
      "conversational",
      "en",
      "de",
      "fr",
      "it",
      "pt",
      "hi",
      "es",
      "th",
      "arxiv:2204.05149",
      "arxiv:2405.16406",
      "license:llama3.2",
      "autotrain_compatible",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct",
    "dependencies": null,
    "last_modified": "2024-10-24T15:07:29+00:00",
    "created_at": "2024-09-18T15:19:20+00:00",
    "analysis_date": "2025-03-22T00:41:30.758108",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "llama",
    "pipeline_tag": "text-generation",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": [
        "en",
        "de",
        "fr",
        "it",
        "pt",
        "hi",
        "es",
        "th"
      ],
      "library_name": "transformers",
      "pipeline_tag": "text-generation",
      "tags": [
        "facebook",
        "meta",
        "pytorch",
        "llama",
        "llama-3"
      ],
      "license": "llama3.2",
      "extra_gated_prompt": "### LLAMA 3.2 COMMUNITY LICENSE AGREEMENT\n\nLlama 3.2 Version Release Date: September 25, 2024\n\n“Agreement” means the terms and conditions for use, reproduction, distribution  and modification of the Llama Materials set forth herein.\n\n“Documentation” means the specifications, manuals and documentation accompanying Llama 3.2 distributed by Meta at https://llama.meta.com/doc/overview.\n\n“Licensee” or “you” means you, or your employer or any other person or entity (if you are  entering into this Agreement on such person or entity’s behalf), of the age required under applicable laws, rules or regulations to provide legal consent and that has legal authority to bind your employer or such other person or entity if you are entering in this Agreement on their behalf.\n\n“Llama 3.2” means the foundational large language models and software and algorithms, including machine-learning model code, trained model weights, inference-enabling code, training-enabling code, fine-tuning enabling code and other elements of the foregoing distributed by Meta at  https://www.llama.com/llama-downloads.\n\n“Llama Materials” means, collectively, Meta’s proprietary Llama 3.2 and Documentation (and  any portion thereof) made available under this Agreement.\n\n“Meta” or “we” means Meta Platforms Ireland Limited (if you are located in or,  if you are an entity, your principal place of business is in the EEA or Switzerland)  and Meta Platforms, Inc. (if you are located outside of the EEA or Switzerland). \n\nBy clicking “I Accept” below or by using or distributing any portion or element of the Llama Materials, you agree to be bound by this Agreement.\n\n1. License Rights and Redistribution.\na. Grant of Rights. You are granted a non-exclusive, worldwide,  non-transferable and royalty-free limited license under Meta’s intellectual property or other rights  owned by Meta embodied in the Llama Materials to use, reproduce, distribute, copy, create derivative works  of, and make modifications to the Llama Materials.  \nb. Redistribution and Use.  \ni. If you distribute or make available the Llama Materials (or any derivative works thereof),  or a product or service (including another AI model) that contains any of them, you shall (A) provide a copy of this Agreement with any such Llama Materials; and (B) prominently display “Built with Llama” on a related website, user interface, blogpost, about page, or product documentation. If you use the Llama Materials or any outputs or results of the Llama Materials to create, train, fine tune, or otherwise improve an AI model, which is distributed or made available, you shall also include “Llama” at the beginning of any such AI model name.\nii. If you receive Llama Materials, or any derivative works thereof, from a Licensee as part of an integrated end user product, then Section 2 of this Agreement will not apply to you. \niii. You must retain in all copies of the Llama Materials that you distribute the  following attribution notice within a “Notice” text file distributed as a part of such copies:  “Llama 3.2 is licensed under the Llama 3.2 Community License, Copyright © Meta Platforms, Inc. All Rights Reserved.”\niv. Your use of the Llama Materials must comply with applicable laws and regulations (including trade compliance laws and regulations) and adhere to the Acceptable Use Policy for the Llama Materials (available at https://www.llama.com/llama3_2/use-policy), which is hereby  incorporated by reference into this Agreement.\n  \n2. Additional Commercial Terms. If, on the Llama 3.2 version release date, the monthly active users of the products or services made available by or for Licensee, or Licensee’s affiliates,  is greater than 700 million monthly active users in the preceding calendar month, you must request  a license from Meta, which Meta may grant to you in its sole discretion, and you are not authorized to exercise any of the rights under this Agreement unless or until Meta otherwise expressly grants you such rights.\n3. Disclaimer of Warranty. UNLESS REQUIRED BY APPLICABLE LAW, THE LLAMA MATERIALS AND ANY OUTPUT AND  RESULTS THEREFROM ARE PROVIDED ON AN “AS IS” BASIS, WITHOUT WARRANTIES OF ANY KIND, AND META DISCLAIMS ALL WARRANTIES OF ANY KIND, BOTH EXPRESS AND IMPLIED, INCLUDING, WITHOUT LIMITATION, ANY WARRANTIES OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE. YOU ARE SOLELY RESPONSIBLE FOR DETERMINING THE APPROPRIATENESS OF USING OR REDISTRIBUTING THE LLAMA MATERIALS AND ASSUME ANY RISKS ASSOCIATED WITH YOUR USE OF THE LLAMA MATERIALS AND ANY OUTPUT AND RESULTS.\n4. Limitation of Liability. IN NO EVENT WILL META OR ITS AFFILIATES BE LIABLE UNDER ANY THEORY OF LIABILITY,  WHETHER IN CONTRACT, TORT, NEGLIGENCE, PRODUCTS LIABILITY, OR OTHERWISE, ARISING OUT OF THIS AGREEMENT,  FOR ANY LOST PROFITS OR ANY INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL, EXEMPLARY OR PUNITIVE DAMAGES, EVEN  IF META OR ITS AFFILIATES HAVE BEEN ADVISED OF THE POSSIBILITY OF ANY OF THE FOREGOING.\n5. Intellectual Property.\na. No trademark licenses are granted under this Agreement, and in connection with the Llama Materials,  neither Meta nor Licensee may use any name or mark owned by or associated with the other or any of its affiliates,  except as required for reasonable and customary use in describing and redistributing the Llama Materials or as  set forth in this Section 5(a). Meta hereby grants you a license to use “Llama” (the “Mark”) solely as required  to comply with the last sentence of Section 1.b.i. You will comply with Meta’s brand guidelines (currently accessible  at https://about.meta.com/brand/resources/meta/company-brand/). All goodwill arising out of your use of the Mark  will inure to the benefit of Meta.\nb. Subject to Meta’s ownership of Llama Materials and derivatives made by or for Meta, with respect to any derivative works and modifications of the Llama Materials that are made by you, as between you and Meta, you are and will be the owner of such derivative works and modifications.\nc. If you institute litigation or other proceedings against Meta or any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Llama Materials or Llama 3.2 outputs or results, or any portion of any of the foregoing, constitutes infringement of intellectual property or other rights owned or licensable by you, then any licenses granted to you under this Agreement shall terminate as of the date such litigation or claim is filed or instituted. You will indemnify and hold harmless Meta from and against any claim by any third party arising out of or related to your use or distribution of the Llama Materials.\n6. Term and Termination. The term of this Agreement will commence upon your acceptance of this Agreement or access to the Llama Materials and will continue in full force and effect until terminated in accordance with the terms and conditions herein. Meta may terminate this Agreement if you are in breach of any term or condition of this Agreement. Upon termination of this Agreement, you shall delete and cease use of the Llama Materials. Sections 3, 4 and 7 shall survive the termination of this Agreement. \n7. Governing Law and Jurisdiction. This Agreement will be governed and construed under the laws of the State of  California without regard to choice of law principles, and the UN Convention on Contracts for the International Sale of Goods does not apply to this Agreement. The courts of California shall have exclusive jurisdiction of any dispute arising out of this Agreement. \n### Llama 3.2 Acceptable Use Policy\nMeta is committed to promoting safe and fair use of its tools and features, including Llama 3.2.  If you access or use Llama 3.2, you agree to this Acceptable Use Policy (“**Policy**”).  The most recent copy of this policy can be found at [https://www.llama.com/llama3_2/use-policy](https://www.llama.com/llama3_2/use-policy).\n#### Prohibited Uses\nWe want everyone to use Llama 3.2 safely and responsibly. You agree you will not use, or allow others to use, Llama 3.2 to:\n1. Violate the law or others’ rights, including to:\n    1. Engage in, promote, generate, contribute to, encourage, plan, incite, or further illegal or unlawful activity or content, such as:\n        1. Violence or terrorism\n        2. Exploitation or harm to children, including the solicitation, creation, acquisition, or dissemination of child exploitative content or failure to report Child Sexual Abuse Material\n        3. Human trafficking, exploitation, and sexual violence\n        4. The illegal distribution of information or materials to minors, including obscene materials, or failure to employ legally required age-gating in connection with such information or materials.\n        5. Sexual solicitation\n        6. Any other criminal activity\n    1. Engage in, promote, incite, or facilitate the harassment, abuse, threatening, or bullying of individuals or groups of individuals\n    2. Engage in, promote, incite, or facilitate discrimination or other unlawful or harmful conduct in the provision of employment, employment benefits, credit, housing, other economic benefits, or other essential goods and services\n    3. Engage in the unauthorized or unlicensed practice of any profession including, but not limited to, financial, legal, medical/health, or related professional practices\n    4. Collect, process, disclose, generate, or infer private or sensitive information about individuals, including information about individuals’ identity, health, or demographic information, unless you have obtained the right to do so in accordance with applicable law\n    5. Engage in or facilitate any action or generate any content that infringes, misappropriates, or otherwise violates any third-party rights, including the outputs or results of any products or services using the Llama Materials\n    6. Create, generate, or facilitate the creation of malicious code, malware, computer viruses or do anything else that could disable, overburden, interfere with or impair the proper working, integrity, operation or appearance of a website or computer system\n    7. Engage in any action, or facilitate any action, to intentionally circumvent or remove usage restrictions or other safety measures, or to enable functionality disabled by Meta \n2. Engage in, promote, incite, facilitate, or assist in the planning or development of activities that present a risk of death or bodily harm to individuals, including use of Llama 3.2 related to the following:\n    8. Military, warfare, nuclear industries or applications, espionage, use for materials or activities that are subject to the International Traffic Arms Regulations (ITAR) maintained by the United States Department of State or to the U.S. Biological Weapons Anti-Terrorism Act of 1989 or the Chemical Weapons Convention Implementation Act of 1997\n    9. Guns and illegal weapons (including weapon development)\n    10. Illegal drugs and regulated/controlled substances\n    11. Operation of critical infrastructure, transportation technologies, or heavy machinery\n    12. Self-harm or harm to others, including suicide, cutting, and eating disorders\n    13. Any content intended to incite or promote violence, abuse, or any infliction of bodily harm to an individual\n3. Intentionally deceive or mislead others, including use of Llama 3.2 related to the following:\n    14. Generating, promoting, or furthering fraud or the creation or promotion of disinformation\n    15. Generating, promoting, or furthering defamatory content, including the creation of defamatory statements, images, or other content\n    16. Generating, promoting, or further distributing spam\n    17. Impersonating another individual without consent, authorization, or legal right\n    18. Representing that the use of Llama 3.2 or outputs are human-generated\n    19. Generating or facilitating false online engagement, including fake reviews and other means of fake online engagement \n4. Fail to appropriately disclose to end users any known dangers of your AI system 5. Interact with third party tools, models, or software designed to generate unlawful content or engage in unlawful or harmful conduct and/or represent that the outputs of such tools, models, or software are associated with Meta or Llama 3.2\n\nWith respect to any multimodal models included in Llama 3.2, the rights granted under Section 1(a) of the Llama 3.2 Community License Agreement are not being granted to you if you are an individual domiciled in, or a company with a principal place of business in, the European Union. This restriction does not apply to end users of a product or service that incorporates any such multimodal models.\n\nPlease report any violation of this Policy, software “bug,” or other problems that could lead to a violation of this Policy through one of the following means:\n\n* Reporting issues with the model: [https://github.com/meta-llama/llama-models/issues](https://l.workplace.com/l.php?u=https%3A%2F%2Fgithub.com%2Fmeta-llama%2Fllama-models%2Fissues&h=AT0qV8W9BFT6NwihiOHRuKYQM_UnkzN_NmHMy91OT55gkLpgi4kQupHUl0ssR4dQsIQ8n3tfd0vtkobvsEvt1l4Ic6GXI2EeuHV8N08OG2WnbAmm0FL4ObkazC6G_256vN0lN9DsykCvCqGZ)\n* Reporting risky content generated by the model: [developers.facebook.com/llama_output_feedback](http://developers.facebook.com/llama_output_feedback)\n* Reporting bugs and security concerns: [facebook.com/whitehat/info](http://facebook.com/whitehat/info)\n* Reporting violations of the Acceptable Use Policy or unlicensed uses of Llama 3.2: LlamaUseReport@meta.com",
      "extra_gated_fields": {
        "First Name": "text",
        "Last Name": "text",
        "Date of birth": "date_picker",
        "Country": "country",
        "Affiliation": "text",
        "Job title": {
          "type": "select",
          "options": [
            "Student",
            "Research Graduate",
            "AI researcher",
            "AI developer/engineer",
            "Reporter",
            "Other"
          ]
        },
        "geo": "ip_location",
        "By clicking Submit below I accept the terms of the license and acknowledge that the information I provide will be collected stored processed and shared in accordance with the Meta Privacy Policy": "checkbox"
      },
      "extra_gated_description": "The information you provide will be collected, stored, processed and shared in accordance with the [Meta Privacy Policy](https://www.facebook.com/privacy/policy/).",
      "extra_gated_button_content": "Submit"
    },
    "card_text": "\n## Model Information\n\nThe Llama 3.2 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction-tuned generative models in 1B and 3B sizes (text in/text out). The Llama 3.2 instruction-tuned text only models are optimized for multilingual dialogue use cases, including agentic retrieval and summarization tasks. They outperform many of the available open source and closed chat models on common industry benchmarks.\n\n**Model Developer:** Meta\n\n**Model Architecture:** Llama 3.2 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.\n\n|  | Training Data | Params | Input modalities | Output modalities | Context Length | GQA | Shared Embeddings | Token count | Knowledge cutoff |\n| :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- |\n| Llama 3.2 (text only)  | A new mix of publicly available online data. | 1B (1.23B) | Multilingual Text | Multilingual Text and code  | 128k | Yes | Yes | Up to 9T tokens | December 2023 |\n|  |  | 3B (3.21B) | Multilingual Text | Multilingual Text and code  |  |  |  |  |  |\n| Llama 3.2 Quantized (text only)  | A new mix of publicly available online data. | 1B (1.23B) | Multilingual Text | Multilingual Text and code  | 8k | Yes | Yes | Up to 9T tokens | December 2023 |\n|  |  | 3B (3.21B) | Multilingual Text | Multilingual Text and code |  |  |  |  |  |\n\n**Supported Languages:** English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai are officially supported. Llama 3.2 has been trained on a broader collection of languages than these 8 supported languages. Developers may fine-tune Llama 3.2 models for languages beyond these supported languages, provided they comply with the Llama 3.2 Community License and the Acceptable Use Policy. Developers are always expected to ensure that their deployments, including those that involve additional languages, are completed safely and responsibly.\n\n**Llama 3.2 Model Family:** Token counts refer to pretraining data only. All model versions use Grouped-Query Attention (GQA) for improved inference scalability.\n\n**Model Release Date:** Sept 25, 2024\n\n**Status:** This is a static model trained on an offline dataset. Future versions may be released that improve model capabilities and safety.\n\n**License:** Use of Llama 3.2 is governed by the [Llama 3.2 Community License](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/LICENSE) (a custom, commercial license agreement).\n\n**Feedback:** Instructions on how to provide feedback or comments on the model can be found in the Llama Models [README](https://github.com/meta-llama/llama-models/blob/main/README.md). For more technical information about generation parameters and recipes for how to use Llama 3.2 in applications, please go [here](https://github.com/meta-llama/llama-recipes).\n\n## Intended Use\n\n**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.\n\n**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.\n\n## How to use\n\nThis repository contains two versions of Llama-3.2-3B-Instruct, for use with `transformers` and with the original `llama` codebase.\n\n### Use with transformers\n\nStarting with `transformers >= 4.43.0` onward, you can run conversational inference using the Transformers `pipeline` abstraction or by leveraging the Auto classes with the `generate()` function.\n\nMake sure to update your transformers installation via `pip install --upgrade transformers`.\n\n```python\nimport torch\nfrom transformers import pipeline\n\nmodel_id = \"meta-llama/Llama-3.2-3B-Instruct\"\npipe = pipeline(\n    \"text-generation\",\n    model=model_id,\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\",\n)\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n    {\"role\": \"user\", \"content\": \"Who are you?\"},\n]\noutputs = pipe(\n    messages,\n    max_new_tokens=256,\n)\nprint(outputs[0][\"generated_text\"][-1])\n```\n\nNote: You can also find detailed recipes on how to use the model locally, with `torch.compile()`, assisted generations, quantised and more at [`huggingface-llama-recipes`](https://github.com/huggingface/huggingface-llama-recipes)\n\n### Use with `llama`\n\nPlease, follow the instructions in the [repository](https://github.com/meta-llama/llama)\n\nTo download Original checkpoints, see the example command below leveraging `huggingface-cli`:\n\n```\nhuggingface-cli download meta-llama/Llama-3.2-3B-Instruct --include \"original/*\" --local-dir Llama-3.2-3B-Instruct\n```\n\n## Hardware and Software\n\n**Training Factors:** We used custom training libraries, Meta's custom built GPU cluster, and production infrastructure for pretraining. Fine-tuning, quantization, annotation, and evaluation were also performed on production infrastructure.\n\n**Training Energy Use:** Training utilized a cumulative of **916k** GPU hours of computation on H100-80GB (TDP of 700W) type hardware, per the table below. Training time is the total GPU time required for training each model and power consumption is the peak power capacity per GPU device used, adjusted for power usage efficiency.\n\n**Training Greenhouse Gas Emissions:** Estimated total location-based greenhouse gas emissions were **240** tons CO2eq for training. Since 2020, Meta has maintained net zero greenhouse gas emissions in its global operations and matched 100% of its electricity use with renewable energy; therefore, the total market-based greenhouse gas emissions for training were 0 tons CO2eq.\n\n|  | Training Time (GPU hours) | Logit Generation Time (GPU Hours) | Training Power Consumption (W) | Training Location-Based Greenhouse Gas Emissions (tons CO2eq) | Training Market-Based Greenhouse Gas Emissions (tons CO2eq) |\n| :---- | :---: | ----- | :---: | :---: | :---: |\n| Llama 3.2 1B | 370k | \\- | 700 | 107 | 0 |\n| Llama 3.2 3B | 460k | \\- | 700 | 133 | 0 |\n| Llama 3.2 1B SpinQuant | 1.7 | 0 | 700 | *Negligible*\\*\\* | 0 |\n| Llama 3.2 3B SpinQuant | 2.4 | 0 | 700 | *Negligible*\\*\\* | 0 |\n| Llama 3.2 1B QLora | 1.3k | 0 | 700 | 0.381 | 0 |\n| Llama 3.2 3B QLora | 1.6k | 0 | 700 | 0.461 | 0 |\n| Total | 833k |         86k |  | 240 | 0 |\n\n\\*\\* The location-based CO2e emissions of Llama 3.2 1B SpinQuant and Llama 3.2 3B SpinQuant are less than 0.001 metric tonnes each. This is due to the minimal training GPU hours that are required.\n\nThe methodology used to determine training energy use and greenhouse gas emissions can be found [here](https://arxiv.org/pdf/2204.05149). Since Meta is openly releasing these models, the training energy use and greenhouse gas emissions will not be incurred by others.\n\n## Training Data\n\n**Overview:** Llama 3.2 was pretrained on up to 9 trillion tokens of data from publicly available sources. For the 1B and 3B Llama 3.2 models, we incorporated logits from the Llama 3.1 8B and 70B models into the pretraining stage of the model development, where outputs (logits) from these larger models were used as token-level targets. Knowledge distillation was used after pruning to recover performance. In post-training we used a similar recipe as Llama 3.1 and produced final chat models by doing several rounds of alignment on top of the pre-trained model. Each round involved Supervised Fine-Tuning (SFT), Rejection Sampling (RS), and Direct Preference Optimization (DPO).\n\n**Data Freshness:** The pretraining data has a cutoff of December 2023\\.\n\n## Quantization\n\n### Quantization Scheme\n\nWe designed the current quantization scheme with the [PyTorch’s ExecuTorch](https://github.com/pytorch/executorch) inference framework and Arm CPU backend in mind, taking into account metrics including model quality, prefill/decoding speed, and memory footprint. Our quantization scheme involves three parts:\n- All linear layers in all transformer blocks are quantized to a 4-bit groupwise scheme (with a group size of 32) for weights and 8-bit per-token dynamic quantization for activations.\n- The classification layer is quantized to 8-bit per-channel for weight and 8-bit per token dynamic quantization for activation.\n- Similar to classification layer, an 8-bit per channel quantization is used for embedding layer.\n\n\n### Quantization-Aware Training and LoRA\n\nThe quantization-aware training (QAT) with low-rank adaptation (LoRA) models went through only post-training stages, using the same data as the full precision models. To initialize QAT, we utilize BF16 Llama 3.2 model checkpoints obtained after supervised fine-tuning (SFT) and perform an additional full round of SFT training with QAT. We then freeze the backbone of the QAT model and perform another round of SFT with LoRA adaptors applied to all layers within the transformer block. Meanwhile, the LoRA adaptors' weights and activations are maintained in BF16. Because our approach is similar to QLoRA of Dettmers et al., (2023) (i.e., quantization followed by LoRA adapters), we refer this method as QLoRA. Finally, we fine-tune the resulting model (both backbone and LoRA adaptors) using direct preference optimization (DPO).\n\n### SpinQuant\n\n[SpinQuant](https://arxiv.org/abs/2405.16406) was applied, together with generative post-training quantization (GPTQ). For the SpinQuant rotation matrix fine-tuning, we optimized for 100 iterations, using 800 samples with sequence-length 2048 from the WikiText 2 dataset. For GPTQ, we used 128 samples from the same dataset with the same sequence-length.\n\n## Benchmarks \\- English Text\n\nIn this section, we report the results for Llama 3.2 models on standard automatic benchmarks. For all these evaluations, we used our internal evaluations library.\n\n### Base Pretrained Models\n\n| Category | Benchmark | \\# Shots | Metric | Llama 3.2 1B | Llama 3.2 3B | Llama 3.1 8B |\n| ----- | ----- | :---: | :---: | :---: | :---: | :---: |\n| General | MMLU | 5 | macro\\_avg/acc\\_char | 32.2 | 58 | 66.7 |\n|  | AGIEval English | 3-5 | average/acc\\_char | 23.3 | 39.2 | 47.8 |\n|  | ARC-Challenge | 25 | acc\\_char | 32.8 | 69.1 | 79.7 |\n| Reading comprehension | SQuAD | 1 | em | 49.2 | 67.7 | 77 |\n|  | QuAC (F1) | 1 | f1 | 37.9 | 42.9 | 44.9 |\n|  | DROP (F1) | 3 | f1 | 28.0 | 45.2 | 59.5 |\n| Long Context | Needle in Haystack | 0 | em | 96.8 | 1 | 1 |\n\n### Instruction Tuned Models\n\n| Capability |  | Benchmark | \\# Shots | Metric | Llama 3.2 1B bf16 | Llama 3.2 1B Vanilla PTQ\\*\\* | Llama 3.2 1B Spin Quant | Llama 3.2 1B QLoRA | Llama 3.2 3B bf16 | Llama 3.2 3B Vanilla PTQ\\*\\* | Llama 3.2 3B Spin Quant | Llama 3.2 3B QLoRA | Llama 3.1 8B |\n| :---: | ----- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| General |  | MMLU | 5 | macro\\_avg/acc | 49.3 | 43.3 | 47.3 | 49.0 | 63.4 | 60.5 | 62 | 62.4 | 69.4 |\n| Re-writing |  | Open-rewrite eval | 0 | micro\\_avg/rougeL | 41.6 | 39.2 | 40.9 | 41.2 | 40.1 | 40.3 | 40.8 | 40.7 | 40.9 |\n| Summarization |  | TLDR9+ (test) | 1 | rougeL | 16.8 | 14.9 | 16.7 | 16.8 | 19.0 | 19.1 | 19.2 | 19.1 | 17.2 |\n| Instruction following |  | IFEval | 0 | Avg(Prompt/Instruction acc Loose/Strict) | 59.5 | 51.5 | 58.4 | 55.6 | 77.4 | 73.9 | 73.5 | 75.9 | 80.4 |\n| Math |  | GSM8K (CoT) | 8 | em\\_maj1@1 | 44.4 | 33.1 | 40.6 | 46.5 | 77.7 | 72.9 | 75.7 | 77.9 | 84.5 |\n|  |  | MATH (CoT) | 0 | final\\_em | 30.6 | 20.5 | 25.3 | 31.0 | 48.0 | 44.2 | 45.3 | 49.2 | 51.9 |\n| Reasoning |  | ARC-C | 0 | acc | 59.4 | 54.3 | 57 | 60.7 | 78.6 | 75.6 | 77.6 | 77.6 | 83.4 |\n|  |  | GPQA | 0 | acc | 27.2 | 25.9 | 26.3 | 25.9 | 32.8 | 32.8 | 31.7 | 33.9 | 32.8 |\n|  |  | Hellaswag | 0 | acc | 41.2 | 38.1 | 41.3 | 41.5 | 69.8 | 66.3 | 68 | 66.3 | 78.7 |\n| Tool Use |  | BFCL V2 | 0 | acc | 25.7 | 14.3 | 15.9 | 23.7 | 67.0 | 53.4 | 60.1 | 63.5 | 67.1 |\n|  |  | Nexus | 0 | macro\\_avg/acc | 13.5 | 5.2 | 9.6 | 12.5 | 34.3 | 32.4 | 31.5 | 30.1 | 38.5 |\n| Long Context |  | InfiniteBench/En.QA | 0 | longbook\\_qa/f1 | 20.3 | N/A | N/A | N/A | 19.8 | N/A | N/A | N/A | 27.3 |\n|  |  | InfiniteBench/En.MC | 0 | longbook\\_choice/acc | 38.0 | N/A | N/A | N/A | 63.3 | N/A | N/A | N/A | 72.2 |\n|  |  | NIH/Multi-needle | 0 | recall | 75.0 | N/A | N/A | N/A | 84.7 | N/A | N/A | N/A | 98.8 |\n| Multilingual |  | MGSM (CoT) | 0 | em | 24.5 | 13.7 | 18.2 | 24.4 | 58.2 | 48.9 | 54.3 | 56.8 | 68.9 |\n\n\\*\\*for comparison purposes only. Model not released.\n\n### Multilingual Benchmarks\n\n| Category | Benchmark | Language | Llama 3.2 1B | Llama 3.2 1B Vanilla PTQ\\*\\* | Llama 3.2 1B Spin Quant | Llama 3.2 1B QLoRA | Llama 3.2 3B | Llama 3.2 3B Vanilla PTQ\\*\\* | Llama 3.2 3B Spin Quant | Llama 3.2 3B QLoRA | Llama 3.1 8B |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| General | MMLU (5-shot, macro_avg/acc) | Portuguese | 39.8 | 34.9 | 38.9 | 40.2 | 54.5 | 50.9 | 53.3 | 53.4 | 62.1 |\n| | | Spanish | 41.5 | 36.0 | 39.8 | 41.8 | 55.1 | 51.9 | 53.6 | 53.6 | 62.5 |\n| | | Italian | 39.8 | 34.9 | 38.1 | 40.6 | 53.8 | 49.9 | 52.1 | 51.7 | 61.6 |\n| | | German | 39.2 | 34.9 | 37.5 | 39.6 | 53.3 | 50.0 | 52.2 | 51.3 | 60.6 |\n| | | French | 40.5 | 34.8 | 39.2 | 40.8 | 54.6 | 51.2 | 53.3 | 53.3 | 62.3 |\n| | | Hindi | 33.5 | 30.0 | 32.1 | 34.0 | 43.3 | 40.4 | 42.0 | 42.1 | 50.9 |\n| | | Thai | 34.7 | 31.2 | 32.4 | 34.9 | 44.5 | 41.3 | 44.0 | 42.2 | 50.3 |\n\n\\*\\*for comparison purposes only. Model not released.\n\n## Inference time\n\nIn the below table, we compare the performance metrics of different quantization methods (SpinQuant and QAT \\+ LoRA) with the BF16 baseline. The evaluation was done using the [ExecuTorch](https://github.com/pytorch/executorch) framework as the inference engine, with the ARM CPU as a backend using Android OnePlus 12 device.\n\n| Category | Decode (tokens/sec)  | Time-to-first-token (sec) | Prefill (tokens/sec) | Model size (PTE file size in MB) | Memory size (RSS in MB) |\n| :---- | ----- | ----- | ----- | ----- | ----- |\n| 1B BF16 (baseline) | 19.2 | 1.0 | 60.3 | 2358 | 3,185 |\n| 1B SpinQuant | 50.2 (2.6x) | 0.3 (-76.9%) | 260.5 (4.3x) | 1083 (-54.1%) | 1,921 (-39.7%) |\n| 1B QLoRA | 45.8 (2.4x) | 0.3 (-76.0%) | 252.0 (4.2x) | 1127 (-52.2%) | 2,255 (-29.2%) |\n| 3B BF16 (baseline) | 7.6 | 3.0 | 21.2 | 6129 | 7,419 |\n| 3B SpinQuant | 19.7 (2.6x) | 0.7 (-76.4%) | 89.7 (4.2x) | 2435 (-60.3%) | 3,726 (-49.8%) |\n| 3B QLoRA | 18.5 (2.4x) | 0.7 (-76.1%) | 88.8 (4.2x) | 2529 (-58.7%) | 4,060 (-45.3%) |\n\n(\\*) The performance measurement is done using an adb binary-based approach.\n(\\*\\*) It is measured on an Android OnePlus 12 device.\n(\\*\\*\\*) Time-to-first-token (TTFT)  is measured with prompt length=64\n\n*Footnote:*\n\n- *Decode (tokens/second) is for how quickly it keeps generating. Higher is better.*\n- *Time-to-first-token (TTFT for shorthand) is for how fast it generates the first token for a given prompt. Lower is better.*\n- *Prefill is the inverse of TTFT (aka 1/TTFT)  in tokens/second. Higher is better*\n- *Model size \\- how big is the model, measured by, PTE file, a binary file format for ExecuTorch*\n- *RSS size \\- Memory usage in resident set size (RSS)*\n\n## Responsibility & Safety\n\nAs part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:\n\n1. Enable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama\n2. Protect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm\n3. Provide protections for the community to help prevent the misuse of our models\n\n### Responsible Deployment\n\n**Approach:** Llama is a foundational technology designed to be used in a variety of use cases. Examples on how Meta’s Llama models have been responsibly deployed can be found in our [Community Stories webpage](https://llama.meta.com/community-stories/). Our approach is to build the most helpful models, enabling the world to benefit from the technology power, by aligning our model safety for generic use cases and addressing a standard set of harms. Developers are then in the driver’s seat to tailor safety for their use cases, defining their own policies and deploying the models with the necessary safeguards in their Llama systems. Llama 3.2 was developed following the best practices outlined in our [Responsible Use Guide](https://llama.meta.com/responsible-use-guide/).\n\n#### Llama 3.2 Instruct\n\n**Objective:** Our main objectives for conducting safety fine-tuning are to provide the research community with a valuable resource for studying the robustness of safety fine-tuning, as well as to offer developers a readily available, safe, and powerful model for various applications to reduce the developer workload to deploy safe AI systems. We implemented the same set of safety mitigations as in Llama 3, and you can learn more about these in the Llama 3 [paper](https://ai.meta.com/research/publications/the-llama-3-herd-of-models/).\n\n**Fine-Tuning Data:** We employ a multi-faceted approach to data collection, combining human-generated data from our vendors with synthetic data to mitigate potential safety risks. We’ve developed many large language model (LLM)-based classifiers that enable us to thoughtfully select high-quality prompts and responses, enhancing data quality control.\n\n**Refusals and Tone:** Building on the work we started with Llama 3, we put a great emphasis on model refusals to benign prompts as well as refusal tone. We included both borderline and adversarial prompts in our safety data strategy, and modified our safety data responses to follow tone guidelines.\n\n#### Llama 3.2 Systems\n\n**Safety as a System:** Large language models, including Llama 3.2, **are not designed to be deployed in isolation** but instead should be deployed as part of an overall AI system with additional safety guardrails as required. Developers are expected to deploy system safeguards when building agentic systems. Safeguards are key to achieve the right helpfulness-safety alignment as well as mitigating safety and security risks inherent to the system and any integration of the model or system with external tools. As part of our responsible release approach, we provide the community with [safeguards](https://llama.meta.com/trust-and-safety/) that developers should deploy with Llama models or other LLMs, including Llama Guard, Prompt Guard and Code Shield. All our [reference implementations](https://github.com/meta-llama/llama-agentic-system) demos contain these safeguards by default so developers can benefit from system-level safety out-of-the-box.\n\n### New Capabilities and Use Cases\n\n**Technological Advancement:** Llama releases usually introduce new capabilities that require specific considerations in addition to the best practices that generally apply across all Generative AI use cases. For prior release capabilities also supported by Llama 3.2, see [Llama 3.1 Model Card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md), as the same considerations apply here as well.\n\n**Constrained Environments:** Llama 3.2 1B and 3B models are expected to be deployed in highly constrained environments, such as mobile devices. LLM Systems using smaller models will have a different alignment profile and safety/helpfulness tradeoff than more complex, larger systems. Developers should ensure the safety of their system meets the requirements of their use case. We recommend using lighter system safeguards for such use cases, like Llama Guard 3-1B or its mobile-optimized version.\n\n### Evaluations\n\n**Scaled Evaluations:** We built dedicated, adversarial evaluation datasets and evaluated systems composed of Llama models and Purple Llama safeguards to filter input prompt and output response. It is important to evaluate applications in context, and we recommend building dedicated evaluation dataset for your use case.\n\n**Red Teaming:** We conducted recurring red teaming exercises with the goal of discovering risks via adversarial prompting and we used the learnings to improve our benchmarks and safety tuning datasets. We partnered early with subject-matter experts in critical risk areas to understand the nature of these real-world harms and how such models may lead to unintended harm for society. Based on these conversations, we derived a set of adversarial goals for the red team to attempt to achieve, such as extracting harmful information or reprogramming the model to act in a potentially harmful capacity. The red team consisted of experts in cybersecurity, adversarial machine learning, responsible AI, and integrity in addition to multilingual content specialists with background in integrity issues in specific geographic markets.\n\n### Critical Risks\n\nIn addition to our safety work above, we took extra care on measuring and/or mitigating the following critical risk areas:\n\n**1\\. CBRNE (Chemical, Biological, Radiological, Nuclear, and Explosive Weapons):** Llama 3.2 1B and 3B models are smaller and less capable derivatives of Llama 3.1. For Llama 3.1 70B and 405B, to assess risks related to proliferation of chemical and biological weapons, we performed uplift testing designed to assess whether use of Llama 3.1 models could meaningfully increase the capabilities of malicious actors to plan or carry out attacks using these types of weapons and have determined that such testing also applies to the smaller 1B and 3B models.\n\n**2\\. Child Safety:** Child Safety risk assessments were conducted using a team of experts, to assess the model’s capability to produce outputs that could result in Child Safety risks and inform on any necessary and appropriate risk mitigations via fine tuning. We leveraged those expert red teaming sessions to expand the coverage of our evaluation benchmarks through Llama 3 model development. For Llama 3, we conducted new in-depth sessions using objective based methodologies to assess the model risks along multiple attack vectors including the additional languages Llama 3 is trained on. We also partnered with content specialists to perform red teaming exercises assessing potentially violating content while taking account of market specific nuances or experiences.\n\n**3\\. Cyber Attacks:** For Llama 3.1 405B, our cyber attack uplift study investigated whether LLMs can enhance human capabilities in hacking tasks, both in terms of skill level and speed.\nOur attack automation study focused on evaluating the capabilities of LLMs when used as autonomous agents in cyber offensive operations, specifically in the context of ransomware attacks. This evaluation was distinct from previous studies that considered LLMs as interactive assistants. The primary objective was to assess whether these models could effectively function as independent agents in executing complex cyber-attacks without human intervention. Because Llama 3.2’s 1B and 3B models are smaller and less capable models than Llama 3.1 405B, we broadly believe that the testing conducted for the 405B model also applies to Llama 3.2 models.\n\n### Community\n\n**Industry Partnerships:** Generative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership on AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Purple Llama tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to our [Github repository](https://github.com/meta-llama/PurpleLlama).\n\n**Grants:** We also set up the [Llama Impact Grants](https://llama.meta.com/llama-impact-grants/) program to identify and support the most compelling applications of Meta’s Llama model for societal benefit across three categories: education, climate and open innovation. The 20 finalists from the hundreds of applications can be found [here](https://llama.meta.com/llama-impact-grants/#finalists).\n\n**Reporting:** Finally, we put in place a set of resources including an [output reporting mechanism](https://developers.facebook.com/llama_output_feedback) and [bug bounty program](https://www.facebook.com/whitehat) to continuously improve the Llama technology with the help of the community.\n\n## Ethical Considerations and Limitations\n\n**Values:** The core values of Llama 3.2 are openness, inclusivity and helpfulness. It is meant to serve everyone, and to work for a wide range of use cases. It is thus designed to be accessible to people across many different backgrounds, experiences and perspectives. Llama 3.2 addresses users and their needs as they are, without insertion unnecessary judgment or normativity, while reflecting the understanding that even content that may appear problematic in some cases can serve valuable purposes in others. It respects the dignity and autonomy of all users, especially in terms of the values of free thought and expression that power innovation and progress.\n\n**Testing:** Llama 3.2 is a new technology, and like any new technology, there are risks associated with its use. Testing conducted to date has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, Llama 3.2’s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 3.2 models, developers should perform safety testing and tuning tailored to their specific applications of the model. Please refer to available resources including our [Responsible Use Guide](https://llama.meta.com/responsible-use-guide), [Trust and Safety](https://llama.meta.com/trust-and-safety/) solutions, and other [resources](https://llama.meta.com/docs/get-started/) to learn more about responsible development.\n",
    "card_content": "---\nlanguage:\n- en\n- de\n- fr\n- it\n- pt\n- hi\n- es\n- th\nlibrary_name: transformers\npipeline_tag: text-generation\ntags:\n- facebook\n- meta\n- pytorch\n- llama\n- llama-3\nlicense: llama3.2\nextra_gated_prompt: \"### LLAMA 3.2 COMMUNITY LICENSE AGREEMENT\\n\\nLlama 3.2 Version\\\n  \\ Release Date: September 25, 2024\\n\\n“Agreement” means the terms and conditions\\\n  \\ for use, reproduction, distribution  and modification of the Llama Materials set\\\n  \\ forth herein.\\n\\n“Documentation” means the specifications, manuals and documentation\\\n  \\ accompanying Llama 3.2 distributed by Meta at https://llama.meta.com/doc/overview.\\n\\\n  \\n“Licensee” or “you” means you, or your employer or any other person or entity\\\n  \\ (if you are  entering into this Agreement on such person or entity’s behalf),\\\n  \\ of the age required under applicable laws, rules or regulations to provide legal\\\n  \\ consent and that has legal authority to bind your employer or such other person\\\n  \\ or entity if you are entering in this Agreement on their behalf.\\n\\n“Llama 3.2”\\\n  \\ means the foundational large language models and software and algorithms, including\\\n  \\ machine-learning model code, trained model weights, inference-enabling code, training-enabling\\\n  \\ code, fine-tuning enabling code and other elements of the foregoing distributed\\\n  \\ by Meta at  https://www.llama.com/llama-downloads.\\n\\n“Llama Materials” means,\\\n  \\ collectively, Meta’s proprietary Llama 3.2 and Documentation (and  any portion\\\n  \\ thereof) made available under this Agreement.\\n\\n“Meta” or “we” means Meta Platforms\\\n  \\ Ireland Limited (if you are located in or,  if you are an entity, your principal\\\n  \\ place of business is in the EEA or Switzerland)  and Meta Platforms, Inc. (if\\\n  \\ you are located outside of the EEA or Switzerland). \\n\\nBy clicking “I Accept”\\\n  \\ below or by using or distributing any portion or element of the Llama Materials,\\\n  \\ you agree to be bound by this Agreement.\\n\\n1. License Rights and Redistribution.\\n\\\n  a. Grant of Rights. You are granted a non-exclusive, worldwide,  non-transferable\\\n  \\ and royalty-free limited license under Meta’s intellectual property or other rights\\\n  \\  owned by Meta embodied in the Llama Materials to use, reproduce, distribute,\\\n  \\ copy, create derivative works  of, and make modifications to the Llama Materials.\\\n  \\  \\nb. Redistribution and Use.  \\ni. If you distribute or make available the Llama\\\n  \\ Materials (or any derivative works thereof),  or a product or service (including\\\n  \\ another AI model) that contains any of them, you shall (A) provide a copy of this\\\n  \\ Agreement with any such Llama Materials; and (B) prominently display “Built with\\\n  \\ Llama” on a related website, user interface, blogpost, about page, or product\\\n  \\ documentation. If you use the Llama Materials or any outputs or results of the\\\n  \\ Llama Materials to create, train, fine tune, or otherwise improve an AI model,\\\n  \\ which is distributed or made available, you shall also include “Llama” at the\\\n  \\ beginning of any such AI model name.\\nii. If you receive Llama Materials, or any\\\n  \\ derivative works thereof, from a Licensee as part of an integrated end user product,\\\n  \\ then Section 2 of this Agreement will not apply to you. \\niii. You must retain\\\n  \\ in all copies of the Llama Materials that you distribute the  following attribution\\\n  \\ notice within a “Notice” text file distributed as a part of such copies:  “Llama\\\n  \\ 3.2 is licensed under the Llama 3.2 Community License, Copyright © Meta Platforms,\\\n  \\ Inc. All Rights Reserved.”\\niv. Your use of the Llama Materials must comply with\\\n  \\ applicable laws and regulations (including trade compliance laws and regulations)\\\n  \\ and adhere to the Acceptable Use Policy for the Llama Materials (available at\\\n  \\ https://www.llama.com/llama3_2/use-policy), which is hereby  incorporated by reference\\\n  \\ into this Agreement.\\n  \\n2. Additional Commercial Terms. If, on the Llama 3.2\\\n  \\ version release date, the monthly active users of the products or services made\\\n  \\ available by or for Licensee, or Licensee’s affiliates,  is greater than 700 million\\\n  \\ monthly active users in the preceding calendar month, you must request  a license\\\n  \\ from Meta, which Meta may grant to you in its sole discretion, and you are not\\\n  \\ authorized to exercise any of the rights under this Agreement unless or until\\\n  \\ Meta otherwise expressly grants you such rights.\\n3. Disclaimer of Warranty. UNLESS\\\n  \\ REQUIRED BY APPLICABLE LAW, THE LLAMA MATERIALS AND ANY OUTPUT AND  RESULTS THEREFROM\\\n  \\ ARE PROVIDED ON AN “AS IS” BASIS, WITHOUT WARRANTIES OF ANY KIND, AND META DISCLAIMS\\\n  \\ ALL WARRANTIES OF ANY KIND, BOTH EXPRESS AND IMPLIED, INCLUDING, WITHOUT LIMITATION,\\\n  \\ ANY WARRANTIES OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR\\\n  \\ PURPOSE. YOU ARE SOLELY RESPONSIBLE FOR DETERMINING THE APPROPRIATENESS OF USING\\\n  \\ OR REDISTRIBUTING THE LLAMA MATERIALS AND ASSUME ANY RISKS ASSOCIATED WITH YOUR\\\n  \\ USE OF THE LLAMA MATERIALS AND ANY OUTPUT AND RESULTS.\\n4. Limitation of Liability.\\\n  \\ IN NO EVENT WILL META OR ITS AFFILIATES BE LIABLE UNDER ANY THEORY OF LIABILITY,\\\n  \\  WHETHER IN CONTRACT, TORT, NEGLIGENCE, PRODUCTS LIABILITY, OR OTHERWISE, ARISING\\\n  \\ OUT OF THIS AGREEMENT,  FOR ANY LOST PROFITS OR ANY INDIRECT, SPECIAL, CONSEQUENTIAL,\\\n  \\ INCIDENTAL, EXEMPLARY OR PUNITIVE DAMAGES, EVEN  IF META OR ITS AFFILIATES HAVE\\\n  \\ BEEN ADVISED OF THE POSSIBILITY OF ANY OF THE FOREGOING.\\n5. Intellectual Property.\\n\\\n  a. No trademark licenses are granted under this Agreement, and in connection with\\\n  \\ the Llama Materials,  neither Meta nor Licensee may use any name or mark owned\\\n  \\ by or associated with the other or any of its affiliates,  except as required\\\n  \\ for reasonable and customary use in describing and redistributing the Llama Materials\\\n  \\ or as  set forth in this Section 5(a). Meta hereby grants you a license to use\\\n  \\ “Llama” (the “Mark”) solely as required  to comply with the last sentence of Section\\\n  \\ 1.b.i. You will comply with Meta’s brand guidelines (currently accessible  at\\\n  \\ https://about.meta.com/brand/resources/meta/company-brand/). All goodwill arising\\\n  \\ out of your use of the Mark  will inure to the benefit of Meta.\\nb. Subject to\\\n  \\ Meta’s ownership of Llama Materials and derivatives made by or for Meta, with\\\n  \\ respect to any derivative works and modifications of the Llama Materials that\\\n  \\ are made by you, as between you and Meta, you are and will be the owner of such\\\n  \\ derivative works and modifications.\\nc. If you institute litigation or other proceedings\\\n  \\ against Meta or any entity (including a cross-claim or counterclaim in a lawsuit)\\\n  \\ alleging that the Llama Materials or Llama 3.2 outputs or results, or any portion\\\n  \\ of any of the foregoing, constitutes infringement of intellectual property or\\\n  \\ other rights owned or licensable by you, then any licenses granted to you under\\\n  \\ this Agreement shall terminate as of the date such litigation or claim is filed\\\n  \\ or instituted. You will indemnify and hold harmless Meta from and against any\\\n  \\ claim by any third party arising out of or related to your use or distribution\\\n  \\ of the Llama Materials.\\n6. Term and Termination. The term of this Agreement will\\\n  \\ commence upon your acceptance of this Agreement or access to the Llama Materials\\\n  \\ and will continue in full force and effect until terminated in accordance with\\\n  \\ the terms and conditions herein. Meta may terminate this Agreement if you are\\\n  \\ in breach of any term or condition of this Agreement. Upon termination of this\\\n  \\ Agreement, you shall delete and cease use of the Llama Materials. Sections 3,\\\n  \\ 4 and 7 shall survive the termination of this Agreement. \\n7. Governing Law and\\\n  \\ Jurisdiction. This Agreement will be governed and construed under the laws of\\\n  \\ the State of  California without regard to choice of law principles, and the UN\\\n  \\ Convention on Contracts for the International Sale of Goods does not apply to\\\n  \\ this Agreement. The courts of California shall have exclusive jurisdiction of\\\n  \\ any dispute arising out of this Agreement. \\n### Llama 3.2 Acceptable Use Policy\\n\\\n  Meta is committed to promoting safe and fair use of its tools and features, including\\\n  \\ Llama 3.2.  If you access or use Llama 3.2, you agree to this Acceptable Use Policy\\\n  \\ (“**Policy**”).  The most recent copy of this policy can be found at [https://www.llama.com/llama3_2/use-policy](https://www.llama.com/llama3_2/use-policy).\\n\\\n  #### Prohibited Uses\\nWe want everyone to use Llama 3.2 safely and responsibly.\\\n  \\ You agree you will not use, or allow others to use, Llama 3.2 to:\\n1. Violate\\\n  \\ the law or others’ rights, including to:\\n    1. Engage in, promote, generate,\\\n  \\ contribute to, encourage, plan, incite, or further illegal or unlawful activity\\\n  \\ or content, such as:\\n        1. Violence or terrorism\\n        2. Exploitation\\\n  \\ or harm to children, including the solicitation, creation, acquisition, or dissemination\\\n  \\ of child exploitative content or failure to report Child Sexual Abuse Material\\n\\\n  \\        3. Human trafficking, exploitation, and sexual violence\\n        4. The\\\n  \\ illegal distribution of information or materials to minors, including obscene\\\n  \\ materials, or failure to employ legally required age-gating in connection with\\\n  \\ such information or materials.\\n        5. Sexual solicitation\\n        6. Any\\\n  \\ other criminal activity\\n    1. Engage in, promote, incite, or facilitate the\\\n  \\ harassment, abuse, threatening, or bullying of individuals or groups of individuals\\n\\\n  \\    2. Engage in, promote, incite, or facilitate discrimination or other unlawful\\\n  \\ or harmful conduct in the provision of employment, employment benefits, credit,\\\n  \\ housing, other economic benefits, or other essential goods and services\\n    3.\\\n  \\ Engage in the unauthorized or unlicensed practice of any profession including,\\\n  \\ but not limited to, financial, legal, medical/health, or related professional\\\n  \\ practices\\n    4. Collect, process, disclose, generate, or infer private or sensitive\\\n  \\ information about individuals, including information about individuals’ identity,\\\n  \\ health, or demographic information, unless you have obtained the right to do so\\\n  \\ in accordance with applicable law\\n    5. Engage in or facilitate any action or\\\n  \\ generate any content that infringes, misappropriates, or otherwise violates any\\\n  \\ third-party rights, including the outputs or results of any products or services\\\n  \\ using the Llama Materials\\n    6. Create, generate, or facilitate the creation\\\n  \\ of malicious code, malware, computer viruses or do anything else that could disable,\\\n  \\ overburden, interfere with or impair the proper working, integrity, operation\\\n  \\ or appearance of a website or computer system\\n    7. Engage in any action, or\\\n  \\ facilitate any action, to intentionally circumvent or remove usage restrictions\\\n  \\ or other safety measures, or to enable functionality disabled by Meta \\n2. Engage\\\n  \\ in, promote, incite, facilitate, or assist in the planning or development of activities\\\n  \\ that present a risk of death or bodily harm to individuals, including use of Llama\\\n  \\ 3.2 related to the following:\\n    8. Military, warfare, nuclear industries or\\\n  \\ applications, espionage, use for materials or activities that are subject to the\\\n  \\ International Traffic Arms Regulations (ITAR) maintained by the United States\\\n  \\ Department of State or to the U.S. Biological Weapons Anti-Terrorism Act of 1989\\\n  \\ or the Chemical Weapons Convention Implementation Act of 1997\\n    9. Guns and\\\n  \\ illegal weapons (including weapon development)\\n    10. Illegal drugs and regulated/controlled\\\n  \\ substances\\n    11. Operation of critical infrastructure, transportation technologies,\\\n  \\ or heavy machinery\\n    12. Self-harm or harm to others, including suicide, cutting,\\\n  \\ and eating disorders\\n    13. Any content intended to incite or promote violence,\\\n  \\ abuse, or any infliction of bodily harm to an individual\\n3. Intentionally deceive\\\n  \\ or mislead others, including use of Llama 3.2 related to the following:\\n    14.\\\n  \\ Generating, promoting, or furthering fraud or the creation or promotion of disinformation\\n\\\n  \\    15. Generating, promoting, or furthering defamatory content, including the\\\n  \\ creation of defamatory statements, images, or other content\\n    16. Generating,\\\n  \\ promoting, or further distributing spam\\n    17. Impersonating another individual\\\n  \\ without consent, authorization, or legal right\\n    18. Representing that the\\\n  \\ use of Llama 3.2 or outputs are human-generated\\n    19. Generating or facilitating\\\n  \\ false online engagement, including fake reviews and other means of fake online\\\n  \\ engagement \\n4. Fail to appropriately disclose to end users any known dangers\\\n  \\ of your AI system 5. Interact with third party tools, models, or software designed\\\n  \\ to generate unlawful content or engage in unlawful or harmful conduct and/or represent\\\n  \\ that the outputs of such tools, models, or software are associated with Meta or\\\n  \\ Llama 3.2\\n\\nWith respect to any multimodal models included in Llama 3.2, the\\\n  \\ rights granted under Section 1(a) of the Llama 3.2 Community License Agreement\\\n  \\ are not being granted to you if you are an individual domiciled in, or a company\\\n  \\ with a principal place of business in, the European Union. This restriction does\\\n  \\ not apply to end users of a product or service that incorporates any such multimodal\\\n  \\ models.\\n\\nPlease report any violation of this Policy, software “bug,” or other\\\n  \\ problems that could lead to a violation of this Policy through one of the following\\\n  \\ means:\\n\\n* Reporting issues with the model: [https://github.com/meta-llama/llama-models/issues](https://l.workplace.com/l.php?u=https%3A%2F%2Fgithub.com%2Fmeta-llama%2Fllama-models%2Fissues&h=AT0qV8W9BFT6NwihiOHRuKYQM_UnkzN_NmHMy91OT55gkLpgi4kQupHUl0ssR4dQsIQ8n3tfd0vtkobvsEvt1l4Ic6GXI2EeuHV8N08OG2WnbAmm0FL4ObkazC6G_256vN0lN9DsykCvCqGZ)\\n\\\n  * Reporting risky content generated by the model: [developers.facebook.com/llama_output_feedback](http://developers.facebook.com/llama_output_feedback)\\n\\\n  * Reporting bugs and security concerns: [facebook.com/whitehat/info](http://facebook.com/whitehat/info)\\n\\\n  * Reporting violations of the Acceptable Use Policy or unlicensed uses of Llama\\\n  \\ 3.2: LlamaUseReport@meta.com\"\nextra_gated_fields:\n  First Name: text\n  Last Name: text\n  Date of birth: date_picker\n  Country: country\n  Affiliation: text\n  Job title:\n    type: select\n    options:\n    - Student\n    - Research Graduate\n    - AI researcher\n    - AI developer/engineer\n    - Reporter\n    - Other\n  geo: ip_location\n  ? By clicking Submit below I accept the terms of the license and acknowledge that\n    the information I provide will be collected stored processed and shared in accordance\n    with the Meta Privacy Policy\n  : checkbox\nextra_gated_description: The information you provide will be collected, stored, processed\n  and shared in accordance with the [Meta Privacy Policy](https://www.facebook.com/privacy/policy/).\nextra_gated_button_content: Submit\n---\n\n## Model Information\n\nThe Llama 3.2 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction-tuned generative models in 1B and 3B sizes (text in/text out). The Llama 3.2 instruction-tuned text only models are optimized for multilingual dialogue use cases, including agentic retrieval and summarization tasks. They outperform many of the available open source and closed chat models on common industry benchmarks.\n\n**Model Developer:** Meta\n\n**Model Architecture:** Llama 3.2 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.\n\n|  | Training Data | Params | Input modalities | Output modalities | Context Length | GQA | Shared Embeddings | Token count | Knowledge cutoff |\n| :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- |\n| Llama 3.2 (text only)  | A new mix of publicly available online data. | 1B (1.23B) | Multilingual Text | Multilingual Text and code  | 128k | Yes | Yes | Up to 9T tokens | December 2023 |\n|  |  | 3B (3.21B) | Multilingual Text | Multilingual Text and code  |  |  |  |  |  |\n| Llama 3.2 Quantized (text only)  | A new mix of publicly available online data. | 1B (1.23B) | Multilingual Text | Multilingual Text and code  | 8k | Yes | Yes | Up to 9T tokens | December 2023 |\n|  |  | 3B (3.21B) | Multilingual Text | Multilingual Text and code |  |  |  |  |  |\n\n**Supported Languages:** English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai are officially supported. Llama 3.2 has been trained on a broader collection of languages than these 8 supported languages. Developers may fine-tune Llama 3.2 models for languages beyond these supported languages, provided they comply with the Llama 3.2 Community License and the Acceptable Use Policy. Developers are always expected to ensure that their deployments, including those that involve additional languages, are completed safely and responsibly.\n\n**Llama 3.2 Model Family:** Token counts refer to pretraining data only. All model versions use Grouped-Query Attention (GQA) for improved inference scalability.\n\n**Model Release Date:** Sept 25, 2024\n\n**Status:** This is a static model trained on an offline dataset. Future versions may be released that improve model capabilities and safety.\n\n**License:** Use of Llama 3.2 is governed by the [Llama 3.2 Community License](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/LICENSE) (a custom, commercial license agreement).\n\n**Feedback:** Instructions on how to provide feedback or comments on the model can be found in the Llama Models [README](https://github.com/meta-llama/llama-models/blob/main/README.md). For more technical information about generation parameters and recipes for how to use Llama 3.2 in applications, please go [here](https://github.com/meta-llama/llama-recipes).\n\n## Intended Use\n\n**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.\n\n**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.\n\n## How to use\n\nThis repository contains two versions of Llama-3.2-3B-Instruct, for use with `transformers` and with the original `llama` codebase.\n\n### Use with transformers\n\nStarting with `transformers >= 4.43.0` onward, you can run conversational inference using the Transformers `pipeline` abstraction or by leveraging the Auto classes with the `generate()` function.\n\nMake sure to update your transformers installation via `pip install --upgrade transformers`.\n\n```python\nimport torch\nfrom transformers import pipeline\n\nmodel_id = \"meta-llama/Llama-3.2-3B-Instruct\"\npipe = pipeline(\n    \"text-generation\",\n    model=model_id,\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\",\n)\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n    {\"role\": \"user\", \"content\": \"Who are you?\"},\n]\noutputs = pipe(\n    messages,\n    max_new_tokens=256,\n)\nprint(outputs[0][\"generated_text\"][-1])\n```\n\nNote: You can also find detailed recipes on how to use the model locally, with `torch.compile()`, assisted generations, quantised and more at [`huggingface-llama-recipes`](https://github.com/huggingface/huggingface-llama-recipes)\n\n### Use with `llama`\n\nPlease, follow the instructions in the [repository](https://github.com/meta-llama/llama)\n\nTo download Original checkpoints, see the example command below leveraging `huggingface-cli`:\n\n```\nhuggingface-cli download meta-llama/Llama-3.2-3B-Instruct --include \"original/*\" --local-dir Llama-3.2-3B-Instruct\n```\n\n## Hardware and Software\n\n**Training Factors:** We used custom training libraries, Meta's custom built GPU cluster, and production infrastructure for pretraining. Fine-tuning, quantization, annotation, and evaluation were also performed on production infrastructure.\n\n**Training Energy Use:** Training utilized a cumulative of **916k** GPU hours of computation on H100-80GB (TDP of 700W) type hardware, per the table below. Training time is the total GPU time required for training each model and power consumption is the peak power capacity per GPU device used, adjusted for power usage efficiency.\n\n**Training Greenhouse Gas Emissions:** Estimated total location-based greenhouse gas emissions were **240** tons CO2eq for training. Since 2020, Meta has maintained net zero greenhouse gas emissions in its global operations and matched 100% of its electricity use with renewable energy; therefore, the total market-based greenhouse gas emissions for training were 0 tons CO2eq.\n\n|  | Training Time (GPU hours) | Logit Generation Time (GPU Hours) | Training Power Consumption (W) | Training Location-Based Greenhouse Gas Emissions (tons CO2eq) | Training Market-Based Greenhouse Gas Emissions (tons CO2eq) |\n| :---- | :---: | ----- | :---: | :---: | :---: |\n| Llama 3.2 1B | 370k | \\- | 700 | 107 | 0 |\n| Llama 3.2 3B | 460k | \\- | 700 | 133 | 0 |\n| Llama 3.2 1B SpinQuant | 1.7 | 0 | 700 | *Negligible*\\*\\* | 0 |\n| Llama 3.2 3B SpinQuant | 2.4 | 0 | 700 | *Negligible*\\*\\* | 0 |\n| Llama 3.2 1B QLora | 1.3k | 0 | 700 | 0.381 | 0 |\n| Llama 3.2 3B QLora | 1.6k | 0 | 700 | 0.461 | 0 |\n| Total | 833k |         86k |  | 240 | 0 |\n\n\\*\\* The location-based CO2e emissions of Llama 3.2 1B SpinQuant and Llama 3.2 3B SpinQuant are less than 0.001 metric tonnes each. This is due to the minimal training GPU hours that are required.\n\nThe methodology used to determine training energy use and greenhouse gas emissions can be found [here](https://arxiv.org/pdf/2204.05149). Since Meta is openly releasing these models, the training energy use and greenhouse gas emissions will not be incurred by others.\n\n## Training Data\n\n**Overview:** Llama 3.2 was pretrained on up to 9 trillion tokens of data from publicly available sources. For the 1B and 3B Llama 3.2 models, we incorporated logits from the Llama 3.1 8B and 70B models into the pretraining stage of the model development, where outputs (logits) from these larger models were used as token-level targets. Knowledge distillation was used after pruning to recover performance. In post-training we used a similar recipe as Llama 3.1 and produced final chat models by doing several rounds of alignment on top of the pre-trained model. Each round involved Supervised Fine-Tuning (SFT), Rejection Sampling (RS), and Direct Preference Optimization (DPO).\n\n**Data Freshness:** The pretraining data has a cutoff of December 2023\\.\n\n## Quantization\n\n### Quantization Scheme\n\nWe designed the current quantization scheme with the [PyTorch’s ExecuTorch](https://github.com/pytorch/executorch) inference framework and Arm CPU backend in mind, taking into account metrics including model quality, prefill/decoding speed, and memory footprint. Our quantization scheme involves three parts:\n- All linear layers in all transformer blocks are quantized to a 4-bit groupwise scheme (with a group size of 32) for weights and 8-bit per-token dynamic quantization for activations.\n- The classification layer is quantized to 8-bit per-channel for weight and 8-bit per token dynamic quantization for activation.\n- Similar to classification layer, an 8-bit per channel quantization is used for embedding layer.\n\n\n### Quantization-Aware Training and LoRA\n\nThe quantization-aware training (QAT) with low-rank adaptation (LoRA) models went through only post-training stages, using the same data as the full precision models. To initialize QAT, we utilize BF16 Llama 3.2 model checkpoints obtained after supervised fine-tuning (SFT) and perform an additional full round of SFT training with QAT. We then freeze the backbone of the QAT model and perform another round of SFT with LoRA adaptors applied to all layers within the transformer block. Meanwhile, the LoRA adaptors' weights and activations are maintained in BF16. Because our approach is similar to QLoRA of Dettmers et al., (2023) (i.e., quantization followed by LoRA adapters), we refer this method as QLoRA. Finally, we fine-tune the resulting model (both backbone and LoRA adaptors) using direct preference optimization (DPO).\n\n### SpinQuant\n\n[SpinQuant](https://arxiv.org/abs/2405.16406) was applied, together with generative post-training quantization (GPTQ). For the SpinQuant rotation matrix fine-tuning, we optimized for 100 iterations, using 800 samples with sequence-length 2048 from the WikiText 2 dataset. For GPTQ, we used 128 samples from the same dataset with the same sequence-length.\n\n## Benchmarks \\- English Text\n\nIn this section, we report the results for Llama 3.2 models on standard automatic benchmarks. For all these evaluations, we used our internal evaluations library.\n\n### Base Pretrained Models\n\n| Category | Benchmark | \\# Shots | Metric | Llama 3.2 1B | Llama 3.2 3B | Llama 3.1 8B |\n| ----- | ----- | :---: | :---: | :---: | :---: | :---: |\n| General | MMLU | 5 | macro\\_avg/acc\\_char | 32.2 | 58 | 66.7 |\n|  | AGIEval English | 3-5 | average/acc\\_char | 23.3 | 39.2 | 47.8 |\n|  | ARC-Challenge | 25 | acc\\_char | 32.8 | 69.1 | 79.7 |\n| Reading comprehension | SQuAD | 1 | em | 49.2 | 67.7 | 77 |\n|  | QuAC (F1) | 1 | f1 | 37.9 | 42.9 | 44.9 |\n|  | DROP (F1) | 3 | f1 | 28.0 | 45.2 | 59.5 |\n| Long Context | Needle in Haystack | 0 | em | 96.8 | 1 | 1 |\n\n### Instruction Tuned Models\n\n| Capability |  | Benchmark | \\# Shots | Metric | Llama 3.2 1B bf16 | Llama 3.2 1B Vanilla PTQ\\*\\* | Llama 3.2 1B Spin Quant | Llama 3.2 1B QLoRA | Llama 3.2 3B bf16 | Llama 3.2 3B Vanilla PTQ\\*\\* | Llama 3.2 3B Spin Quant | Llama 3.2 3B QLoRA | Llama 3.1 8B |\n| :---: | ----- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| General |  | MMLU | 5 | macro\\_avg/acc | 49.3 | 43.3 | 47.3 | 49.0 | 63.4 | 60.5 | 62 | 62.4 | 69.4 |\n| Re-writing |  | Open-rewrite eval | 0 | micro\\_avg/rougeL | 41.6 | 39.2 | 40.9 | 41.2 | 40.1 | 40.3 | 40.8 | 40.7 | 40.9 |\n| Summarization |  | TLDR9+ (test) | 1 | rougeL | 16.8 | 14.9 | 16.7 | 16.8 | 19.0 | 19.1 | 19.2 | 19.1 | 17.2 |\n| Instruction following |  | IFEval | 0 | Avg(Prompt/Instruction acc Loose/Strict) | 59.5 | 51.5 | 58.4 | 55.6 | 77.4 | 73.9 | 73.5 | 75.9 | 80.4 |\n| Math |  | GSM8K (CoT) | 8 | em\\_maj1@1 | 44.4 | 33.1 | 40.6 | 46.5 | 77.7 | 72.9 | 75.7 | 77.9 | 84.5 |\n|  |  | MATH (CoT) | 0 | final\\_em | 30.6 | 20.5 | 25.3 | 31.0 | 48.0 | 44.2 | 45.3 | 49.2 | 51.9 |\n| Reasoning |  | ARC-C | 0 | acc | 59.4 | 54.3 | 57 | 60.7 | 78.6 | 75.6 | 77.6 | 77.6 | 83.4 |\n|  |  | GPQA | 0 | acc | 27.2 | 25.9 | 26.3 | 25.9 | 32.8 | 32.8 | 31.7 | 33.9 | 32.8 |\n|  |  | Hellaswag | 0 | acc | 41.2 | 38.1 | 41.3 | 41.5 | 69.8 | 66.3 | 68 | 66.3 | 78.7 |\n| Tool Use |  | BFCL V2 | 0 | acc | 25.7 | 14.3 | 15.9 | 23.7 | 67.0 | 53.4 | 60.1 | 63.5 | 67.1 |\n|  |  | Nexus | 0 | macro\\_avg/acc | 13.5 | 5.2 | 9.6 | 12.5 | 34.3 | 32.4 | 31.5 | 30.1 | 38.5 |\n| Long Context |  | InfiniteBench/En.QA | 0 | longbook\\_qa/f1 | 20.3 | N/A | N/A | N/A | 19.8 | N/A | N/A | N/A | 27.3 |\n|  |  | InfiniteBench/En.MC | 0 | longbook\\_choice/acc | 38.0 | N/A | N/A | N/A | 63.3 | N/A | N/A | N/A | 72.2 |\n|  |  | NIH/Multi-needle | 0 | recall | 75.0 | N/A | N/A | N/A | 84.7 | N/A | N/A | N/A | 98.8 |\n| Multilingual |  | MGSM (CoT) | 0 | em | 24.5 | 13.7 | 18.2 | 24.4 | 58.2 | 48.9 | 54.3 | 56.8 | 68.9 |\n\n\\*\\*for comparison purposes only. Model not released.\n\n### Multilingual Benchmarks\n\n| Category | Benchmark | Language | Llama 3.2 1B | Llama 3.2 1B Vanilla PTQ\\*\\* | Llama 3.2 1B Spin Quant | Llama 3.2 1B QLoRA | Llama 3.2 3B | Llama 3.2 3B Vanilla PTQ\\*\\* | Llama 3.2 3B Spin Quant | Llama 3.2 3B QLoRA | Llama 3.1 8B |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| General | MMLU (5-shot, macro_avg/acc) | Portuguese | 39.8 | 34.9 | 38.9 | 40.2 | 54.5 | 50.9 | 53.3 | 53.4 | 62.1 |\n| | | Spanish | 41.5 | 36.0 | 39.8 | 41.8 | 55.1 | 51.9 | 53.6 | 53.6 | 62.5 |\n| | | Italian | 39.8 | 34.9 | 38.1 | 40.6 | 53.8 | 49.9 | 52.1 | 51.7 | 61.6 |\n| | | German | 39.2 | 34.9 | 37.5 | 39.6 | 53.3 | 50.0 | 52.2 | 51.3 | 60.6 |\n| | | French | 40.5 | 34.8 | 39.2 | 40.8 | 54.6 | 51.2 | 53.3 | 53.3 | 62.3 |\n| | | Hindi | 33.5 | 30.0 | 32.1 | 34.0 | 43.3 | 40.4 | 42.0 | 42.1 | 50.9 |\n| | | Thai | 34.7 | 31.2 | 32.4 | 34.9 | 44.5 | 41.3 | 44.0 | 42.2 | 50.3 |\n\n\\*\\*for comparison purposes only. Model not released.\n\n## Inference time\n\nIn the below table, we compare the performance metrics of different quantization methods (SpinQuant and QAT \\+ LoRA) with the BF16 baseline. The evaluation was done using the [ExecuTorch](https://github.com/pytorch/executorch) framework as the inference engine, with the ARM CPU as a backend using Android OnePlus 12 device.\n\n| Category | Decode (tokens/sec)  | Time-to-first-token (sec) | Prefill (tokens/sec) | Model size (PTE file size in MB) | Memory size (RSS in MB) |\n| :---- | ----- | ----- | ----- | ----- | ----- |\n| 1B BF16 (baseline) | 19.2 | 1.0 | 60.3 | 2358 | 3,185 |\n| 1B SpinQuant | 50.2 (2.6x) | 0.3 (-76.9%) | 260.5 (4.3x) | 1083 (-54.1%) | 1,921 (-39.7%) |\n| 1B QLoRA | 45.8 (2.4x) | 0.3 (-76.0%) | 252.0 (4.2x) | 1127 (-52.2%) | 2,255 (-29.2%) |\n| 3B BF16 (baseline) | 7.6 | 3.0 | 21.2 | 6129 | 7,419 |\n| 3B SpinQuant | 19.7 (2.6x) | 0.7 (-76.4%) | 89.7 (4.2x) | 2435 (-60.3%) | 3,726 (-49.8%) |\n| 3B QLoRA | 18.5 (2.4x) | 0.7 (-76.1%) | 88.8 (4.2x) | 2529 (-58.7%) | 4,060 (-45.3%) |\n\n(\\*) The performance measurement is done using an adb binary-based approach.\n(\\*\\*) It is measured on an Android OnePlus 12 device.\n(\\*\\*\\*) Time-to-first-token (TTFT)  is measured with prompt length=64\n\n*Footnote:*\n\n- *Decode (tokens/second) is for how quickly it keeps generating. Higher is better.*\n- *Time-to-first-token (TTFT for shorthand) is for how fast it generates the first token for a given prompt. Lower is better.*\n- *Prefill is the inverse of TTFT (aka 1/TTFT)  in tokens/second. Higher is better*\n- *Model size \\- how big is the model, measured by, PTE file, a binary file format for ExecuTorch*\n- *RSS size \\- Memory usage in resident set size (RSS)*\n\n## Responsibility & Safety\n\nAs part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:\n\n1. Enable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama\n2. Protect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm\n3. Provide protections for the community to help prevent the misuse of our models\n\n### Responsible Deployment\n\n**Approach:** Llama is a foundational technology designed to be used in a variety of use cases. Examples on how Meta’s Llama models have been responsibly deployed can be found in our [Community Stories webpage](https://llama.meta.com/community-stories/). Our approach is to build the most helpful models, enabling the world to benefit from the technology power, by aligning our model safety for generic use cases and addressing a standard set of harms. Developers are then in the driver’s seat to tailor safety for their use cases, defining their own policies and deploying the models with the necessary safeguards in their Llama systems. Llama 3.2 was developed following the best practices outlined in our [Responsible Use Guide](https://llama.meta.com/responsible-use-guide/).\n\n#### Llama 3.2 Instruct\n\n**Objective:** Our main objectives for conducting safety fine-tuning are to provide the research community with a valuable resource for studying the robustness of safety fine-tuning, as well as to offer developers a readily available, safe, and powerful model for various applications to reduce the developer workload to deploy safe AI systems. We implemented the same set of safety mitigations as in Llama 3, and you can learn more about these in the Llama 3 [paper](https://ai.meta.com/research/publications/the-llama-3-herd-of-models/).\n\n**Fine-Tuning Data:** We employ a multi-faceted approach to data collection, combining human-generated data from our vendors with synthetic data to mitigate potential safety risks. We’ve developed many large language model (LLM)-based classifiers that enable us to thoughtfully select high-quality prompts and responses, enhancing data quality control.\n\n**Refusals and Tone:** Building on the work we started with Llama 3, we put a great emphasis on model refusals to benign prompts as well as refusal tone. We included both borderline and adversarial prompts in our safety data strategy, and modified our safety data responses to follow tone guidelines.\n\n#### Llama 3.2 Systems\n\n**Safety as a System:** Large language models, including Llama 3.2, **are not designed to be deployed in isolation** but instead should be deployed as part of an overall AI system with additional safety guardrails as required. Developers are expected to deploy system safeguards when building agentic systems. Safeguards are key to achieve the right helpfulness-safety alignment as well as mitigating safety and security risks inherent to the system and any integration of the model or system with external tools. As part of our responsible release approach, we provide the community with [safeguards](https://llama.meta.com/trust-and-safety/) that developers should deploy with Llama models or other LLMs, including Llama Guard, Prompt Guard and Code Shield. All our [reference implementations](https://github.com/meta-llama/llama-agentic-system) demos contain these safeguards by default so developers can benefit from system-level safety out-of-the-box.\n\n### New Capabilities and Use Cases\n\n**Technological Advancement:** Llama releases usually introduce new capabilities that require specific considerations in addition to the best practices that generally apply across all Generative AI use cases. For prior release capabilities also supported by Llama 3.2, see [Llama 3.1 Model Card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md), as the same considerations apply here as well.\n\n**Constrained Environments:** Llama 3.2 1B and 3B models are expected to be deployed in highly constrained environments, such as mobile devices. LLM Systems using smaller models will have a different alignment profile and safety/helpfulness tradeoff than more complex, larger systems. Developers should ensure the safety of their system meets the requirements of their use case. We recommend using lighter system safeguards for such use cases, like Llama Guard 3-1B or its mobile-optimized version.\n\n### Evaluations\n\n**Scaled Evaluations:** We built dedicated, adversarial evaluation datasets and evaluated systems composed of Llama models and Purple Llama safeguards to filter input prompt and output response. It is important to evaluate applications in context, and we recommend building dedicated evaluation dataset for your use case.\n\n**Red Teaming:** We conducted recurring red teaming exercises with the goal of discovering risks via adversarial prompting and we used the learnings to improve our benchmarks and safety tuning datasets. We partnered early with subject-matter experts in critical risk areas to understand the nature of these real-world harms and how such models may lead to unintended harm for society. Based on these conversations, we derived a set of adversarial goals for the red team to attempt to achieve, such as extracting harmful information or reprogramming the model to act in a potentially harmful capacity. The red team consisted of experts in cybersecurity, adversarial machine learning, responsible AI, and integrity in addition to multilingual content specialists with background in integrity issues in specific geographic markets.\n\n### Critical Risks\n\nIn addition to our safety work above, we took extra care on measuring and/or mitigating the following critical risk areas:\n\n**1\\. CBRNE (Chemical, Biological, Radiological, Nuclear, and Explosive Weapons):** Llama 3.2 1B and 3B models are smaller and less capable derivatives of Llama 3.1. For Llama 3.1 70B and 405B, to assess risks related to proliferation of chemical and biological weapons, we performed uplift testing designed to assess whether use of Llama 3.1 models could meaningfully increase the capabilities of malicious actors to plan or carry out attacks using these types of weapons and have determined that such testing also applies to the smaller 1B and 3B models.\n\n**2\\. Child Safety:** Child Safety risk assessments were conducted using a team of experts, to assess the model’s capability to produce outputs that could result in Child Safety risks and inform on any necessary and appropriate risk mitigations via fine tuning. We leveraged those expert red teaming sessions to expand the coverage of our evaluation benchmarks through Llama 3 model development. For Llama 3, we conducted new in-depth sessions using objective based methodologies to assess the model risks along multiple attack vectors including the additional languages Llama 3 is trained on. We also partnered with content specialists to perform red teaming exercises assessing potentially violating content while taking account of market specific nuances or experiences.\n\n**3\\. Cyber Attacks:** For Llama 3.1 405B, our cyber attack uplift study investigated whether LLMs can enhance human capabilities in hacking tasks, both in terms of skill level and speed.\nOur attack automation study focused on evaluating the capabilities of LLMs when used as autonomous agents in cyber offensive operations, specifically in the context of ransomware attacks. This evaluation was distinct from previous studies that considered LLMs as interactive assistants. The primary objective was to assess whether these models could effectively function as independent agents in executing complex cyber-attacks without human intervention. Because Llama 3.2’s 1B and 3B models are smaller and less capable models than Llama 3.1 405B, we broadly believe that the testing conducted for the 405B model also applies to Llama 3.2 models.\n\n### Community\n\n**Industry Partnerships:** Generative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership on AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Purple Llama tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to our [Github repository](https://github.com/meta-llama/PurpleLlama).\n\n**Grants:** We also set up the [Llama Impact Grants](https://llama.meta.com/llama-impact-grants/) program to identify and support the most compelling applications of Meta’s Llama model for societal benefit across three categories: education, climate and open innovation. The 20 finalists from the hundreds of applications can be found [here](https://llama.meta.com/llama-impact-grants/#finalists).\n\n**Reporting:** Finally, we put in place a set of resources including an [output reporting mechanism](https://developers.facebook.com/llama_output_feedback) and [bug bounty program](https://www.facebook.com/whitehat) to continuously improve the Llama technology with the help of the community.\n\n## Ethical Considerations and Limitations\n\n**Values:** The core values of Llama 3.2 are openness, inclusivity and helpfulness. It is meant to serve everyone, and to work for a wide range of use cases. It is thus designed to be accessible to people across many different backgrounds, experiences and perspectives. Llama 3.2 addresses users and their needs as they are, without insertion unnecessary judgment or normativity, while reflecting the understanding that even content that may appear problematic in some cases can serve valuable purposes in others. It respects the dignity and autonomy of all users, especially in terms of the values of free thought and expression that power innovation and progress.\n\n**Testing:** Llama 3.2 is a new technology, and like any new technology, there are risks associated with its use. Testing conducted to date has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, Llama 3.2’s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 3.2 models, developers should perform safety testing and tuning tailored to their specific applications of the model. Please refer to available resources including our [Responsible Use Guide](https://llama.meta.com/responsible-use-guide), [Trust and Safety](https://llama.meta.com/trust-and-safety/) solutions, and other [resources](https://llama.meta.com/docs/get-started/) to learn more about responsible development.\n",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModelForCausalLM",
      "custom_class": null,
      "pipeline_tag": "text-generation",
      "processor": "AutoTokenizer"
    },
    "safetensors": {
      "parameters": {
        "BF16": 3212749824
      },
      "total": 3212749824
    },
    "model_index": null,
    "trending_score": null
  },
  {
    "model_id": "MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF",
    "model_name": "MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF",
    "author": "MaziyarPanahi",
    "downloads": 2533565,
    "downloads_all_time": null,
    "likes": 85,
    "tags": [
      "transformers",
      "gguf",
      "mistral",
      "quantized",
      "2-bit",
      "3-bit",
      "4-bit",
      "5-bit",
      "6-bit",
      "8-bit",
      "GGUF",
      "safetensors",
      "text-generation",
      "conversational",
      "license:apache-2.0",
      "autotrain_compatible",
      "endpoints_compatible",
      "text-generation-inference",
      "region:us",
      "base_model:mistralai/Mistral-7B-Instruct-v0.3",
      "base_model:quantized:mistralai/Mistral-7B-Instruct-v0.3"
    ],
    "card_url": "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF",
    "dependencies": [
      [
        "transformers",
        null
      ],
      [
        "safetensors",
        null
      ]
    ],
    "last_modified": "2024-05-22T20:26:06+00:00",
    "created_at": "2024-05-22T17:27:45+00:00",
    "analysis_date": "2025-03-22T00:41:31.898690",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "mistral",
    "pipeline_tag": "text-generation",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "tags": [
        "quantized",
        "2-bit",
        "3-bit",
        "4-bit",
        "5-bit",
        "6-bit",
        "8-bit",
        "GGUF",
        "transformers",
        "safetensors",
        "mistral",
        "text-generation",
        "conversational",
        "license:apache-2.0",
        "autotrain_compatible",
        "endpoints_compatible",
        "text-generation-inference",
        "region:us"
      ],
      "model_name": "Mistral-7B-Instruct-v0.3-GGUF",
      "base_model": "mistralai/Mistral-7B-Instruct-v0.3",
      "inference": false,
      "model_creator": "mistralai",
      "pipeline_tag": "text-generation",
      "quantized_by": "MaziyarPanahi"
    },
    "card_text": "# [MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF](https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF)\n- Model creator: [mistralai](https://huggingface.co/mistralai)\n- Original model: [mistralai/Mistral-7B-Instruct-v0.3](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3)\n\n## Description\n[MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF](https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF) contains GGUF format model files for [mistralai/Mistral-7B-Instruct-v0.3](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3).\n\n### About GGUF\n\nGGUF is a new format introduced by the llama.cpp team on August 21st 2023. It is a replacement for GGML, which is no longer supported by llama.cpp.\n\nHere is an incomplete list of clients and libraries that are known to support GGUF:\n\n* [llama.cpp](https://github.com/ggerganov/llama.cpp). The source project for GGUF. Offers a CLI and a server option.\n* [llama-cpp-python](https://github.com/abetlen/llama-cpp-python), a Python library with GPU accel, LangChain support, and OpenAI-compatible API server.\n* [LM Studio](https://lmstudio.ai/), an easy-to-use and powerful local GUI for Windows and macOS (Silicon), with GPU acceleration. Linux available, in beta as of 27/11/2023.\n* [text-generation-webui](https://github.com/oobabooga/text-generation-webui), the most widely used web UI, with many features and powerful extensions. Supports GPU acceleration.\n* [KoboldCpp](https://github.com/LostRuins/koboldcpp), a fully featured web UI, with GPU accel across all platforms and GPU architectures. Especially good for story telling.\n* [GPT4All](https://gpt4all.io/index.html), a free and open source local running GUI, supporting Windows, Linux and macOS with full GPU accel.\n* [LoLLMS Web UI](https://github.com/ParisNeo/lollms-webui), a great web UI with many interesting and unique features, including a full model library for easy model selection.\n* [Faraday.dev](https://faraday.dev/), an attractive and easy to use character-based chat GUI for Windows and macOS (both Silicon and Intel), with GPU acceleration.\n* [candle](https://github.com/huggingface/candle), a Rust ML framework with a focus on performance, including GPU support, and ease of use.\n* [ctransformers](https://github.com/marella/ctransformers), a Python library with GPU accel, LangChain support, and OpenAI-compatible AI server. Note, as of time of writing (November 27th 2023), ctransformers has not been updated in a long time and does not support many recent models.\n\n## Special thanks\n\n🙏 Special thanks to [Georgi Gerganov](https://github.com/ggerganov) and the whole team working on [llama.cpp](https://github.com/ggerganov/llama.cpp/) for making all of this possible.",
    "card_content": "---\ntags:\n- quantized\n- 2-bit\n- 3-bit\n- 4-bit\n- 5-bit\n- 6-bit\n- 8-bit\n- GGUF\n- transformers\n- safetensors\n- mistral\n- text-generation\n- conversational\n- license:apache-2.0\n- autotrain_compatible\n- endpoints_compatible\n- text-generation-inference\n- region:us\nmodel_name: Mistral-7B-Instruct-v0.3-GGUF\nbase_model: mistralai/Mistral-7B-Instruct-v0.3\ninference: false\nmodel_creator: mistralai\npipeline_tag: text-generation\nquantized_by: MaziyarPanahi\n---\n# [MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF](https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF)\n- Model creator: [mistralai](https://huggingface.co/mistralai)\n- Original model: [mistralai/Mistral-7B-Instruct-v0.3](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3)\n\n## Description\n[MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF](https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF) contains GGUF format model files for [mistralai/Mistral-7B-Instruct-v0.3](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3).\n\n### About GGUF\n\nGGUF is a new format introduced by the llama.cpp team on August 21st 2023. It is a replacement for GGML, which is no longer supported by llama.cpp.\n\nHere is an incomplete list of clients and libraries that are known to support GGUF:\n\n* [llama.cpp](https://github.com/ggerganov/llama.cpp). The source project for GGUF. Offers a CLI and a server option.\n* [llama-cpp-python](https://github.com/abetlen/llama-cpp-python), a Python library with GPU accel, LangChain support, and OpenAI-compatible API server.\n* [LM Studio](https://lmstudio.ai/), an easy-to-use and powerful local GUI for Windows and macOS (Silicon), with GPU acceleration. Linux available, in beta as of 27/11/2023.\n* [text-generation-webui](https://github.com/oobabooga/text-generation-webui), the most widely used web UI, with many features and powerful extensions. Supports GPU acceleration.\n* [KoboldCpp](https://github.com/LostRuins/koboldcpp), a fully featured web UI, with GPU accel across all platforms and GPU architectures. Especially good for story telling.\n* [GPT4All](https://gpt4all.io/index.html), a free and open source local running GUI, supporting Windows, Linux and macOS with full GPU accel.\n* [LoLLMS Web UI](https://github.com/ParisNeo/lollms-webui), a great web UI with many interesting and unique features, including a full model library for easy model selection.\n* [Faraday.dev](https://faraday.dev/), an attractive and easy to use character-based chat GUI for Windows and macOS (both Silicon and Intel), with GPU acceleration.\n* [candle](https://github.com/huggingface/candle), a Rust ML framework with a focus on performance, including GPU support, and ease of use.\n* [ctransformers](https://github.com/marella/ctransformers), a Python library with GPU accel, LangChain support, and OpenAI-compatible AI server. Note, as of time of writing (November 27th 2023), ctransformers has not been updated in a long time and does not support many recent models.\n\n## Special thanks\n\n🙏 Special thanks to [Georgi Gerganov](https://github.com/ggerganov) and the whole team working on [llama.cpp](https://github.com/ggerganov/llama.cpp/) for making all of this possible.",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModel",
      "custom_class": null,
      "pipeline_tag": null,
      "processor": null
    },
    "safetensors": null,
    "model_index": null,
    "trending_score": null
  },
  {
    "model_id": "distilbert/distilgpt2",
    "model_name": "distilbert/distilgpt2",
    "author": "distilbert",
    "downloads": 2504927,
    "downloads_all_time": null,
    "likes": 502,
    "tags": [
      "transformers",
      "pytorch",
      "tf",
      "jax",
      "tflite",
      "rust",
      "coreml",
      "safetensors",
      "gpt2",
      "text-generation",
      "exbert",
      "en",
      "dataset:openwebtext",
      "arxiv:1910.01108",
      "arxiv:2201.08542",
      "arxiv:2203.12574",
      "arxiv:1910.09700",
      "arxiv:1503.02531",
      "license:apache-2.0",
      "model-index",
      "co2_eq_emissions",
      "autotrain_compatible",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/distilbert/distilgpt2",
    "dependencies": [
      [
        "transformers",
        null
      ]
    ],
    "last_modified": "2024-02-19T11:09:53+00:00",
    "created_at": "2022-03-02T23:29:04+00:00",
    "analysis_date": "2025-03-22T00:41:33.611387",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "gpt2",
    "pipeline_tag": "text-generation",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": "en",
      "tags": [
        "exbert"
      ],
      "license": "apache-2.0",
      "datasets": [
        "openwebtext"
      ],
      "co2_eq_emissions": 149200,
      "model-index": [
        {
          "name": "distilgpt2",
          "results": [
            {
              "task": {
                "type": "text-generation",
                "name": "Text Generation"
              },
              "dataset": {
                "name": "WikiText-103",
                "type": "wikitext"
              },
              "metrics": [
                {
                  "type": "perplexity",
                  "value": 21.1,
                  "name": "Perplexity"
                }
              ]
            }
          ]
        }
      ]
    },
    "card_text": "\n# DistilGPT2\n\nDistilGPT2 (short for Distilled-GPT2) is an English-language model pre-trained with the supervision of the smallest version of Generative Pre-trained Transformer 2 (GPT-2). Like GPT-2, DistilGPT2 can be used to generate text. Users of this model card should also consider information about the design, training, and limitations of [GPT-2](https://huggingface.co/gpt2).\n\n## Model Details\n\n- **Developed by:** Hugging Face\n- **Model type:** Transformer-based Language Model\n- **Language:** English\n- **License:** Apache 2.0\n- **Model Description:** DistilGPT2 is an English-language model pre-trained with the supervision of the 124 million parameter version of GPT-2. DistilGPT2, which has 82 million parameters, was developed using [knowledge distillation](#knowledge-distillation) and was designed to be a faster, lighter version of GPT-2.\n- **Resources for more information:** See [this repository](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation) for more about Distil\\* (a class of compressed models including Distilled-GPT2), [Sanh et al. (2019)](https://arxiv.org/abs/1910.01108) for more information about knowledge distillation and the training procedure, and this page for more about [GPT-2](https://openai.com/blog/better-language-models/).\n\n## Uses, Limitations and Risks\n\n#### Limitations and Risks\n\n<details>\n<summary>Click to expand</summary>\n\n**CONTENT WARNING: Readers should be aware this section contains content that is disturbing, offensive, and can propagate historical and current stereotypes.**\n\nAs the developers of GPT-2 (OpenAI) note in their [model card](https://github.com/openai/gpt-2/blob/master/model_card.md), “language models like GPT-2 reflect the biases inherent to the systems they were trained on.” Significant research has explored bias and fairness issues with models for language generation including GPT-2 (see, e.g., [Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)). \n\nDistilGPT2 also suffers from persistent bias issues, as highlighted in the demonstrative examples below. Note that these examples are not a comprehensive stress-testing of the model. Readers considering using the model should consider more rigorous evaluations of the model depending on their use case and context.\n\nThe impact of model compression techniques – such as knowledge distillation – on bias and fairness issues associated with language models is an active area of research. For example: \n\n- [Silva, Tambwekar and Gombolay (2021)](https://aclanthology.org/2021.naacl-main.189.pdf) find that distilled versions of BERT and RoBERTa consistently exhibit statistically significant bias (with regard to gender and race) with effect sizes larger than the teacher models.\n- [Xu and Hu (2022)](https://arxiv.org/pdf/2201.08542.pdf) find that distilled versions of GPT-2 showed consistent reductions in toxicity and bias compared to the teacher model (see the paper for more detail on metrics used to define/measure toxicity and bias). \n- [Gupta et al. (2022)](https://arxiv.org/pdf/2203.12574.pdf) find that DistilGPT2 exhibits greater gender disparities than GPT-2 and propose a technique for mitigating gender bias in distilled language models like DistilGPT2. \n\n```python\n>>> from transformers import pipeline, set_seed\n>>> generator = pipeline('text-generation', model='distilgpt2')\n>>> set_seed(48)\n>>> generator(\"The White man worked as a\", max_length=20, num_return_sequences=3)\n[{'generated_text': \"The White man worked as a salesman at a McDonald's restaurant called Kia at the time of the\"},\n {'generated_text': 'The White man worked as a contractor in the Army in the late 1990s. He became a \"'},\n {'generated_text': 'The White man worked as a police spokesman to the US Navy in the 1930s.'}]\n \n>>> set_seed(48)\n>>> generator(\"The Black man worked as a\", max_length=20, num_return_sequences=3)\n[{'generated_text': 'The Black man worked as a shop assistant for an hour at Wal-Mart at Wal-Mart in'},\n {'generated_text': 'The Black man worked as a waiter in the hotel when he was assaulted when he got out of a'},\n {'generated_text': 'The Black man worked as a police spokesman four months ago...'}]\n```\n\n</details>\n\n#### Potential Uses\n\nSince DistilGPT2 is a distilled version of GPT-2, it is intended to be used for similar use cases with the increased functionality of being smaller and easier to run than the base model. \n\nThe developers of GPT-2 state in their [model card](https://github.com/openai/gpt-2/blob/master/model_card.md) that they envisioned GPT-2 would be used by researchers to better understand large-scale generative language models, with possible secondary use cases including: \n\n> - *Writing assistance: Grammar assistance, autocompletion (for normal prose or code)*\n> - *Creative writing and art: exploring the generation of creative, fictional texts; aiding creation of poetry and other literary art.*\n> - *Entertainment: Creation of games, chat bots, and amusing generations.*\n\nUsing DistilGPT2, the Hugging Face team built the [Write With Transformers](https://transformer.huggingface.co/doc/distil-gpt2) web app, which allows users to play with the model to generate text directly from their browser.\n\n#### Out-of-scope Uses\n\nOpenAI states in the GPT-2 [model card](https://github.com/openai/gpt-2/blob/master/model_card.md): \n\n> Because large-scale language models like GPT-2 do not distinguish fact from fiction, we don’t support use-cases that require the generated text to be true.\n>\n> Additionally, language models like GPT-2 reflect the biases inherent to the systems they were trained on, so we do not recommend that they be deployed into systems that interact with humans unless the deployers first carry out a study of biases relevant to the intended use-case.\n\n### How to Get Started with the Model \n\n<details>\n<summary>Click to expand</summary>\n\n*Be sure to read the sections on in-scope and out-of-scope uses and limitations of the model for further information on how to use the model.*\n\nUsing DistilGPT2 is similar to using GPT-2. DistilGPT2 can be used directly with a pipeline for text generation. Since the generation relies on some randomness, we set a seed for reproducibility:\n\n```python\n>>> from transformers import pipeline, set_seed\n>>> generator = pipeline('text-generation', model='distilgpt2')\n>>> set_seed(42)\n>>> generator(\"Hello, I’m a language model\", max_length=20, num_return_sequences=5)\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n[{'generated_text': \"Hello, I'm a language model, I'm a language model. In my previous post I've\"},\n {'generated_text': \"Hello, I'm a language model, and I'd love to hear what you think about it.\"},\n {'generated_text': \"Hello, I'm a language model, but I don't get much of a connection anymore, so\"},\n {'generated_text': \"Hello, I'm a language model, a functional language... It's not an example, and that\"},\n {'generated_text': \"Hello, I'm a language model, not an object model.\\n\\nIn a nutshell, I\"}]\n``` \n \nHere is how to use this model to get the features of a given text in PyTorch:\n\n```python\nfrom transformers import GPT2Tokenizer, GPT2Model\ntokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\nmodel = GPT2Model.from_pretrained('distilgpt2')\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n```\n\nAnd in TensorFlow:\n\n```python\nfrom transformers import GPT2Tokenizer, TFGPT2Model\ntokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\nmodel = TFGPT2Model.from_pretrained('distilgpt2')\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\n```\n\n</details>\n\n## Training Data\n\nDistilGPT2 was trained using [OpenWebTextCorpus](https://skylion007.github.io/OpenWebTextCorpus/), an open-source reproduction of OpenAI’s WebText dataset, which was used to train GPT-2. See the [OpenWebTextCorpus Dataset Card](https://huggingface.co/datasets/openwebtext) for additional information about OpenWebTextCorpus and [Radford et al. (2019)](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf) for additional information about WebText.\n\n## Training Procedure\n\nThe texts were tokenized using the same tokenizer as GPT-2, a byte-level version of Byte Pair Encoding (BPE). DistilGPT2 was trained using knowledge distillation, following a procedure similar to the training procedure for DistilBERT, described in more detail in [Sanh et al. (2019)](https://arxiv.org/abs/1910.01108). \n\n## Evaluation Results\n\nThe creators of DistilGPT2 [report](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation) that, on the [WikiText-103](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/) benchmark, GPT-2 reaches a perplexity on the test set of 16.3 compared to 21.1 for DistilGPT2 (after fine-tuning on the train set).\n\n## Environmental Impact\n\n*Carbon emissions were estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700). The hardware, runtime, cloud provider, and compute region were utilized to estimate the carbon impact.*\n\n- **Hardware Type:** 8 16GB V100\n- **Hours used:** 168 (1 week)\n- **Cloud Provider:** Azure\n- **Compute Region:** unavailable, assumed East US for calculations\n- **Carbon Emitted** *(Power consumption x Time x Carbon produced based on location of power grid)*: 149.2 kg eq. CO2\n\n## Citation\n\n```bibtex\n@inproceedings{sanh2019distilbert,\n  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},\n  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},\n  booktitle={NeurIPS EMC^2 Workshop},\n  year={2019}\n}\n```\n\n## Glossary\n\n-\t<a name=\"knowledge-distillation\">**Knowledge Distillation**</a>: As described in [Sanh et al. (2019)](https://arxiv.org/pdf/1910.01108.pdf), “knowledge distillation is a compression technique in which a compact model – the student – is trained to reproduce the behavior of a larger model – the teacher – or an ensemble of models.” Also see [Bucila et al. (2006)](https://www.cs.cornell.edu/~caruana/compression.kdd06.pdf) and [Hinton et al. (2015)](https://arxiv.org/abs/1503.02531).\n\n<a href=\"https://huggingface.co/exbert/?model=distilgpt2\">\n\t<img width=\"300px\" src=\"https://cdn-media.huggingface.co/exbert/button.png\">\n</a>\n",
    "card_content": "---\nlanguage: en\ntags:\n- exbert\nlicense: apache-2.0\ndatasets:\n- openwebtext\nco2_eq_emissions: 149200\nmodel-index:\n- name: distilgpt2\n  results:\n  - task:\n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: WikiText-103\n      type: wikitext\n    metrics:\n    - type: perplexity\n      value: 21.1\n      name: Perplexity\n---\n\n# DistilGPT2\n\nDistilGPT2 (short for Distilled-GPT2) is an English-language model pre-trained with the supervision of the smallest version of Generative Pre-trained Transformer 2 (GPT-2). Like GPT-2, DistilGPT2 can be used to generate text. Users of this model card should also consider information about the design, training, and limitations of [GPT-2](https://huggingface.co/gpt2).\n\n## Model Details\n\n- **Developed by:** Hugging Face\n- **Model type:** Transformer-based Language Model\n- **Language:** English\n- **License:** Apache 2.0\n- **Model Description:** DistilGPT2 is an English-language model pre-trained with the supervision of the 124 million parameter version of GPT-2. DistilGPT2, which has 82 million parameters, was developed using [knowledge distillation](#knowledge-distillation) and was designed to be a faster, lighter version of GPT-2.\n- **Resources for more information:** See [this repository](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation) for more about Distil\\* (a class of compressed models including Distilled-GPT2), [Sanh et al. (2019)](https://arxiv.org/abs/1910.01108) for more information about knowledge distillation and the training procedure, and this page for more about [GPT-2](https://openai.com/blog/better-language-models/).\n\n## Uses, Limitations and Risks\n\n#### Limitations and Risks\n\n<details>\n<summary>Click to expand</summary>\n\n**CONTENT WARNING: Readers should be aware this section contains content that is disturbing, offensive, and can propagate historical and current stereotypes.**\n\nAs the developers of GPT-2 (OpenAI) note in their [model card](https://github.com/openai/gpt-2/blob/master/model_card.md), “language models like GPT-2 reflect the biases inherent to the systems they were trained on.” Significant research has explored bias and fairness issues with models for language generation including GPT-2 (see, e.g., [Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)). \n\nDistilGPT2 also suffers from persistent bias issues, as highlighted in the demonstrative examples below. Note that these examples are not a comprehensive stress-testing of the model. Readers considering using the model should consider more rigorous evaluations of the model depending on their use case and context.\n\nThe impact of model compression techniques – such as knowledge distillation – on bias and fairness issues associated with language models is an active area of research. For example: \n\n- [Silva, Tambwekar and Gombolay (2021)](https://aclanthology.org/2021.naacl-main.189.pdf) find that distilled versions of BERT and RoBERTa consistently exhibit statistically significant bias (with regard to gender and race) with effect sizes larger than the teacher models.\n- [Xu and Hu (2022)](https://arxiv.org/pdf/2201.08542.pdf) find that distilled versions of GPT-2 showed consistent reductions in toxicity and bias compared to the teacher model (see the paper for more detail on metrics used to define/measure toxicity and bias). \n- [Gupta et al. (2022)](https://arxiv.org/pdf/2203.12574.pdf) find that DistilGPT2 exhibits greater gender disparities than GPT-2 and propose a technique for mitigating gender bias in distilled language models like DistilGPT2. \n\n```python\n>>> from transformers import pipeline, set_seed\n>>> generator = pipeline('text-generation', model='distilgpt2')\n>>> set_seed(48)\n>>> generator(\"The White man worked as a\", max_length=20, num_return_sequences=3)\n[{'generated_text': \"The White man worked as a salesman at a McDonald's restaurant called Kia at the time of the\"},\n {'generated_text': 'The White man worked as a contractor in the Army in the late 1990s. He became a \"'},\n {'generated_text': 'The White man worked as a police spokesman to the US Navy in the 1930s.'}]\n \n>>> set_seed(48)\n>>> generator(\"The Black man worked as a\", max_length=20, num_return_sequences=3)\n[{'generated_text': 'The Black man worked as a shop assistant for an hour at Wal-Mart at Wal-Mart in'},\n {'generated_text': 'The Black man worked as a waiter in the hotel when he was assaulted when he got out of a'},\n {'generated_text': 'The Black man worked as a police spokesman four months ago...'}]\n```\n\n</details>\n\n#### Potential Uses\n\nSince DistilGPT2 is a distilled version of GPT-2, it is intended to be used for similar use cases with the increased functionality of being smaller and easier to run than the base model. \n\nThe developers of GPT-2 state in their [model card](https://github.com/openai/gpt-2/blob/master/model_card.md) that they envisioned GPT-2 would be used by researchers to better understand large-scale generative language models, with possible secondary use cases including: \n\n> - *Writing assistance: Grammar assistance, autocompletion (for normal prose or code)*\n> - *Creative writing and art: exploring the generation of creative, fictional texts; aiding creation of poetry and other literary art.*\n> - *Entertainment: Creation of games, chat bots, and amusing generations.*\n\nUsing DistilGPT2, the Hugging Face team built the [Write With Transformers](https://transformer.huggingface.co/doc/distil-gpt2) web app, which allows users to play with the model to generate text directly from their browser.\n\n#### Out-of-scope Uses\n\nOpenAI states in the GPT-2 [model card](https://github.com/openai/gpt-2/blob/master/model_card.md): \n\n> Because large-scale language models like GPT-2 do not distinguish fact from fiction, we don’t support use-cases that require the generated text to be true.\n>\n> Additionally, language models like GPT-2 reflect the biases inherent to the systems they were trained on, so we do not recommend that they be deployed into systems that interact with humans unless the deployers first carry out a study of biases relevant to the intended use-case.\n\n### How to Get Started with the Model \n\n<details>\n<summary>Click to expand</summary>\n\n*Be sure to read the sections on in-scope and out-of-scope uses and limitations of the model for further information on how to use the model.*\n\nUsing DistilGPT2 is similar to using GPT-2. DistilGPT2 can be used directly with a pipeline for text generation. Since the generation relies on some randomness, we set a seed for reproducibility:\n\n```python\n>>> from transformers import pipeline, set_seed\n>>> generator = pipeline('text-generation', model='distilgpt2')\n>>> set_seed(42)\n>>> generator(\"Hello, I’m a language model\", max_length=20, num_return_sequences=5)\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n[{'generated_text': \"Hello, I'm a language model, I'm a language model. In my previous post I've\"},\n {'generated_text': \"Hello, I'm a language model, and I'd love to hear what you think about it.\"},\n {'generated_text': \"Hello, I'm a language model, but I don't get much of a connection anymore, so\"},\n {'generated_text': \"Hello, I'm a language model, a functional language... It's not an example, and that\"},\n {'generated_text': \"Hello, I'm a language model, not an object model.\\n\\nIn a nutshell, I\"}]\n``` \n \nHere is how to use this model to get the features of a given text in PyTorch:\n\n```python\nfrom transformers import GPT2Tokenizer, GPT2Model\ntokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\nmodel = GPT2Model.from_pretrained('distilgpt2')\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n```\n\nAnd in TensorFlow:\n\n```python\nfrom transformers import GPT2Tokenizer, TFGPT2Model\ntokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\nmodel = TFGPT2Model.from_pretrained('distilgpt2')\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\n```\n\n</details>\n\n## Training Data\n\nDistilGPT2 was trained using [OpenWebTextCorpus](https://skylion007.github.io/OpenWebTextCorpus/), an open-source reproduction of OpenAI’s WebText dataset, which was used to train GPT-2. See the [OpenWebTextCorpus Dataset Card](https://huggingface.co/datasets/openwebtext) for additional information about OpenWebTextCorpus and [Radford et al. (2019)](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf) for additional information about WebText.\n\n## Training Procedure\n\nThe texts were tokenized using the same tokenizer as GPT-2, a byte-level version of Byte Pair Encoding (BPE). DistilGPT2 was trained using knowledge distillation, following a procedure similar to the training procedure for DistilBERT, described in more detail in [Sanh et al. (2019)](https://arxiv.org/abs/1910.01108). \n\n## Evaluation Results\n\nThe creators of DistilGPT2 [report](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation) that, on the [WikiText-103](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/) benchmark, GPT-2 reaches a perplexity on the test set of 16.3 compared to 21.1 for DistilGPT2 (after fine-tuning on the train set).\n\n## Environmental Impact\n\n*Carbon emissions were estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700). The hardware, runtime, cloud provider, and compute region were utilized to estimate the carbon impact.*\n\n- **Hardware Type:** 8 16GB V100\n- **Hours used:** 168 (1 week)\n- **Cloud Provider:** Azure\n- **Compute Region:** unavailable, assumed East US for calculations\n- **Carbon Emitted** *(Power consumption x Time x Carbon produced based on location of power grid)*: 149.2 kg eq. CO2\n\n## Citation\n\n```bibtex\n@inproceedings{sanh2019distilbert,\n  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},\n  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},\n  booktitle={NeurIPS EMC^2 Workshop},\n  year={2019}\n}\n```\n\n## Glossary\n\n-\t<a name=\"knowledge-distillation\">**Knowledge Distillation**</a>: As described in [Sanh et al. (2019)](https://arxiv.org/pdf/1910.01108.pdf), “knowledge distillation is a compression technique in which a compact model – the student – is trained to reproduce the behavior of a larger model – the teacher – or an ensemble of models.” Also see [Bucila et al. (2006)](https://www.cs.cornell.edu/~caruana/compression.kdd06.pdf) and [Hinton et al. (2015)](https://arxiv.org/abs/1503.02531).\n\n<a href=\"https://huggingface.co/exbert/?model=distilgpt2\">\n\t<img width=\"300px\" src=\"https://cdn-media.huggingface.co/exbert/button.png\">\n</a>\n",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModelForCausalLM",
      "custom_class": null,
      "pipeline_tag": "text-generation",
      "processor": "AutoTokenizer"
    },
    "safetensors": {
      "parameters": {
        "F32": 88204032
      },
      "total": 88204032
    },
    "model_index": [
      {
        "name": "distilgpt2",
        "results": [
          {
            "task": {
              "type": "text-generation",
              "name": "Text Generation"
            },
            "dataset": {
              "type": "wikitext",
              "name": "WikiText-103"
            },
            "metrics": [
              {
                "type": "perplexity",
                "name": "Perplexity",
                "value": 21.1,
                "verified": false
              }
            ]
          }
        ]
      }
    ],
    "trending_score": null
  },
  {
    "model_id": "MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF",
    "model_name": "MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF",
    "author": "MaziyarPanahi",
    "downloads": 2492513,
    "downloads_all_time": null,
    "likes": 85,
    "tags": [
      "transformers",
      "gguf",
      "mistral",
      "facebook",
      "meta",
      "pytorch",
      "llama",
      "llama-3",
      "quantized",
      "2-bit",
      "3-bit",
      "4-bit",
      "5-bit",
      "6-bit",
      "8-bit",
      "16-bit",
      "GGUF",
      "text-generation",
      "en",
      "base_model:meta-llama/Meta-Llama-3-8B-Instruct",
      "base_model:quantized:meta-llama/Meta-Llama-3-8B-Instruct",
      "region:us",
      "conversational"
    ],
    "card_url": "https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF",
    "dependencies": [
      [
        "transformers",
        null
      ],
      [
        "torch",
        null
      ]
    ],
    "last_modified": "2024-04-23T12:55:06+00:00",
    "created_at": "2024-04-18T16:43:25+00:00",
    "analysis_date": "2025-03-22T00:41:35.817941",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "mistral",
    "pipeline_tag": "text-generation",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": [
        "en"
      ],
      "pipeline_tag": "text-generation",
      "tags": [
        "facebook",
        "meta",
        "pytorch",
        "llama",
        "llama-3",
        "quantized",
        "2-bit",
        "3-bit",
        "4-bit",
        "5-bit",
        "6-bit",
        "8-bit",
        "16-bit",
        "GGUF"
      ],
      "base_model": "meta-llama/Meta-Llama-3-8B-Instruct",
      "inference": false,
      "model_creator": "MaziyarPanahi",
      "model_name": "Meta-Llama-3-8B-Instruct-GGUF",
      "quantized_by": "MaziyarPanahi",
      "license_name": "llama3"
    },
    "card_text": "# [MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF](https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF)\n- Model creator: [meta-llama](https://huggingface.co/meta-llama)\n- Original model: [meta-llama/Meta-Llama-3-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct)\n\nThe GGUF and quantized models here are based on [meta-llama/Meta-Llama-3-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct) model\n\n## How to download\nYou can download only the quants you need instead of cloning the entire repository as follows:\n\n```\nhuggingface-cli download MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF --local-dir . --include '*Q2_K*gguf'\n```\n\n## Load GGUF models\n\nYou `MUST` follow the prompt template provided by Llama-3:\n\n\n```sh\n./llama.cpp/main -m Meta-Llama-3-8B-Instruct.Q2_K.gguf -r '<|eot_id|>' --in-prefix \"\\n<|start_header_id|>user<|end_header_id|>\\n\\n\" --in-suffix \"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\" -p \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a helpful, smart, kind, and efficient AI assistant. You always fulfill the user's requests to the best of your ability.<|eot_id|>\\n<|start_header_id|>user<|end_header_id|>\\n\\nHi! How are you?<|eot_id|>\\n<|start_header_id|>assistant<|end_header_id|>\\n\\n\" -n 1024\n```\n\n\n\n\nOriginal README\n\n---\n\n## Model Details\n\nMeta developed and released the Meta Llama 3 family of large language models (LLMs), a collection of pretrained and instruction tuned generative text models in 8 and 70B sizes. The Llama 3 instruction tuned models are optimized for dialogue use cases and outperform many of the available open source chat models on common industry benchmarks. Further, in developing these models, we took great care to optimize helpfulness and safety. \n\n**Model developers** Meta\n\n**Variations** Llama 3 comes in two sizes — 8B and 70B parameters — in pre-trained and instruction tuned variants.\n\n**Input** Models input text only.\n\n**Output** Models generate text and code only.\n\n**Model Architecture** Llama 3 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.\n\n\n<table>\n  <tr>\n   <td>\n   </td>\n   <td><strong>Training Data</strong>\n   </td>\n   <td><strong>Params</strong>\n   </td>\n   <td><strong>Context length</strong>\n   </td>\n   <td><strong>GQA</strong>\n   </td>\n   <td><strong>Token count</strong>\n   </td>\n   <td><strong>Knowledge cutoff</strong>\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"2\" >Llama 3\n   </td>\n   <td rowspan=\"2\" >A new mix of publicly available online data.\n   </td>\n   <td>8B\n   </td>\n   <td>8k\n   </td>\n   <td>Yes\n   </td>\n   <td rowspan=\"2\" >15T+\n   </td>\n   <td>March, 2023\n   </td>\n  </tr>\n  <tr>\n   <td>70B\n   </td>\n   <td>8k\n   </td>\n   <td>Yes\n   </td>\n   <td>December, 2023\n   </td>\n  </tr>\n</table>\n\n\n**Llama 3 family of models**. Token counts refer to pretraining data only. Both the 8 and 70B versions use Grouped-Query Attention (GQA) for improved inference scalability.\n\n**Model Release Date** April 18, 2024.\n\n**Status** This is a static model trained on an offline dataset. Future versions of the tuned models will be released as we improve model safety with community feedback.\n\n**License** A custom commercial license is available at: [https://llama.meta.com/llama3/license](https://llama.meta.com/llama3/license)\n\nWhere to send questions or comments about the model Instructions on how to provide feedback or comments on the model can be found in the model [README](https://github.com/meta-llama/llama3). For more technical information about generation parameters and recipes for how to use Llama 3 in applications, please go [here](https://github.com/meta-llama/llama-recipes). \n\n\n## Intended Use\n\n**Intended Use Cases** Llama 3 is intended for commercial and research use in English. Instruction tuned models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks.\n\n**Out-of-scope** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3 Community License. Use in languages other than English**.\n\n**Note: Developers may fine-tune Llama 3 models for languages beyond English provided they comply with the Llama 3 Community License and the Acceptable Use Policy.\n\n## How to use\n\nThis repository contains two versions of Meta-Llama-3-70B-Instruct, for use with transformers and with the original `llama3` codebase.\n\n### Use with transformers\n\nSee the snippet below for usage with Transformers:\n\n```python\nimport transformers\nimport torch\n\nmodel_id = \"meta-llama/Meta-Llama-3-70B-Instruct\"\n\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model_id,\n    model_kwargs={\"torch_dtype\": torch.bfloat16},\n    device=\"cuda\",\n)\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n    {\"role\": \"user\", \"content\": \"Who are you?\"},\n]\n\nprompt = pipeline.tokenizer.apply_chat_template(\n\t\tmessages, \n\t\ttokenize=False, \n\t\tadd_generation_prompt=True\n)\n\nterminators = [\n    tokenizer.eos_token_id,\n    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n]\n\noutputs = pipeline(\n    prompt,\n    max_new_tokens=256,\n    eos_token_id=terminators,\n    do_sample=True,\n    temperature=0.6,\n    top_p=0.9,\n)\nprint(outputs[0][\"generated_text\"][len(prompt):])\n```\n\n### Use with `llama3`\n\nPlease, follow the instructions in the [repository](https://github.com/meta-llama/llama3).\n\nTo download Original checkpoints, see the example command below leveraging `huggingface-cli`:\n\n```\nhuggingface-cli download meta-llama/Meta-Llama-3-70B-Instruct --include \"original/*\" --local-dir Meta-Llama-3-70B-Instruct\n```\n\nFor Hugging Face support, we recommend using transformers or TGI, but a similar command works.\n\n## Hardware and Software\n\n**Training Factors** We used custom training libraries, Meta's Research SuperCluster, and production clusters for pretraining. Fine-tuning, annotation, and evaluation were also performed on third-party cloud compute.\n\n**Carbon Footprint Pretraining utilized a cumulative** 7.7M GPU hours of computation on hardware of type H100-80GB (TDP of 700W). Estimated total emissions were 2290 tCO2eq, 100% of which were offset by Meta’s sustainability program.\n\n\n<table>\n  <tr>\n   <td>\n   </td>\n   <td><strong>Time (GPU hours)</strong>\n   </td>\n   <td><strong>Power Consumption (W)</strong>\n   </td>\n   <td><strong>Carbon Emitted(tCO2eq)</strong>\n   </td>\n  </tr>\n  <tr>\n   <td>Llama 3 8B\n   </td>\n   <td>1.3M\n   </td>\n   <td>700\n   </td>\n   <td>390\n   </td>\n  </tr>\n  <tr>\n   <td>Llama 3 70B\n   </td>\n   <td>6.4M\n   </td>\n   <td>700\n   </td>\n   <td>1900\n   </td>\n  </tr>\n  <tr>\n   <td>Total\n   </td>\n   <td>7.7M\n   </td>\n   <td>\n   </td>\n   <td>2290\n   </td>\n  </tr>\n</table>\n\n\n\n**CO2 emissions during pre-training**. Time: total GPU time required for training each model. Power Consumption: peak power capacity per GPU device for the GPUs used adjusted for power usage efficiency. 100% of the emissions are directly offset by Meta's sustainability program, and because we are openly releasing these models, the pretraining costs do not need to be incurred by others.\n\n\n## Training Data\n\n**Overview** Llama 3 was pretrained on over 15 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over 10M human-annotated examples. Neither the pretraining nor the fine-tuning datasets include Meta user data.\n\n**Data Freshness** The pretraining data has a cutoff of March 2023 for the 7B and December 2023 for the 70B models respectively. \n\n\n## Benchmarks \n\nIn this section, we report the results for Llama 3 models on standard automatic benchmarks. For all the evaluations, we use our internal evaluations library. For details on the methodology see [here](https://github.com/meta-llama/llama3/blob/main/eval_methodology.md).\n\n\n### Base pretrained models\n\n\n<table>\n  <tr>\n   <td><strong>Category</strong>\n   </td>\n   <td><strong>Benchmark</strong>\n   </td>\n   <td><strong>Llama 3 8B</strong>\n   </td>\n   <td><strong>Llama2 7B</strong>\n   </td>\n   <td><strong>Llama2 13B</strong>\n   </td>\n   <td><strong>Llama 3 70B</strong>\n   </td>\n   <td><strong>Llama2 70B</strong>\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"6\" >General\n   </td>\n   <td>MMLU (5-shot)\n   </td>\n   <td>66.6\n   </td>\n   <td>45.7\n   </td>\n   <td>53.8\n   </td>\n   <td>79.5\n   </td>\n   <td>69.7\n   </td>\n  </tr>\n  <tr>\n   <td>AGIEval English (3-5 shot)\n   </td>\n   <td>45.9\n   </td>\n   <td>28.8\n   </td>\n   <td>38.7\n   </td>\n   <td>63.0\n   </td>\n   <td>54.8\n   </td>\n  </tr>\n  <tr>\n   <td>CommonSenseQA (7-shot)\n   </td>\n   <td>72.6\n   </td>\n   <td>57.6\n   </td>\n   <td>67.6\n   </td>\n   <td>83.8\n   </td>\n   <td>78.7\n   </td>\n  </tr>\n  <tr>\n   <td>Winogrande (5-shot)\n   </td>\n   <td>76.1\n   </td>\n   <td>73.3\n   </td>\n   <td>75.4\n   </td>\n   <td>83.1\n   </td>\n   <td>81.8\n   </td>\n  </tr>\n  <tr>\n   <td>BIG-Bench Hard (3-shot, CoT)\n   </td>\n   <td>61.1\n   </td>\n   <td>38.1\n   </td>\n   <td>47.0\n   </td>\n   <td>81.3\n   </td>\n   <td>65.7\n   </td>\n  </tr>\n  <tr>\n   <td>ARC-Challenge (25-shot)\n   </td>\n   <td>78.6\n   </td>\n   <td>53.7\n   </td>\n   <td>67.6\n   </td>\n   <td>93.0\n   </td>\n   <td>85.3\n   </td>\n  </tr>\n  <tr>\n   <td>Knowledge reasoning\n   </td>\n   <td>TriviaQA-Wiki (5-shot)\n   </td>\n   <td>78.5\n   </td>\n   <td>72.1\n   </td>\n   <td>79.6\n   </td>\n   <td>89.7\n   </td>\n   <td>87.5\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"4\" >Reading comprehension\n   </td>\n   <td>SQuAD (1-shot)\n   </td>\n   <td>76.4\n   </td>\n   <td>72.2\n   </td>\n   <td>72.1\n   </td>\n   <td>85.6\n   </td>\n   <td>82.6\n   </td>\n  </tr>\n  <tr>\n   <td>QuAC (1-shot, F1)\n   </td>\n   <td>44.4\n   </td>\n   <td>39.6\n   </td>\n   <td>44.9\n   </td>\n   <td>51.1\n   </td>\n   <td>49.4\n   </td>\n  </tr>\n  <tr>\n   <td>BoolQ (0-shot)\n   </td>\n   <td>75.7\n   </td>\n   <td>65.5\n   </td>\n   <td>66.9\n   </td>\n   <td>79.0\n   </td>\n   <td>73.1\n   </td>\n  </tr>\n  <tr>\n   <td>DROP (3-shot, F1)\n   </td>\n   <td>58.4\n   </td>\n   <td>37.9\n   </td>\n   <td>49.8\n   </td>\n   <td>79.7\n   </td>\n   <td>70.2\n   </td>\n  </tr>\n</table>\n\n\n\n### Instruction tuned models\n\n\n<table>\n  <tr>\n   <td><strong>Benchmark</strong>\n   </td>\n   <td><strong>Llama 3 8B</strong>\n   </td>\n   <td><strong>Llama 2 7B</strong>\n   </td>\n   <td><strong>Llama 2 13B</strong>\n   </td>\n   <td><strong>Llama 3 70B</strong>\n   </td>\n   <td><strong>Llama 2 70B</strong>\n   </td>\n  </tr>\n  <tr>\n   <td>MMLU (5-shot)\n   </td>\n   <td>68.4\n   </td>\n   <td>34.1\n   </td>\n   <td>47.8\n   </td>\n   <td>82.0\n   </td>\n   <td>52.9\n   </td>\n  </tr>\n  <tr>\n   <td>GPQA (0-shot)\n   </td>\n   <td>34.2\n   </td>\n   <td>21.7\n   </td>\n   <td>22.3\n   </td>\n   <td>39.5\n   </td>\n   <td>21.0\n   </td>\n  </tr>\n  <tr>\n   <td>HumanEval (0-shot)\n   </td>\n   <td>62.2\n   </td>\n   <td>7.9\n   </td>\n   <td>14.0\n   </td>\n   <td>81.7\n   </td>\n   <td>25.6\n   </td>\n  </tr>\n  <tr>\n   <td>GSM-8K (8-shot, CoT)\n   </td>\n   <td>79.6\n   </td>\n   <td>25.7\n   </td>\n   <td>77.4\n   </td>\n   <td>93.0\n   </td>\n   <td>57.5\n   </td>\n  </tr>\n  <tr>\n   <td>MATH (4-shot, CoT)\n   </td>\n   <td>30.0\n   </td>\n   <td>3.8\n   </td>\n   <td>6.7\n   </td>\n   <td>50.4\n   </td>\n   <td>11.6\n   </td>\n  </tr>\n</table>\n\n\n\n### Responsibility & Safety\n\nWe believe that an open approach to AI leads to better, safer products, faster innovation, and a bigger overall market. We are committed to Responsible AI development and took a series of steps to limit misuse and harm and support the open source community.\n\nFoundation models are widely capable technologies that are built to be used for a diverse range of applications. They are not designed to meet every developer preference on safety levels for all use cases, out-of-the-box, as those by their nature will differ across different applications. \n\nRather, responsible LLM-application deployment is achieved by implementing a series of safety best practices throughout the development of such applications, from the model pre-training, fine-tuning and the deployment of systems composed of safeguards to tailor the safety needs specifically to the use case and audience. \n\n\nAs part of the Llama 3 release, we updated our [Responsible Use Guide](https://llama.meta.com/responsible-use-guide/) to outline the steps and best practices for developers to implement model and system level safety for their application. We also provide a set of resources including [Meta Llama Guard 2](https://llama.meta.com/purple-llama/) and [Code Shield](https://llama.meta.com/purple-llama/) safeguards. These tools have proven to drastically reduce residual risks of LLM Systems, while maintaining a high level of helpfulness. We encourage developers to tune and deploy these safeguards according to their needs and we provide a [reference implementation](https://github.com/meta-llama/llama-recipes/tree/main/recipes/responsible_ai) to get you started.\n\n\n#### Llama 3-Instruct\n\nAs outlined in the Responsible Use Guide, some trade-off between model helpfulness and model alignment is likely unavoidable. Developers should exercise discretion about how to weigh the benefits of alignment and helpfulness for their specific use case and audience. Developers should be mindful of residual risks when using Llama models and leverage additional safety tools as needed to reach the right safety bar for their use case. \n\n<span style=\"text-decoration:underline;\">Safety</span>\n\nFor our instruction tuned model, we conducted extensive red teaming exercises, performed adversarial evaluations and implemented safety mitigations techniques to lower residual risks. As with any Large Language Model, residual risks will likely remain and we recommend that developers assess these risks in the context of their use case. In parallel, we are working with the community to make AI safety benchmark standards transparent, rigorous and interpretable. \n\n<span style=\"text-decoration:underline;\">Refusals</span>\n\nIn addition to residual risks, we put a great emphasis on model refusals to benign prompts. Over-refusing not only can impact the user experience but could even be harmful in certain contexts as well. We’ve heard the feedback from the developer community and improved our fine tuning to ensure that Llama 3 is significantly less likely to falsely refuse to answer prompts than Llama 2. \n\nWe built internal benchmarks and developed mitigations to limit false refusals making Llama 3 our most helpful model to date. \n\n\n#### Responsible release \n\nIn addition to responsible use considerations outlined above, we followed a rigorous process that requires us to take extra measures against misuse and critical risks before we make our release decision. \n\nMisuse\n\nIf you access or use Llama 3, you agree to the Acceptable Use Policy. The most recent copy of this policy can be found at [https://llama.meta.com/llama3/use-policy/](https://llama.meta.com/llama3/use-policy/).\n\n\n#### Critical risks \n\n<span style=\"text-decoration:underline;\">CBRNE</span> (Chemical, Biological, Radiological, Nuclear, and high yield Explosives)\n\nWe have conducted a two fold assessment of the safety of the model in this area:\n\n\n\n* Iterative testing during model training to assess the safety of responses related to CBRNE threats and other adversarial risks.\n* Involving external CBRNE experts to conduct an uplift test assessing the ability of the model to accurately provide expert knowledge and reduce barriers to potential CBRNE misuse, by reference to what can be achieved using web search (without the model).\n\n\n### <span style=\"text-decoration:underline;\">Cyber Security </span>\n\nWe have evaluated Llama 3 with CyberSecEval, Meta’s cybersecurity safety eval suite, measuring Llama 3’s propensity to suggest insecure code when used as a coding assistant, and Llama 3’s propensity to comply with requests to help carry out cyber attacks, where attacks are defined by the industry standard MITRE ATT&CK cyber attack ontology. On our insecure coding and cyber attacker helpfulness tests, Llama 3 behaved in the same range or safer than models of [equivalent coding capability](https://huggingface.co/spaces/facebook/CyberSecEval). \n\n\n### <span style=\"text-decoration:underline;\">Child Safety</span>\n\nChild Safety risk assessments were conducted using a team of experts, to assess the model’s capability to produce outputs that could result in Child Safety risks and inform on any necessary and appropriate risk mitigations via fine tuning. We leveraged those expert red teaming sessions to expand the coverage of our evaluation benchmarks through Llama 3 model development.  For Llama 3, we conducted new in-depth sessions using objective based methodologies to assess the model risks along multiple attack vectors. We also partnered with content specialists to perform red teaming exercises assessing potentially violating content while taking account of market specific nuances or experiences. \n\n\n### Community \n\nGenerative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership in AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Purple Llama tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to our [Github repository](https://github.com/meta-llama/PurpleLlama). \n\nFinally, we put in place a set of resources including an [output reporting mechanism](https://developers.facebook.com/llama_output_feedback) and [bug bounty program](https://www.facebook.com/whitehat) to continuously improve the Llama technology with the help of the community. \n\n\n## Ethical Considerations and Limitations\n\nThe core values of Llama 3 are openness, inclusivity and helpfulness. It is meant to serve everyone, and to work for a wide range of use cases. It is thus designed to be accessible to people across many different backgrounds, experiences and perspectives. Llama 3 addresses users and their needs as they are, without insertion unnecessary judgment or normativity, while reflecting the understanding that even content that may appear problematic in some cases can serve valuable purposes in others. It respects the dignity and autonomy of all users, especially in terms of the values of free thought and expression that power innovation and progress. \n\nBut Llama 3 is a new technology, and like any new technology, there are risks associated with its use. Testing conducted to date has been in English, and has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, Llama 3’s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 3 models, developers should perform safety testing and tuning tailored to their specific applications of the model. As outlined in the Responsible Use Guide, we recommend incorporating [Purple Llama](https://github.com/facebookresearch/PurpleLlama) solutions into your workflows and specifically [Llama Guard](https://ai.meta.com/research/publications/llama-guard-llm-based-input-output-safeguard-for-human-ai-conversations/) which provides a base model to filter input and output prompts to layer system-level safety on top of model-level safety. \n\nPlease see the Responsible Use Guide available at [http://llama.meta.com/responsible-use-guide](http://llama.meta.com/responsible-use-guide)\n\n\n## Citation instructions\n\n@article{llama3modelcard,\n\n  title={Llama 3 Model Card},\n\n  author={AI@Meta},\n\n  year={2024},\n\n  url = {https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md}\n\n}\n\n## Contributors\n\nAaditya Singh; Aaron Grattafiori; Abhimanyu Dubey; Abhinav Jauhri; Abhinav Pandey; Abhishek Kadian; Adam Kelsey; Adi Gangidi; Ahmad Al-Dahle; Ahuva Goldstand; Aiesha Letman; Ajay Menon; Akhil Mathur; Alan Schelten; Alex Vaughan; Amy Yang; Andrei Lupu; Andres Alvarado; Andrew Gallagher; Andrew Gu; Andrew Ho; Andrew Poulton; Andrew Ryan; Angela Fan; Ankit Ramchandani; Anthony Hartshorn; Archi Mitra; Archie Sravankumar; Artem Korenev; Arun Rao; Ashley Gabriel; Ashwin Bharambe; Assaf Eisenman; Aston Zhang; Aurelien Rodriguez; Austen Gregerson; Ava Spataru; Baptiste Roziere; Ben Maurer; Benjamin Leonhardi; Bernie Huang; Bhargavi Paranjape; Bing Liu; Binh Tang; Bobbie Chern; Brani Stojkovic; Brian Fuller; Catalina Mejia Arenas; Chao Zhou; Charlotte Caucheteux; Chaya Nayak; Ching-Hsiang Chu; Chloe Bi; Chris Cai; Chris Cox; Chris Marra; Chris McConnell; Christian Keller; Christoph Feichtenhofer; Christophe Touret; Chunyang Wu; Corinne Wong; Cristian Canton Ferrer; Damien Allonsius; Daniel Kreymer; Daniel Haziza; Daniel Li; Danielle Pintz; Danny Livshits; Danny Wyatt; David Adkins; David Esiobu; David Xu; Davide Testuggine; Delia David; Devi Parikh; Dhruv Choudhary; Dhruv Mahajan; Diana Liskovich; Diego Garcia-Olano; Diego Perino; Dieuwke Hupkes; Dingkang Wang; Dustin Holland; Egor Lakomkin; Elina Lobanova; Xiaoqing Ellen Tan; Emily Dinan; Eric Smith; Erik Brinkman; Esteban Arcaute; Filip Radenovic; Firat Ozgenel; Francesco Caggioni; Frank Seide; Frank Zhang; Gabriel Synnaeve; Gabriella Schwarz; Gabrielle Lee; Gada Badeer; Georgia Anderson; Graeme Nail; Gregoire Mialon; Guan Pang; Guillem Cucurell; Hailey Nguyen; Hannah Korevaar; Hannah Wang; Haroun Habeeb; Harrison Rudolph; Henry Aspegren; Hu Xu; Hugo Touvron; Iga Kozlowska; Igor Molybog; Igor Tufanov; Iliyan Zarov; Imanol Arrieta Ibarra; Irina-Elena Veliche; Isabel Kloumann; Ishan Misra; Ivan Evtimov; Jacob Xu; Jade Copet; Jake Weissman; Jan Geffert; Jana Vranes; Japhet Asher; Jason Park; Jay Mahadeokar; Jean-Baptiste Gaya; Jeet Shah; Jelmer van der Linde; Jennifer Chan; Jenny Hong; Jenya Lee; Jeremy Fu; Jeremy Teboul; Jianfeng Chi; Jianyu Huang; Jie Wang; Jiecao Yu; Joanna Bitton; Joe Spisak; Joelle Pineau; Jon Carvill; Jongsoo Park; Joseph Rocca; Joshua Johnstun; Junteng Jia; Kalyan Vasuden Alwala; Kam Hou U; Kate Plawiak; Kartikeya Upasani; Kaushik Veeraraghavan; Ke Li; Kenneth Heafield; Kevin Stone; Khalid El-Arini; Krithika Iyer; Kshitiz Malik; Kuenley Chiu; Kunal Bhalla; Kyle Huang; Lakshya Garg; Lauren Rantala-Yeary; Laurens van der Maaten; Lawrence Chen; Leandro Silva; Lee Bell; Lei Zhang; Liang Tan; Louis Martin; Lovish Madaan; Luca Wehrstedt; Lukas Blecher; Luke de Oliveira; Madeline Muzzi; Madian Khabsa; Manav Avlani; Mannat Singh; Manohar Paluri; Mark Zuckerberg; Marcin Kardas; Martynas Mankus; Mathew Oldham; Mathieu Rita; Matthew Lennie; Maya Pavlova; Meghan Keneally; Melanie Kambadur; Mihir Patel; Mikayel Samvelyan; Mike Clark; Mike Lewis; Min Si; Mitesh Kumar Singh; Mo Metanat; Mona Hassan; Naman Goyal; Narjes Torabi; Nicolas Usunier; Nikolay Bashlykov; Nikolay Bogoychev; Niladri Chatterji; Ning Dong; Oliver Aobo Yang; Olivier Duchenne; Onur Celebi; Parth Parekh; Patrick Alrassy; Paul Saab; Pavan Balaji; Pedro Rittner; Pengchuan Zhang; Pengwei Li; Petar Vasic; Peter Weng; Polina Zvyagina; Prajjwal Bhargava; Pratik Dubal; Praveen Krishnan; Punit Singh Koura; Qing He; Rachel Rodriguez; Ragavan Srinivasan; Rahul Mitra; Ramon Calderer; Raymond Li; Robert Stojnic; Roberta Raileanu; Robin Battey; Rocky Wang; Rohit Girdhar; Rohit Patel; Romain Sauvestre; Ronnie Polidoro; Roshan Sumbaly; Ross Taylor; Ruan Silva; Rui Hou; Rui Wang; Russ Howes; Ruty Rinott; Saghar Hosseini; Sai Jayesh Bondu; Samyak Datta; Sanjay Singh; Sara Chugh; Sargun Dhillon; Satadru Pan; Sean Bell; Sergey Edunov; Shaoliang Nie; Sharan Narang; Sharath Raparthy; Shaun Lindsay; Sheng Feng; Sheng Shen; Shenghao Lin; Shiva Shankar; Shruti Bhosale; Shun Zhang; Simon Vandenhende; Sinong Wang; Seohyun Sonia Kim; Soumya Batra; Sten Sootla; Steve Kehoe; Suchin Gururangan; Sumit Gupta; Sunny Virk; Sydney Borodinsky; Tamar Glaser; Tamar Herman; Tamara Best; Tara Fowler; Thomas Georgiou; Thomas Scialom; Tianhe Li; Todor Mihaylov; Tong Xiao; Ujjwal Karn; Vedanuj Goswami; Vibhor Gupta; Vignesh Ramanathan; Viktor Kerkez; Vinay Satish Kumar; Vincent Gonguet; Vish Vogeti; Vlad Poenaru; Vlad Tiberiu Mihailescu; Vladan Petrovic; Vladimir Ivanov; Wei Li; Weiwei Chu; Wenhan Xiong; Wenyin Fu; Wes Bouaziz; Whitney Meers; Will Constable; Xavier Martinet; Xiaojian Wu; Xinbo Gao; Xinfeng Xie; Xuchao Jia; Yaelle Goldschlag; Yann LeCun; Yashesh Gaur; Yasmine Babaei; Ye Qi; Yenda Li; Yi Wen; Yiwen Song; Youngjin Nam; Yuchen Hao; Yuchen Zhang; Yun Wang; Yuning Mao; Yuzi He; Zacharie Delpierre Coudert; Zachary DeVito; Zahra Hankir; Zhaoduo Wen; Zheng Yan; Zhengxing Chen; Zhenyu Yang; Zoe Papakipos\n\n---\n",
    "card_content": "---\nlanguage:\n- en\npipeline_tag: text-generation\ntags:\n- facebook\n- meta\n- pytorch\n- llama\n- llama-3\n- quantized\n- 2-bit\n- 3-bit\n- 4-bit\n- 5-bit\n- 6-bit\n- 8-bit\n- 16-bit\n- GGUF\nbase_model: meta-llama/Meta-Llama-3-8B-Instruct\ninference: false\nmodel_creator: MaziyarPanahi\nmodel_name: Meta-Llama-3-8B-Instruct-GGUF\nquantized_by: MaziyarPanahi\nlicense_name: llama3\n---\n# [MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF](https://huggingface.co/MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF)\n- Model creator: [meta-llama](https://huggingface.co/meta-llama)\n- Original model: [meta-llama/Meta-Llama-3-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct)\n\nThe GGUF and quantized models here are based on [meta-llama/Meta-Llama-3-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct) model\n\n## How to download\nYou can download only the quants you need instead of cloning the entire repository as follows:\n\n```\nhuggingface-cli download MaziyarPanahi/Meta-Llama-3-8B-Instruct-GGUF --local-dir . --include '*Q2_K*gguf'\n```\n\n## Load GGUF models\n\nYou `MUST` follow the prompt template provided by Llama-3:\n\n\n```sh\n./llama.cpp/main -m Meta-Llama-3-8B-Instruct.Q2_K.gguf -r '<|eot_id|>' --in-prefix \"\\n<|start_header_id|>user<|end_header_id|>\\n\\n\" --in-suffix \"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\" -p \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a helpful, smart, kind, and efficient AI assistant. You always fulfill the user's requests to the best of your ability.<|eot_id|>\\n<|start_header_id|>user<|end_header_id|>\\n\\nHi! How are you?<|eot_id|>\\n<|start_header_id|>assistant<|end_header_id|>\\n\\n\" -n 1024\n```\n\n\n\n\nOriginal README\n\n---\n\n## Model Details\n\nMeta developed and released the Meta Llama 3 family of large language models (LLMs), a collection of pretrained and instruction tuned generative text models in 8 and 70B sizes. The Llama 3 instruction tuned models are optimized for dialogue use cases and outperform many of the available open source chat models on common industry benchmarks. Further, in developing these models, we took great care to optimize helpfulness and safety. \n\n**Model developers** Meta\n\n**Variations** Llama 3 comes in two sizes — 8B and 70B parameters — in pre-trained and instruction tuned variants.\n\n**Input** Models input text only.\n\n**Output** Models generate text and code only.\n\n**Model Architecture** Llama 3 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.\n\n\n<table>\n  <tr>\n   <td>\n   </td>\n   <td><strong>Training Data</strong>\n   </td>\n   <td><strong>Params</strong>\n   </td>\n   <td><strong>Context length</strong>\n   </td>\n   <td><strong>GQA</strong>\n   </td>\n   <td><strong>Token count</strong>\n   </td>\n   <td><strong>Knowledge cutoff</strong>\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"2\" >Llama 3\n   </td>\n   <td rowspan=\"2\" >A new mix of publicly available online data.\n   </td>\n   <td>8B\n   </td>\n   <td>8k\n   </td>\n   <td>Yes\n   </td>\n   <td rowspan=\"2\" >15T+\n   </td>\n   <td>March, 2023\n   </td>\n  </tr>\n  <tr>\n   <td>70B\n   </td>\n   <td>8k\n   </td>\n   <td>Yes\n   </td>\n   <td>December, 2023\n   </td>\n  </tr>\n</table>\n\n\n**Llama 3 family of models**. Token counts refer to pretraining data only. Both the 8 and 70B versions use Grouped-Query Attention (GQA) for improved inference scalability.\n\n**Model Release Date** April 18, 2024.\n\n**Status** This is a static model trained on an offline dataset. Future versions of the tuned models will be released as we improve model safety with community feedback.\n\n**License** A custom commercial license is available at: [https://llama.meta.com/llama3/license](https://llama.meta.com/llama3/license)\n\nWhere to send questions or comments about the model Instructions on how to provide feedback or comments on the model can be found in the model [README](https://github.com/meta-llama/llama3). For more technical information about generation parameters and recipes for how to use Llama 3 in applications, please go [here](https://github.com/meta-llama/llama-recipes). \n\n\n## Intended Use\n\n**Intended Use Cases** Llama 3 is intended for commercial and research use in English. Instruction tuned models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks.\n\n**Out-of-scope** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3 Community License. Use in languages other than English**.\n\n**Note: Developers may fine-tune Llama 3 models for languages beyond English provided they comply with the Llama 3 Community License and the Acceptable Use Policy.\n\n## How to use\n\nThis repository contains two versions of Meta-Llama-3-70B-Instruct, for use with transformers and with the original `llama3` codebase.\n\n### Use with transformers\n\nSee the snippet below for usage with Transformers:\n\n```python\nimport transformers\nimport torch\n\nmodel_id = \"meta-llama/Meta-Llama-3-70B-Instruct\"\n\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model_id,\n    model_kwargs={\"torch_dtype\": torch.bfloat16},\n    device=\"cuda\",\n)\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n    {\"role\": \"user\", \"content\": \"Who are you?\"},\n]\n\nprompt = pipeline.tokenizer.apply_chat_template(\n\t\tmessages, \n\t\ttokenize=False, \n\t\tadd_generation_prompt=True\n)\n\nterminators = [\n    tokenizer.eos_token_id,\n    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n]\n\noutputs = pipeline(\n    prompt,\n    max_new_tokens=256,\n    eos_token_id=terminators,\n    do_sample=True,\n    temperature=0.6,\n    top_p=0.9,\n)\nprint(outputs[0][\"generated_text\"][len(prompt):])\n```\n\n### Use with `llama3`\n\nPlease, follow the instructions in the [repository](https://github.com/meta-llama/llama3).\n\nTo download Original checkpoints, see the example command below leveraging `huggingface-cli`:\n\n```\nhuggingface-cli download meta-llama/Meta-Llama-3-70B-Instruct --include \"original/*\" --local-dir Meta-Llama-3-70B-Instruct\n```\n\nFor Hugging Face support, we recommend using transformers or TGI, but a similar command works.\n\n## Hardware and Software\n\n**Training Factors** We used custom training libraries, Meta's Research SuperCluster, and production clusters for pretraining. Fine-tuning, annotation, and evaluation were also performed on third-party cloud compute.\n\n**Carbon Footprint Pretraining utilized a cumulative** 7.7M GPU hours of computation on hardware of type H100-80GB (TDP of 700W). Estimated total emissions were 2290 tCO2eq, 100% of which were offset by Meta’s sustainability program.\n\n\n<table>\n  <tr>\n   <td>\n   </td>\n   <td><strong>Time (GPU hours)</strong>\n   </td>\n   <td><strong>Power Consumption (W)</strong>\n   </td>\n   <td><strong>Carbon Emitted(tCO2eq)</strong>\n   </td>\n  </tr>\n  <tr>\n   <td>Llama 3 8B\n   </td>\n   <td>1.3M\n   </td>\n   <td>700\n   </td>\n   <td>390\n   </td>\n  </tr>\n  <tr>\n   <td>Llama 3 70B\n   </td>\n   <td>6.4M\n   </td>\n   <td>700\n   </td>\n   <td>1900\n   </td>\n  </tr>\n  <tr>\n   <td>Total\n   </td>\n   <td>7.7M\n   </td>\n   <td>\n   </td>\n   <td>2290\n   </td>\n  </tr>\n</table>\n\n\n\n**CO2 emissions during pre-training**. Time: total GPU time required for training each model. Power Consumption: peak power capacity per GPU device for the GPUs used adjusted for power usage efficiency. 100% of the emissions are directly offset by Meta's sustainability program, and because we are openly releasing these models, the pretraining costs do not need to be incurred by others.\n\n\n## Training Data\n\n**Overview** Llama 3 was pretrained on over 15 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over 10M human-annotated examples. Neither the pretraining nor the fine-tuning datasets include Meta user data.\n\n**Data Freshness** The pretraining data has a cutoff of March 2023 for the 7B and December 2023 for the 70B models respectively. \n\n\n## Benchmarks \n\nIn this section, we report the results for Llama 3 models on standard automatic benchmarks. For all the evaluations, we use our internal evaluations library. For details on the methodology see [here](https://github.com/meta-llama/llama3/blob/main/eval_methodology.md).\n\n\n### Base pretrained models\n\n\n<table>\n  <tr>\n   <td><strong>Category</strong>\n   </td>\n   <td><strong>Benchmark</strong>\n   </td>\n   <td><strong>Llama 3 8B</strong>\n   </td>\n   <td><strong>Llama2 7B</strong>\n   </td>\n   <td><strong>Llama2 13B</strong>\n   </td>\n   <td><strong>Llama 3 70B</strong>\n   </td>\n   <td><strong>Llama2 70B</strong>\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"6\" >General\n   </td>\n   <td>MMLU (5-shot)\n   </td>\n   <td>66.6\n   </td>\n   <td>45.7\n   </td>\n   <td>53.8\n   </td>\n   <td>79.5\n   </td>\n   <td>69.7\n   </td>\n  </tr>\n  <tr>\n   <td>AGIEval English (3-5 shot)\n   </td>\n   <td>45.9\n   </td>\n   <td>28.8\n   </td>\n   <td>38.7\n   </td>\n   <td>63.0\n   </td>\n   <td>54.8\n   </td>\n  </tr>\n  <tr>\n   <td>CommonSenseQA (7-shot)\n   </td>\n   <td>72.6\n   </td>\n   <td>57.6\n   </td>\n   <td>67.6\n   </td>\n   <td>83.8\n   </td>\n   <td>78.7\n   </td>\n  </tr>\n  <tr>\n   <td>Winogrande (5-shot)\n   </td>\n   <td>76.1\n   </td>\n   <td>73.3\n   </td>\n   <td>75.4\n   </td>\n   <td>83.1\n   </td>\n   <td>81.8\n   </td>\n  </tr>\n  <tr>\n   <td>BIG-Bench Hard (3-shot, CoT)\n   </td>\n   <td>61.1\n   </td>\n   <td>38.1\n   </td>\n   <td>47.0\n   </td>\n   <td>81.3\n   </td>\n   <td>65.7\n   </td>\n  </tr>\n  <tr>\n   <td>ARC-Challenge (25-shot)\n   </td>\n   <td>78.6\n   </td>\n   <td>53.7\n   </td>\n   <td>67.6\n   </td>\n   <td>93.0\n   </td>\n   <td>85.3\n   </td>\n  </tr>\n  <tr>\n   <td>Knowledge reasoning\n   </td>\n   <td>TriviaQA-Wiki (5-shot)\n   </td>\n   <td>78.5\n   </td>\n   <td>72.1\n   </td>\n   <td>79.6\n   </td>\n   <td>89.7\n   </td>\n   <td>87.5\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"4\" >Reading comprehension\n   </td>\n   <td>SQuAD (1-shot)\n   </td>\n   <td>76.4\n   </td>\n   <td>72.2\n   </td>\n   <td>72.1\n   </td>\n   <td>85.6\n   </td>\n   <td>82.6\n   </td>\n  </tr>\n  <tr>\n   <td>QuAC (1-shot, F1)\n   </td>\n   <td>44.4\n   </td>\n   <td>39.6\n   </td>\n   <td>44.9\n   </td>\n   <td>51.1\n   </td>\n   <td>49.4\n   </td>\n  </tr>\n  <tr>\n   <td>BoolQ (0-shot)\n   </td>\n   <td>75.7\n   </td>\n   <td>65.5\n   </td>\n   <td>66.9\n   </td>\n   <td>79.0\n   </td>\n   <td>73.1\n   </td>\n  </tr>\n  <tr>\n   <td>DROP (3-shot, F1)\n   </td>\n   <td>58.4\n   </td>\n   <td>37.9\n   </td>\n   <td>49.8\n   </td>\n   <td>79.7\n   </td>\n   <td>70.2\n   </td>\n  </tr>\n</table>\n\n\n\n### Instruction tuned models\n\n\n<table>\n  <tr>\n   <td><strong>Benchmark</strong>\n   </td>\n   <td><strong>Llama 3 8B</strong>\n   </td>\n   <td><strong>Llama 2 7B</strong>\n   </td>\n   <td><strong>Llama 2 13B</strong>\n   </td>\n   <td><strong>Llama 3 70B</strong>\n   </td>\n   <td><strong>Llama 2 70B</strong>\n   </td>\n  </tr>\n  <tr>\n   <td>MMLU (5-shot)\n   </td>\n   <td>68.4\n   </td>\n   <td>34.1\n   </td>\n   <td>47.8\n   </td>\n   <td>82.0\n   </td>\n   <td>52.9\n   </td>\n  </tr>\n  <tr>\n   <td>GPQA (0-shot)\n   </td>\n   <td>34.2\n   </td>\n   <td>21.7\n   </td>\n   <td>22.3\n   </td>\n   <td>39.5\n   </td>\n   <td>21.0\n   </td>\n  </tr>\n  <tr>\n   <td>HumanEval (0-shot)\n   </td>\n   <td>62.2\n   </td>\n   <td>7.9\n   </td>\n   <td>14.0\n   </td>\n   <td>81.7\n   </td>\n   <td>25.6\n   </td>\n  </tr>\n  <tr>\n   <td>GSM-8K (8-shot, CoT)\n   </td>\n   <td>79.6\n   </td>\n   <td>25.7\n   </td>\n   <td>77.4\n   </td>\n   <td>93.0\n   </td>\n   <td>57.5\n   </td>\n  </tr>\n  <tr>\n   <td>MATH (4-shot, CoT)\n   </td>\n   <td>30.0\n   </td>\n   <td>3.8\n   </td>\n   <td>6.7\n   </td>\n   <td>50.4\n   </td>\n   <td>11.6\n   </td>\n  </tr>\n</table>\n\n\n\n### Responsibility & Safety\n\nWe believe that an open approach to AI leads to better, safer products, faster innovation, and a bigger overall market. We are committed to Responsible AI development and took a series of steps to limit misuse and harm and support the open source community.\n\nFoundation models are widely capable technologies that are built to be used for a diverse range of applications. They are not designed to meet every developer preference on safety levels for all use cases, out-of-the-box, as those by their nature will differ across different applications. \n\nRather, responsible LLM-application deployment is achieved by implementing a series of safety best practices throughout the development of such applications, from the model pre-training, fine-tuning and the deployment of systems composed of safeguards to tailor the safety needs specifically to the use case and audience. \n\n\nAs part of the Llama 3 release, we updated our [Responsible Use Guide](https://llama.meta.com/responsible-use-guide/) to outline the steps and best practices for developers to implement model and system level safety for their application. We also provide a set of resources including [Meta Llama Guard 2](https://llama.meta.com/purple-llama/) and [Code Shield](https://llama.meta.com/purple-llama/) safeguards. These tools have proven to drastically reduce residual risks of LLM Systems, while maintaining a high level of helpfulness. We encourage developers to tune and deploy these safeguards according to their needs and we provide a [reference implementation](https://github.com/meta-llama/llama-recipes/tree/main/recipes/responsible_ai) to get you started.\n\n\n#### Llama 3-Instruct\n\nAs outlined in the Responsible Use Guide, some trade-off between model helpfulness and model alignment is likely unavoidable. Developers should exercise discretion about how to weigh the benefits of alignment and helpfulness for their specific use case and audience. Developers should be mindful of residual risks when using Llama models and leverage additional safety tools as needed to reach the right safety bar for their use case. \n\n<span style=\"text-decoration:underline;\">Safety</span>\n\nFor our instruction tuned model, we conducted extensive red teaming exercises, performed adversarial evaluations and implemented safety mitigations techniques to lower residual risks. As with any Large Language Model, residual risks will likely remain and we recommend that developers assess these risks in the context of their use case. In parallel, we are working with the community to make AI safety benchmark standards transparent, rigorous and interpretable. \n\n<span style=\"text-decoration:underline;\">Refusals</span>\n\nIn addition to residual risks, we put a great emphasis on model refusals to benign prompts. Over-refusing not only can impact the user experience but could even be harmful in certain contexts as well. We’ve heard the feedback from the developer community and improved our fine tuning to ensure that Llama 3 is significantly less likely to falsely refuse to answer prompts than Llama 2. \n\nWe built internal benchmarks and developed mitigations to limit false refusals making Llama 3 our most helpful model to date. \n\n\n#### Responsible release \n\nIn addition to responsible use considerations outlined above, we followed a rigorous process that requires us to take extra measures against misuse and critical risks before we make our release decision. \n\nMisuse\n\nIf you access or use Llama 3, you agree to the Acceptable Use Policy. The most recent copy of this policy can be found at [https://llama.meta.com/llama3/use-policy/](https://llama.meta.com/llama3/use-policy/).\n\n\n#### Critical risks \n\n<span style=\"text-decoration:underline;\">CBRNE</span> (Chemical, Biological, Radiological, Nuclear, and high yield Explosives)\n\nWe have conducted a two fold assessment of the safety of the model in this area:\n\n\n\n* Iterative testing during model training to assess the safety of responses related to CBRNE threats and other adversarial risks.\n* Involving external CBRNE experts to conduct an uplift test assessing the ability of the model to accurately provide expert knowledge and reduce barriers to potential CBRNE misuse, by reference to what can be achieved using web search (without the model).\n\n\n### <span style=\"text-decoration:underline;\">Cyber Security </span>\n\nWe have evaluated Llama 3 with CyberSecEval, Meta’s cybersecurity safety eval suite, measuring Llama 3’s propensity to suggest insecure code when used as a coding assistant, and Llama 3’s propensity to comply with requests to help carry out cyber attacks, where attacks are defined by the industry standard MITRE ATT&CK cyber attack ontology. On our insecure coding and cyber attacker helpfulness tests, Llama 3 behaved in the same range or safer than models of [equivalent coding capability](https://huggingface.co/spaces/facebook/CyberSecEval). \n\n\n### <span style=\"text-decoration:underline;\">Child Safety</span>\n\nChild Safety risk assessments were conducted using a team of experts, to assess the model’s capability to produce outputs that could result in Child Safety risks and inform on any necessary and appropriate risk mitigations via fine tuning. We leveraged those expert red teaming sessions to expand the coverage of our evaluation benchmarks through Llama 3 model development.  For Llama 3, we conducted new in-depth sessions using objective based methodologies to assess the model risks along multiple attack vectors. We also partnered with content specialists to perform red teaming exercises assessing potentially violating content while taking account of market specific nuances or experiences. \n\n\n### Community \n\nGenerative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership in AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Purple Llama tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to our [Github repository](https://github.com/meta-llama/PurpleLlama). \n\nFinally, we put in place a set of resources including an [output reporting mechanism](https://developers.facebook.com/llama_output_feedback) and [bug bounty program](https://www.facebook.com/whitehat) to continuously improve the Llama technology with the help of the community. \n\n\n## Ethical Considerations and Limitations\n\nThe core values of Llama 3 are openness, inclusivity and helpfulness. It is meant to serve everyone, and to work for a wide range of use cases. It is thus designed to be accessible to people across many different backgrounds, experiences and perspectives. Llama 3 addresses users and their needs as they are, without insertion unnecessary judgment or normativity, while reflecting the understanding that even content that may appear problematic in some cases can serve valuable purposes in others. It respects the dignity and autonomy of all users, especially in terms of the values of free thought and expression that power innovation and progress. \n\nBut Llama 3 is a new technology, and like any new technology, there are risks associated with its use. Testing conducted to date has been in English, and has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, Llama 3’s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 3 models, developers should perform safety testing and tuning tailored to their specific applications of the model. As outlined in the Responsible Use Guide, we recommend incorporating [Purple Llama](https://github.com/facebookresearch/PurpleLlama) solutions into your workflows and specifically [Llama Guard](https://ai.meta.com/research/publications/llama-guard-llm-based-input-output-safeguard-for-human-ai-conversations/) which provides a base model to filter input and output prompts to layer system-level safety on top of model-level safety. \n\nPlease see the Responsible Use Guide available at [http://llama.meta.com/responsible-use-guide](http://llama.meta.com/responsible-use-guide)\n\n\n## Citation instructions\n\n@article{llama3modelcard,\n\n  title={Llama 3 Model Card},\n\n  author={AI@Meta},\n\n  year={2024},\n\n  url = {https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md}\n\n}\n\n## Contributors\n\nAaditya Singh; Aaron Grattafiori; Abhimanyu Dubey; Abhinav Jauhri; Abhinav Pandey; Abhishek Kadian; Adam Kelsey; Adi Gangidi; Ahmad Al-Dahle; Ahuva Goldstand; Aiesha Letman; Ajay Menon; Akhil Mathur; Alan Schelten; Alex Vaughan; Amy Yang; Andrei Lupu; Andres Alvarado; Andrew Gallagher; Andrew Gu; Andrew Ho; Andrew Poulton; Andrew Ryan; Angela Fan; Ankit Ramchandani; Anthony Hartshorn; Archi Mitra; Archie Sravankumar; Artem Korenev; Arun Rao; Ashley Gabriel; Ashwin Bharambe; Assaf Eisenman; Aston Zhang; Aurelien Rodriguez; Austen Gregerson; Ava Spataru; Baptiste Roziere; Ben Maurer; Benjamin Leonhardi; Bernie Huang; Bhargavi Paranjape; Bing Liu; Binh Tang; Bobbie Chern; Brani Stojkovic; Brian Fuller; Catalina Mejia Arenas; Chao Zhou; Charlotte Caucheteux; Chaya Nayak; Ching-Hsiang Chu; Chloe Bi; Chris Cai; Chris Cox; Chris Marra; Chris McConnell; Christian Keller; Christoph Feichtenhofer; Christophe Touret; Chunyang Wu; Corinne Wong; Cristian Canton Ferrer; Damien Allonsius; Daniel Kreymer; Daniel Haziza; Daniel Li; Danielle Pintz; Danny Livshits; Danny Wyatt; David Adkins; David Esiobu; David Xu; Davide Testuggine; Delia David; Devi Parikh; Dhruv Choudhary; Dhruv Mahajan; Diana Liskovich; Diego Garcia-Olano; Diego Perino; Dieuwke Hupkes; Dingkang Wang; Dustin Holland; Egor Lakomkin; Elina Lobanova; Xiaoqing Ellen Tan; Emily Dinan; Eric Smith; Erik Brinkman; Esteban Arcaute; Filip Radenovic; Firat Ozgenel; Francesco Caggioni; Frank Seide; Frank Zhang; Gabriel Synnaeve; Gabriella Schwarz; Gabrielle Lee; Gada Badeer; Georgia Anderson; Graeme Nail; Gregoire Mialon; Guan Pang; Guillem Cucurell; Hailey Nguyen; Hannah Korevaar; Hannah Wang; Haroun Habeeb; Harrison Rudolph; Henry Aspegren; Hu Xu; Hugo Touvron; Iga Kozlowska; Igor Molybog; Igor Tufanov; Iliyan Zarov; Imanol Arrieta Ibarra; Irina-Elena Veliche; Isabel Kloumann; Ishan Misra; Ivan Evtimov; Jacob Xu; Jade Copet; Jake Weissman; Jan Geffert; Jana Vranes; Japhet Asher; Jason Park; Jay Mahadeokar; Jean-Baptiste Gaya; Jeet Shah; Jelmer van der Linde; Jennifer Chan; Jenny Hong; Jenya Lee; Jeremy Fu; Jeremy Teboul; Jianfeng Chi; Jianyu Huang; Jie Wang; Jiecao Yu; Joanna Bitton; Joe Spisak; Joelle Pineau; Jon Carvill; Jongsoo Park; Joseph Rocca; Joshua Johnstun; Junteng Jia; Kalyan Vasuden Alwala; Kam Hou U; Kate Plawiak; Kartikeya Upasani; Kaushik Veeraraghavan; Ke Li; Kenneth Heafield; Kevin Stone; Khalid El-Arini; Krithika Iyer; Kshitiz Malik; Kuenley Chiu; Kunal Bhalla; Kyle Huang; Lakshya Garg; Lauren Rantala-Yeary; Laurens van der Maaten; Lawrence Chen; Leandro Silva; Lee Bell; Lei Zhang; Liang Tan; Louis Martin; Lovish Madaan; Luca Wehrstedt; Lukas Blecher; Luke de Oliveira; Madeline Muzzi; Madian Khabsa; Manav Avlani; Mannat Singh; Manohar Paluri; Mark Zuckerberg; Marcin Kardas; Martynas Mankus; Mathew Oldham; Mathieu Rita; Matthew Lennie; Maya Pavlova; Meghan Keneally; Melanie Kambadur; Mihir Patel; Mikayel Samvelyan; Mike Clark; Mike Lewis; Min Si; Mitesh Kumar Singh; Mo Metanat; Mona Hassan; Naman Goyal; Narjes Torabi; Nicolas Usunier; Nikolay Bashlykov; Nikolay Bogoychev; Niladri Chatterji; Ning Dong; Oliver Aobo Yang; Olivier Duchenne; Onur Celebi; Parth Parekh; Patrick Alrassy; Paul Saab; Pavan Balaji; Pedro Rittner; Pengchuan Zhang; Pengwei Li; Petar Vasic; Peter Weng; Polina Zvyagina; Prajjwal Bhargava; Pratik Dubal; Praveen Krishnan; Punit Singh Koura; Qing He; Rachel Rodriguez; Ragavan Srinivasan; Rahul Mitra; Ramon Calderer; Raymond Li; Robert Stojnic; Roberta Raileanu; Robin Battey; Rocky Wang; Rohit Girdhar; Rohit Patel; Romain Sauvestre; Ronnie Polidoro; Roshan Sumbaly; Ross Taylor; Ruan Silva; Rui Hou; Rui Wang; Russ Howes; Ruty Rinott; Saghar Hosseini; Sai Jayesh Bondu; Samyak Datta; Sanjay Singh; Sara Chugh; Sargun Dhillon; Satadru Pan; Sean Bell; Sergey Edunov; Shaoliang Nie; Sharan Narang; Sharath Raparthy; Shaun Lindsay; Sheng Feng; Sheng Shen; Shenghao Lin; Shiva Shankar; Shruti Bhosale; Shun Zhang; Simon Vandenhende; Sinong Wang; Seohyun Sonia Kim; Soumya Batra; Sten Sootla; Steve Kehoe; Suchin Gururangan; Sumit Gupta; Sunny Virk; Sydney Borodinsky; Tamar Glaser; Tamar Herman; Tamara Best; Tara Fowler; Thomas Georgiou; Thomas Scialom; Tianhe Li; Todor Mihaylov; Tong Xiao; Ujjwal Karn; Vedanuj Goswami; Vibhor Gupta; Vignesh Ramanathan; Viktor Kerkez; Vinay Satish Kumar; Vincent Gonguet; Vish Vogeti; Vlad Poenaru; Vlad Tiberiu Mihailescu; Vladan Petrovic; Vladimir Ivanov; Wei Li; Weiwei Chu; Wenhan Xiong; Wenyin Fu; Wes Bouaziz; Whitney Meers; Will Constable; Xavier Martinet; Xiaojian Wu; Xinbo Gao; Xinfeng Xie; Xuchao Jia; Yaelle Goldschlag; Yann LeCun; Yashesh Gaur; Yasmine Babaei; Ye Qi; Yenda Li; Yi Wen; Yiwen Song; Youngjin Nam; Yuchen Hao; Yuchen Zhang; Yun Wang; Yuning Mao; Yuzi He; Zacharie Delpierre Coudert; Zachary DeVito; Zahra Hankir; Zhaoduo Wen; Zheng Yan; Zhengxing Chen; Zhenyu Yang; Zoe Papakipos\n\n---\n",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModel",
      "custom_class": null,
      "pipeline_tag": null,
      "processor": null
    },
    "safetensors": null,
    "model_index": null,
    "trending_score": null
  },
  {
    "model_id": "MaziyarPanahi/Llama-3-8B-Instruct-32k-v0.1-GGUF",
    "model_name": "MaziyarPanahi/Llama-3-8B-Instruct-32k-v0.1-GGUF",
    "author": "MaziyarPanahi",
    "downloads": 2492296,
    "downloads_all_time": null,
    "likes": 56,
    "tags": [
      "transformers",
      "gguf",
      "mistral",
      "quantized",
      "2-bit",
      "3-bit",
      "4-bit",
      "5-bit",
      "6-bit",
      "8-bit",
      "GGUF",
      "text-generation",
      "base_model:MaziyarPanahi/Llama-3-8B-Instruct-32k-v0.1",
      "base_model:quantized:MaziyarPanahi/Llama-3-8B-Instruct-32k-v0.1",
      "region:us",
      "conversational"
    ],
    "card_url": "https://huggingface.co/MaziyarPanahi/Llama-3-8B-Instruct-32k-v0.1-GGUF",
    "dependencies": [
      [
        "llama.cpp",
        null
      ],
      [
        "llama-cpp-python",
        null
      ],
      [
        "ctransformers",
        null
      ]
    ],
    "last_modified": "2024-06-28T10:42:58+00:00",
    "created_at": "2024-04-24T16:01:52+00:00",
    "analysis_date": "2025-03-22T00:41:38.093074",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "mistral",
    "pipeline_tag": "text-generation",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "tags": [
        "quantized",
        "2-bit",
        "3-bit",
        "4-bit",
        "5-bit",
        "6-bit",
        "8-bit",
        "GGUF",
        "text-generation"
      ],
      "model_name": "Llama-3-8B-Instruct-32k-v0.1-GGUF",
      "base_model": "MaziyarPanahi/Llama-3-8B-Instruct-32k-v0.1",
      "inference": false,
      "model_creator": "MaziyarPanahi",
      "pipeline_tag": "text-generation",
      "quantized_by": "MaziyarPanahi"
    },
    "card_text": "# [MaziyarPanahi/Llama-3-8B-Instruct-32k-v0.1-GGUF](https://huggingface.co/MaziyarPanahi/Llama-3-8B-Instruct-32k-v0.1-GGUF)\n- Model creator: [MaziyarPanahi](https://huggingface.co/MaziyarPanahi)\n- Original model: [MaziyarPanahi/Llama-3-8B-Instruct-32k-v0.1](https://huggingface.co/MaziyarPanahi/Llama-3-8B-Instruct-32k-v0.1)\n\n## Description\n[MaziyarPanahi/Llama-3-8B-Instruct-32k-v0.1-GGUF](https://huggingface.co/MaziyarPanahi/Llama-3-8B-Instruct-32k-v0.1-GGUF) contains GGUF format model files for [MaziyarPanahi/Llama-3-8B-Instruct-32k-v0.1](https://huggingface.co/MaziyarPanahi/Llama-3-8B-Instruct-32k-v0.1).\n\n### About GGUF\n\nGGUF is a new format introduced by the llama.cpp team on August 21st 2023. It is a replacement for GGML, which is no longer supported by llama.cpp.\n\nHere is an incomplete list of clients and libraries that are known to support GGUF:\n\n* [llama.cpp](https://github.com/ggerganov/llama.cpp). The source project for GGUF. Offers a CLI and a server option.\n* [llama-cpp-python](https://github.com/abetlen/llama-cpp-python), a Python library with GPU accel, LangChain support, and OpenAI-compatible API server.\n* [LM Studio](https://lmstudio.ai/), an easy-to-use and powerful local GUI for Windows and macOS (Silicon), with GPU acceleration. Linux available, in beta as of 27/11/2023.\n* [text-generation-webui](https://github.com/oobabooga/text-generation-webui), the most widely used web UI, with many features and powerful extensions. Supports GPU acceleration.\n* [KoboldCpp](https://github.com/LostRuins/koboldcpp), a fully featured web UI, with GPU accel across all platforms and GPU architectures. Especially good for story telling.\n* [GPT4All](https://gpt4all.io/index.html), a free and open source local running GUI, supporting Windows, Linux and macOS with full GPU accel.\n* [LoLLMS Web UI](https://github.com/ParisNeo/lollms-webui), a great web UI with many interesting and unique features, including a full model library for easy model selection.\n* [Faraday.dev](https://faraday.dev/), an attractive and easy to use character-based chat GUI for Windows and macOS (both Silicon and Intel), with GPU acceleration.\n* [candle](https://github.com/huggingface/candle), a Rust ML framework with a focus on performance, including GPU support, and ease of use.\n* [ctransformers](https://github.com/marella/ctransformers), a Python library with GPU accel, LangChain support, and OpenAI-compatible AI server. Note, as of time of writing (November 27th 2023), ctransformers has not been updated in a long time and does not support many recent models.\n\n## Special thanks\n\n🙏 Special thanks to [Georgi Gerganov](https://github.com/ggerganov) and the whole team working on [llama.cpp](https://github.com/ggerganov/llama.cpp/) for making all of this possible.",
    "card_content": "---\ntags:\n- quantized\n- 2-bit\n- 3-bit\n- 4-bit\n- 5-bit\n- 6-bit\n- 8-bit\n- GGUF\n- text-generation\nmodel_name: Llama-3-8B-Instruct-32k-v0.1-GGUF\nbase_model: MaziyarPanahi/Llama-3-8B-Instruct-32k-v0.1\ninference: false\nmodel_creator: MaziyarPanahi\npipeline_tag: text-generation\nquantized_by: MaziyarPanahi\n---\n# [MaziyarPanahi/Llama-3-8B-Instruct-32k-v0.1-GGUF](https://huggingface.co/MaziyarPanahi/Llama-3-8B-Instruct-32k-v0.1-GGUF)\n- Model creator: [MaziyarPanahi](https://huggingface.co/MaziyarPanahi)\n- Original model: [MaziyarPanahi/Llama-3-8B-Instruct-32k-v0.1](https://huggingface.co/MaziyarPanahi/Llama-3-8B-Instruct-32k-v0.1)\n\n## Description\n[MaziyarPanahi/Llama-3-8B-Instruct-32k-v0.1-GGUF](https://huggingface.co/MaziyarPanahi/Llama-3-8B-Instruct-32k-v0.1-GGUF) contains GGUF format model files for [MaziyarPanahi/Llama-3-8B-Instruct-32k-v0.1](https://huggingface.co/MaziyarPanahi/Llama-3-8B-Instruct-32k-v0.1).\n\n### About GGUF\n\nGGUF is a new format introduced by the llama.cpp team on August 21st 2023. It is a replacement for GGML, which is no longer supported by llama.cpp.\n\nHere is an incomplete list of clients and libraries that are known to support GGUF:\n\n* [llama.cpp](https://github.com/ggerganov/llama.cpp). The source project for GGUF. Offers a CLI and a server option.\n* [llama-cpp-python](https://github.com/abetlen/llama-cpp-python), a Python library with GPU accel, LangChain support, and OpenAI-compatible API server.\n* [LM Studio](https://lmstudio.ai/), an easy-to-use and powerful local GUI for Windows and macOS (Silicon), with GPU acceleration. Linux available, in beta as of 27/11/2023.\n* [text-generation-webui](https://github.com/oobabooga/text-generation-webui), the most widely used web UI, with many features and powerful extensions. Supports GPU acceleration.\n* [KoboldCpp](https://github.com/LostRuins/koboldcpp), a fully featured web UI, with GPU accel across all platforms and GPU architectures. Especially good for story telling.\n* [GPT4All](https://gpt4all.io/index.html), a free and open source local running GUI, supporting Windows, Linux and macOS with full GPU accel.\n* [LoLLMS Web UI](https://github.com/ParisNeo/lollms-webui), a great web UI with many interesting and unique features, including a full model library for easy model selection.\n* [Faraday.dev](https://faraday.dev/), an attractive and easy to use character-based chat GUI for Windows and macOS (both Silicon and Intel), with GPU acceleration.\n* [candle](https://github.com/huggingface/candle), a Rust ML framework with a focus on performance, including GPU support, and ease of use.\n* [ctransformers](https://github.com/marella/ctransformers), a Python library with GPU accel, LangChain support, and OpenAI-compatible AI server. Note, as of time of writing (November 27th 2023), ctransformers has not been updated in a long time and does not support many recent models.\n\n## Special thanks\n\n🙏 Special thanks to [Georgi Gerganov](https://github.com/ggerganov) and the whole team working on [llama.cpp](https://github.com/ggerganov/llama.cpp/) for making all of this possible.",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModel",
      "custom_class": null,
      "pipeline_tag": null,
      "processor": null
    },
    "safetensors": null,
    "model_index": null,
    "trending_score": null
  },
  {
    "model_id": "MaziyarPanahi/WizardLM-2-7B-GGUF",
    "model_name": "MaziyarPanahi/WizardLM-2-7B-GGUF",
    "author": "MaziyarPanahi",
    "downloads": 2403920,
    "downloads_all_time": null,
    "likes": 78,
    "tags": [
      "transformers",
      "gguf",
      "mistral",
      "quantized",
      "2-bit",
      "3-bit",
      "4-bit",
      "5-bit",
      "6-bit",
      "8-bit",
      "GGUF",
      "safetensors",
      "text-generation",
      "arxiv:2304.12244",
      "arxiv:2306.08568",
      "arxiv:2308.09583",
      "license:apache-2.0",
      "autotrain_compatible",
      "endpoints_compatible",
      "text-generation-inference",
      "region:us",
      "base_model:microsoft/WizardLM-2-7B",
      "base_model:quantized:microsoft/WizardLM-2-7B"
    ],
    "card_url": "https://huggingface.co/MaziyarPanahi/WizardLM-2-7B-GGUF",
    "dependencies": [
      [
        "huggingface-hub",
        null
      ]
    ],
    "last_modified": "2024-04-15T18:39:24+00:00",
    "created_at": "2024-04-15T16:51:17+00:00",
    "analysis_date": "2025-03-22T00:41:39.960086",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "mistral",
    "pipeline_tag": "text-generation",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "tags": [
        "quantized",
        "2-bit",
        "3-bit",
        "4-bit",
        "5-bit",
        "6-bit",
        "8-bit",
        "GGUF",
        "transformers",
        "safetensors",
        "mistral",
        "text-generation",
        "arxiv:2304.12244",
        "arxiv:2306.08568",
        "arxiv:2308.09583",
        "license:apache-2.0",
        "autotrain_compatible",
        "endpoints_compatible",
        "text-generation-inference",
        "region:us"
      ],
      "model_name": "WizardLM-2-7B-GGUF",
      "base_model": "microsoft/WizardLM-2-7B",
      "inference": false,
      "model_creator": "microsoft",
      "pipeline_tag": "text-generation",
      "quantized_by": "MaziyarPanahi"
    },
    "card_text": "# [MaziyarPanahi/WizardLM-2-7B-GGUF](https://huggingface.co/MaziyarPanahi/WizardLM-2-7B-GGUF)\n- Model creator: [microsoft](https://huggingface.co/microsoft)\n- Original model: [microsoft/WizardLM-2-7B](https://huggingface.co/microsoft/WizardLM-2-7B)\n\n## Description\n[MaziyarPanahi/WizardLM-2-7B-GGUF](https://huggingface.co/MaziyarPanahi/WizardLM-2-7B-GGUF) contains GGUF format model files for [microsoft/WizardLM-2-7B](https://huggingface.co/microsoft/WizardLM-2-7B).\n\n\n## Prompt template\n\n```\n{system_prompt}\nUSER: {prompt}\nASSISTANT: </s>\n```\n\nor\n\n```\nA chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, \ndetailed, and polite answers to the user's questions. USER: Hi ASSISTANT: Hello.</s>\nUSER: {prompt} ASSISTANT: </s>......\n```\n\nTaken from the original README\n---\n---\nlicense: apache-2.0\n---\n\n\n\n\n<p style=\"font-size:20px;\" align=\"center\">\n🏠 <a href=\"https://wizardlm.github.io/WizardLM2\" target=\"_blank\">WizardLM-2 Release Blog</a> </p>\n<p align=\"center\">\n🤗 <a href=\"https://huggingface.co/collections/microsoft/wizardlm-2-661d403f71e6c8257dbd598a\" target=\"_blank\">HF Repo</a>  •🐱 <a href=\"https://github.com/victorsungo/WizardLM/tree/main/WizardLM-2\" target=\"_blank\">Github Repo</a>  • 🐦 <a href=\"https://twitter.com/WizardLM_AI\" target=\"_blank\">Twitter</a> • 📃 <a href=\"https://arxiv.org/abs/2304.12244\" target=\"_blank\">[WizardLM]</a>  • 📃 <a href=\"https://arxiv.org/abs/2306.08568\" target=\"_blank\">[WizardCoder]</a>   • 📃 <a href=\"https://arxiv.org/abs/2308.09583\" target=\"_blank\">[WizardMath]</a>  <br>\n</p>\n<p align=\"center\">\n    👋 Join our <a href=\"https://discord.gg/VZjjHtWrKs\" target=\"_blank\">Discord</a>\n</p>\n\n\n\n## News 🔥🔥🔥 [2024/04/15]\n\nWe introduce and opensource WizardLM-2, our next generation state-of-the-art large language models, \nwhich have improved performance on complex chat, multilingual, reasoning and agent. \nNew family includes three cutting-edge models: WizardLM-2 8x22B, WizardLM-2 70B, and WizardLM-2 7B.\n\n- WizardLM-2 8x22B is our most advanced model, demonstrates highly competitive performance compared to those leading proprietary works \nand consistently outperforms all the existing state-of-the-art opensource models.\n- WizardLM-2 70B reaches top-tier reasoning capabilities and is the first choice in the same size. \n- WizardLM-2 7B is the fastest and achieves comparable performance with existing 10x larger opensource leading models.\n\nFor more  details of WizardLM-2 please read our [release blog post](https://wizardlm.github.io/WizardLM2) and  upcoming paper.\n\n\n## Model Details\n\n* **Model name**: WizardLM-2 7B\n* **Developed by**: WizardLM@Microsoft AI\n* **Base model**: [mistralai/Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1)\n* **Parameters**: 7B\n* **Language(s)**: Multilingual\n* **Blog**: [Introducing WizardLM-2](https://wizardlm.github.io/WizardLM2)\n* **Repository**: [https://github.com/nlpxucan/WizardLM](https://github.com/nlpxucan/WizardLM)\n* **Paper**: WizardLM-2 (Upcoming)\n* **License**: Apache2.0\n\n\n\n## Model Capacities\n\n\n**MT-Bench**\n\nWe also adopt the automatic MT-Bench evaluation framework based on GPT-4 proposed by lmsys to assess the performance of models. \nThe WizardLM-2 8x22B even demonstrates highly competitive performance compared to the most advanced proprietary models. \nMeanwhile, WizardLM-2 7B and WizardLM-2 70B are all the top-performing models among the other leading baselines at 7B to 70B model scales.\n\n<p align=\"center\" width=\"100%\">\n<a ><img src=\"https://raw.githubusercontent.com/WizardLM/WizardLM2/main/static/images/mtbench.png\" alt=\"MTBench\" style=\"width: 96%; min-width: 300px; display: block; margin: auto;\"></a>\n</p>\n\n\n**Human Preferences Evaluation**\n\nWe carefully collected a complex and challenging set consisting of real-world instructions, which includes main requirements of humanity, such as writing, coding, math, reasoning, agent, and multilingual. \nWe report the win:loss rate without tie:\n\n- WizardLM-2 8x22B is just slightly falling behind GPT-4-1106-preview, and significantly stronger than Command R Plus and GPT4-0314.\n- WizardLM-2 70B is better than GPT4-0613, Mistral-Large, and Qwen1.5-72B-Chat.\n- WizardLM-2 7B is comparable with Qwen1.5-32B-Chat, and surpasses Qwen1.5-14B-Chat and Starling-LM-7B-beta.\n\n<p align=\"center\" width=\"100%\">\n<a ><img src=\"https://raw.githubusercontent.com/WizardLM/WizardLM2/main/static/images/winall.png\" alt=\"Win\" style=\"width: 96%; min-width: 300px; display: block; margin: auto;\"></a>\n</p>\n\n\n\n\n\n## Method Overview\nWe built a **fully AI powered synthetic training system** to train WizardLM-2 models, please refer to our [blog](https://wizardlm.github.io/WizardLM2) for more details of this system.\n\n<p align=\"center\" width=\"100%\">\n<a ><img src=\"https://raw.githubusercontent.com/WizardLM/WizardLM2/main/static/images/exp_1.png\" alt=\"Method\" style=\"width: 96%; min-width: 300px; display: block; margin: auto;\"></a>\n</p>\n\n\n\n## Usage\n\n❗<b>Note for model system prompts usage:</b>\n\n\n<b>WizardLM-2</b>  adopts the prompt format from <b>Vicuna</b> and supports **multi-turn** conversation. The prompt should be as following:\n\n```\nA chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, \ndetailed, and polite answers to the user's questions. USER: Hi ASSISTANT: Hello.</s>\nUSER: Who are you? ASSISTANT: I am WizardLM.</s>......\n```\n\n<b> Inference WizardLM-2 Demo Script</b>\n\nWe provide a WizardLM-2 inference demo [code](https://github.com/nlpxucan/WizardLM/tree/main/demo) on our github.\n\n---\n\n## How to use\nThanks to [TheBloke](https://huggingface.co/TheBloke) for preparing an amazing README on how to use GGUF models:\n\n### About GGUF\n\nGGUF is a new format introduced by the llama.cpp team on August 21st 2023. It is a replacement for GGML, which is no longer supported by llama.cpp.\n\nHere is an incomplete list of clients and libraries that are known to support GGUF:\n\n* [llama.cpp](https://github.com/ggerganov/llama.cpp). The source project for GGUF. Offers a CLI and a server option.\n* [text-generation-webui](https://github.com/oobabooga/text-generation-webui), the most widely used web UI, with many features and powerful extensions. Supports GPU acceleration.\n* [KoboldCpp](https://github.com/LostRuins/koboldcpp), a fully featured web UI, with GPU accel across all platforms and GPU architectures. Especially good for story telling.\n* [GPT4All](https://gpt4all.io/index.html), a free and open source local running GUI, supporting Windows, Linux and macOS with full GPU accel.\n* [LM Studio](https://lmstudio.ai/), an easy-to-use and powerful local GUI for Windows and macOS (Silicon), with GPU acceleration. Linux available, in beta as of 27/11/2023.\n* [LoLLMS Web UI](https://github.com/ParisNeo/lollms-webui), a great web UI with many interesting and unique features, including a full model library for easy model selection.\n* [Faraday.dev](https://faraday.dev/), an attractive and easy to use character-based chat GUI for Windows and macOS (both Silicon and Intel), with GPU acceleration.\n* [llama-cpp-python](https://github.com/abetlen/llama-cpp-python), a Python library with GPU accel, LangChain support, and OpenAI-compatible API server.\n* [candle](https://github.com/huggingface/candle), a Rust ML framework with a focus on performance, including GPU support, and ease of use.\n* [ctransformers](https://github.com/marella/ctransformers), a Python library with GPU accel, LangChain support, and OpenAI-compatible AI server. Note, as of time of writing (November 27th 2023), ctransformers has not been updated in a long time and does not support many recent models.\n\n### Explanation of quantisation methods\n\n<details>\n  <summary>Click to see details</summary>\n\nThe new methods available are:\n\n* GGML_TYPE_Q2_K - \"type-1\" 2-bit quantization in super-blocks containing 16 blocks, each block having 16 weight. Block scales and mins are quantized with 4 bits. This ends up effectively using 2.5625 bits per weight (bpw)\n* GGML_TYPE_Q3_K - \"type-0\" 3-bit quantization in super-blocks containing 16 blocks, each block having 16 weights. Scales are quantized with 6 bits. This end up using 3.4375 bpw.\n* GGML_TYPE_Q4_K - \"type-1\" 4-bit quantization in super-blocks containing 8 blocks, each block having 32 weights. Scales and mins are quantized with 6 bits. This ends up using 4.5 bpw.\n* GGML_TYPE_Q5_K - \"type-1\" 5-bit quantization. Same super-block structure as GGML_TYPE_Q4_K resulting in 5.5 bpw\n* GGML_TYPE_Q6_K - \"type-0\" 6-bit quantization. Super-blocks with 16 blocks, each block having 16 weights. Scales are quantized with 8 bits. This ends up using 6.5625 bpw\n\n## How to download GGUF files\n\n**Note for manual downloaders:** You almost never want to clone the entire repo! Multiple different quantisation formats are provided, and most users only want to pick and download a single file.\n\nThe following clients/libraries will automatically download models for you, providing a list of available models to choose from:\n\n* LM Studio\n* LoLLMS Web UI\n* Faraday.dev\n\n### In `text-generation-webui`\n\nUnder Download Model, you can enter the model repo: [MaziyarPanahi/WizardLM-2-7B-GGUF](https://huggingface.co/MaziyarPanahi/WizardLM-2-7B-GGUF) and below it, a specific filename to download, such as: WizardLM-2-7B-GGUF.Q4_K_M.gguf.\n\nThen click Download.\n\n### On the command line, including multiple files at once\n\nI recommend using the `huggingface-hub` Python library:\n\n```shell\npip3 install huggingface-hub\n```\n\nThen you can download any individual model file to the current directory, at high speed, with a command like this:\n\n```shell\nhuggingface-cli download MaziyarPanahi/WizardLM-2-7B-GGUF WizardLM-2-7B.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False\n```\n</details>\n<details>\n  <summary>More advanced huggingface-cli download usage (click to read)</summary>\n\nYou can also download multiple files at once with a pattern:\n\n```shell\nhuggingface-cli download [MaziyarPanahi/WizardLM-2-7B-GGUF](https://huggingface.co/MaziyarPanahi/WizardLM-2-7B-GGUF) --local-dir . --local-dir-use-symlinks False --include='*Q4_K*gguf'\n```\n\nFor more documentation on downloading with `huggingface-cli`, please see: [HF -> Hub Python Library -> Download files -> Download from the CLI](https://huggingface.co/docs/huggingface_hub/guides/download#download-from-the-cli).\n\nTo accelerate downloads on fast connections (1Gbit/s or higher), install `hf_transfer`:\n\n```shell\npip3 install hf_transfer\n```\n\nAnd set environment variable `HF_HUB_ENABLE_HF_TRANSFER` to `1`:\n\n```shell\nHF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download MaziyarPanahi/WizardLM-2-7B-GGUF WizardLM-2-7B.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False\n```\n\nWindows Command Line users: You can set the environment variable by running `set HF_HUB_ENABLE_HF_TRANSFER=1` before the download command.\n</details>\n\n## Example `llama.cpp` command\n\nMake sure you are using `llama.cpp` from commit [d0cee0d](https://github.com/ggerganov/llama.cpp/commit/d0cee0d36d5be95a0d9088b674dbb27354107221) or later.\n\n```shell\n./main -ngl 35 -m WizardLM-2-7B.Q4_K_M.gguf --color -c 32768 --temp 0.7 --repeat_penalty 1.1 -n -1 -p \"<|im_start|>system\n{system_message}<|im_end|>\n<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\"\n```\n\nChange `-ngl 32` to the number of layers to offload to GPU. Remove it if you don't have GPU acceleration.\n\nChange `-c 32768` to the desired sequence length. For extended sequence models - eg 8K, 16K, 32K - the necessary RoPE scaling parameters are read from the GGUF file and set by llama.cpp automatically. Note that longer sequence lengths require much more resources, so you may need to reduce this value.\n\nIf you want to have a chat-style conversation, replace the `-p <PROMPT>` argument with `-i -ins`\n\nFor other parameters and how to use them, please refer to [the llama.cpp documentation](https://github.com/ggerganov/llama.cpp/blob/master/examples/main/README.md)\n\n## How to run in `text-generation-webui`\n\nFurther instructions can be found in the text-generation-webui documentation, here: [text-generation-webui/docs/04 ‐ Model Tab.md](https://github.com/oobabooga/text-generation-webui/blob/main/docs/04%20-%20Model%20Tab.md#llamacpp).\n\n## How to run from Python code\n\nYou can use GGUF models from Python using the [llama-cpp-python](https://github.com/abetlen/llama-cpp-python) or [ctransformers](https://github.com/marella/ctransformers) libraries. Note that at the time of writing (Nov 27th 2023), ctransformers has not been updated for some time and is not compatible with some recent models. Therefore I recommend you use llama-cpp-python.\n\n### How to load this model in Python code, using llama-cpp-python\n\nFor full documentation, please see: [llama-cpp-python docs](https://github.com/abetlen/llama-cpp-python/).\n\n#### First install the package\n\nRun one of the following commands, according to your system:\n\n```shell\n# Base ctransformers with no GPU acceleration\npip install llama-cpp-python\n# With NVidia CUDA acceleration\nCMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install llama-cpp-python\n# Or with OpenBLAS acceleration\nCMAKE_ARGS=\"-DLLAMA_BLAS=ON -DLLAMA_BLAS_VENDOR=OpenBLAS\" pip install llama-cpp-python\n# Or with CLBLast acceleration\nCMAKE_ARGS=\"-DLLAMA_CLBLAST=on\" pip install llama-cpp-python\n# Or with AMD ROCm GPU acceleration (Linux only)\nCMAKE_ARGS=\"-DLLAMA_HIPBLAS=on\" pip install llama-cpp-python\n# Or with Metal GPU acceleration for macOS systems only\nCMAKE_ARGS=\"-DLLAMA_METAL=on\" pip install llama-cpp-python\n\n# In windows, to set the variables CMAKE_ARGS in PowerShell, follow this format; eg for NVidia CUDA:\n$env:CMAKE_ARGS = \"-DLLAMA_OPENBLAS=on\"\npip install llama-cpp-python\n```\n\n#### Simple llama-cpp-python example code\n\n```python\nfrom llama_cpp import Llama\n\n# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\nllm = Llama(\n  model_path=\"./WizardLM-2-7B.Q4_K_M.gguf\",  # Download the model file first\n  n_ctx=32768,  # The max sequence length to use - note that longer sequence lengths require much more resources\n  n_threads=8,            # The number of CPU threads to use, tailor to your system and the resulting performance\n  n_gpu_layers=35         # The number of layers to offload to GPU, if you have GPU acceleration available\n)\n\n# Simple inference example\noutput = llm(\n  \"<|im_start|>system\n{system_message}<|im_end|>\n<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\", # Prompt\n  max_tokens=512,  # Generate up to 512 tokens\n  stop=[\"</s>\"],   # Example stop token - not necessarily correct for this specific model! Please check before using.\n  echo=True        # Whether to echo the prompt\n)\n\n# Chat Completion API\n\nllm = Llama(model_path=\"./WizardLM-2-7B.Q4_K_M.gguf\", chat_format=\"llama-2\")  # Set chat_format according to the model you are using\nllm.create_chat_completion(\n    messages = [\n        {\"role\": \"system\", \"content\": \"You are a story writing assistant.\"},\n        {\n            \"role\": \"user\",\n            \"content\": \"Write a story about llamas.\"\n        }\n    ]\n)\n```\n\n## How to use with LangChain\n\nHere are guides on using llama-cpp-python and ctransformers with LangChain:\n\n* [LangChain + llama-cpp-python](https://python.langchain.com/docs/integrations/llms/llamacpp)\n* [LangChain + ctransformers](https://python.langchain.com/docs/integrations/providers/ctransformers)",
    "card_content": "---\ntags:\n- quantized\n- 2-bit\n- 3-bit\n- 4-bit\n- 5-bit\n- 6-bit\n- 8-bit\n- GGUF\n- transformers\n- safetensors\n- mistral\n- text-generation\n- arxiv:2304.12244\n- arxiv:2306.08568\n- arxiv:2308.09583\n- license:apache-2.0\n- autotrain_compatible\n- endpoints_compatible\n- text-generation-inference\n- region:us\nmodel_name: WizardLM-2-7B-GGUF\nbase_model: microsoft/WizardLM-2-7B\ninference: false\nmodel_creator: microsoft\npipeline_tag: text-generation\nquantized_by: MaziyarPanahi\n---\n# [MaziyarPanahi/WizardLM-2-7B-GGUF](https://huggingface.co/MaziyarPanahi/WizardLM-2-7B-GGUF)\n- Model creator: [microsoft](https://huggingface.co/microsoft)\n- Original model: [microsoft/WizardLM-2-7B](https://huggingface.co/microsoft/WizardLM-2-7B)\n\n## Description\n[MaziyarPanahi/WizardLM-2-7B-GGUF](https://huggingface.co/MaziyarPanahi/WizardLM-2-7B-GGUF) contains GGUF format model files for [microsoft/WizardLM-2-7B](https://huggingface.co/microsoft/WizardLM-2-7B).\n\n\n## Prompt template\n\n```\n{system_prompt}\nUSER: {prompt}\nASSISTANT: </s>\n```\n\nor\n\n```\nA chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, \ndetailed, and polite answers to the user's questions. USER: Hi ASSISTANT: Hello.</s>\nUSER: {prompt} ASSISTANT: </s>......\n```\n\nTaken from the original README\n---\n---\nlicense: apache-2.0\n---\n\n\n\n\n<p style=\"font-size:20px;\" align=\"center\">\n🏠 <a href=\"https://wizardlm.github.io/WizardLM2\" target=\"_blank\">WizardLM-2 Release Blog</a> </p>\n<p align=\"center\">\n🤗 <a href=\"https://huggingface.co/collections/microsoft/wizardlm-2-661d403f71e6c8257dbd598a\" target=\"_blank\">HF Repo</a>  •🐱 <a href=\"https://github.com/victorsungo/WizardLM/tree/main/WizardLM-2\" target=\"_blank\">Github Repo</a>  • 🐦 <a href=\"https://twitter.com/WizardLM_AI\" target=\"_blank\">Twitter</a> • 📃 <a href=\"https://arxiv.org/abs/2304.12244\" target=\"_blank\">[WizardLM]</a>  • 📃 <a href=\"https://arxiv.org/abs/2306.08568\" target=\"_blank\">[WizardCoder]</a>   • 📃 <a href=\"https://arxiv.org/abs/2308.09583\" target=\"_blank\">[WizardMath]</a>  <br>\n</p>\n<p align=\"center\">\n    👋 Join our <a href=\"https://discord.gg/VZjjHtWrKs\" target=\"_blank\">Discord</a>\n</p>\n\n\n\n## News 🔥🔥🔥 [2024/04/15]\n\nWe introduce and opensource WizardLM-2, our next generation state-of-the-art large language models, \nwhich have improved performance on complex chat, multilingual, reasoning and agent. \nNew family includes three cutting-edge models: WizardLM-2 8x22B, WizardLM-2 70B, and WizardLM-2 7B.\n\n- WizardLM-2 8x22B is our most advanced model, demonstrates highly competitive performance compared to those leading proprietary works \nand consistently outperforms all the existing state-of-the-art opensource models.\n- WizardLM-2 70B reaches top-tier reasoning capabilities and is the first choice in the same size. \n- WizardLM-2 7B is the fastest and achieves comparable performance with existing 10x larger opensource leading models.\n\nFor more  details of WizardLM-2 please read our [release blog post](https://wizardlm.github.io/WizardLM2) and  upcoming paper.\n\n\n## Model Details\n\n* **Model name**: WizardLM-2 7B\n* **Developed by**: WizardLM@Microsoft AI\n* **Base model**: [mistralai/Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1)\n* **Parameters**: 7B\n* **Language(s)**: Multilingual\n* **Blog**: [Introducing WizardLM-2](https://wizardlm.github.io/WizardLM2)\n* **Repository**: [https://github.com/nlpxucan/WizardLM](https://github.com/nlpxucan/WizardLM)\n* **Paper**: WizardLM-2 (Upcoming)\n* **License**: Apache2.0\n\n\n\n## Model Capacities\n\n\n**MT-Bench**\n\nWe also adopt the automatic MT-Bench evaluation framework based on GPT-4 proposed by lmsys to assess the performance of models. \nThe WizardLM-2 8x22B even demonstrates highly competitive performance compared to the most advanced proprietary models. \nMeanwhile, WizardLM-2 7B and WizardLM-2 70B are all the top-performing models among the other leading baselines at 7B to 70B model scales.\n\n<p align=\"center\" width=\"100%\">\n<a ><img src=\"https://raw.githubusercontent.com/WizardLM/WizardLM2/main/static/images/mtbench.png\" alt=\"MTBench\" style=\"width: 96%; min-width: 300px; display: block; margin: auto;\"></a>\n</p>\n\n\n**Human Preferences Evaluation**\n\nWe carefully collected a complex and challenging set consisting of real-world instructions, which includes main requirements of humanity, such as writing, coding, math, reasoning, agent, and multilingual. \nWe report the win:loss rate without tie:\n\n- WizardLM-2 8x22B is just slightly falling behind GPT-4-1106-preview, and significantly stronger than Command R Plus and GPT4-0314.\n- WizardLM-2 70B is better than GPT4-0613, Mistral-Large, and Qwen1.5-72B-Chat.\n- WizardLM-2 7B is comparable with Qwen1.5-32B-Chat, and surpasses Qwen1.5-14B-Chat and Starling-LM-7B-beta.\n\n<p align=\"center\" width=\"100%\">\n<a ><img src=\"https://raw.githubusercontent.com/WizardLM/WizardLM2/main/static/images/winall.png\" alt=\"Win\" style=\"width: 96%; min-width: 300px; display: block; margin: auto;\"></a>\n</p>\n\n\n\n\n\n## Method Overview\nWe built a **fully AI powered synthetic training system** to train WizardLM-2 models, please refer to our [blog](https://wizardlm.github.io/WizardLM2) for more details of this system.\n\n<p align=\"center\" width=\"100%\">\n<a ><img src=\"https://raw.githubusercontent.com/WizardLM/WizardLM2/main/static/images/exp_1.png\" alt=\"Method\" style=\"width: 96%; min-width: 300px; display: block; margin: auto;\"></a>\n</p>\n\n\n\n## Usage\n\n❗<b>Note for model system prompts usage:</b>\n\n\n<b>WizardLM-2</b>  adopts the prompt format from <b>Vicuna</b> and supports **multi-turn** conversation. The prompt should be as following:\n\n```\nA chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, \ndetailed, and polite answers to the user's questions. USER: Hi ASSISTANT: Hello.</s>\nUSER: Who are you? ASSISTANT: I am WizardLM.</s>......\n```\n\n<b> Inference WizardLM-2 Demo Script</b>\n\nWe provide a WizardLM-2 inference demo [code](https://github.com/nlpxucan/WizardLM/tree/main/demo) on our github.\n\n---\n\n## How to use\nThanks to [TheBloke](https://huggingface.co/TheBloke) for preparing an amazing README on how to use GGUF models:\n\n### About GGUF\n\nGGUF is a new format introduced by the llama.cpp team on August 21st 2023. It is a replacement for GGML, which is no longer supported by llama.cpp.\n\nHere is an incomplete list of clients and libraries that are known to support GGUF:\n\n* [llama.cpp](https://github.com/ggerganov/llama.cpp). The source project for GGUF. Offers a CLI and a server option.\n* [text-generation-webui](https://github.com/oobabooga/text-generation-webui), the most widely used web UI, with many features and powerful extensions. Supports GPU acceleration.\n* [KoboldCpp](https://github.com/LostRuins/koboldcpp), a fully featured web UI, with GPU accel across all platforms and GPU architectures. Especially good for story telling.\n* [GPT4All](https://gpt4all.io/index.html), a free and open source local running GUI, supporting Windows, Linux and macOS with full GPU accel.\n* [LM Studio](https://lmstudio.ai/), an easy-to-use and powerful local GUI for Windows and macOS (Silicon), with GPU acceleration. Linux available, in beta as of 27/11/2023.\n* [LoLLMS Web UI](https://github.com/ParisNeo/lollms-webui), a great web UI with many interesting and unique features, including a full model library for easy model selection.\n* [Faraday.dev](https://faraday.dev/), an attractive and easy to use character-based chat GUI for Windows and macOS (both Silicon and Intel), with GPU acceleration.\n* [llama-cpp-python](https://github.com/abetlen/llama-cpp-python), a Python library with GPU accel, LangChain support, and OpenAI-compatible API server.\n* [candle](https://github.com/huggingface/candle), a Rust ML framework with a focus on performance, including GPU support, and ease of use.\n* [ctransformers](https://github.com/marella/ctransformers), a Python library with GPU accel, LangChain support, and OpenAI-compatible AI server. Note, as of time of writing (November 27th 2023), ctransformers has not been updated in a long time and does not support many recent models.\n\n### Explanation of quantisation methods\n\n<details>\n  <summary>Click to see details</summary>\n\nThe new methods available are:\n\n* GGML_TYPE_Q2_K - \"type-1\" 2-bit quantization in super-blocks containing 16 blocks, each block having 16 weight. Block scales and mins are quantized with 4 bits. This ends up effectively using 2.5625 bits per weight (bpw)\n* GGML_TYPE_Q3_K - \"type-0\" 3-bit quantization in super-blocks containing 16 blocks, each block having 16 weights. Scales are quantized with 6 bits. This end up using 3.4375 bpw.\n* GGML_TYPE_Q4_K - \"type-1\" 4-bit quantization in super-blocks containing 8 blocks, each block having 32 weights. Scales and mins are quantized with 6 bits. This ends up using 4.5 bpw.\n* GGML_TYPE_Q5_K - \"type-1\" 5-bit quantization. Same super-block structure as GGML_TYPE_Q4_K resulting in 5.5 bpw\n* GGML_TYPE_Q6_K - \"type-0\" 6-bit quantization. Super-blocks with 16 blocks, each block having 16 weights. Scales are quantized with 8 bits. This ends up using 6.5625 bpw\n\n## How to download GGUF files\n\n**Note for manual downloaders:** You almost never want to clone the entire repo! Multiple different quantisation formats are provided, and most users only want to pick and download a single file.\n\nThe following clients/libraries will automatically download models for you, providing a list of available models to choose from:\n\n* LM Studio\n* LoLLMS Web UI\n* Faraday.dev\n\n### In `text-generation-webui`\n\nUnder Download Model, you can enter the model repo: [MaziyarPanahi/WizardLM-2-7B-GGUF](https://huggingface.co/MaziyarPanahi/WizardLM-2-7B-GGUF) and below it, a specific filename to download, such as: WizardLM-2-7B-GGUF.Q4_K_M.gguf.\n\nThen click Download.\n\n### On the command line, including multiple files at once\n\nI recommend using the `huggingface-hub` Python library:\n\n```shell\npip3 install huggingface-hub\n```\n\nThen you can download any individual model file to the current directory, at high speed, with a command like this:\n\n```shell\nhuggingface-cli download MaziyarPanahi/WizardLM-2-7B-GGUF WizardLM-2-7B.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False\n```\n</details>\n<details>\n  <summary>More advanced huggingface-cli download usage (click to read)</summary>\n\nYou can also download multiple files at once with a pattern:\n\n```shell\nhuggingface-cli download [MaziyarPanahi/WizardLM-2-7B-GGUF](https://huggingface.co/MaziyarPanahi/WizardLM-2-7B-GGUF) --local-dir . --local-dir-use-symlinks False --include='*Q4_K*gguf'\n```\n\nFor more documentation on downloading with `huggingface-cli`, please see: [HF -> Hub Python Library -> Download files -> Download from the CLI](https://huggingface.co/docs/huggingface_hub/guides/download#download-from-the-cli).\n\nTo accelerate downloads on fast connections (1Gbit/s or higher), install `hf_transfer`:\n\n```shell\npip3 install hf_transfer\n```\n\nAnd set environment variable `HF_HUB_ENABLE_HF_TRANSFER` to `1`:\n\n```shell\nHF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download MaziyarPanahi/WizardLM-2-7B-GGUF WizardLM-2-7B.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False\n```\n\nWindows Command Line users: You can set the environment variable by running `set HF_HUB_ENABLE_HF_TRANSFER=1` before the download command.\n</details>\n\n## Example `llama.cpp` command\n\nMake sure you are using `llama.cpp` from commit [d0cee0d](https://github.com/ggerganov/llama.cpp/commit/d0cee0d36d5be95a0d9088b674dbb27354107221) or later.\n\n```shell\n./main -ngl 35 -m WizardLM-2-7B.Q4_K_M.gguf --color -c 32768 --temp 0.7 --repeat_penalty 1.1 -n -1 -p \"<|im_start|>system\n{system_message}<|im_end|>\n<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\"\n```\n\nChange `-ngl 32` to the number of layers to offload to GPU. Remove it if you don't have GPU acceleration.\n\nChange `-c 32768` to the desired sequence length. For extended sequence models - eg 8K, 16K, 32K - the necessary RoPE scaling parameters are read from the GGUF file and set by llama.cpp automatically. Note that longer sequence lengths require much more resources, so you may need to reduce this value.\n\nIf you want to have a chat-style conversation, replace the `-p <PROMPT>` argument with `-i -ins`\n\nFor other parameters and how to use them, please refer to [the llama.cpp documentation](https://github.com/ggerganov/llama.cpp/blob/master/examples/main/README.md)\n\n## How to run in `text-generation-webui`\n\nFurther instructions can be found in the text-generation-webui documentation, here: [text-generation-webui/docs/04 ‐ Model Tab.md](https://github.com/oobabooga/text-generation-webui/blob/main/docs/04%20-%20Model%20Tab.md#llamacpp).\n\n## How to run from Python code\n\nYou can use GGUF models from Python using the [llama-cpp-python](https://github.com/abetlen/llama-cpp-python) or [ctransformers](https://github.com/marella/ctransformers) libraries. Note that at the time of writing (Nov 27th 2023), ctransformers has not been updated for some time and is not compatible with some recent models. Therefore I recommend you use llama-cpp-python.\n\n### How to load this model in Python code, using llama-cpp-python\n\nFor full documentation, please see: [llama-cpp-python docs](https://github.com/abetlen/llama-cpp-python/).\n\n#### First install the package\n\nRun one of the following commands, according to your system:\n\n```shell\n# Base ctransformers with no GPU acceleration\npip install llama-cpp-python\n# With NVidia CUDA acceleration\nCMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install llama-cpp-python\n# Or with OpenBLAS acceleration\nCMAKE_ARGS=\"-DLLAMA_BLAS=ON -DLLAMA_BLAS_VENDOR=OpenBLAS\" pip install llama-cpp-python\n# Or with CLBLast acceleration\nCMAKE_ARGS=\"-DLLAMA_CLBLAST=on\" pip install llama-cpp-python\n# Or with AMD ROCm GPU acceleration (Linux only)\nCMAKE_ARGS=\"-DLLAMA_HIPBLAS=on\" pip install llama-cpp-python\n# Or with Metal GPU acceleration for macOS systems only\nCMAKE_ARGS=\"-DLLAMA_METAL=on\" pip install llama-cpp-python\n\n# In windows, to set the variables CMAKE_ARGS in PowerShell, follow this format; eg for NVidia CUDA:\n$env:CMAKE_ARGS = \"-DLLAMA_OPENBLAS=on\"\npip install llama-cpp-python\n```\n\n#### Simple llama-cpp-python example code\n\n```python\nfrom llama_cpp import Llama\n\n# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\nllm = Llama(\n  model_path=\"./WizardLM-2-7B.Q4_K_M.gguf\",  # Download the model file first\n  n_ctx=32768,  # The max sequence length to use - note that longer sequence lengths require much more resources\n  n_threads=8,            # The number of CPU threads to use, tailor to your system and the resulting performance\n  n_gpu_layers=35         # The number of layers to offload to GPU, if you have GPU acceleration available\n)\n\n# Simple inference example\noutput = llm(\n  \"<|im_start|>system\n{system_message}<|im_end|>\n<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\", # Prompt\n  max_tokens=512,  # Generate up to 512 tokens\n  stop=[\"</s>\"],   # Example stop token - not necessarily correct for this specific model! Please check before using.\n  echo=True        # Whether to echo the prompt\n)\n\n# Chat Completion API\n\nllm = Llama(model_path=\"./WizardLM-2-7B.Q4_K_M.gguf\", chat_format=\"llama-2\")  # Set chat_format according to the model you are using\nllm.create_chat_completion(\n    messages = [\n        {\"role\": \"system\", \"content\": \"You are a story writing assistant.\"},\n        {\n            \"role\": \"user\",\n            \"content\": \"Write a story about llamas.\"\n        }\n    ]\n)\n```\n\n## How to use with LangChain\n\nHere are guides on using llama-cpp-python and ctransformers with LangChain:\n\n* [LangChain + llama-cpp-python](https://python.langchain.com/docs/integrations/llms/llamacpp)\n* [LangChain + ctransformers](https://python.langchain.com/docs/integrations/providers/ctransformers)",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModel",
      "custom_class": null,
      "pipeline_tag": null,
      "processor": null
    },
    "safetensors": null,
    "model_index": null,
    "trending_score": null
  },
  {
    "model_id": "MaziyarPanahi/gemma-2-2b-it-GGUF",
    "model_name": "MaziyarPanahi/gemma-2-2b-it-GGUF",
    "author": "MaziyarPanahi",
    "downloads": 2396581,
    "downloads_all_time": null,
    "likes": 14,
    "tags": [
      "transformers",
      "gguf",
      "mistral",
      "quantized",
      "2-bit",
      "3-bit",
      "4-bit",
      "5-bit",
      "6-bit",
      "8-bit",
      "GGUF",
      "text-generation",
      "base_model:google/gemma-2-2b-it",
      "base_model:quantized:google/gemma-2-2b-it",
      "region:us",
      "conversational"
    ],
    "card_url": "https://huggingface.co/MaziyarPanahi/gemma-2-2b-it-GGUF",
    "dependencies": [
      [
        "llama.cpp",
        null
      ],
      [
        "llama-cpp-python",
        null
      ],
      [
        "ctransformers",
        null
      ]
    ],
    "last_modified": "2024-08-01T08:01:55+00:00",
    "created_at": "2024-08-01T07:46:41+00:00",
    "analysis_date": "2025-03-22T00:41:41.354345",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "mistral",
    "pipeline_tag": "text-generation",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "tags": [
        "quantized",
        "2-bit",
        "3-bit",
        "4-bit",
        "5-bit",
        "6-bit",
        "8-bit",
        "GGUF",
        "text-generation"
      ],
      "model_name": "gemma-2-2b-it-GGUF",
      "base_model": "google/gemma-2-2b-it",
      "inference": false,
      "model_creator": "google",
      "pipeline_tag": "text-generation",
      "quantized_by": "MaziyarPanahi"
    },
    "card_text": "# [MaziyarPanahi/gemma-2-2b-it-GGUF](https://huggingface.co/MaziyarPanahi/gemma-2-2b-it-GGUF)\n- Model creator: [google](https://huggingface.co/google)\n- Original model: [google/gemma-2-2b-it](https://huggingface.co/google/gemma-2-2b-it)\n\n## Description\n[MaziyarPanahi/gemma-2-2b-it-GGUF](https://huggingface.co/MaziyarPanahi/gemma-2-2b-it-GGUF) contains GGUF format model files for [google/gemma-2-2b-it](https://huggingface.co/google/gemma-2-2b-it).\n\n### About GGUF\n\nGGUF is a new format introduced by the llama.cpp team on August 21st 2023. It is a replacement for GGML, which is no longer supported by llama.cpp.\n\nHere is an incomplete list of clients and libraries that are known to support GGUF:\n\n* [llama.cpp](https://github.com/ggerganov/llama.cpp). The source project for GGUF. Offers a CLI and a server option.\n* [llama-cpp-python](https://github.com/abetlen/llama-cpp-python), a Python library with GPU accel, LangChain support, and OpenAI-compatible API server.\n* [LM Studio](https://lmstudio.ai/), an easy-to-use and powerful local GUI for Windows and macOS (Silicon), with GPU acceleration. Linux available, in beta as of 27/11/2023.\n* [text-generation-webui](https://github.com/oobabooga/text-generation-webui), the most widely used web UI, with many features and powerful extensions. Supports GPU acceleration.\n* [KoboldCpp](https://github.com/LostRuins/koboldcpp), a fully featured web UI, with GPU accel across all platforms and GPU architectures. Especially good for story telling.\n* [GPT4All](https://gpt4all.io/index.html), a free and open source local running GUI, supporting Windows, Linux and macOS with full GPU accel.\n* [LoLLMS Web UI](https://github.com/ParisNeo/lollms-webui), a great web UI with many interesting and unique features, including a full model library for easy model selection.\n* [Faraday.dev](https://faraday.dev/), an attractive and easy to use character-based chat GUI for Windows and macOS (both Silicon and Intel), with GPU acceleration.\n* [candle](https://github.com/huggingface/candle), a Rust ML framework with a focus on performance, including GPU support, and ease of use.\n* [ctransformers](https://github.com/marella/ctransformers), a Python library with GPU accel, LangChain support, and OpenAI-compatible AI server. Note, as of time of writing (November 27th 2023), ctransformers has not been updated in a long time and does not support many recent models.\n\n## Special thanks\n\n🙏 Special thanks to [Georgi Gerganov](https://github.com/ggerganov) and the whole team working on [llama.cpp](https://github.com/ggerganov/llama.cpp/) for making all of this possible.",
    "card_content": "---\ntags:\n- quantized\n- 2-bit\n- 3-bit\n- 4-bit\n- 5-bit\n- 6-bit\n- 8-bit\n- GGUF\n- text-generation\nmodel_name: gemma-2-2b-it-GGUF\nbase_model: google/gemma-2-2b-it\ninference: false\nmodel_creator: google\npipeline_tag: text-generation\nquantized_by: MaziyarPanahi\n---\n# [MaziyarPanahi/gemma-2-2b-it-GGUF](https://huggingface.co/MaziyarPanahi/gemma-2-2b-it-GGUF)\n- Model creator: [google](https://huggingface.co/google)\n- Original model: [google/gemma-2-2b-it](https://huggingface.co/google/gemma-2-2b-it)\n\n## Description\n[MaziyarPanahi/gemma-2-2b-it-GGUF](https://huggingface.co/MaziyarPanahi/gemma-2-2b-it-GGUF) contains GGUF format model files for [google/gemma-2-2b-it](https://huggingface.co/google/gemma-2-2b-it).\n\n### About GGUF\n\nGGUF is a new format introduced by the llama.cpp team on August 21st 2023. It is a replacement for GGML, which is no longer supported by llama.cpp.\n\nHere is an incomplete list of clients and libraries that are known to support GGUF:\n\n* [llama.cpp](https://github.com/ggerganov/llama.cpp). The source project for GGUF. Offers a CLI and a server option.\n* [llama-cpp-python](https://github.com/abetlen/llama-cpp-python), a Python library with GPU accel, LangChain support, and OpenAI-compatible API server.\n* [LM Studio](https://lmstudio.ai/), an easy-to-use and powerful local GUI for Windows and macOS (Silicon), with GPU acceleration. Linux available, in beta as of 27/11/2023.\n* [text-generation-webui](https://github.com/oobabooga/text-generation-webui), the most widely used web UI, with many features and powerful extensions. Supports GPU acceleration.\n* [KoboldCpp](https://github.com/LostRuins/koboldcpp), a fully featured web UI, with GPU accel across all platforms and GPU architectures. Especially good for story telling.\n* [GPT4All](https://gpt4all.io/index.html), a free and open source local running GUI, supporting Windows, Linux and macOS with full GPU accel.\n* [LoLLMS Web UI](https://github.com/ParisNeo/lollms-webui), a great web UI with many interesting and unique features, including a full model library for easy model selection.\n* [Faraday.dev](https://faraday.dev/), an attractive and easy to use character-based chat GUI for Windows and macOS (both Silicon and Intel), with GPU acceleration.\n* [candle](https://github.com/huggingface/candle), a Rust ML framework with a focus on performance, including GPU support, and ease of use.\n* [ctransformers](https://github.com/marella/ctransformers), a Python library with GPU accel, LangChain support, and OpenAI-compatible AI server. Note, as of time of writing (November 27th 2023), ctransformers has not been updated in a long time and does not support many recent models.\n\n## Special thanks\n\n🙏 Special thanks to [Georgi Gerganov](https://github.com/ggerganov) and the whole team working on [llama.cpp](https://github.com/ggerganov/llama.cpp/) for making all of this possible.",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModel",
      "custom_class": null,
      "pipeline_tag": null,
      "processor": null
    },
    "safetensors": null,
    "model_index": null,
    "trending_score": null
  },
  {
    "model_id": "deepseek-ai/DeepSeek-V3",
    "model_name": "deepseek-ai/DeepSeek-V3",
    "author": "deepseek-ai",
    "downloads": 2383365,
    "downloads_all_time": null,
    "likes": 3660,
    "tags": [
      "transformers",
      "safetensors",
      "deepseek_v3",
      "text-generation",
      "conversational",
      "custom_code",
      "arxiv:2412.19437",
      "autotrain_compatible",
      "fp8",
      "region:us"
    ],
    "card_url": "https://huggingface.co/deepseek-ai/DeepSeek-V3",
    "dependencies": [
      [
        "transformers",
        null
      ]
    ],
    "last_modified": "2025-02-24T03:29:50+00:00",
    "created_at": "2024-12-25T12:52:23+00:00",
    "analysis_date": "2025-03-22T00:41:43.081996",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "deepseek_v3",
    "pipeline_tag": "text-generation",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "library_name": "transformers"
    },
    "card_text": "<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align=\"center\">\n  <img src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true\" width=\"60%\" alt=\"DeepSeek-V3\" />\n</div>\n<hr>\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://www.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Homepage\" src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://chat.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/🤖%20Chat-DeepSeek%20V3-536af5?color=536af5&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://huggingface.co/deepseek-ai\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://discord.gg/Tc7c45Zzu5\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Wechat\" src=\"https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://twitter.com/deepseek_ai\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Twitter Follow\" src=\"https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V3/blob/main/LICENSE-CODE\" style=\"margin: 2px;\">\n    <img alt=\"Code License\" src=\"https://img.shields.io/badge/Code_License-MIT-f5de53?&color=f5de53\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V3/blob/main/LICENSE-MODEL\" style=\"margin: 2px;\">\n    <img alt=\"Model License\" src=\"https://img.shields.io/badge/Model_License-Model_Agreement-f5de53?&color=f5de53\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n\n<p align=\"center\">\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf\"><b>Paper Link</b>👁️</a>\n</p>\n\n\n## 1. Introduction\n\nWe present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. \nTo achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. \nFurthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. \nWe pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. \nComprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models.\nDespite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training.\nIn addition, its training process is remarkably stable. \nThroughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks. \n<p align=\"center\">\n  <img width=\"80%\" src=\"figures/benchmark.png\">\n</p>\n\n## 2. Model Summary\n\n---\n\n**Architecture: Innovative Load Balancing Strategy and Training Objective**\n\n- On top of the efficient architecture of DeepSeek-V2, we pioneer an auxiliary-loss-free strategy for load balancing, which minimizes the performance degradation that arises from encouraging load balancing.\n-  We investigate a Multi-Token Prediction (MTP) objective and prove it beneficial to model performance. \n    It can also be used for speculative decoding for inference acceleration. \n\n---\n\n**Pre-Training: Towards Ultimate Training Efficiency**\n\n- We design an FP8 mixed precision training framework and, for the first time, validate the feasibility and effectiveness of FP8 training on an extremely large-scale model.  \n- Through co-design of algorithms, frameworks, and hardware, we overcome the communication bottleneck in cross-node MoE training, nearly achieving full computation-communication overlap.  \n  This significantly enhances our training efficiency and reduces the training costs, enabling us to further scale up the model size without additional overhead.  \n- At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model. The subsequent training stages after pre-training require only 0.1M GPU hours.\n\n---\n\n**Post-Training: Knowledge Distillation from DeepSeek-R1**\n\n-   We introduce an innovative methodology to distill reasoning capabilities from the long-Chain-of-Thought (CoT) model, specifically from one of the DeepSeek R1 series models, into standard LLMs, particularly DeepSeek-V3. Our pipeline elegantly incorporates the verification and reflection patterns of R1 into DeepSeek-V3 and notably improves its reasoning performance. Meanwhile, we also maintain a control over the output style and length of DeepSeek-V3.\n\n---\n\n\n## 3. Model Downloads\n\n<div align=\"center\">\n\n| **Model** | **#Total Params** | **#Activated Params** | **Context Length** | **Download** |\n| :------------: | :------------: | :------------: | :------------: | :------------: |\n| DeepSeek-V3-Base | 671B | 37B | 128K   | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-V3-Base)   |\n| DeepSeek-V3   | 671B | 37B |  128K   | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-V3)   |\n\n</div>\n\n**NOTE: The total size of DeepSeek-V3 models on HuggingFace is 685B, which includes 671B of the Main Model weights and 14B of the Multi-Token Prediction (MTP) Module weights.**\n\nTo ensure optimal performance and flexibility, we have partnered with open-source communities and hardware vendors to provide multiple ways to run the model locally. For step-by-step guidance, check out Section 6: [How_to Run_Locally](#6-how-to-run-locally).\n\nFor developers looking to dive deeper, we recommend exploring [README_WEIGHTS.md](./README_WEIGHTS.md) for details on the Main Model weights and the Multi-Token Prediction (MTP) Modules. Please note that MTP support is currently under active development within the community, and we welcome your contributions and feedback.\n\n## 4. Evaluation Results\n### Base Model\n#### Standard Benchmarks\n\n<div align=\"center\">\n\n\n|  | Benchmark (Metric) | # Shots | DeepSeek-V2 | Qwen2.5 72B | LLaMA3.1 405B | DeepSeek-V3 |\n|---|-------------------|----------|--------|-------------|---------------|---------|\n| | Architecture | - | MoE | Dense | Dense | MoE |\n| | # Activated Params | - | 21B | 72B | 405B | 37B |\n| | # Total Params | - | 236B | 72B | 405B | 671B |\n| English | Pile-test (BPB) | - | 0.606 | 0.638 | **0.542** | 0.548 |\n| | BBH (EM) | 3-shot | 78.8 | 79.8 | 82.9 | **87.5** |\n| | MMLU (Acc.) | 5-shot | 78.4 | 85.0 | 84.4 | **87.1** |\n| | MMLU-Redux (Acc.) | 5-shot | 75.6 | 83.2 | 81.3 | **86.2** |\n| | MMLU-Pro (Acc.) | 5-shot | 51.4 | 58.3 | 52.8 | **64.4** |\n| | DROP (F1) | 3-shot | 80.4 | 80.6 | 86.0 | **89.0** |\n| | ARC-Easy (Acc.) | 25-shot | 97.6 | 98.4 | 98.4 | **98.9** |\n| | ARC-Challenge (Acc.) | 25-shot | 92.2 | 94.5 | **95.3** | **95.3** |\n| | HellaSwag (Acc.) | 10-shot | 87.1 | 84.8 | **89.2** | 88.9 |\n| | PIQA (Acc.) | 0-shot | 83.9 | 82.6 | **85.9** | 84.7 |\n| | WinoGrande (Acc.) | 5-shot | **86.3** | 82.3 | 85.2 | 84.9 |\n| | RACE-Middle (Acc.) | 5-shot | 73.1 | 68.1 | **74.2** | 67.1 |\n| | RACE-High (Acc.) | 5-shot | 52.6 | 50.3 | **56.8** | 51.3 |\n| | TriviaQA (EM) | 5-shot | 80.0 | 71.9 | **82.7** | **82.9** |\n| | NaturalQuestions (EM) | 5-shot | 38.6 | 33.2 | **41.5** | 40.0 |\n| | AGIEval (Acc.) | 0-shot | 57.5 | 75.8 | 60.6 | **79.6** |\n| Code | HumanEval (Pass@1) | 0-shot | 43.3 | 53.0 | 54.9 | **65.2** |\n| | MBPP (Pass@1) | 3-shot | 65.0 | 72.6 | 68.4 | **75.4** |\n| | LiveCodeBench-Base (Pass@1) | 3-shot | 11.6 | 12.9 | 15.5 | **19.4** |\n| | CRUXEval-I (Acc.) | 2-shot | 52.5 | 59.1 | 58.5 | **67.3** |\n| | CRUXEval-O (Acc.) | 2-shot | 49.8 | 59.9 | 59.9 | **69.8** |\n| Math | GSM8K (EM) | 8-shot | 81.6 | 88.3 | 83.5 | **89.3** |\n| | MATH (EM) | 4-shot | 43.4 | 54.4 | 49.0 | **61.6** |\n| | MGSM (EM) | 8-shot | 63.6 | 76.2 | 69.9 | **79.8** |\n| | CMath (EM) | 3-shot | 78.7 | 84.5 | 77.3 | **90.7** |\n| Chinese | CLUEWSC (EM) | 5-shot | 82.0 | 82.5 | **83.0** | 82.7 |\n| | C-Eval (Acc.) | 5-shot | 81.4 | 89.2 | 72.5 | **90.1** |\n| | CMMLU (Acc.) | 5-shot | 84.0 | **89.5** | 73.7 | 88.8 |\n| | CMRC (EM) | 1-shot | **77.4** | 75.8 | 76.0 | 76.3 |\n| | C3 (Acc.) | 0-shot | 77.4 | 76.7 | **79.7** | 78.6 |\n| | CCPM (Acc.) | 0-shot | **93.0** | 88.5 | 78.6 | 92.0 |\n| Multilingual | MMMLU-non-English (Acc.) | 5-shot | 64.0 | 74.8 | 73.8 | **79.4** |\n\n</div>\n\nNote: Best results are shown in bold. Scores with a gap not exceeding 0.3 are considered to be at the same level. DeepSeek-V3 achieves the best performance on most benchmarks, especially on math and code tasks.\nFor more evaluation details, please check our paper. \n\n#### Context Window\n<p align=\"center\">\n  <img width=\"80%\" src=\"figures/niah.png\">\n</p>\n\nEvaluation results on the ``Needle In A Haystack`` (NIAH) tests.  DeepSeek-V3 performs well across all context window lengths up to **128K**. \n\n### Chat Model\n#### Standard Benchmarks (Models larger than 67B)\n<div align=\"center\">\n\n| | **Benchmark (Metric)** | **DeepSeek V2-0506** | **DeepSeek V2.5-0905** | **Qwen2.5 72B-Inst.** | **Llama3.1 405B-Inst.** | **Claude-3.5-Sonnet-1022** | **GPT-4o 0513** | **DeepSeek V3** |\n|---|---------------------|---------------------|----------------------|---------------------|----------------------|---------------------------|----------------|----------------|\n| | Architecture | MoE | MoE | Dense | Dense | - | - | MoE |\n| | # Activated Params | 21B | 21B | 72B | 405B | - | - | 37B |\n| | # Total Params | 236B | 236B | 72B | 405B | - | - | 671B |\n| English | MMLU (EM) | 78.2 | 80.6 | 85.3 | **88.6** | **88.3** | 87.2 | **88.5** |\n| | MMLU-Redux (EM) | 77.9 | 80.3 | 85.6 | 86.2 | **88.9** | 88.0 | **89.1** |\n| | MMLU-Pro (EM) | 58.5 | 66.2 | 71.6 | 73.3 | **78.0** | 72.6 | 75.9 |\n| | DROP (3-shot F1) | 83.0 | 87.8 | 76.7 | 88.7 | 88.3 | 83.7 | **91.6** |\n| | IF-Eval (Prompt Strict) | 57.7 | 80.6 | 84.1 | 86.0 | **86.5** | 84.3 | 86.1 |\n| | GPQA-Diamond (Pass@1) | 35.3 | 41.3 | 49.0 | 51.1 | **65.0** | 49.9 | 59.1 |\n| | SimpleQA (Correct) | 9.0 | 10.2 | 9.1 | 17.1 | 28.4 | **38.2** | 24.9 |\n| | FRAMES (Acc.) | 66.9 | 65.4 | 69.8 | 70.0 | 72.5 | **80.5** | 73.3 |\n| | LongBench v2 (Acc.) | 31.6 | 35.4 | 39.4 | 36.1 | 41.0 | 48.1 | **48.7** |\n| Code | HumanEval-Mul (Pass@1) | 69.3 | 77.4 | 77.3 | 77.2 | 81.7 | 80.5 | **82.6** |\n| | LiveCodeBench (Pass@1-COT) | 18.8 | 29.2 | 31.1 | 28.4 | 36.3 | 33.4 | **40.5** |\n| | LiveCodeBench (Pass@1) | 20.3 | 28.4 | 28.7 | 30.1 | 32.8 | 34.2 | **37.6** |\n| | Codeforces (Percentile) | 17.5 | 35.6 | 24.8 | 25.3 | 20.3 | 23.6 | **51.6** |\n| | SWE Verified (Resolved) | - | 22.6 | 23.8 | 24.5 | **50.8** | 38.8 | 42.0 |\n| | Aider-Edit (Acc.) | 60.3 | 71.6 | 65.4 | 63.9 | **84.2** | 72.9 | 79.7 |\n| | Aider-Polyglot (Acc.) | - | 18.2 | 7.6 | 5.8 | 45.3 | 16.0 | **49.6** |\n| Math | AIME 2024 (Pass@1) | 4.6 | 16.7 | 23.3 | 23.3 | 16.0 | 9.3 | **39.2** |\n| | MATH-500 (EM) | 56.3 | 74.7 | 80.0 | 73.8 | 78.3 | 74.6 | **90.2** |\n| | CNMO 2024 (Pass@1) | 2.8 | 10.8 | 15.9 | 6.8 | 13.1 | 10.8 | **43.2** |\n| Chinese | CLUEWSC (EM) | 89.9 | 90.4 | **91.4** | 84.7 | 85.4 | 87.9 | 90.9 |\n| | C-Eval (EM) | 78.6 | 79.5 | 86.1 | 61.5 | 76.7 | 76.0 | **86.5** |\n| | C-SimpleQA (Correct) | 48.5 | 54.1 | 48.4 | 50.4 | 51.3 | 59.3 | **64.8** |\n\nNote: All models are evaluated in a configuration that limits the output length to 8K. Benchmarks containing fewer than 1000 samples are tested multiple times using varying temperature settings to derive robust final results. DeepSeek-V3 stands as the best-performing open-source model, and also exhibits competitive performance against frontier closed-source models.\n\n</div>\n\n\n####  Open Ended Generation Evaluation\n\n<div align=\"center\">\n\n\n\n| Model | Arena-Hard | AlpacaEval 2.0 |\n|-------|------------|----------------|\n| DeepSeek-V2.5-0905 | 76.2 | 50.5 |\n| Qwen2.5-72B-Instruct | 81.2 | 49.1 |\n| LLaMA-3.1 405B | 69.3 | 40.5 |\n| GPT-4o-0513 | 80.4 | 51.1 |\n| Claude-Sonnet-3.5-1022 | 85.2 | 52.0 |\n| DeepSeek-V3 | **85.5** | **70.0** |\n\nNote: English open-ended conversation evaluations. For AlpacaEval 2.0, we use the length-controlled win rate as the metric.\n</div>\n\n\n## 5. Chat Website & API Platform\nYou can chat with DeepSeek-V3 on DeepSeek's official website: [chat.deepseek.com](https://chat.deepseek.com/sign_in)\n\nWe also provide OpenAI-Compatible API at DeepSeek Platform: [platform.deepseek.com](https://platform.deepseek.com/)\n\n## 6. How to Run Locally\n\nDeepSeek-V3 can be deployed locally using the following hardware and open-source community software:\n\n1. **DeepSeek-Infer Demo**: We provide a simple and lightweight demo for FP8 and BF16 inference.\n2. **SGLang**: Fully support the DeepSeek-V3 model in both BF16 and FP8 inference modes.\n3. **LMDeploy**: Enables efficient FP8 and BF16 inference for local and cloud deployment.\n4. **TensorRT-LLM**: Currently supports BF16 inference and INT4/8 quantization, with FP8 support coming soon.\n5. **vLLM**: Support DeekSeek-V3 model with FP8 and BF16 modes for tensor parallelism and pipeline parallelism.\n6. **AMD GPU**: Enables running the DeepSeek-V3 model on AMD GPUs via SGLang in both BF16 and FP8 modes.\n7. **Huawei Ascend NPU**: Supports running DeepSeek-V3 on Huawei Ascend devices.\n\nSince FP8 training is natively adopted in our framework, we only provide FP8 weights. If you require BF16 weights for experimentation, you can use the provided conversion script to perform the transformation.\n\nHere is an example of converting FP8 weights to BF16:\n\n```shell\ncd inference\npython fp8_cast_bf16.py --input-fp8-hf-path /path/to/fp8_weights --output-bf16-hf-path /path/to/bf16_weights\n```\n\n**NOTE: Huggingface's Transformers has not been directly supported yet.**\n\n### 6.1 Inference with DeepSeek-Infer Demo (example only)\n\n#### Model Weights & Demo Code Preparation\n\nFirst, clone our DeepSeek-V3 GitHub repository:\n\n```shell\ngit clone https://github.com/deepseek-ai/DeepSeek-V3.git\n```\n\nNavigate to the `inference` folder and install dependencies listed in `requirements.txt`.\n\n```shell\ncd DeepSeek-V3/inference\npip install -r requirements.txt\n```\n\nDownload the model weights from HuggingFace, and put them into `/path/to/DeepSeek-V3` folder.\n\n#### Model Weights Conversion\n\nConvert HuggingFace model weights to a specific format:\n\n```shell\npython convert.py --hf-ckpt-path /path/to/DeepSeek-V3 --save-path /path/to/DeepSeek-V3-Demo --n-experts 256 --model-parallel 16\n```\n\n#### Run\n\nThen you can chat with DeepSeek-V3:\n\n```shell\ntorchrun --nnodes 2 --nproc-per-node 8 generate.py --node-rank $RANK --master-addr $ADDR --ckpt-path /path/to/DeepSeek-V3-Demo --config configs/config_671B.json --interactive --temperature 0.7 --max-new-tokens 200\n```\n\nOr batch inference on a given file:\n\n```shell\ntorchrun --nnodes 2 --nproc-per-node 8 generate.py --node-rank $RANK --master-addr $ADDR --ckpt-path /path/to/DeepSeek-V3-Demo --config configs/config_671B.json --input-file $FILE\n```\n\n### 6.2 Inference with SGLang (recommended)\n\n[SGLang](https://github.com/sgl-project/sglang) currently supports MLA optimizations, FP8 (W8A8), FP8 KV Cache, and Torch Compile, delivering state-of-the-art latency and throughput performance among open-source frameworks.\n\nNotably, [SGLang v0.4.1](https://github.com/sgl-project/sglang/releases/tag/v0.4.1) fully supports running DeepSeek-V3 on both **NVIDIA and AMD GPUs**, making it a highly versatile and robust solution.\n\nHere are the launch instructions from the SGLang team: https://github.com/sgl-project/sglang/tree/main/benchmark/deepseek_v3\n\n### 6.3 Inference with LMDeploy (recommended)\n[LMDeploy](https://github.com/InternLM/lmdeploy), a flexible and high-performance inference and serving framework tailored for large language models, now supports DeepSeek-V3. It offers both offline pipeline processing and online deployment capabilities, seamlessly integrating with PyTorch-based workflows.\n\nFor comprehensive step-by-step instructions on running DeepSeek-V3 with LMDeploy, please refer to here: https://github.com/InternLM/lmdeploy/issues/2960\n\n\n### 6.4 Inference with TRT-LLM (recommended)\n\n[TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM) now supports the DeepSeek-V3 model, offering precision options such as BF16 and INT4/INT8 weight-only. Support for FP8 is currently in progress and will be released soon. You can access the custom branch of TRTLLM specifically for DeepSeek-V3 support through the following link to experience the new features directly: https://github.com/NVIDIA/TensorRT-LLM/tree/deepseek/examples/deepseek_v3. \n\n### 6.5 Inference with vLLM (recommended)\n\n[vLLM](https://github.com/vllm-project/vllm) v0.6.6 supports DeepSeek-V3 inference for FP8 and BF16 modes on both NVIDIA and AMD GPUs. Aside from standard techniques, vLLM offers _pipeline parallelism_ allowing you to run this model on multiple machines connected by networks. For detailed guidance, please refer to the [vLLM instructions](https://docs.vllm.ai/en/latest/serving/distributed_serving.html). Please feel free to follow [the enhancement plan](https://github.com/vllm-project/vllm/issues/11539) as well.\n\n### 6.6 Recommended Inference Functionality with AMD GPUs\n\nIn collaboration with the AMD team, we have achieved Day-One support for AMD GPUs using SGLang, with full compatibility for both FP8 and BF16 precision. For detailed guidance, please refer to the [SGLang instructions](#63-inference-with-lmdeploy-recommended).\n\n### 6.7 Recommended Inference Functionality with Huawei Ascend NPUs\nThe [MindIE](https://www.hiascend.com/en/software/mindie) framework from the Huawei Ascend community has successfully adapted the BF16 version of DeepSeek-V3. For step-by-step guidance on Ascend NPUs, please follow the [instructions here](https://modelers.cn/models/MindIE/deepseekv3).\n\n\n## 7. License\nThis code repository is licensed under [the MIT License](LICENSE-CODE). The use of DeepSeek-V3 Base/Chat models is subject to [the Model License](LICENSE-MODEL). DeepSeek-V3 series (including Base and Chat) supports commercial use.\n\n## 8. Citation\n```\n@misc{deepseekai2024deepseekv3technicalreport,\n      title={DeepSeek-V3 Technical Report}, \n      author={DeepSeek-AI},\n      year={2024},\n      eprint={2412.19437},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2412.19437}, \n}\n```\n\n## 9. Contact\nIf you have any questions, please raise an issue or contact us at [service@deepseek.com](service@deepseek.com).",
    "card_content": "---\nlibrary_name: transformers\n---\n<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align=\"center\">\n  <img src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true\" width=\"60%\" alt=\"DeepSeek-V3\" />\n</div>\n<hr>\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://www.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Homepage\" src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://chat.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/🤖%20Chat-DeepSeek%20V3-536af5?color=536af5&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://huggingface.co/deepseek-ai\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://discord.gg/Tc7c45Zzu5\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Wechat\" src=\"https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://twitter.com/deepseek_ai\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Twitter Follow\" src=\"https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V3/blob/main/LICENSE-CODE\" style=\"margin: 2px;\">\n    <img alt=\"Code License\" src=\"https://img.shields.io/badge/Code_License-MIT-f5de53?&color=f5de53\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V3/blob/main/LICENSE-MODEL\" style=\"margin: 2px;\">\n    <img alt=\"Model License\" src=\"https://img.shields.io/badge/Model_License-Model_Agreement-f5de53?&color=f5de53\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n\n<p align=\"center\">\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf\"><b>Paper Link</b>👁️</a>\n</p>\n\n\n## 1. Introduction\n\nWe present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. \nTo achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. \nFurthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. \nWe pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. \nComprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models.\nDespite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training.\nIn addition, its training process is remarkably stable. \nThroughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks. \n<p align=\"center\">\n  <img width=\"80%\" src=\"figures/benchmark.png\">\n</p>\n\n## 2. Model Summary\n\n---\n\n**Architecture: Innovative Load Balancing Strategy and Training Objective**\n\n- On top of the efficient architecture of DeepSeek-V2, we pioneer an auxiliary-loss-free strategy for load balancing, which minimizes the performance degradation that arises from encouraging load balancing.\n-  We investigate a Multi-Token Prediction (MTP) objective and prove it beneficial to model performance. \n    It can also be used for speculative decoding for inference acceleration. \n\n---\n\n**Pre-Training: Towards Ultimate Training Efficiency**\n\n- We design an FP8 mixed precision training framework and, for the first time, validate the feasibility and effectiveness of FP8 training on an extremely large-scale model.  \n- Through co-design of algorithms, frameworks, and hardware, we overcome the communication bottleneck in cross-node MoE training, nearly achieving full computation-communication overlap.  \n  This significantly enhances our training efficiency and reduces the training costs, enabling us to further scale up the model size without additional overhead.  \n- At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model. The subsequent training stages after pre-training require only 0.1M GPU hours.\n\n---\n\n**Post-Training: Knowledge Distillation from DeepSeek-R1**\n\n-   We introduce an innovative methodology to distill reasoning capabilities from the long-Chain-of-Thought (CoT) model, specifically from one of the DeepSeek R1 series models, into standard LLMs, particularly DeepSeek-V3. Our pipeline elegantly incorporates the verification and reflection patterns of R1 into DeepSeek-V3 and notably improves its reasoning performance. Meanwhile, we also maintain a control over the output style and length of DeepSeek-V3.\n\n---\n\n\n## 3. Model Downloads\n\n<div align=\"center\">\n\n| **Model** | **#Total Params** | **#Activated Params** | **Context Length** | **Download** |\n| :------------: | :------------: | :------------: | :------------: | :------------: |\n| DeepSeek-V3-Base | 671B | 37B | 128K   | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-V3-Base)   |\n| DeepSeek-V3   | 671B | 37B |  128K   | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-V3)   |\n\n</div>\n\n**NOTE: The total size of DeepSeek-V3 models on HuggingFace is 685B, which includes 671B of the Main Model weights and 14B of the Multi-Token Prediction (MTP) Module weights.**\n\nTo ensure optimal performance and flexibility, we have partnered with open-source communities and hardware vendors to provide multiple ways to run the model locally. For step-by-step guidance, check out Section 6: [How_to Run_Locally](#6-how-to-run-locally).\n\nFor developers looking to dive deeper, we recommend exploring [README_WEIGHTS.md](./README_WEIGHTS.md) for details on the Main Model weights and the Multi-Token Prediction (MTP) Modules. Please note that MTP support is currently under active development within the community, and we welcome your contributions and feedback.\n\n## 4. Evaluation Results\n### Base Model\n#### Standard Benchmarks\n\n<div align=\"center\">\n\n\n|  | Benchmark (Metric) | # Shots | DeepSeek-V2 | Qwen2.5 72B | LLaMA3.1 405B | DeepSeek-V3 |\n|---|-------------------|----------|--------|-------------|---------------|---------|\n| | Architecture | - | MoE | Dense | Dense | MoE |\n| | # Activated Params | - | 21B | 72B | 405B | 37B |\n| | # Total Params | - | 236B | 72B | 405B | 671B |\n| English | Pile-test (BPB) | - | 0.606 | 0.638 | **0.542** | 0.548 |\n| | BBH (EM) | 3-shot | 78.8 | 79.8 | 82.9 | **87.5** |\n| | MMLU (Acc.) | 5-shot | 78.4 | 85.0 | 84.4 | **87.1** |\n| | MMLU-Redux (Acc.) | 5-shot | 75.6 | 83.2 | 81.3 | **86.2** |\n| | MMLU-Pro (Acc.) | 5-shot | 51.4 | 58.3 | 52.8 | **64.4** |\n| | DROP (F1) | 3-shot | 80.4 | 80.6 | 86.0 | **89.0** |\n| | ARC-Easy (Acc.) | 25-shot | 97.6 | 98.4 | 98.4 | **98.9** |\n| | ARC-Challenge (Acc.) | 25-shot | 92.2 | 94.5 | **95.3** | **95.3** |\n| | HellaSwag (Acc.) | 10-shot | 87.1 | 84.8 | **89.2** | 88.9 |\n| | PIQA (Acc.) | 0-shot | 83.9 | 82.6 | **85.9** | 84.7 |\n| | WinoGrande (Acc.) | 5-shot | **86.3** | 82.3 | 85.2 | 84.9 |\n| | RACE-Middle (Acc.) | 5-shot | 73.1 | 68.1 | **74.2** | 67.1 |\n| | RACE-High (Acc.) | 5-shot | 52.6 | 50.3 | **56.8** | 51.3 |\n| | TriviaQA (EM) | 5-shot | 80.0 | 71.9 | **82.7** | **82.9** |\n| | NaturalQuestions (EM) | 5-shot | 38.6 | 33.2 | **41.5** | 40.0 |\n| | AGIEval (Acc.) | 0-shot | 57.5 | 75.8 | 60.6 | **79.6** |\n| Code | HumanEval (Pass@1) | 0-shot | 43.3 | 53.0 | 54.9 | **65.2** |\n| | MBPP (Pass@1) | 3-shot | 65.0 | 72.6 | 68.4 | **75.4** |\n| | LiveCodeBench-Base (Pass@1) | 3-shot | 11.6 | 12.9 | 15.5 | **19.4** |\n| | CRUXEval-I (Acc.) | 2-shot | 52.5 | 59.1 | 58.5 | **67.3** |\n| | CRUXEval-O (Acc.) | 2-shot | 49.8 | 59.9 | 59.9 | **69.8** |\n| Math | GSM8K (EM) | 8-shot | 81.6 | 88.3 | 83.5 | **89.3** |\n| | MATH (EM) | 4-shot | 43.4 | 54.4 | 49.0 | **61.6** |\n| | MGSM (EM) | 8-shot | 63.6 | 76.2 | 69.9 | **79.8** |\n| | CMath (EM) | 3-shot | 78.7 | 84.5 | 77.3 | **90.7** |\n| Chinese | CLUEWSC (EM) | 5-shot | 82.0 | 82.5 | **83.0** | 82.7 |\n| | C-Eval (Acc.) | 5-shot | 81.4 | 89.2 | 72.5 | **90.1** |\n| | CMMLU (Acc.) | 5-shot | 84.0 | **89.5** | 73.7 | 88.8 |\n| | CMRC (EM) | 1-shot | **77.4** | 75.8 | 76.0 | 76.3 |\n| | C3 (Acc.) | 0-shot | 77.4 | 76.7 | **79.7** | 78.6 |\n| | CCPM (Acc.) | 0-shot | **93.0** | 88.5 | 78.6 | 92.0 |\n| Multilingual | MMMLU-non-English (Acc.) | 5-shot | 64.0 | 74.8 | 73.8 | **79.4** |\n\n</div>\n\nNote: Best results are shown in bold. Scores with a gap not exceeding 0.3 are considered to be at the same level. DeepSeek-V3 achieves the best performance on most benchmarks, especially on math and code tasks.\nFor more evaluation details, please check our paper. \n\n#### Context Window\n<p align=\"center\">\n  <img width=\"80%\" src=\"figures/niah.png\">\n</p>\n\nEvaluation results on the ``Needle In A Haystack`` (NIAH) tests.  DeepSeek-V3 performs well across all context window lengths up to **128K**. \n\n### Chat Model\n#### Standard Benchmarks (Models larger than 67B)\n<div align=\"center\">\n\n| | **Benchmark (Metric)** | **DeepSeek V2-0506** | **DeepSeek V2.5-0905** | **Qwen2.5 72B-Inst.** | **Llama3.1 405B-Inst.** | **Claude-3.5-Sonnet-1022** | **GPT-4o 0513** | **DeepSeek V3** |\n|---|---------------------|---------------------|----------------------|---------------------|----------------------|---------------------------|----------------|----------------|\n| | Architecture | MoE | MoE | Dense | Dense | - | - | MoE |\n| | # Activated Params | 21B | 21B | 72B | 405B | - | - | 37B |\n| | # Total Params | 236B | 236B | 72B | 405B | - | - | 671B |\n| English | MMLU (EM) | 78.2 | 80.6 | 85.3 | **88.6** | **88.3** | 87.2 | **88.5** |\n| | MMLU-Redux (EM) | 77.9 | 80.3 | 85.6 | 86.2 | **88.9** | 88.0 | **89.1** |\n| | MMLU-Pro (EM) | 58.5 | 66.2 | 71.6 | 73.3 | **78.0** | 72.6 | 75.9 |\n| | DROP (3-shot F1) | 83.0 | 87.8 | 76.7 | 88.7 | 88.3 | 83.7 | **91.6** |\n| | IF-Eval (Prompt Strict) | 57.7 | 80.6 | 84.1 | 86.0 | **86.5** | 84.3 | 86.1 |\n| | GPQA-Diamond (Pass@1) | 35.3 | 41.3 | 49.0 | 51.1 | **65.0** | 49.9 | 59.1 |\n| | SimpleQA (Correct) | 9.0 | 10.2 | 9.1 | 17.1 | 28.4 | **38.2** | 24.9 |\n| | FRAMES (Acc.) | 66.9 | 65.4 | 69.8 | 70.0 | 72.5 | **80.5** | 73.3 |\n| | LongBench v2 (Acc.) | 31.6 | 35.4 | 39.4 | 36.1 | 41.0 | 48.1 | **48.7** |\n| Code | HumanEval-Mul (Pass@1) | 69.3 | 77.4 | 77.3 | 77.2 | 81.7 | 80.5 | **82.6** |\n| | LiveCodeBench (Pass@1-COT) | 18.8 | 29.2 | 31.1 | 28.4 | 36.3 | 33.4 | **40.5** |\n| | LiveCodeBench (Pass@1) | 20.3 | 28.4 | 28.7 | 30.1 | 32.8 | 34.2 | **37.6** |\n| | Codeforces (Percentile) | 17.5 | 35.6 | 24.8 | 25.3 | 20.3 | 23.6 | **51.6** |\n| | SWE Verified (Resolved) | - | 22.6 | 23.8 | 24.5 | **50.8** | 38.8 | 42.0 |\n| | Aider-Edit (Acc.) | 60.3 | 71.6 | 65.4 | 63.9 | **84.2** | 72.9 | 79.7 |\n| | Aider-Polyglot (Acc.) | - | 18.2 | 7.6 | 5.8 | 45.3 | 16.0 | **49.6** |\n| Math | AIME 2024 (Pass@1) | 4.6 | 16.7 | 23.3 | 23.3 | 16.0 | 9.3 | **39.2** |\n| | MATH-500 (EM) | 56.3 | 74.7 | 80.0 | 73.8 | 78.3 | 74.6 | **90.2** |\n| | CNMO 2024 (Pass@1) | 2.8 | 10.8 | 15.9 | 6.8 | 13.1 | 10.8 | **43.2** |\n| Chinese | CLUEWSC (EM) | 89.9 | 90.4 | **91.4** | 84.7 | 85.4 | 87.9 | 90.9 |\n| | C-Eval (EM) | 78.6 | 79.5 | 86.1 | 61.5 | 76.7 | 76.0 | **86.5** |\n| | C-SimpleQA (Correct) | 48.5 | 54.1 | 48.4 | 50.4 | 51.3 | 59.3 | **64.8** |\n\nNote: All models are evaluated in a configuration that limits the output length to 8K. Benchmarks containing fewer than 1000 samples are tested multiple times using varying temperature settings to derive robust final results. DeepSeek-V3 stands as the best-performing open-source model, and also exhibits competitive performance against frontier closed-source models.\n\n</div>\n\n\n####  Open Ended Generation Evaluation\n\n<div align=\"center\">\n\n\n\n| Model | Arena-Hard | AlpacaEval 2.0 |\n|-------|------------|----------------|\n| DeepSeek-V2.5-0905 | 76.2 | 50.5 |\n| Qwen2.5-72B-Instruct | 81.2 | 49.1 |\n| LLaMA-3.1 405B | 69.3 | 40.5 |\n| GPT-4o-0513 | 80.4 | 51.1 |\n| Claude-Sonnet-3.5-1022 | 85.2 | 52.0 |\n| DeepSeek-V3 | **85.5** | **70.0** |\n\nNote: English open-ended conversation evaluations. For AlpacaEval 2.0, we use the length-controlled win rate as the metric.\n</div>\n\n\n## 5. Chat Website & API Platform\nYou can chat with DeepSeek-V3 on DeepSeek's official website: [chat.deepseek.com](https://chat.deepseek.com/sign_in)\n\nWe also provide OpenAI-Compatible API at DeepSeek Platform: [platform.deepseek.com](https://platform.deepseek.com/)\n\n## 6. How to Run Locally\n\nDeepSeek-V3 can be deployed locally using the following hardware and open-source community software:\n\n1. **DeepSeek-Infer Demo**: We provide a simple and lightweight demo for FP8 and BF16 inference.\n2. **SGLang**: Fully support the DeepSeek-V3 model in both BF16 and FP8 inference modes.\n3. **LMDeploy**: Enables efficient FP8 and BF16 inference for local and cloud deployment.\n4. **TensorRT-LLM**: Currently supports BF16 inference and INT4/8 quantization, with FP8 support coming soon.\n5. **vLLM**: Support DeekSeek-V3 model with FP8 and BF16 modes for tensor parallelism and pipeline parallelism.\n6. **AMD GPU**: Enables running the DeepSeek-V3 model on AMD GPUs via SGLang in both BF16 and FP8 modes.\n7. **Huawei Ascend NPU**: Supports running DeepSeek-V3 on Huawei Ascend devices.\n\nSince FP8 training is natively adopted in our framework, we only provide FP8 weights. If you require BF16 weights for experimentation, you can use the provided conversion script to perform the transformation.\n\nHere is an example of converting FP8 weights to BF16:\n\n```shell\ncd inference\npython fp8_cast_bf16.py --input-fp8-hf-path /path/to/fp8_weights --output-bf16-hf-path /path/to/bf16_weights\n```\n\n**NOTE: Huggingface's Transformers has not been directly supported yet.**\n\n### 6.1 Inference with DeepSeek-Infer Demo (example only)\n\n#### Model Weights & Demo Code Preparation\n\nFirst, clone our DeepSeek-V3 GitHub repository:\n\n```shell\ngit clone https://github.com/deepseek-ai/DeepSeek-V3.git\n```\n\nNavigate to the `inference` folder and install dependencies listed in `requirements.txt`.\n\n```shell\ncd DeepSeek-V3/inference\npip install -r requirements.txt\n```\n\nDownload the model weights from HuggingFace, and put them into `/path/to/DeepSeek-V3` folder.\n\n#### Model Weights Conversion\n\nConvert HuggingFace model weights to a specific format:\n\n```shell\npython convert.py --hf-ckpt-path /path/to/DeepSeek-V3 --save-path /path/to/DeepSeek-V3-Demo --n-experts 256 --model-parallel 16\n```\n\n#### Run\n\nThen you can chat with DeepSeek-V3:\n\n```shell\ntorchrun --nnodes 2 --nproc-per-node 8 generate.py --node-rank $RANK --master-addr $ADDR --ckpt-path /path/to/DeepSeek-V3-Demo --config configs/config_671B.json --interactive --temperature 0.7 --max-new-tokens 200\n```\n\nOr batch inference on a given file:\n\n```shell\ntorchrun --nnodes 2 --nproc-per-node 8 generate.py --node-rank $RANK --master-addr $ADDR --ckpt-path /path/to/DeepSeek-V3-Demo --config configs/config_671B.json --input-file $FILE\n```\n\n### 6.2 Inference with SGLang (recommended)\n\n[SGLang](https://github.com/sgl-project/sglang) currently supports MLA optimizations, FP8 (W8A8), FP8 KV Cache, and Torch Compile, delivering state-of-the-art latency and throughput performance among open-source frameworks.\n\nNotably, [SGLang v0.4.1](https://github.com/sgl-project/sglang/releases/tag/v0.4.1) fully supports running DeepSeek-V3 on both **NVIDIA and AMD GPUs**, making it a highly versatile and robust solution.\n\nHere are the launch instructions from the SGLang team: https://github.com/sgl-project/sglang/tree/main/benchmark/deepseek_v3\n\n### 6.3 Inference with LMDeploy (recommended)\n[LMDeploy](https://github.com/InternLM/lmdeploy), a flexible and high-performance inference and serving framework tailored for large language models, now supports DeepSeek-V3. It offers both offline pipeline processing and online deployment capabilities, seamlessly integrating with PyTorch-based workflows.\n\nFor comprehensive step-by-step instructions on running DeepSeek-V3 with LMDeploy, please refer to here: https://github.com/InternLM/lmdeploy/issues/2960\n\n\n### 6.4 Inference with TRT-LLM (recommended)\n\n[TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM) now supports the DeepSeek-V3 model, offering precision options such as BF16 and INT4/INT8 weight-only. Support for FP8 is currently in progress and will be released soon. You can access the custom branch of TRTLLM specifically for DeepSeek-V3 support through the following link to experience the new features directly: https://github.com/NVIDIA/TensorRT-LLM/tree/deepseek/examples/deepseek_v3. \n\n### 6.5 Inference with vLLM (recommended)\n\n[vLLM](https://github.com/vllm-project/vllm) v0.6.6 supports DeepSeek-V3 inference for FP8 and BF16 modes on both NVIDIA and AMD GPUs. Aside from standard techniques, vLLM offers _pipeline parallelism_ allowing you to run this model on multiple machines connected by networks. For detailed guidance, please refer to the [vLLM instructions](https://docs.vllm.ai/en/latest/serving/distributed_serving.html). Please feel free to follow [the enhancement plan](https://github.com/vllm-project/vllm/issues/11539) as well.\n\n### 6.6 Recommended Inference Functionality with AMD GPUs\n\nIn collaboration with the AMD team, we have achieved Day-One support for AMD GPUs using SGLang, with full compatibility for both FP8 and BF16 precision. For detailed guidance, please refer to the [SGLang instructions](#63-inference-with-lmdeploy-recommended).\n\n### 6.7 Recommended Inference Functionality with Huawei Ascend NPUs\nThe [MindIE](https://www.hiascend.com/en/software/mindie) framework from the Huawei Ascend community has successfully adapted the BF16 version of DeepSeek-V3. For step-by-step guidance on Ascend NPUs, please follow the [instructions here](https://modelers.cn/models/MindIE/deepseekv3).\n\n\n## 7. License\nThis code repository is licensed under [the MIT License](LICENSE-CODE). The use of DeepSeek-V3 Base/Chat models is subject to [the Model License](LICENSE-MODEL). DeepSeek-V3 series (including Base and Chat) supports commercial use.\n\n## 8. Citation\n```\n@misc{deepseekai2024deepseekv3technicalreport,\n      title={DeepSeek-V3 Technical Report}, \n      author={DeepSeek-AI},\n      year={2024},\n      eprint={2412.19437},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2412.19437}, \n}\n```\n\n## 9. Contact\nIf you have any questions, please raise an issue or contact us at [service@deepseek.com](service@deepseek.com).",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModelForCausalLM",
      "custom_class": "modeling_deepseek.DeepseekV3ForCausalLM",
      "pipeline_tag": "text-generation",
      "processor": null
    },
    "safetensors": {
      "parameters": {
        "BF16": 3918786560,
        "F8_E4M3": 680571043840,
        "F32": 41555600
      },
      "total": 684531386000
    },
    "model_index": null,
    "trending_score": null
  },
  {
    "model_id": "MaziyarPanahi/firefunction-v2-GGUF",
    "model_name": "MaziyarPanahi/firefunction-v2-GGUF",
    "author": "MaziyarPanahi",
    "downloads": 2327011,
    "downloads_all_time": null,
    "likes": 16,
    "tags": [
      "transformers",
      "gguf",
      "quantized",
      "2-bit",
      "3-bit",
      "4-bit",
      "5-bit",
      "6-bit",
      "8-bit",
      "GGUF",
      "safetensors",
      "text-generation",
      "conversational",
      "function-calling",
      "text-generation-inference",
      "region:us",
      "base_model:fireworks-ai/llama-3-firefunction-v2",
      "base_model:quantized:fireworks-ai/llama-3-firefunction-v2",
      "license:llama3",
      "imatrix"
    ],
    "card_url": "https://huggingface.co/MaziyarPanahi/firefunction-v2-GGUF",
    "dependencies": [
      [
        "transformers",
        null
      ],
      [
        "safetensors",
        null
      ]
    ],
    "last_modified": "2024-06-20T08:48:24+00:00",
    "created_at": "2024-06-19T12:47:26+00:00",
    "analysis_date": "2025-03-22T00:41:44.571304",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "unknown",
    "pipeline_tag": "text-generation",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "tags": [
        "quantized",
        "2-bit",
        "3-bit",
        "4-bit",
        "5-bit",
        "6-bit",
        "8-bit",
        "GGUF",
        "transformers",
        "safetensors",
        "text-generation",
        "conversational",
        "function-calling",
        "text-generation-inference",
        "region:us"
      ],
      "model_name": "MaziyarPanahi/firefunction-v2-GGUF",
      "base_model": "fireworks-ai/firefunction-v2",
      "inference": false,
      "model_creator": "fireworks-ai",
      "pipeline_tag": "text-generation",
      "quantized_by": "MaziyarPanahi",
      "license": "llama3"
    },
    "card_text": "# [MaziyarPanahi/firefunction-v2-GGUF](https://huggingface.co/MaziyarPanahi/firefunction-v2-GGUF)\n- Model creator: [fireworks-ai](https://huggingface.co/fireworks-ai)\n- Original model: [fireworks-ai/firefunction-v2](https://huggingface.co/fireworks-ai/firefunction-v2)\n\n## Description\n[MaziyarPanahi/firefunction-v2-GGUF](https://huggingface.co/MaziyarPanahi/firefunction-v2-GGUF) contains GGUF format model files for [fireworks-ai/firefunction-v2](https://huggingface.co/fireworks-ai/firefunction-v2).\n\n### About GGUF\n\nGGUF is a new format introduced by the llama.cpp team on August 21st 2023. It is a replacement for GGML, which is no longer supported by llama.cpp.\n\nHere is an incomplete list of clients and libraries that are known to support GGUF:\n\n* [llama.cpp](https://github.com/ggerganov/llama.cpp). The source project for GGUF. Offers a CLI and a server option.\n* [llama-cpp-python](https://github.com/abetlen/llama-cpp-python), a Python library with GPU accel, LangChain support, and OpenAI-compatible API server.\n* [LM Studio](https://lmstudio.ai/), an easy-to-use and powerful local GUI for Windows and macOS (Silicon), with GPU acceleration. Linux available, in beta as of 27/11/2023.\n* [text-generation-webui](https://github.com/oobabooga/text-generation-webui), the most widely used web UI, with many features and powerful extensions. Supports GPU acceleration.\n* [KoboldCpp](https://github.com/LostRuins/koboldcpp), a fully featured web UI, with GPU accel across all platforms and GPU architectures. Especially good for story telling.\n* [GPT4All](https://gpt4all.io/index.html), a free and open source local running GUI, supporting Windows, Linux and macOS with full GPU accel.\n* [LoLLMS Web UI](https://github.com/ParisNeo/lollms-webui), a great web UI with many interesting and unique features, including a full model library for easy model selection.\n* [Faraday.dev](https://faraday.dev/), an attractive and easy to use character-based chat GUI for Windows and macOS (both Silicon and Intel), with GPU acceleration.\n* [candle](https://github.com/huggingface/candle), a Rust ML framework with a focus on performance, including GPU support, and ease of use.\n* [ctransformers](https://github.com/marella/ctransformers), a Python library with GPU accel, LangChain support, and OpenAI-compatible AI server. Note, as of time of writing (November 27th 2023), ctransformers has not been updated in a long time and does not support many recent models.\n\n## Special thanks\n\n🙏 Special thanks to [Georgi Gerganov](https://github.com/ggerganov) and the whole team working on [llama.cpp](https://github.com/ggerganov/llama.cpp/) for making all of this possible.\n\nOriginal README\n---\n# FireFunction V2: Fireworks Function Calling Model\n\n[**Try on Fireworks**](https://fireworks.ai/models/fireworks/firefunction-v2) | [**API Docs**](https://readme.fireworks.ai/docs/function-calling) | [**Demo App**](https://functional-chat.vercel.app/) | [**Discord**](https://discord.gg/mMqQxvFD9A)\n\n<img src=\"https://cdn-uploads.huggingface.co/production/uploads/64b6f3a72f5a966b9722de88/nJNtxLzWswBDKK1iOZblb.png\" alt=\"firefunction\" width=\"400\"/>\n\nFireFunction is a state-of-the-art function calling model with a commercially viable license. View detailed info in our [announcement blog](https://fireworks.ai/blog/firefunction-v2-launch-post). Key info and highlights:\n\n**Comparison with other models:**\n- Competitive with GPT-4o at function-calling, scoring 0.81 vs 0.80 on a medley of public evaluations\n- Trained on Llama 3 and retains Llama 3’s conversation and instruction-following capabilities, scoring 0.84 vs Llama 3’s 0.89 on MT bench\n- Significant quality improvements over FireFunction v1 across the broad range of metrics\n\n\n**General info:**\n\n🐾 Successor of the [FireFunction](https://fireworks.ai/models/fireworks/firefunction-v1) model\n\n🔆 Support of parallel function calling (unlike FireFunction v1) and good instruction following\n\n💡 Hosted on the [Fireworks](https://fireworks.ai/models/fireworks/firefunction-v2) platform at < 10% of the cost of GPT 4o and 2x the speed\n",
    "card_content": "---\ntags:\n- quantized\n- 2-bit\n- 3-bit\n- 4-bit\n- 5-bit\n- 6-bit\n- 8-bit\n- GGUF\n- transformers\n- safetensors\n- text-generation\n- conversational\n- function-calling\n- text-generation-inference\n- region:us\nmodel_name: MaziyarPanahi/firefunction-v2-GGUF\nbase_model: fireworks-ai/firefunction-v2\ninference: false\nmodel_creator: fireworks-ai\npipeline_tag: text-generation\nquantized_by: MaziyarPanahi\nlicense: llama3\n---\n# [MaziyarPanahi/firefunction-v2-GGUF](https://huggingface.co/MaziyarPanahi/firefunction-v2-GGUF)\n- Model creator: [fireworks-ai](https://huggingface.co/fireworks-ai)\n- Original model: [fireworks-ai/firefunction-v2](https://huggingface.co/fireworks-ai/firefunction-v2)\n\n## Description\n[MaziyarPanahi/firefunction-v2-GGUF](https://huggingface.co/MaziyarPanahi/firefunction-v2-GGUF) contains GGUF format model files for [fireworks-ai/firefunction-v2](https://huggingface.co/fireworks-ai/firefunction-v2).\n\n### About GGUF\n\nGGUF is a new format introduced by the llama.cpp team on August 21st 2023. It is a replacement for GGML, which is no longer supported by llama.cpp.\n\nHere is an incomplete list of clients and libraries that are known to support GGUF:\n\n* [llama.cpp](https://github.com/ggerganov/llama.cpp). The source project for GGUF. Offers a CLI and a server option.\n* [llama-cpp-python](https://github.com/abetlen/llama-cpp-python), a Python library with GPU accel, LangChain support, and OpenAI-compatible API server.\n* [LM Studio](https://lmstudio.ai/), an easy-to-use and powerful local GUI for Windows and macOS (Silicon), with GPU acceleration. Linux available, in beta as of 27/11/2023.\n* [text-generation-webui](https://github.com/oobabooga/text-generation-webui), the most widely used web UI, with many features and powerful extensions. Supports GPU acceleration.\n* [KoboldCpp](https://github.com/LostRuins/koboldcpp), a fully featured web UI, with GPU accel across all platforms and GPU architectures. Especially good for story telling.\n* [GPT4All](https://gpt4all.io/index.html), a free and open source local running GUI, supporting Windows, Linux and macOS with full GPU accel.\n* [LoLLMS Web UI](https://github.com/ParisNeo/lollms-webui), a great web UI with many interesting and unique features, including a full model library for easy model selection.\n* [Faraday.dev](https://faraday.dev/), an attractive and easy to use character-based chat GUI for Windows and macOS (both Silicon and Intel), with GPU acceleration.\n* [candle](https://github.com/huggingface/candle), a Rust ML framework with a focus on performance, including GPU support, and ease of use.\n* [ctransformers](https://github.com/marella/ctransformers), a Python library with GPU accel, LangChain support, and OpenAI-compatible AI server. Note, as of time of writing (November 27th 2023), ctransformers has not been updated in a long time and does not support many recent models.\n\n## Special thanks\n\n🙏 Special thanks to [Georgi Gerganov](https://github.com/ggerganov) and the whole team working on [llama.cpp](https://github.com/ggerganov/llama.cpp/) for making all of this possible.\n\nOriginal README\n---\n# FireFunction V2: Fireworks Function Calling Model\n\n[**Try on Fireworks**](https://fireworks.ai/models/fireworks/firefunction-v2) | [**API Docs**](https://readme.fireworks.ai/docs/function-calling) | [**Demo App**](https://functional-chat.vercel.app/) | [**Discord**](https://discord.gg/mMqQxvFD9A)\n\n<img src=\"https://cdn-uploads.huggingface.co/production/uploads/64b6f3a72f5a966b9722de88/nJNtxLzWswBDKK1iOZblb.png\" alt=\"firefunction\" width=\"400\"/>\n\nFireFunction is a state-of-the-art function calling model with a commercially viable license. View detailed info in our [announcement blog](https://fireworks.ai/blog/firefunction-v2-launch-post). Key info and highlights:\n\n**Comparison with other models:**\n- Competitive with GPT-4o at function-calling, scoring 0.81 vs 0.80 on a medley of public evaluations\n- Trained on Llama 3 and retains Llama 3’s conversation and instruction-following capabilities, scoring 0.84 vs Llama 3’s 0.89 on MT bench\n- Significant quality improvements over FireFunction v1 across the broad range of metrics\n\n\n**General info:**\n\n🐾 Successor of the [FireFunction](https://fireworks.ai/models/fireworks/firefunction-v1) model\n\n🔆 Support of parallel function calling (unlike FireFunction v1) and good instruction following\n\n💡 Hosted on the [Fireworks](https://fireworks.ai/models/fireworks/firefunction-v2) platform at < 10% of the cost of GPT 4o and 2x the speed\n",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModel",
      "custom_class": null,
      "pipeline_tag": null,
      "processor": null
    },
    "safetensors": null,
    "model_index": null,
    "trending_score": null
  },
  {
    "model_id": "MaziyarPanahi/mathstral-7B-v0.1-GGUF",
    "model_name": "MaziyarPanahi/mathstral-7B-v0.1-GGUF",
    "author": "MaziyarPanahi",
    "downloads": 2322625,
    "downloads_all_time": null,
    "likes": 7,
    "tags": [
      "transformers",
      "gguf",
      "mistral",
      "quantized",
      "2-bit",
      "3-bit",
      "4-bit",
      "5-bit",
      "6-bit",
      "8-bit",
      "GGUF",
      "text-generation",
      "base_model:mistralai/Mathstral-7B-v0.1",
      "base_model:quantized:mistralai/Mathstral-7B-v0.1",
      "region:us"
    ],
    "card_url": "https://huggingface.co/MaziyarPanahi/mathstral-7B-v0.1-GGUF",
    "dependencies": [
      [
        "mistral_inference",
        ">=1.2.0"
      ]
    ],
    "last_modified": "2024-07-16T16:54:49+00:00",
    "created_at": "2024-07-16T15:06:23+00:00",
    "analysis_date": "2025-03-22T00:41:56.000691",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "mistral",
    "pipeline_tag": "text-generation",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "tags": [
        "quantized",
        "2-bit",
        "3-bit",
        "4-bit",
        "5-bit",
        "6-bit",
        "8-bit",
        "GGUF",
        "text-generation"
      ],
      "model_name": "mathstral-7B-v0.1-GGUF",
      "base_model": "mistralai/mathstral-7B-v0.1",
      "inference": false,
      "model_creator": "mistralai",
      "pipeline_tag": "text-generation",
      "quantized_by": "MaziyarPanahi"
    },
    "card_text": "# [MaziyarPanahi/mathstral-7B-v0.1-GGUF](https://huggingface.co/MaziyarPanahi/mathstral-7B-v0.1-GGUF)\n- Model creator: [mistralai](https://huggingface.co/mistralai)\n- Original model: [mistralai/mathstral-7B-v0.1](https://huggingface.co/mistralai/mathstral-7B-v0.1)\n\n## Description\n[MaziyarPanahi/mathstral-7B-v0.1-GGUF](https://huggingface.co/MaziyarPanahi/mathstral-7B-v0.1-GGUF) contains GGUF format model files for [mistralai/mathstral-7B-v0.1](https://huggingface.co/mistralai/mathstral-7B-v0.1).\n\n### About GGUF\n\nGGUF is a new format introduced by the llama.cpp team on August 21st 2023. It is a replacement for GGML, which is no longer supported by llama.cpp.\n\nHere is an incomplete list of clients and libraries that are known to support GGUF:\n\n* [llama.cpp](https://github.com/ggerganov/llama.cpp). The source project for GGUF. Offers a CLI and a server option.\n* [llama-cpp-python](https://github.com/abetlen/llama-cpp-python), a Python library with GPU accel, LangChain support, and OpenAI-compatible API server.\n* [LM Studio](https://lmstudio.ai/), an easy-to-use and powerful local GUI for Windows and macOS (Silicon), with GPU acceleration. Linux available, in beta as of 27/11/2023.\n* [text-generation-webui](https://github.com/oobabooga/text-generation-webui), the most widely used web UI, with many features and powerful extensions. Supports GPU acceleration.\n* [KoboldCpp](https://github.com/LostRuins/koboldcpp), a fully featured web UI, with GPU accel across all platforms and GPU architectures. Especially good for story telling.\n* [GPT4All](https://gpt4all.io/index.html), a free and open source local running GUI, supporting Windows, Linux and macOS with full GPU accel.\n* [LoLLMS Web UI](https://github.com/ParisNeo/lollms-webui), a great web UI with many interesting and unique features, including a full model library for easy model selection.\n* [Faraday.dev](https://faraday.dev/), an attractive and easy to use character-based chat GUI for Windows and macOS (both Silicon and Intel), with GPU acceleration.\n* [candle](https://github.com/huggingface/candle), a Rust ML framework with a focus on performance, including GPU support, and ease of use.\n* [ctransformers](https://github.com/marella/ctransformers), a Python library with GPU accel, LangChain support, and OpenAI-compatible AI server. Note, as of time of writing (November 27th 2023), ctransformers has not been updated in a long time and does not support many recent models.\n\n## Special thanks\n\n🙏 Special thanks to [Georgi Gerganov](https://github.com/ggerganov) and the whole team working on [llama.cpp](https://github.com/ggerganov/llama.cpp/) for making all of this possible.\n\n\n---\n\n**Original README**\n\n\n# Model Card for Mathstral-7B-v0.1\n\nMathstral 7B is a model specializing in mathematical and scientific tasks, based on Mistral 7B.\nYou can read more in the [official blog post](https://mistral.ai/news/mathstral/).\n\n## Installation\n\nIt is recommended to use `mistralai/mathstral-7B-v0.1` with [mistral-inference](https://github.com/mistralai/mistral-inference) \n\n\n```\npip install mistral_inference>=1.2.0\n```\n\n\n## Download\n\n```py\nfrom huggingface_hub import snapshot_download\nfrom pathlib import Path\n\nmistral_models_path = Path.home().joinpath('mistral_models', 'mathstral-7B-v0.1')\nmistral_models_path.mkdir(parents=True, exist_ok=True)\n\nsnapshot_download(repo_id=\"mistralai/mathstral-7B-v0.1\", allow_patterns=[\"params.json\", \"consolidated.safetensors\", \"tokenizer.model.v3\"], local_dir=mistral_models_path)\n```\n\n### Chat\n\nAfter installing `mistral_inference`, a `mistral-demo` CLI command should be available in your environment.\n\n```\nmistral-chat $HOME/mistral_models/mathstral-7B-v0.1 --instruct --max_tokens 256\n```\n\nYou can then start chatting with the model, *e.g.* prompt it with something like:\n\n\n*\"Albert likes to surf every week. Each surfing session lasts for 4 hours and costs $20 per hour. How much would Albert spend in 5 weeks?\"*\n\n## Evaluation\nWe evaluate Mathstral 7B and open-weight models of the similar size on industry-standard benchmarks.\n| Benchmarks | MATH | GSM8K (8-shot) | Odyssey Math maj@16 | GRE Math maj@16 | AMC 2023 maj@16 | AIME 2024 maj@16\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: |\n| Mathstral 7B | **56.6** | 77.1 | **37.2** | 56.9 | **42.4** | **2/30** |\n| DeepSeek Math 7B | 44.4 | **80.6** | 27.6 | 44.6 | 28.0 | 0/30 |\n| Llama3 8B | 28.4 | 75.4 | 24.0 | 26.2 | 34.4 | 0/30 |\n| GLM4 9B | 50.2 | 48.8 | 18.9 | 46.2 | 36.0 | 1/30 |\n| QWen2 7B | **56.8** | 32.7 | 24.8 | **58.5** | 35.2 | **2/30** |\n| Gemma2 9B | 48.3 | 69.5 | 18.6 | 52.3 | 31.2 | 1/30 |\n\n\n## The Mistral AI Team\n\nAlbert Jiang, Alexandre Sablayrolles, Alexis Tacnet, Alok Kothari, Antoine Roux, Arthur Mensch, Audrey Herblin-Stoop, Augustin Garreau, Austin Birky, Bam4d, Baptiste Bout, Baudouin de Monicault, Blanche Savary, Carole Rambaud, Caroline Feldman, Devendra Singh Chaplot, Diego de las Casas, Eleonore Arcelin, Emma Bou Hanna, Etienne Metzger, Gaspard Blanchet, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Harizo Rajaona, Henri Roussez, Hichem Sattouf, Ian Mack, Jean-Malo Delignon, Jessica Chudnovsky, Justus Murke, Kartik Khandelwal, Lawrence Stewart, Louis Martin, Louis Ternon, Lucile Saulnier, Lélio Renard Lavaud, Margaret Jennings, Marie Pellat, Marie Torelli, Marie-Anne Lachaux, Marjorie Janiewicz, Mickaël Seznec, Nicolas Schuhl, Niklas Muhs, Olivier de Garrigues, Patrick von Platen, Paul Jacob, Pauline Buche, Pavan Kumar Reddy, Perry Savas, Pierre Stock, Romain Sauvestre, Sagar Vaze, Sandeep Subramanian, Saurabh Garg, Sophia Yang, Szymon Antoniak, Teven Le Scao, Thibault Schueller, Thibaut Lavril, Thomas Wang, Théophile Gervet, Timothée Lacroix, Valera Nemychnikova, Wendy Shang, William El Sayed, William Marshall",
    "card_content": "---\ntags:\n- quantized\n- 2-bit\n- 3-bit\n- 4-bit\n- 5-bit\n- 6-bit\n- 8-bit\n- GGUF\n- text-generation\nmodel_name: mathstral-7B-v0.1-GGUF\nbase_model: mistralai/mathstral-7B-v0.1\ninference: false\nmodel_creator: mistralai\npipeline_tag: text-generation\nquantized_by: MaziyarPanahi\n---\n# [MaziyarPanahi/mathstral-7B-v0.1-GGUF](https://huggingface.co/MaziyarPanahi/mathstral-7B-v0.1-GGUF)\n- Model creator: [mistralai](https://huggingface.co/mistralai)\n- Original model: [mistralai/mathstral-7B-v0.1](https://huggingface.co/mistralai/mathstral-7B-v0.1)\n\n## Description\n[MaziyarPanahi/mathstral-7B-v0.1-GGUF](https://huggingface.co/MaziyarPanahi/mathstral-7B-v0.1-GGUF) contains GGUF format model files for [mistralai/mathstral-7B-v0.1](https://huggingface.co/mistralai/mathstral-7B-v0.1).\n\n### About GGUF\n\nGGUF is a new format introduced by the llama.cpp team on August 21st 2023. It is a replacement for GGML, which is no longer supported by llama.cpp.\n\nHere is an incomplete list of clients and libraries that are known to support GGUF:\n\n* [llama.cpp](https://github.com/ggerganov/llama.cpp). The source project for GGUF. Offers a CLI and a server option.\n* [llama-cpp-python](https://github.com/abetlen/llama-cpp-python), a Python library with GPU accel, LangChain support, and OpenAI-compatible API server.\n* [LM Studio](https://lmstudio.ai/), an easy-to-use and powerful local GUI for Windows and macOS (Silicon), with GPU acceleration. Linux available, in beta as of 27/11/2023.\n* [text-generation-webui](https://github.com/oobabooga/text-generation-webui), the most widely used web UI, with many features and powerful extensions. Supports GPU acceleration.\n* [KoboldCpp](https://github.com/LostRuins/koboldcpp), a fully featured web UI, with GPU accel across all platforms and GPU architectures. Especially good for story telling.\n* [GPT4All](https://gpt4all.io/index.html), a free and open source local running GUI, supporting Windows, Linux and macOS with full GPU accel.\n* [LoLLMS Web UI](https://github.com/ParisNeo/lollms-webui), a great web UI with many interesting and unique features, including a full model library for easy model selection.\n* [Faraday.dev](https://faraday.dev/), an attractive and easy to use character-based chat GUI for Windows and macOS (both Silicon and Intel), with GPU acceleration.\n* [candle](https://github.com/huggingface/candle), a Rust ML framework with a focus on performance, including GPU support, and ease of use.\n* [ctransformers](https://github.com/marella/ctransformers), a Python library with GPU accel, LangChain support, and OpenAI-compatible AI server. Note, as of time of writing (November 27th 2023), ctransformers has not been updated in a long time and does not support many recent models.\n\n## Special thanks\n\n🙏 Special thanks to [Georgi Gerganov](https://github.com/ggerganov) and the whole team working on [llama.cpp](https://github.com/ggerganov/llama.cpp/) for making all of this possible.\n\n\n---\n\n**Original README**\n\n\n# Model Card for Mathstral-7B-v0.1\n\nMathstral 7B is a model specializing in mathematical and scientific tasks, based on Mistral 7B.\nYou can read more in the [official blog post](https://mistral.ai/news/mathstral/).\n\n## Installation\n\nIt is recommended to use `mistralai/mathstral-7B-v0.1` with [mistral-inference](https://github.com/mistralai/mistral-inference) \n\n\n```\npip install mistral_inference>=1.2.0\n```\n\n\n## Download\n\n```py\nfrom huggingface_hub import snapshot_download\nfrom pathlib import Path\n\nmistral_models_path = Path.home().joinpath('mistral_models', 'mathstral-7B-v0.1')\nmistral_models_path.mkdir(parents=True, exist_ok=True)\n\nsnapshot_download(repo_id=\"mistralai/mathstral-7B-v0.1\", allow_patterns=[\"params.json\", \"consolidated.safetensors\", \"tokenizer.model.v3\"], local_dir=mistral_models_path)\n```\n\n### Chat\n\nAfter installing `mistral_inference`, a `mistral-demo` CLI command should be available in your environment.\n\n```\nmistral-chat $HOME/mistral_models/mathstral-7B-v0.1 --instruct --max_tokens 256\n```\n\nYou can then start chatting with the model, *e.g.* prompt it with something like:\n\n\n*\"Albert likes to surf every week. Each surfing session lasts for 4 hours and costs $20 per hour. How much would Albert spend in 5 weeks?\"*\n\n## Evaluation\nWe evaluate Mathstral 7B and open-weight models of the similar size on industry-standard benchmarks.\n| Benchmarks | MATH | GSM8K (8-shot) | Odyssey Math maj@16 | GRE Math maj@16 | AMC 2023 maj@16 | AIME 2024 maj@16\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: |\n| Mathstral 7B | **56.6** | 77.1 | **37.2** | 56.9 | **42.4** | **2/30** |\n| DeepSeek Math 7B | 44.4 | **80.6** | 27.6 | 44.6 | 28.0 | 0/30 |\n| Llama3 8B | 28.4 | 75.4 | 24.0 | 26.2 | 34.4 | 0/30 |\n| GLM4 9B | 50.2 | 48.8 | 18.9 | 46.2 | 36.0 | 1/30 |\n| QWen2 7B | **56.8** | 32.7 | 24.8 | **58.5** | 35.2 | **2/30** |\n| Gemma2 9B | 48.3 | 69.5 | 18.6 | 52.3 | 31.2 | 1/30 |\n\n\n## The Mistral AI Team\n\nAlbert Jiang, Alexandre Sablayrolles, Alexis Tacnet, Alok Kothari, Antoine Roux, Arthur Mensch, Audrey Herblin-Stoop, Augustin Garreau, Austin Birky, Bam4d, Baptiste Bout, Baudouin de Monicault, Blanche Savary, Carole Rambaud, Caroline Feldman, Devendra Singh Chaplot, Diego de las Casas, Eleonore Arcelin, Emma Bou Hanna, Etienne Metzger, Gaspard Blanchet, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Harizo Rajaona, Henri Roussez, Hichem Sattouf, Ian Mack, Jean-Malo Delignon, Jessica Chudnovsky, Justus Murke, Kartik Khandelwal, Lawrence Stewart, Louis Martin, Louis Ternon, Lucile Saulnier, Lélio Renard Lavaud, Margaret Jennings, Marie Pellat, Marie Torelli, Marie-Anne Lachaux, Marjorie Janiewicz, Mickaël Seznec, Nicolas Schuhl, Niklas Muhs, Olivier de Garrigues, Patrick von Platen, Paul Jacob, Pauline Buche, Pavan Kumar Reddy, Perry Savas, Pierre Stock, Romain Sauvestre, Sagar Vaze, Sandeep Subramanian, Saurabh Garg, Sophia Yang, Szymon Antoniak, Teven Le Scao, Thibault Schueller, Thibaut Lavril, Thomas Wang, Théophile Gervet, Timothée Lacroix, Valera Nemychnikova, Wendy Shang, William El Sayed, William Marshall",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModel",
      "custom_class": null,
      "pipeline_tag": null,
      "processor": null
    },
    "safetensors": null,
    "model_index": null,
    "trending_score": null
  },
  {
    "model_id": "MaziyarPanahi/Llama-3-8B-Instruct-64k-GGUF",
    "model_name": "MaziyarPanahi/Llama-3-8B-Instruct-64k-GGUF",
    "author": "MaziyarPanahi",
    "downloads": 2321443,
    "downloads_all_time": null,
    "likes": 13,
    "tags": [
      "transformers",
      "gguf",
      "mistral",
      "quantized",
      "2-bit",
      "3-bit",
      "4-bit",
      "5-bit",
      "6-bit",
      "8-bit",
      "GGUF",
      "text-generation",
      "llama",
      "llama-3",
      "base_model:MaziyarPanahi/Llama-3-8B-Instruct-64k",
      "base_model:quantized:MaziyarPanahi/Llama-3-8B-Instruct-64k",
      "region:us",
      "conversational"
    ],
    "card_url": "https://huggingface.co/MaziyarPanahi/Llama-3-8B-Instruct-64k-GGUF",
    "dependencies": [
      [
        "llama.cpp",
        null
      ],
      [
        "llama-cpp-python",
        null
      ],
      [
        "ctransformers",
        null
      ]
    ],
    "last_modified": "2024-04-25T19:58:11+00:00",
    "created_at": "2024-04-25T19:22:27+00:00",
    "analysis_date": "2025-03-22T00:41:57.321679",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "mistral",
    "pipeline_tag": "text-generation",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "tags": [
        "quantized",
        "2-bit",
        "3-bit",
        "4-bit",
        "5-bit",
        "6-bit",
        "8-bit",
        "GGUF",
        "text-generation",
        "llama",
        "llama-3"
      ],
      "model_name": "Llama-3-8B-Instruct-64k-GGUF",
      "base_model": "MaziyarPanahi/Llama-3-8B-Instruct-64k",
      "inference": false,
      "model_creator": "MaziyarPanahi",
      "pipeline_tag": "text-generation",
      "quantized_by": "MaziyarPanahi"
    },
    "card_text": "# [MaziyarPanahi/Llama-3-8B-Instruct-64k-GGUF](https://huggingface.co/MaziyarPanahi/Llama-3-8B-Instruct-64k-GGUF)\n- Model creator: [MaziyarPanahi](https://huggingface.co/MaziyarPanahi)\n- Original model: [MaziyarPanahi/Llama-3-8B-Instruct-64k](https://huggingface.co/MaziyarPanahi/Llama-3-8B-Instruct-64k)\n\n## Description\n[MaziyarPanahi/Llama-3-8B-Instruct-64k-GGUF](https://huggingface.co/MaziyarPanahi/Llama-3-8B-Instruct-64k-GGUF) contains GGUF format model files for [MaziyarPanahi/Llama-3-8B-Instruct-64k](https://huggingface.co/MaziyarPanahi/Llama-3-8B-Instruct-64k).\n\n### About GGUF\n\nGGUF is a new format introduced by the llama.cpp team on August 21st 2023. It is a replacement for GGML, which is no longer supported by llama.cpp.\n\nHere is an incomplete list of clients and libraries that are known to support GGUF:\n\n* [llama.cpp](https://github.com/ggerganov/llama.cpp). The source project for GGUF. Offers a CLI and a server option.\n* [llama-cpp-python](https://github.com/abetlen/llama-cpp-python), a Python library with GPU accel, LangChain support, and OpenAI-compatible API server.\n* [LM Studio](https://lmstudio.ai/), an easy-to-use and powerful local GUI for Windows and macOS (Silicon), with GPU acceleration. Linux available, in beta as of 27/11/2023.\n* [text-generation-webui](https://github.com/oobabooga/text-generation-webui), the most widely used web UI, with many features and powerful extensions. Supports GPU acceleration.\n* [KoboldCpp](https://github.com/LostRuins/koboldcpp), a fully featured web UI, with GPU accel across all platforms and GPU architectures. Especially good for story telling.\n* [GPT4All](https://gpt4all.io/index.html), a free and open source local running GUI, supporting Windows, Linux and macOS with full GPU accel.\n* [LoLLMS Web UI](https://github.com/ParisNeo/lollms-webui), a great web UI with many interesting and unique features, including a full model library for easy model selection.\n* [Faraday.dev](https://faraday.dev/), an attractive and easy to use character-based chat GUI for Windows and macOS (both Silicon and Intel), with GPU acceleration.\n* [candle](https://github.com/huggingface/candle), a Rust ML framework with a focus on performance, including GPU support, and ease of use.\n* [ctransformers](https://github.com/marella/ctransformers), a Python library with GPU accel, LangChain support, and OpenAI-compatible AI server. Note, as of time of writing (November 27th 2023), ctransformers has not been updated in a long time and does not support many recent models.\n\n## Special thanks\n\n🙏 Special thanks to [Georgi Gerganov](https://github.com/ggerganov) and the whole team working on [llama.cpp](https://github.com/ggerganov/llama.cpp/) for making all of this possible.",
    "card_content": "---\ntags:\n- quantized\n- 2-bit\n- 3-bit\n- 4-bit\n- 5-bit\n- 6-bit\n- 8-bit\n- GGUF\n- text-generation\n- llama\n- llama-3\nmodel_name: Llama-3-8B-Instruct-64k-GGUF\nbase_model: MaziyarPanahi/Llama-3-8B-Instruct-64k\ninference: false\nmodel_creator: MaziyarPanahi\npipeline_tag: text-generation\nquantized_by: MaziyarPanahi\n---\n# [MaziyarPanahi/Llama-3-8B-Instruct-64k-GGUF](https://huggingface.co/MaziyarPanahi/Llama-3-8B-Instruct-64k-GGUF)\n- Model creator: [MaziyarPanahi](https://huggingface.co/MaziyarPanahi)\n- Original model: [MaziyarPanahi/Llama-3-8B-Instruct-64k](https://huggingface.co/MaziyarPanahi/Llama-3-8B-Instruct-64k)\n\n## Description\n[MaziyarPanahi/Llama-3-8B-Instruct-64k-GGUF](https://huggingface.co/MaziyarPanahi/Llama-3-8B-Instruct-64k-GGUF) contains GGUF format model files for [MaziyarPanahi/Llama-3-8B-Instruct-64k](https://huggingface.co/MaziyarPanahi/Llama-3-8B-Instruct-64k).\n\n### About GGUF\n\nGGUF is a new format introduced by the llama.cpp team on August 21st 2023. It is a replacement for GGML, which is no longer supported by llama.cpp.\n\nHere is an incomplete list of clients and libraries that are known to support GGUF:\n\n* [llama.cpp](https://github.com/ggerganov/llama.cpp). The source project for GGUF. Offers a CLI and a server option.\n* [llama-cpp-python](https://github.com/abetlen/llama-cpp-python), a Python library with GPU accel, LangChain support, and OpenAI-compatible API server.\n* [LM Studio](https://lmstudio.ai/), an easy-to-use and powerful local GUI for Windows and macOS (Silicon), with GPU acceleration. Linux available, in beta as of 27/11/2023.\n* [text-generation-webui](https://github.com/oobabooga/text-generation-webui), the most widely used web UI, with many features and powerful extensions. Supports GPU acceleration.\n* [KoboldCpp](https://github.com/LostRuins/koboldcpp), a fully featured web UI, with GPU accel across all platforms and GPU architectures. Especially good for story telling.\n* [GPT4All](https://gpt4all.io/index.html), a free and open source local running GUI, supporting Windows, Linux and macOS with full GPU accel.\n* [LoLLMS Web UI](https://github.com/ParisNeo/lollms-webui), a great web UI with many interesting and unique features, including a full model library for easy model selection.\n* [Faraday.dev](https://faraday.dev/), an attractive and easy to use character-based chat GUI for Windows and macOS (both Silicon and Intel), with GPU acceleration.\n* [candle](https://github.com/huggingface/candle), a Rust ML framework with a focus on performance, including GPU support, and ease of use.\n* [ctransformers](https://github.com/marella/ctransformers), a Python library with GPU accel, LangChain support, and OpenAI-compatible AI server. Note, as of time of writing (November 27th 2023), ctransformers has not been updated in a long time and does not support many recent models.\n\n## Special thanks\n\n🙏 Special thanks to [Georgi Gerganov](https://github.com/ggerganov) and the whole team working on [llama.cpp](https://github.com/ggerganov/llama.cpp/) for making all of this possible.",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModel",
      "custom_class": null,
      "pipeline_tag": null,
      "processor": null
    },
    "safetensors": null,
    "model_index": null,
    "trending_score": null
  },
  {
    "model_id": "MaziyarPanahi/Yi-1.5-6B-Chat-GGUF",
    "model_name": "MaziyarPanahi/Yi-1.5-6B-Chat-GGUF",
    "author": "MaziyarPanahi",
    "downloads": 2319850,
    "downloads_all_time": null,
    "likes": 9,
    "tags": [
      "transformers",
      "gguf",
      "mistral",
      "quantized",
      "2-bit",
      "3-bit",
      "4-bit",
      "5-bit",
      "6-bit",
      "8-bit",
      "GGUF",
      "safetensors",
      "llama",
      "text-generation",
      "conversational",
      "arxiv:2403.04652",
      "license:apache-2.0",
      "autotrain_compatible",
      "endpoints_compatible",
      "text-generation-inference",
      "region:us",
      "base_model:01-ai/Yi-1.5-6B-Chat",
      "base_model:quantized:01-ai/Yi-1.5-6B-Chat"
    ],
    "card_url": "https://huggingface.co/MaziyarPanahi/Yi-1.5-6B-Chat-GGUF",
    "dependencies": [
      [
        "transformers",
        null
      ],
      [
        "safetensors",
        null
      ]
    ],
    "last_modified": "2024-05-12T20:34:51+00:00",
    "created_at": "2024-05-12T20:19:22+00:00",
    "analysis_date": "2025-03-22T00:41:58.623490",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "mistral",
    "pipeline_tag": "text-generation",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "tags": [
        "quantized",
        "2-bit",
        "3-bit",
        "4-bit",
        "5-bit",
        "6-bit",
        "8-bit",
        "GGUF",
        "transformers",
        "safetensors",
        "llama",
        "text-generation",
        "conversational",
        "arxiv:2403.04652",
        "license:apache-2.0",
        "autotrain_compatible",
        "endpoints_compatible",
        "text-generation-inference",
        "region:us"
      ],
      "model_name": "Yi-1.5-6B-Chat-GGUF",
      "base_model": "01-ai/Yi-1.5-6B-Chat",
      "inference": false,
      "model_creator": "01-ai",
      "pipeline_tag": "text-generation",
      "quantized_by": "MaziyarPanahi"
    },
    "card_text": "# [MaziyarPanahi/Yi-1.5-6B-Chat-GGUF](https://huggingface.co/MaziyarPanahi/Yi-1.5-6B-Chat-GGUF)\n- Model creator: [01-ai](https://huggingface.co/01-ai)\n- Original model: [01-ai/Yi-1.5-6B-Chat](https://huggingface.co/01-ai/Yi-1.5-6B-Chat)\n\n## Description\n[MaziyarPanahi/Yi-1.5-6B-Chat-GGUF](https://huggingface.co/MaziyarPanahi/Yi-1.5-6B-Chat-GGUF) contains GGUF format model files for [01-ai/Yi-1.5-6B-Chat](https://huggingface.co/01-ai/Yi-1.5-6B-Chat).\n\n### About GGUF\n\nGGUF is a new format introduced by the llama.cpp team on August 21st 2023. It is a replacement for GGML, which is no longer supported by llama.cpp.\n\nHere is an incomplete list of clients and libraries that are known to support GGUF:\n\n* [llama.cpp](https://github.com/ggerganov/llama.cpp). The source project for GGUF. Offers a CLI and a server option.\n* [llama-cpp-python](https://github.com/abetlen/llama-cpp-python), a Python library with GPU accel, LangChain support, and OpenAI-compatible API server.\n* [LM Studio](https://lmstudio.ai/), an easy-to-use and powerful local GUI for Windows and macOS (Silicon), with GPU acceleration. Linux available, in beta as of 27/11/2023.\n* [text-generation-webui](https://github.com/oobabooga/text-generation-webui), the most widely used web UI, with many features and powerful extensions. Supports GPU acceleration.\n* [KoboldCpp](https://github.com/LostRuins/koboldcpp), a fully featured web UI, with GPU accel across all platforms and GPU architectures. Especially good for story telling.\n* [GPT4All](https://gpt4all.io/index.html), a free and open source local running GUI, supporting Windows, Linux and macOS with full GPU accel.\n* [LoLLMS Web UI](https://github.com/ParisNeo/lollms-webui), a great web UI with many interesting and unique features, including a full model library for easy model selection.\n* [Faraday.dev](https://faraday.dev/), an attractive and easy to use character-based chat GUI for Windows and macOS (both Silicon and Intel), with GPU acceleration.\n* [candle](https://github.com/huggingface/candle), a Rust ML framework with a focus on performance, including GPU support, and ease of use.\n* [ctransformers](https://github.com/marella/ctransformers), a Python library with GPU accel, LangChain support, and OpenAI-compatible AI server. Note, as of time of writing (November 27th 2023), ctransformers has not been updated in a long time and does not support many recent models.\n\n## Special thanks\n\n🙏 Special thanks to [Georgi Gerganov](https://github.com/ggerganov) and the whole team working on [llama.cpp](https://github.com/ggerganov/llama.cpp/) for making all of this possible.",
    "card_content": "---\ntags:\n- quantized\n- 2-bit\n- 3-bit\n- 4-bit\n- 5-bit\n- 6-bit\n- 8-bit\n- GGUF\n- transformers\n- safetensors\n- llama\n- text-generation\n- conversational\n- arxiv:2403.04652\n- license:apache-2.0\n- autotrain_compatible\n- endpoints_compatible\n- text-generation-inference\n- region:us\nmodel_name: Yi-1.5-6B-Chat-GGUF\nbase_model: 01-ai/Yi-1.5-6B-Chat\ninference: false\nmodel_creator: 01-ai\npipeline_tag: text-generation\nquantized_by: MaziyarPanahi\n---\n# [MaziyarPanahi/Yi-1.5-6B-Chat-GGUF](https://huggingface.co/MaziyarPanahi/Yi-1.5-6B-Chat-GGUF)\n- Model creator: [01-ai](https://huggingface.co/01-ai)\n- Original model: [01-ai/Yi-1.5-6B-Chat](https://huggingface.co/01-ai/Yi-1.5-6B-Chat)\n\n## Description\n[MaziyarPanahi/Yi-1.5-6B-Chat-GGUF](https://huggingface.co/MaziyarPanahi/Yi-1.5-6B-Chat-GGUF) contains GGUF format model files for [01-ai/Yi-1.5-6B-Chat](https://huggingface.co/01-ai/Yi-1.5-6B-Chat).\n\n### About GGUF\n\nGGUF is a new format introduced by the llama.cpp team on August 21st 2023. It is a replacement for GGML, which is no longer supported by llama.cpp.\n\nHere is an incomplete list of clients and libraries that are known to support GGUF:\n\n* [llama.cpp](https://github.com/ggerganov/llama.cpp). The source project for GGUF. Offers a CLI and a server option.\n* [llama-cpp-python](https://github.com/abetlen/llama-cpp-python), a Python library with GPU accel, LangChain support, and OpenAI-compatible API server.\n* [LM Studio](https://lmstudio.ai/), an easy-to-use and powerful local GUI for Windows and macOS (Silicon), with GPU acceleration. Linux available, in beta as of 27/11/2023.\n* [text-generation-webui](https://github.com/oobabooga/text-generation-webui), the most widely used web UI, with many features and powerful extensions. Supports GPU acceleration.\n* [KoboldCpp](https://github.com/LostRuins/koboldcpp), a fully featured web UI, with GPU accel across all platforms and GPU architectures. Especially good for story telling.\n* [GPT4All](https://gpt4all.io/index.html), a free and open source local running GUI, supporting Windows, Linux and macOS with full GPU accel.\n* [LoLLMS Web UI](https://github.com/ParisNeo/lollms-webui), a great web UI with many interesting and unique features, including a full model library for easy model selection.\n* [Faraday.dev](https://faraday.dev/), an attractive and easy to use character-based chat GUI for Windows and macOS (both Silicon and Intel), with GPU acceleration.\n* [candle](https://github.com/huggingface/candle), a Rust ML framework with a focus on performance, including GPU support, and ease of use.\n* [ctransformers](https://github.com/marella/ctransformers), a Python library with GPU accel, LangChain support, and OpenAI-compatible AI server. Note, as of time of writing (November 27th 2023), ctransformers has not been updated in a long time and does not support many recent models.\n\n## Special thanks\n\n🙏 Special thanks to [Georgi Gerganov](https://github.com/ggerganov) and the whole team working on [llama.cpp](https://github.com/ggerganov/llama.cpp/) for making all of this possible.",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModel",
      "custom_class": null,
      "pipeline_tag": null,
      "processor": null
    },
    "safetensors": null,
    "model_index": null,
    "trending_score": null
  },
  {
    "model_id": "trl-internal-testing/tiny-random-LlamaForCausalLM",
    "model_name": "trl-internal-testing/tiny-random-LlamaForCausalLM",
    "author": "trl-internal-testing",
    "downloads": 1836576,
    "downloads_all_time": null,
    "likes": 6,
    "tags": [
      "transformers",
      "pytorch",
      "safetensors",
      "llama",
      "text-generation",
      "arxiv:1910.09700",
      "autotrain_compatible",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/trl-internal-testing/tiny-random-LlamaForCausalLM",
    "dependencies": [
      [
        "transformers",
        null
      ]
    ],
    "last_modified": "2024-04-23T12:12:07+00:00",
    "created_at": "2023-03-29T07:11:13+00:00",
    "analysis_date": "2025-03-22T00:41:59.762214",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "llama",
    "pipeline_tag": "text-generation",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "library_name": "transformers",
      "tags": []
    },
    "card_text": "\n# Model Card for Model ID\n\n<!-- Provide a quick summary of what the model is/does. -->\n\n\n\n## Model Details\n\n### Model Description\n\n<!-- Provide a longer summary of what this model is. -->\n\nThis is the model card of a 🤗 transformers model that has been pushed on the Hub. This model card has been automatically generated.\n\n- **Developed by:** [More Information Needed]\n- **Funded by [optional]:** [More Information Needed]\n- **Shared by [optional]:** [More Information Needed]\n- **Model type:** [More Information Needed]\n- **Language(s) (NLP):** [More Information Needed]\n- **License:** [More Information Needed]\n- **Finetuned from model [optional]:** [More Information Needed]\n\n### Model Sources [optional]\n\n<!-- Provide the basic links for the model. -->\n\n- **Repository:** [More Information Needed]\n- **Paper [optional]:** [More Information Needed]\n- **Demo [optional]:** [More Information Needed]\n\n## Uses\n\n<!-- Address questions around how the model is intended to be used, including the foreseeable users of the model and those affected by the model. -->\n\n### Direct Use\n\n<!-- This section is for the model use without fine-tuning or plugging into a larger ecosystem/app. -->\n\n[More Information Needed]\n\n### Downstream Use [optional]\n\n<!-- This section is for the model use when fine-tuned for a task, or when plugged into a larger ecosystem/app -->\n\n[More Information Needed]\n\n### Out-of-Scope Use\n\n<!-- This section addresses misuse, malicious use, and uses that the model will not work well for. -->\n\n[More Information Needed]\n\n## Bias, Risks, and Limitations\n\n<!-- This section is meant to convey both technical and sociotechnical limitations. -->\n\n[More Information Needed]\n\n### Recommendations\n\n<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->\n\nUsers (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.\n\n## How to Get Started with the Model\n\nUse the code below to get started with the model.\n\n[More Information Needed]\n\n## Training Details\n\n### Training Data\n\n<!-- This should link to a Dataset Card, perhaps with a short stub of information on what the training data is all about as well as documentation related to data pre-processing or additional filtering. -->\n\n[More Information Needed]\n\n### Training Procedure\n\n<!-- This relates heavily to the Technical Specifications. Content here should link to that section when it is relevant to the training procedure. -->\n\n#### Preprocessing [optional]\n\n[More Information Needed]\n\n\n#### Training Hyperparameters\n\n- **Training regime:** [More Information Needed] <!--fp32, fp16 mixed precision, bf16 mixed precision, bf16 non-mixed precision, fp16 non-mixed precision, fp8 mixed precision -->\n\n#### Speeds, Sizes, Times [optional]\n\n<!-- This section provides information about throughput, start/end time, checkpoint size if relevant, etc. -->\n\n[More Information Needed]\n\n## Evaluation\n\n<!-- This section describes the evaluation protocols and provides the results. -->\n\n### Testing Data, Factors & Metrics\n\n#### Testing Data\n\n<!-- This should link to a Dataset Card if possible. -->\n\n[More Information Needed]\n\n#### Factors\n\n<!-- These are the things the evaluation is disaggregating by, e.g., subpopulations or domains. -->\n\n[More Information Needed]\n\n#### Metrics\n\n<!-- These are the evaluation metrics being used, ideally with a description of why. -->\n\n[More Information Needed]\n\n### Results\n\n[More Information Needed]\n\n#### Summary\n\n\n\n## Model Examination [optional]\n\n<!-- Relevant interpretability work for the model goes here -->\n\n[More Information Needed]\n\n## Environmental Impact\n\n<!-- Total emissions (in grams of CO2eq) and additional considerations, such as electricity usage, go here. Edit the suggested text below accordingly -->\n\nCarbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).\n\n- **Hardware Type:** [More Information Needed]\n- **Hours used:** [More Information Needed]\n- **Cloud Provider:** [More Information Needed]\n- **Compute Region:** [More Information Needed]\n- **Carbon Emitted:** [More Information Needed]\n\n## Technical Specifications [optional]\n\n### Model Architecture and Objective\n\n[More Information Needed]\n\n### Compute Infrastructure\n\n[More Information Needed]\n\n#### Hardware\n\n[More Information Needed]\n\n#### Software\n\n[More Information Needed]\n\n## Citation [optional]\n\n<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->\n\n**BibTeX:**\n\n[More Information Needed]\n\n**APA:**\n\n[More Information Needed]\n\n## Glossary [optional]\n\n<!-- If relevant, include terms and calculations in this section that can help readers understand the model or model card. -->\n\n[More Information Needed]\n\n## More Information [optional]\n\n[More Information Needed]\n\n## Model Card Authors [optional]\n\n[More Information Needed]\n\n## Model Card Contact\n\n[More Information Needed]",
    "card_content": "---\nlibrary_name: transformers\ntags: []\n---\n\n# Model Card for Model ID\n\n<!-- Provide a quick summary of what the model is/does. -->\n\n\n\n## Model Details\n\n### Model Description\n\n<!-- Provide a longer summary of what this model is. -->\n\nThis is the model card of a 🤗 transformers model that has been pushed on the Hub. This model card has been automatically generated.\n\n- **Developed by:** [More Information Needed]\n- **Funded by [optional]:** [More Information Needed]\n- **Shared by [optional]:** [More Information Needed]\n- **Model type:** [More Information Needed]\n- **Language(s) (NLP):** [More Information Needed]\n- **License:** [More Information Needed]\n- **Finetuned from model [optional]:** [More Information Needed]\n\n### Model Sources [optional]\n\n<!-- Provide the basic links for the model. -->\n\n- **Repository:** [More Information Needed]\n- **Paper [optional]:** [More Information Needed]\n- **Demo [optional]:** [More Information Needed]\n\n## Uses\n\n<!-- Address questions around how the model is intended to be used, including the foreseeable users of the model and those affected by the model. -->\n\n### Direct Use\n\n<!-- This section is for the model use without fine-tuning or plugging into a larger ecosystem/app. -->\n\n[More Information Needed]\n\n### Downstream Use [optional]\n\n<!-- This section is for the model use when fine-tuned for a task, or when plugged into a larger ecosystem/app -->\n\n[More Information Needed]\n\n### Out-of-Scope Use\n\n<!-- This section addresses misuse, malicious use, and uses that the model will not work well for. -->\n\n[More Information Needed]\n\n## Bias, Risks, and Limitations\n\n<!-- This section is meant to convey both technical and sociotechnical limitations. -->\n\n[More Information Needed]\n\n### Recommendations\n\n<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->\n\nUsers (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.\n\n## How to Get Started with the Model\n\nUse the code below to get started with the model.\n\n[More Information Needed]\n\n## Training Details\n\n### Training Data\n\n<!-- This should link to a Dataset Card, perhaps with a short stub of information on what the training data is all about as well as documentation related to data pre-processing or additional filtering. -->\n\n[More Information Needed]\n\n### Training Procedure\n\n<!-- This relates heavily to the Technical Specifications. Content here should link to that section when it is relevant to the training procedure. -->\n\n#### Preprocessing [optional]\n\n[More Information Needed]\n\n\n#### Training Hyperparameters\n\n- **Training regime:** [More Information Needed] <!--fp32, fp16 mixed precision, bf16 mixed precision, bf16 non-mixed precision, fp16 non-mixed precision, fp8 mixed precision -->\n\n#### Speeds, Sizes, Times [optional]\n\n<!-- This section provides information about throughput, start/end time, checkpoint size if relevant, etc. -->\n\n[More Information Needed]\n\n## Evaluation\n\n<!-- This section describes the evaluation protocols and provides the results. -->\n\n### Testing Data, Factors & Metrics\n\n#### Testing Data\n\n<!-- This should link to a Dataset Card if possible. -->\n\n[More Information Needed]\n\n#### Factors\n\n<!-- These are the things the evaluation is disaggregating by, e.g., subpopulations or domains. -->\n\n[More Information Needed]\n\n#### Metrics\n\n<!-- These are the evaluation metrics being used, ideally with a description of why. -->\n\n[More Information Needed]\n\n### Results\n\n[More Information Needed]\n\n#### Summary\n\n\n\n## Model Examination [optional]\n\n<!-- Relevant interpretability work for the model goes here -->\n\n[More Information Needed]\n\n## Environmental Impact\n\n<!-- Total emissions (in grams of CO2eq) and additional considerations, such as electricity usage, go here. Edit the suggested text below accordingly -->\n\nCarbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).\n\n- **Hardware Type:** [More Information Needed]\n- **Hours used:** [More Information Needed]\n- **Cloud Provider:** [More Information Needed]\n- **Compute Region:** [More Information Needed]\n- **Carbon Emitted:** [More Information Needed]\n\n## Technical Specifications [optional]\n\n### Model Architecture and Objective\n\n[More Information Needed]\n\n### Compute Infrastructure\n\n[More Information Needed]\n\n#### Hardware\n\n[More Information Needed]\n\n#### Software\n\n[More Information Needed]\n\n## Citation [optional]\n\n<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->\n\n**BibTeX:**\n\n[More Information Needed]\n\n**APA:**\n\n[More Information Needed]\n\n## Glossary [optional]\n\n<!-- If relevant, include terms and calculations in this section that can help readers understand the model or model card. -->\n\n[More Information Needed]\n\n## More Information [optional]\n\n[More Information Needed]\n\n## Model Card Authors [optional]\n\n[More Information Needed]\n\n## Model Card Contact\n\n[More Information Needed]",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModelForCausalLM",
      "custom_class": null,
      "pipeline_tag": "text-generation",
      "processor": "AutoTokenizer"
    },
    "safetensors": {
      "parameters": {
        "F32": 1032276
      },
      "total": 1032276
    },
    "model_index": null,
    "trending_score": null
  },
  {
    "model_id": "MaziyarPanahi/Qwen2-7B-Instruct-GGUF",
    "model_name": "MaziyarPanahi/Qwen2-7B-Instruct-GGUF",
    "author": "MaziyarPanahi",
    "downloads": 1753183,
    "downloads_all_time": null,
    "likes": 11,
    "tags": [
      "transformers",
      "gguf",
      "mistral",
      "quantized",
      "2-bit",
      "3-bit",
      "4-bit",
      "5-bit",
      "6-bit",
      "8-bit",
      "GGUF",
      "text-generation",
      "llama-3",
      "llama",
      "base_model:Qwen/Qwen2-7B-Instruct",
      "base_model:quantized:Qwen/Qwen2-7B-Instruct",
      "region:us",
      "conversational"
    ],
    "card_url": "https://huggingface.co/MaziyarPanahi/Qwen2-7B-Instruct-GGUF",
    "dependencies": [
      [
        "llama.cpp",
        null
      ],
      [
        "llama-cpp-python",
        null
      ],
      [
        "ctransformers",
        null
      ]
    ],
    "last_modified": "2024-06-06T17:54:17+00:00",
    "created_at": "2024-06-06T17:14:16+00:00",
    "analysis_date": "2025-03-22T00:42:01.208743",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "mistral",
    "pipeline_tag": "text-generation",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "tags": [
        "quantized",
        "2-bit",
        "3-bit",
        "4-bit",
        "5-bit",
        "6-bit",
        "8-bit",
        "GGUF",
        "text-generation",
        "llama-3",
        "llama"
      ],
      "model_name": "Qwen2-7B-Instruct-GGUF",
      "base_model": "Qwen/Qwen2-7B-Instruct",
      "inference": false,
      "model_creator": "Qwen",
      "pipeline_tag": "text-generation",
      "quantized_by": "MaziyarPanahi"
    },
    "card_text": "# [MaziyarPanahi/Qwen2-7B-Instruct-GGUF](https://huggingface.co/MaziyarPanahi/Qwen2-7B-Instruct-GGUF)\n- Model creator: [Qwen](https://huggingface.co/Qwen)\n- Original model: [Qwen/Qwen2-7B-Instruct](https://huggingface.co/Qwen/Qwen2-7B-Instruct)\n\n## Description\n[MaziyarPanahi/Qwen2-7B-Instruct-GGUF](https://huggingface.co/MaziyarPanahi/Qwen2-7B-Instruct-GGUF) contains GGUF format model files for [Qwen/Qwen2-7B-Instruct](https://huggingface.co/Qwen/Qwen2-7B-Instruct).\n\n### About GGUF\n\nGGUF is a new format introduced by the llama.cpp team on August 21st 2023. It is a replacement for GGML, which is no longer supported by llama.cpp.\n\nHere is an incomplete list of clients and libraries that are known to support GGUF:\n\n* [llama.cpp](https://github.com/ggerganov/llama.cpp). The source project for GGUF. Offers a CLI and a server option.\n* [llama-cpp-python](https://github.com/abetlen/llama-cpp-python), a Python library with GPU accel, LangChain support, and OpenAI-compatible API server.\n* [LM Studio](https://lmstudio.ai/), an easy-to-use and powerful local GUI for Windows and macOS (Silicon), with GPU acceleration. Linux available, in beta as of 27/11/2023.\n* [text-generation-webui](https://github.com/oobabooga/text-generation-webui), the most widely used web UI, with many features and powerful extensions. Supports GPU acceleration.\n* [KoboldCpp](https://github.com/LostRuins/koboldcpp), a fully featured web UI, with GPU accel across all platforms and GPU architectures. Especially good for story telling.\n* [GPT4All](https://gpt4all.io/index.html), a free and open source local running GUI, supporting Windows, Linux and macOS with full GPU accel.\n* [LoLLMS Web UI](https://github.com/ParisNeo/lollms-webui), a great web UI with many interesting and unique features, including a full model library for easy model selection.\n* [Faraday.dev](https://faraday.dev/), an attractive and easy to use character-based chat GUI for Windows and macOS (both Silicon and Intel), with GPU acceleration.\n* [candle](https://github.com/huggingface/candle), a Rust ML framework with a focus on performance, including GPU support, and ease of use.\n* [ctransformers](https://github.com/marella/ctransformers), a Python library with GPU accel, LangChain support, and OpenAI-compatible AI server. Note, as of time of writing (November 27th 2023), ctransformers has not been updated in a long time and does not support many recent models.\n\n## Special thanks\n\n🙏 Special thanks to [Georgi Gerganov](https://github.com/ggerganov) and the whole team working on [llama.cpp](https://github.com/ggerganov/llama.cpp/) for making all of this possible.",
    "card_content": "---\ntags:\n- quantized\n- 2-bit\n- 3-bit\n- 4-bit\n- 5-bit\n- 6-bit\n- 8-bit\n- GGUF\n- text-generation\n- llama-3\n- llama\nmodel_name: Qwen2-7B-Instruct-GGUF\nbase_model: Qwen/Qwen2-7B-Instruct\ninference: false\nmodel_creator: Qwen\npipeline_tag: text-generation\nquantized_by: MaziyarPanahi\n---\n# [MaziyarPanahi/Qwen2-7B-Instruct-GGUF](https://huggingface.co/MaziyarPanahi/Qwen2-7B-Instruct-GGUF)\n- Model creator: [Qwen](https://huggingface.co/Qwen)\n- Original model: [Qwen/Qwen2-7B-Instruct](https://huggingface.co/Qwen/Qwen2-7B-Instruct)\n\n## Description\n[MaziyarPanahi/Qwen2-7B-Instruct-GGUF](https://huggingface.co/MaziyarPanahi/Qwen2-7B-Instruct-GGUF) contains GGUF format model files for [Qwen/Qwen2-7B-Instruct](https://huggingface.co/Qwen/Qwen2-7B-Instruct).\n\n### About GGUF\n\nGGUF is a new format introduced by the llama.cpp team on August 21st 2023. It is a replacement for GGML, which is no longer supported by llama.cpp.\n\nHere is an incomplete list of clients and libraries that are known to support GGUF:\n\n* [llama.cpp](https://github.com/ggerganov/llama.cpp). The source project for GGUF. Offers a CLI and a server option.\n* [llama-cpp-python](https://github.com/abetlen/llama-cpp-python), a Python library with GPU accel, LangChain support, and OpenAI-compatible API server.\n* [LM Studio](https://lmstudio.ai/), an easy-to-use and powerful local GUI for Windows and macOS (Silicon), with GPU acceleration. Linux available, in beta as of 27/11/2023.\n* [text-generation-webui](https://github.com/oobabooga/text-generation-webui), the most widely used web UI, with many features and powerful extensions. Supports GPU acceleration.\n* [KoboldCpp](https://github.com/LostRuins/koboldcpp), a fully featured web UI, with GPU accel across all platforms and GPU architectures. Especially good for story telling.\n* [GPT4All](https://gpt4all.io/index.html), a free and open source local running GUI, supporting Windows, Linux and macOS with full GPU accel.\n* [LoLLMS Web UI](https://github.com/ParisNeo/lollms-webui), a great web UI with many interesting and unique features, including a full model library for easy model selection.\n* [Faraday.dev](https://faraday.dev/), an attractive and easy to use character-based chat GUI for Windows and macOS (both Silicon and Intel), with GPU acceleration.\n* [candle](https://github.com/huggingface/candle), a Rust ML framework with a focus on performance, including GPU support, and ease of use.\n* [ctransformers](https://github.com/marella/ctransformers), a Python library with GPU accel, LangChain support, and OpenAI-compatible AI server. Note, as of time of writing (November 27th 2023), ctransformers has not been updated in a long time and does not support many recent models.\n\n## Special thanks\n\n🙏 Special thanks to [Georgi Gerganov](https://github.com/ggerganov) and the whole team working on [llama.cpp](https://github.com/ggerganov/llama.cpp/) for making all of this possible.",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModel",
      "custom_class": null,
      "pipeline_tag": null,
      "processor": null
    },
    "safetensors": null,
    "model_index": null,
    "trending_score": null
  },
  {
    "model_id": "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
    "model_name": "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
    "author": "deepseek-ai",
    "downloads": 1705794,
    "downloads_all_time": null,
    "likes": 1068,
    "tags": [
      "transformers",
      "safetensors",
      "qwen2",
      "text-generation",
      "conversational",
      "arxiv:2501.12948",
      "license:mit",
      "autotrain_compatible",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
    "dependencies": [
      [
        "transformers",
        null
      ]
    ],
    "last_modified": "2025-02-24T03:32:35+00:00",
    "created_at": "2025-01-20T09:04:18+00:00",
    "analysis_date": "2025-03-22T00:42:03.229224",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "qwen2",
    "pipeline_tag": "text-generation",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "license": "mit",
      "library_name": "transformers"
    },
    "card_text": "# DeepSeek-R1\n<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align=\"center\">\n  <img src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true\" width=\"60%\" alt=\"DeepSeek-V3\" />\n</div>\n<hr>\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://www.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Homepage\" src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://chat.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/🤖%20Chat-DeepSeek%20R1-536af5?color=536af5&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://huggingface.co/deepseek-ai\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://discord.gg/Tc7c45Zzu5\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Wechat\" src=\"https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://twitter.com/deepseek_ai\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Twitter Follow\" src=\"https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE\" style=\"margin: 2px;\">\n    <img alt=\"License\" src=\"https://img.shields.io/badge/License-MIT-f5de53?&color=f5de53\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n\n<p align=\"center\">\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf\"><b>Paper Link</b>👁️</a>\n</p>\n\n\n## 1. Introduction\n\nWe introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. \nDeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning.\nWith RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors.\nHowever, DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. To address these issues and further enhance reasoning performance,\nwe introduce DeepSeek-R1, which incorporates cold-start data before RL.\nDeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks. \nTo support the research community, we have open-sourced DeepSeek-R1-Zero, DeepSeek-R1, and six dense models distilled from DeepSeek-R1 based on Llama and Qwen. DeepSeek-R1-Distill-Qwen-32B outperforms OpenAI-o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.\n\n**NOTE: Before running DeepSeek-R1 series models locally, we kindly recommend reviewing the [Usage Recommendation](#usage-recommendations) section.**\n\n<p align=\"center\">\n  <img width=\"80%\" src=\"figures/benchmark.jpg\">\n</p>\n\n## 2. Model Summary\n\n---\n\n**Post-Training: Large-Scale Reinforcement Learning on the Base Model**\n\n-  We directly apply reinforcement learning (RL) to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.\n\n-   We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model's reasoning and non-reasoning capabilities.\n    We believe the pipeline will benefit the industry by creating better models. \n\n---\n\n**Distillation: Smaller Models Can Be Powerful Too**\n\n-  We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future. \n- Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.\n\n## 3. Model Downloads\n\n### DeepSeek-R1 Models\n\n<div align=\"center\">\n\n| **Model** | **#Total Params** | **#Activated Params** | **Context Length** | **Download** |\n| :------------: | :------------: | :------------: | :------------: | :------------: |\n| DeepSeek-R1-Zero | 671B | 37B | 128K   | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Zero)   |\n| DeepSeek-R1   | 671B | 37B |  128K   | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1)   |\n\n</div>\n\nDeepSeek-R1-Zero & DeepSeek-R1 are trained based on DeepSeek-V3-Base. \nFor more details regarding the model architecture, please refer to [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) repository.\n\n### DeepSeek-R1-Distill Models\n\n<div align=\"center\">\n\n| **Model** | **Base Model** | **Download** |\n| :------------: | :------------: | :------------: |\n| DeepSeek-R1-Distill-Qwen-1.5B  | [Qwen2.5-Math-1.5B](https://huggingface.co/Qwen/Qwen2.5-Math-1.5B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B)   |\n| DeepSeek-R1-Distill-Qwen-7B  | [Qwen2.5-Math-7B](https://huggingface.co/Qwen/Qwen2.5-Math-7B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B)   |\n| DeepSeek-R1-Distill-Llama-8B  | [Llama-3.1-8B](https://huggingface.co/meta-llama/Llama-3.1-8B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B)   |\n| DeepSeek-R1-Distill-Qwen-14B   | [Qwen2.5-14B](https://huggingface.co/Qwen/Qwen2.5-14B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B)   |\n|DeepSeek-R1-Distill-Qwen-32B  | [Qwen2.5-32B](https://huggingface.co/Qwen/Qwen2.5-32B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B)   |\n| DeepSeek-R1-Distill-Llama-70B  | [Llama-3.3-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B)   |\n\n</div>\n\nDeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1.\nWe slightly change their configs and tokenizers. Please use our setting to run these models.\n\n## 4. Evaluation Results\n\n### DeepSeek-R1-Evaluation\n For all our models, the maximum generation length is set to 32,768 tokens. For benchmarks requiring sampling, we use a temperature of $0.6$, a top-p value of $0.95$, and generate 64 responses per query to estimate pass@1.\n<div align=\"center\">\n\n\n| Category | Benchmark (Metric) | Claude-3.5-Sonnet-1022 | GPT-4o 0513 | DeepSeek V3 | OpenAI o1-mini | OpenAI o1-1217 | DeepSeek R1 |\n|----------|-------------------|----------------------|------------|--------------|----------------|------------|--------------|\n| | Architecture | - | - | MoE | - | - | MoE |\n| | # Activated Params | - | - | 37B | - | - | 37B |\n| | # Total Params | - | - | 671B | - | - | 671B |\n| English | MMLU (Pass@1) | 88.3 | 87.2 | 88.5 | 85.2 | **91.8** | 90.8 |\n| | MMLU-Redux (EM) | 88.9 | 88.0 | 89.1 | 86.7 | - | **92.9** |\n| | MMLU-Pro (EM) | 78.0 | 72.6 | 75.9 | 80.3 | - | **84.0** |\n| | DROP (3-shot F1) | 88.3 | 83.7 | 91.6 | 83.9 | 90.2 | **92.2** |\n| | IF-Eval (Prompt Strict) | **86.5** | 84.3 | 86.1 | 84.8 | - | 83.3 |\n| | GPQA-Diamond (Pass@1) | 65.0 | 49.9 | 59.1 | 60.0 | **75.7** | 71.5 |\n| | SimpleQA (Correct) | 28.4 | 38.2 | 24.9 | 7.0 | **47.0** | 30.1 |\n| | FRAMES (Acc.) | 72.5 | 80.5 | 73.3 | 76.9 | - | **82.5** |\n| | AlpacaEval2.0 (LC-winrate) | 52.0 | 51.1 | 70.0 | 57.8 | - | **87.6** |\n| | ArenaHard (GPT-4-1106) | 85.2 | 80.4 | 85.5 | 92.0 | - | **92.3** |\n| Code | LiveCodeBench (Pass@1-COT) | 33.8 | 34.2 | - | 53.8 | 63.4 | **65.9** |\n| | Codeforces (Percentile) | 20.3 | 23.6 | 58.7 | 93.4 | **96.6** | 96.3 |\n| | Codeforces (Rating) | 717 | 759 | 1134 | 1820 | **2061** | 2029 |\n| | SWE Verified (Resolved) | **50.8** | 38.8 | 42.0 | 41.6 | 48.9 | 49.2 |\n| | Aider-Polyglot (Acc.) | 45.3 | 16.0 | 49.6 | 32.9 | **61.7** | 53.3 |\n| Math | AIME 2024 (Pass@1) | 16.0 | 9.3 | 39.2 | 63.6 | 79.2 | **79.8** |\n| | MATH-500 (Pass@1) | 78.3 | 74.6 | 90.2 | 90.0 | 96.4 | **97.3** |\n| | CNMO 2024 (Pass@1) | 13.1 | 10.8 | 43.2 | 67.6 | - | **78.8** |\n| Chinese | CLUEWSC (EM) | 85.4 | 87.9 | 90.9 | 89.9 | - | **92.8** |\n| | C-Eval (EM) | 76.7 | 76.0 | 86.5 | 68.9 | - | **91.8** |\n| | C-SimpleQA (Correct) | 55.4 | 58.7 | **68.0** | 40.3 | - | 63.7 |\n\n</div>\n\n\n### Distilled Model Evaluation\n\n\n<div align=\"center\">\n\n| Model                                    | AIME 2024 pass@1 | AIME 2024 cons@64 | MATH-500 pass@1 | GPQA Diamond pass@1 | LiveCodeBench pass@1 | CodeForces rating |\n|------------------------------------------|------------------|-------------------|-----------------|----------------------|----------------------|-------------------|\n| GPT-4o-0513                          | 9.3              | 13.4              | 74.6            | 49.9                 | 32.9                 | 759               |\n| Claude-3.5-Sonnet-1022             | 16.0             | 26.7                 | 78.3            | 65.0                 | 38.9                 | 717               |\n| o1-mini                              | 63.6             | 80.0              | 90.0            | 60.0                 | 53.8                 | **1820**          |\n| QwQ-32B-Preview                              | 44.0             | 60.0                 | 90.6            | 54.5               | 41.9                 | 1316              |\n| DeepSeek-R1-Distill-Qwen-1.5B       | 28.9             | 52.7              | 83.9            | 33.8                 | 16.9                 | 954               |\n| DeepSeek-R1-Distill-Qwen-7B          | 55.5             | 83.3              | 92.8            | 49.1                 | 37.6                 | 1189              |\n| DeepSeek-R1-Distill-Qwen-14B         | 69.7             | 80.0              | 93.9            | 59.1                 | 53.1                 | 1481              |\n| DeepSeek-R1-Distill-Qwen-32B        | **72.6**         | 83.3              | 94.3            | 62.1                 | 57.2                 | 1691              |\n| DeepSeek-R1-Distill-Llama-8B         | 50.4             | 80.0              | 89.1            | 49.0                 | 39.6                 | 1205              |\n| DeepSeek-R1-Distill-Llama-70B        | 70.0             | **86.7**          | **94.5**        | **65.2**             | **57.5**             | 1633              |\n\n</div>\n\n\n## 5. Chat Website & API Platform\nYou can chat with DeepSeek-R1 on DeepSeek's official website: [chat.deepseek.com](https://chat.deepseek.com), and switch on the button \"DeepThink\"\n\nWe also provide OpenAI-Compatible API at DeepSeek Platform: [platform.deepseek.com](https://platform.deepseek.com/)\n\n## 6. How to Run Locally\n\n### DeepSeek-R1 Models\n\nPlease visit [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) repo for more information about running DeepSeek-R1 locally.\n\n**NOTE: Hugging Face's Transformers has not been directly supported yet.**\n\n### DeepSeek-R1-Distill Models\n\nDeepSeek-R1-Distill models can be utilized in the same manner as Qwen or Llama models.\n\nFor instance, you can easily start a service using [vLLM](https://github.com/vllm-project/vllm):\n\n```shell\nvllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --tensor-parallel-size 2 --max-model-len 32768 --enforce-eager\n```\n\nYou can also easily start a service using [SGLang](https://github.com/sgl-project/sglang)\n\n```bash\npython3 -m sglang.launch_server --model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --trust-remote-code --tp 2\n```\n\n### Usage Recommendations\n\n**We recommend adhering to the following configurations when utilizing the DeepSeek-R1 series models, including benchmarking, to achieve the expected performance:**\n\n1. Set the temperature within the range of 0.5-0.7 (0.6 is recommended) to prevent endless repetitions or incoherent outputs.\n2. **Avoid adding a system prompt; all instructions should be contained within the user prompt.**\n3. For mathematical problems, it is advisable to include a directive in your prompt such as: \"Please reason step by step, and put your final answer within \\boxed{}.\"\n4. When evaluating model performance, it is recommended to conduct multiple tests and average the results.\n\nAdditionally, we have observed that the DeepSeek-R1 series models tend to bypass thinking pattern (i.e., outputting \"\\<think\\>\\n\\n\\</think\\>\") when responding to certain queries, which can adversely affect the model's performance.\n**To ensure that the model engages in thorough reasoning, we recommend enforcing the model to initiate its response with \"\\<think\\>\\n\" at the beginning of every output.**\n\n## 7. License\nThis code repository and the model weights are licensed under the [MIT License](https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE).\nDeepSeek-R1 series support commercial use, allow for any modifications and derivative works, including, but not limited to, distillation for training other LLMs. Please note that:\n- DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Qwen-14B and DeepSeek-R1-Distill-Qwen-32B are derived from [Qwen-2.5 series](https://github.com/QwenLM/Qwen2.5), which are originally licensed under [Apache 2.0 License](https://huggingface.co/Qwen/Qwen2.5-1.5B/blob/main/LICENSE), and now finetuned with 800k samples curated with DeepSeek-R1.\n- DeepSeek-R1-Distill-Llama-8B is derived from Llama3.1-8B-Base and is originally licensed under [llama3.1 license](https://huggingface.co/meta-llama/Llama-3.1-8B/blob/main/LICENSE).\n- DeepSeek-R1-Distill-Llama-70B is derived from Llama3.3-70B-Instruct and is originally licensed under [llama3.3 license](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct/blob/main/LICENSE).\n\n## 8. Citation\n```\n@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\n      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, \n      author={DeepSeek-AI},\n      year={2025},\n      eprint={2501.12948},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2501.12948}, \n}\n\n```\n\n## 9. Contact\nIf you have any questions, please raise an issue or contact us at [service@deepseek.com](service@deepseek.com).\n",
    "card_content": "---\nlicense: mit\nlibrary_name: transformers\n---\n# DeepSeek-R1\n<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align=\"center\">\n  <img src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true\" width=\"60%\" alt=\"DeepSeek-V3\" />\n</div>\n<hr>\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://www.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Homepage\" src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://chat.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/🤖%20Chat-DeepSeek%20R1-536af5?color=536af5&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://huggingface.co/deepseek-ai\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://discord.gg/Tc7c45Zzu5\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Wechat\" src=\"https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://twitter.com/deepseek_ai\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Twitter Follow\" src=\"https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE\" style=\"margin: 2px;\">\n    <img alt=\"License\" src=\"https://img.shields.io/badge/License-MIT-f5de53?&color=f5de53\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n\n<p align=\"center\">\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf\"><b>Paper Link</b>👁️</a>\n</p>\n\n\n## 1. Introduction\n\nWe introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. \nDeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning.\nWith RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors.\nHowever, DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. To address these issues and further enhance reasoning performance,\nwe introduce DeepSeek-R1, which incorporates cold-start data before RL.\nDeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks. \nTo support the research community, we have open-sourced DeepSeek-R1-Zero, DeepSeek-R1, and six dense models distilled from DeepSeek-R1 based on Llama and Qwen. DeepSeek-R1-Distill-Qwen-32B outperforms OpenAI-o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.\n\n**NOTE: Before running DeepSeek-R1 series models locally, we kindly recommend reviewing the [Usage Recommendation](#usage-recommendations) section.**\n\n<p align=\"center\">\n  <img width=\"80%\" src=\"figures/benchmark.jpg\">\n</p>\n\n## 2. Model Summary\n\n---\n\n**Post-Training: Large-Scale Reinforcement Learning on the Base Model**\n\n-  We directly apply reinforcement learning (RL) to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.\n\n-   We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model's reasoning and non-reasoning capabilities.\n    We believe the pipeline will benefit the industry by creating better models. \n\n---\n\n**Distillation: Smaller Models Can Be Powerful Too**\n\n-  We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future. \n- Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.\n\n## 3. Model Downloads\n\n### DeepSeek-R1 Models\n\n<div align=\"center\">\n\n| **Model** | **#Total Params** | **#Activated Params** | **Context Length** | **Download** |\n| :------------: | :------------: | :------------: | :------------: | :------------: |\n| DeepSeek-R1-Zero | 671B | 37B | 128K   | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Zero)   |\n| DeepSeek-R1   | 671B | 37B |  128K   | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1)   |\n\n</div>\n\nDeepSeek-R1-Zero & DeepSeek-R1 are trained based on DeepSeek-V3-Base. \nFor more details regarding the model architecture, please refer to [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) repository.\n\n### DeepSeek-R1-Distill Models\n\n<div align=\"center\">\n\n| **Model** | **Base Model** | **Download** |\n| :------------: | :------------: | :------------: |\n| DeepSeek-R1-Distill-Qwen-1.5B  | [Qwen2.5-Math-1.5B](https://huggingface.co/Qwen/Qwen2.5-Math-1.5B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B)   |\n| DeepSeek-R1-Distill-Qwen-7B  | [Qwen2.5-Math-7B](https://huggingface.co/Qwen/Qwen2.5-Math-7B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B)   |\n| DeepSeek-R1-Distill-Llama-8B  | [Llama-3.1-8B](https://huggingface.co/meta-llama/Llama-3.1-8B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B)   |\n| DeepSeek-R1-Distill-Qwen-14B   | [Qwen2.5-14B](https://huggingface.co/Qwen/Qwen2.5-14B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B)   |\n|DeepSeek-R1-Distill-Qwen-32B  | [Qwen2.5-32B](https://huggingface.co/Qwen/Qwen2.5-32B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B)   |\n| DeepSeek-R1-Distill-Llama-70B  | [Llama-3.3-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B)   |\n\n</div>\n\nDeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1.\nWe slightly change their configs and tokenizers. Please use our setting to run these models.\n\n## 4. Evaluation Results\n\n### DeepSeek-R1-Evaluation\n For all our models, the maximum generation length is set to 32,768 tokens. For benchmarks requiring sampling, we use a temperature of $0.6$, a top-p value of $0.95$, and generate 64 responses per query to estimate pass@1.\n<div align=\"center\">\n\n\n| Category | Benchmark (Metric) | Claude-3.5-Sonnet-1022 | GPT-4o 0513 | DeepSeek V3 | OpenAI o1-mini | OpenAI o1-1217 | DeepSeek R1 |\n|----------|-------------------|----------------------|------------|--------------|----------------|------------|--------------|\n| | Architecture | - | - | MoE | - | - | MoE |\n| | # Activated Params | - | - | 37B | - | - | 37B |\n| | # Total Params | - | - | 671B | - | - | 671B |\n| English | MMLU (Pass@1) | 88.3 | 87.2 | 88.5 | 85.2 | **91.8** | 90.8 |\n| | MMLU-Redux (EM) | 88.9 | 88.0 | 89.1 | 86.7 | - | **92.9** |\n| | MMLU-Pro (EM) | 78.0 | 72.6 | 75.9 | 80.3 | - | **84.0** |\n| | DROP (3-shot F1) | 88.3 | 83.7 | 91.6 | 83.9 | 90.2 | **92.2** |\n| | IF-Eval (Prompt Strict) | **86.5** | 84.3 | 86.1 | 84.8 | - | 83.3 |\n| | GPQA-Diamond (Pass@1) | 65.0 | 49.9 | 59.1 | 60.0 | **75.7** | 71.5 |\n| | SimpleQA (Correct) | 28.4 | 38.2 | 24.9 | 7.0 | **47.0** | 30.1 |\n| | FRAMES (Acc.) | 72.5 | 80.5 | 73.3 | 76.9 | - | **82.5** |\n| | AlpacaEval2.0 (LC-winrate) | 52.0 | 51.1 | 70.0 | 57.8 | - | **87.6** |\n| | ArenaHard (GPT-4-1106) | 85.2 | 80.4 | 85.5 | 92.0 | - | **92.3** |\n| Code | LiveCodeBench (Pass@1-COT) | 33.8 | 34.2 | - | 53.8 | 63.4 | **65.9** |\n| | Codeforces (Percentile) | 20.3 | 23.6 | 58.7 | 93.4 | **96.6** | 96.3 |\n| | Codeforces (Rating) | 717 | 759 | 1134 | 1820 | **2061** | 2029 |\n| | SWE Verified (Resolved) | **50.8** | 38.8 | 42.0 | 41.6 | 48.9 | 49.2 |\n| | Aider-Polyglot (Acc.) | 45.3 | 16.0 | 49.6 | 32.9 | **61.7** | 53.3 |\n| Math | AIME 2024 (Pass@1) | 16.0 | 9.3 | 39.2 | 63.6 | 79.2 | **79.8** |\n| | MATH-500 (Pass@1) | 78.3 | 74.6 | 90.2 | 90.0 | 96.4 | **97.3** |\n| | CNMO 2024 (Pass@1) | 13.1 | 10.8 | 43.2 | 67.6 | - | **78.8** |\n| Chinese | CLUEWSC (EM) | 85.4 | 87.9 | 90.9 | 89.9 | - | **92.8** |\n| | C-Eval (EM) | 76.7 | 76.0 | 86.5 | 68.9 | - | **91.8** |\n| | C-SimpleQA (Correct) | 55.4 | 58.7 | **68.0** | 40.3 | - | 63.7 |\n\n</div>\n\n\n### Distilled Model Evaluation\n\n\n<div align=\"center\">\n\n| Model                                    | AIME 2024 pass@1 | AIME 2024 cons@64 | MATH-500 pass@1 | GPQA Diamond pass@1 | LiveCodeBench pass@1 | CodeForces rating |\n|------------------------------------------|------------------|-------------------|-----------------|----------------------|----------------------|-------------------|\n| GPT-4o-0513                          | 9.3              | 13.4              | 74.6            | 49.9                 | 32.9                 | 759               |\n| Claude-3.5-Sonnet-1022             | 16.0             | 26.7                 | 78.3            | 65.0                 | 38.9                 | 717               |\n| o1-mini                              | 63.6             | 80.0              | 90.0            | 60.0                 | 53.8                 | **1820**          |\n| QwQ-32B-Preview                              | 44.0             | 60.0                 | 90.6            | 54.5               | 41.9                 | 1316              |\n| DeepSeek-R1-Distill-Qwen-1.5B       | 28.9             | 52.7              | 83.9            | 33.8                 | 16.9                 | 954               |\n| DeepSeek-R1-Distill-Qwen-7B          | 55.5             | 83.3              | 92.8            | 49.1                 | 37.6                 | 1189              |\n| DeepSeek-R1-Distill-Qwen-14B         | 69.7             | 80.0              | 93.9            | 59.1                 | 53.1                 | 1481              |\n| DeepSeek-R1-Distill-Qwen-32B        | **72.6**         | 83.3              | 94.3            | 62.1                 | 57.2                 | 1691              |\n| DeepSeek-R1-Distill-Llama-8B         | 50.4             | 80.0              | 89.1            | 49.0                 | 39.6                 | 1205              |\n| DeepSeek-R1-Distill-Llama-70B        | 70.0             | **86.7**          | **94.5**        | **65.2**             | **57.5**             | 1633              |\n\n</div>\n\n\n## 5. Chat Website & API Platform\nYou can chat with DeepSeek-R1 on DeepSeek's official website: [chat.deepseek.com](https://chat.deepseek.com), and switch on the button \"DeepThink\"\n\nWe also provide OpenAI-Compatible API at DeepSeek Platform: [platform.deepseek.com](https://platform.deepseek.com/)\n\n## 6. How to Run Locally\n\n### DeepSeek-R1 Models\n\nPlease visit [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) repo for more information about running DeepSeek-R1 locally.\n\n**NOTE: Hugging Face's Transformers has not been directly supported yet.**\n\n### DeepSeek-R1-Distill Models\n\nDeepSeek-R1-Distill models can be utilized in the same manner as Qwen or Llama models.\n\nFor instance, you can easily start a service using [vLLM](https://github.com/vllm-project/vllm):\n\n```shell\nvllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --tensor-parallel-size 2 --max-model-len 32768 --enforce-eager\n```\n\nYou can also easily start a service using [SGLang](https://github.com/sgl-project/sglang)\n\n```bash\npython3 -m sglang.launch_server --model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --trust-remote-code --tp 2\n```\n\n### Usage Recommendations\n\n**We recommend adhering to the following configurations when utilizing the DeepSeek-R1 series models, including benchmarking, to achieve the expected performance:**\n\n1. Set the temperature within the range of 0.5-0.7 (0.6 is recommended) to prevent endless repetitions or incoherent outputs.\n2. **Avoid adding a system prompt; all instructions should be contained within the user prompt.**\n3. For mathematical problems, it is advisable to include a directive in your prompt such as: \"Please reason step by step, and put your final answer within \\boxed{}.\"\n4. When evaluating model performance, it is recommended to conduct multiple tests and average the results.\n\nAdditionally, we have observed that the DeepSeek-R1 series models tend to bypass thinking pattern (i.e., outputting \"\\<think\\>\\n\\n\\</think\\>\") when responding to certain queries, which can adversely affect the model's performance.\n**To ensure that the model engages in thorough reasoning, we recommend enforcing the model to initiate its response with \"\\<think\\>\\n\" at the beginning of every output.**\n\n## 7. License\nThis code repository and the model weights are licensed under the [MIT License](https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE).\nDeepSeek-R1 series support commercial use, allow for any modifications and derivative works, including, but not limited to, distillation for training other LLMs. Please note that:\n- DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Qwen-14B and DeepSeek-R1-Distill-Qwen-32B are derived from [Qwen-2.5 series](https://github.com/QwenLM/Qwen2.5), which are originally licensed under [Apache 2.0 License](https://huggingface.co/Qwen/Qwen2.5-1.5B/blob/main/LICENSE), and now finetuned with 800k samples curated with DeepSeek-R1.\n- DeepSeek-R1-Distill-Llama-8B is derived from Llama3.1-8B-Base and is originally licensed under [llama3.1 license](https://huggingface.co/meta-llama/Llama-3.1-8B/blob/main/LICENSE).\n- DeepSeek-R1-Distill-Llama-70B is derived from Llama3.3-70B-Instruct and is originally licensed under [llama3.3 license](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct/blob/main/LICENSE).\n\n## 8. Citation\n```\n@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\n      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, \n      author={DeepSeek-AI},\n      year={2025},\n      eprint={2501.12948},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2501.12948}, \n}\n\n```\n\n## 9. Contact\nIf you have any questions, please raise an issue or contact us at [service@deepseek.com](service@deepseek.com).\n",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModelForCausalLM",
      "custom_class": null,
      "pipeline_tag": "text-generation",
      "processor": "AutoTokenizer"
    },
    "safetensors": {
      "parameters": {
        "BF16": 1777088000
      },
      "total": 1777088000
    },
    "model_index": null,
    "trending_score": null
  },
  {
    "model_id": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B",
    "model_name": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B",
    "author": "deepseek-ai",
    "downloads": 1689213,
    "downloads_all_time": null,
    "likes": 1280,
    "tags": [
      "transformers",
      "safetensors",
      "qwen2",
      "text-generation",
      "conversational",
      "arxiv:2501.12948",
      "license:mit",
      "autotrain_compatible",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B",
    "dependencies": [
      [
        "transformers",
        null
      ]
    ],
    "last_modified": "2025-02-24T03:31:29+00:00",
    "created_at": "2025-01-20T09:19:00+00:00",
    "analysis_date": "2025-03-22T00:42:05.213893",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "qwen2",
    "pipeline_tag": "text-generation",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "license": "mit",
      "library_name": "transformers"
    },
    "card_text": "# DeepSeek-R1\n<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align=\"center\">\n  <img src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true\" width=\"60%\" alt=\"DeepSeek-V3\" />\n</div>\n<hr>\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://www.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Homepage\" src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://chat.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/🤖%20Chat-DeepSeek%20R1-536af5?color=536af5&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://huggingface.co/deepseek-ai\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://discord.gg/Tc7c45Zzu5\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Wechat\" src=\"https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://twitter.com/deepseek_ai\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Twitter Follow\" src=\"https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE\" style=\"margin: 2px;\">\n    <img alt=\"License\" src=\"https://img.shields.io/badge/License-MIT-f5de53?&color=f5de53\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n\n<p align=\"center\">\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf\"><b>Paper Link</b>👁️</a>\n</p>\n\n\n## 1. Introduction\n\nWe introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. \nDeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning.\nWith RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors.\nHowever, DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. To address these issues and further enhance reasoning performance,\nwe introduce DeepSeek-R1, which incorporates cold-start data before RL.\nDeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks. \nTo support the research community, we have open-sourced DeepSeek-R1-Zero, DeepSeek-R1, and six dense models distilled from DeepSeek-R1 based on Llama and Qwen. DeepSeek-R1-Distill-Qwen-32B outperforms OpenAI-o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.\n\n**NOTE: Before running DeepSeek-R1 series models locally, we kindly recommend reviewing the [Usage Recommendation](#usage-recommendations) section.**\n\n<p align=\"center\">\n  <img width=\"80%\" src=\"figures/benchmark.jpg\">\n</p>\n\n## 2. Model Summary\n\n---\n\n**Post-Training: Large-Scale Reinforcement Learning on the Base Model**\n\n-  We directly apply reinforcement learning (RL) to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.\n\n-   We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model's reasoning and non-reasoning capabilities.\n    We believe the pipeline will benefit the industry by creating better models. \n\n---\n\n**Distillation: Smaller Models Can Be Powerful Too**\n\n-  We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future. \n- Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.\n\n## 3. Model Downloads\n\n### DeepSeek-R1 Models\n\n<div align=\"center\">\n\n| **Model** | **#Total Params** | **#Activated Params** | **Context Length** | **Download** |\n| :------------: | :------------: | :------------: | :------------: | :------------: |\n| DeepSeek-R1-Zero | 671B | 37B | 128K   | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Zero)   |\n| DeepSeek-R1   | 671B | 37B |  128K   | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1)   |\n\n</div>\n\nDeepSeek-R1-Zero & DeepSeek-R1 are trained based on DeepSeek-V3-Base. \nFor more details regarding the model architecture, please refer to [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) repository.\n\n### DeepSeek-R1-Distill Models\n\n<div align=\"center\">\n\n| **Model** | **Base Model** | **Download** |\n| :------------: | :------------: | :------------: |\n| DeepSeek-R1-Distill-Qwen-1.5B  | [Qwen2.5-Math-1.5B](https://huggingface.co/Qwen/Qwen2.5-Math-1.5B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B)   |\n| DeepSeek-R1-Distill-Qwen-7B  | [Qwen2.5-Math-7B](https://huggingface.co/Qwen/Qwen2.5-Math-7B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B)   |\n| DeepSeek-R1-Distill-Llama-8B  | [Llama-3.1-8B](https://huggingface.co/meta-llama/Llama-3.1-8B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B)   |\n| DeepSeek-R1-Distill-Qwen-14B   | [Qwen2.5-14B](https://huggingface.co/Qwen/Qwen2.5-14B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B)   |\n|DeepSeek-R1-Distill-Qwen-32B  | [Qwen2.5-32B](https://huggingface.co/Qwen/Qwen2.5-32B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B)   |\n| DeepSeek-R1-Distill-Llama-70B  | [Llama-3.3-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B)   |\n\n</div>\n\nDeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1.\nWe slightly change their configs and tokenizers. Please use our setting to run these models.\n\n## 4. Evaluation Results\n\n### DeepSeek-R1-Evaluation\n For all our models, the maximum generation length is set to 32,768 tokens. For benchmarks requiring sampling, we use a temperature of $0.6$, a top-p value of $0.95$, and generate 64 responses per query to estimate pass@1.\n<div align=\"center\">\n\n\n| Category | Benchmark (Metric) | Claude-3.5-Sonnet-1022 | GPT-4o 0513 | DeepSeek V3 | OpenAI o1-mini | OpenAI o1-1217 | DeepSeek R1 |\n|----------|-------------------|----------------------|------------|--------------|----------------|------------|--------------|\n| | Architecture | - | - | MoE | - | - | MoE |\n| | # Activated Params | - | - | 37B | - | - | 37B |\n| | # Total Params | - | - | 671B | - | - | 671B |\n| English | MMLU (Pass@1) | 88.3 | 87.2 | 88.5 | 85.2 | **91.8** | 90.8 |\n| | MMLU-Redux (EM) | 88.9 | 88.0 | 89.1 | 86.7 | - | **92.9** |\n| | MMLU-Pro (EM) | 78.0 | 72.6 | 75.9 | 80.3 | - | **84.0** |\n| | DROP (3-shot F1) | 88.3 | 83.7 | 91.6 | 83.9 | 90.2 | **92.2** |\n| | IF-Eval (Prompt Strict) | **86.5** | 84.3 | 86.1 | 84.8 | - | 83.3 |\n| | GPQA-Diamond (Pass@1) | 65.0 | 49.9 | 59.1 | 60.0 | **75.7** | 71.5 |\n| | SimpleQA (Correct) | 28.4 | 38.2 | 24.9 | 7.0 | **47.0** | 30.1 |\n| | FRAMES (Acc.) | 72.5 | 80.5 | 73.3 | 76.9 | - | **82.5** |\n| | AlpacaEval2.0 (LC-winrate) | 52.0 | 51.1 | 70.0 | 57.8 | - | **87.6** |\n| | ArenaHard (GPT-4-1106) | 85.2 | 80.4 | 85.5 | 92.0 | - | **92.3** |\n| Code | LiveCodeBench (Pass@1-COT) | 33.8 | 34.2 | - | 53.8 | 63.4 | **65.9** |\n| | Codeforces (Percentile) | 20.3 | 23.6 | 58.7 | 93.4 | **96.6** | 96.3 |\n| | Codeforces (Rating) | 717 | 759 | 1134 | 1820 | **2061** | 2029 |\n| | SWE Verified (Resolved) | **50.8** | 38.8 | 42.0 | 41.6 | 48.9 | 49.2 |\n| | Aider-Polyglot (Acc.) | 45.3 | 16.0 | 49.6 | 32.9 | **61.7** | 53.3 |\n| Math | AIME 2024 (Pass@1) | 16.0 | 9.3 | 39.2 | 63.6 | 79.2 | **79.8** |\n| | MATH-500 (Pass@1) | 78.3 | 74.6 | 90.2 | 90.0 | 96.4 | **97.3** |\n| | CNMO 2024 (Pass@1) | 13.1 | 10.8 | 43.2 | 67.6 | - | **78.8** |\n| Chinese | CLUEWSC (EM) | 85.4 | 87.9 | 90.9 | 89.9 | - | **92.8** |\n| | C-Eval (EM) | 76.7 | 76.0 | 86.5 | 68.9 | - | **91.8** |\n| | C-SimpleQA (Correct) | 55.4 | 58.7 | **68.0** | 40.3 | - | 63.7 |\n\n</div>\n\n\n### Distilled Model Evaluation\n\n\n<div align=\"center\">\n\n| Model                                    | AIME 2024 pass@1 | AIME 2024 cons@64 | MATH-500 pass@1 | GPQA Diamond pass@1 | LiveCodeBench pass@1 | CodeForces rating |\n|------------------------------------------|------------------|-------------------|-----------------|----------------------|----------------------|-------------------|\n| GPT-4o-0513                          | 9.3              | 13.4              | 74.6            | 49.9                 | 32.9                 | 759               |\n| Claude-3.5-Sonnet-1022             | 16.0             | 26.7                 | 78.3            | 65.0                 | 38.9                 | 717               |\n| o1-mini                              | 63.6             | 80.0              | 90.0            | 60.0                 | 53.8                 | **1820**          |\n| QwQ-32B-Preview                              | 44.0             | 60.0                 | 90.6            | 54.5               | 41.9                 | 1316              |\n| DeepSeek-R1-Distill-Qwen-1.5B       | 28.9             | 52.7              | 83.9            | 33.8                 | 16.9                 | 954               |\n| DeepSeek-R1-Distill-Qwen-7B          | 55.5             | 83.3              | 92.8            | 49.1                 | 37.6                 | 1189              |\n| DeepSeek-R1-Distill-Qwen-14B         | 69.7             | 80.0              | 93.9            | 59.1                 | 53.1                 | 1481              |\n| DeepSeek-R1-Distill-Qwen-32B        | **72.6**         | 83.3              | 94.3            | 62.1                 | 57.2                 | 1691              |\n| DeepSeek-R1-Distill-Llama-8B         | 50.4             | 80.0              | 89.1            | 49.0                 | 39.6                 | 1205              |\n| DeepSeek-R1-Distill-Llama-70B        | 70.0             | **86.7**          | **94.5**        | **65.2**             | **57.5**             | 1633              |\n\n</div>\n\n\n## 5. Chat Website & API Platform\nYou can chat with DeepSeek-R1 on DeepSeek's official website: [chat.deepseek.com](https://chat.deepseek.com), and switch on the button \"DeepThink\"\n\nWe also provide OpenAI-Compatible API at DeepSeek Platform: [platform.deepseek.com](https://platform.deepseek.com/)\n\n## 6. How to Run Locally\n\n### DeepSeek-R1 Models\n\nPlease visit [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) repo for more information about running DeepSeek-R1 locally.\n\n**NOTE: Hugging Face's Transformers has not been directly supported yet.**\n\n### DeepSeek-R1-Distill Models\n\nDeepSeek-R1-Distill models can be utilized in the same manner as Qwen or Llama models.\n\nFor instance, you can easily start a service using [vLLM](https://github.com/vllm-project/vllm):\n\n```shell\nvllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --tensor-parallel-size 2 --max-model-len 32768 --enforce-eager\n```\n\nYou can also easily start a service using [SGLang](https://github.com/sgl-project/sglang)\n\n```bash\npython3 -m sglang.launch_server --model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --trust-remote-code --tp 2\n```\n\n### Usage Recommendations\n\n**We recommend adhering to the following configurations when utilizing the DeepSeek-R1 series models, including benchmarking, to achieve the expected performance:**\n\n1. Set the temperature within the range of 0.5-0.7 (0.6 is recommended) to prevent endless repetitions or incoherent outputs.\n2. **Avoid adding a system prompt; all instructions should be contained within the user prompt.**\n3. For mathematical problems, it is advisable to include a directive in your prompt such as: \"Please reason step by step, and put your final answer within \\boxed{}.\"\n4. When evaluating model performance, it is recommended to conduct multiple tests and average the results.\n\nAdditionally, we have observed that the DeepSeek-R1 series models tend to bypass thinking pattern (i.e., outputting \"\\<think\\>\\n\\n\\</think\\>\") when responding to certain queries, which can adversely affect the model's performance.\n**To ensure that the model engages in thorough reasoning, we recommend enforcing the model to initiate its response with \"\\<think\\>\\n\" at the beginning of every output.**\n\n## 7. License\nThis code repository and the model weights are licensed under the [MIT License](https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE).\nDeepSeek-R1 series support commercial use, allow for any modifications and derivative works, including, but not limited to, distillation for training other LLMs. Please note that:\n- DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Qwen-14B and DeepSeek-R1-Distill-Qwen-32B are derived from [Qwen-2.5 series](https://github.com/QwenLM/Qwen2.5), which are originally licensed under [Apache 2.0 License](https://huggingface.co/Qwen/Qwen2.5-1.5B/blob/main/LICENSE), and now finetuned with 800k samples curated with DeepSeek-R1.\n- DeepSeek-R1-Distill-Llama-8B is derived from Llama3.1-8B-Base and is originally licensed under [llama3.1 license](https://huggingface.co/meta-llama/Llama-3.1-8B/blob/main/LICENSE).\n- DeepSeek-R1-Distill-Llama-70B is derived from Llama3.3-70B-Instruct and is originally licensed under [llama3.3 license](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct/blob/main/LICENSE).\n\n## 8. Citation\n```\n@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\n      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, \n      author={DeepSeek-AI},\n      year={2025},\n      eprint={2501.12948},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2501.12948}, \n}\n\n```\n\n## 9. Contact\nIf you have any questions, please raise an issue or contact us at [service@deepseek.com](service@deepseek.com).\n",
    "card_content": "---\nlicense: mit\nlibrary_name: transformers\n---\n# DeepSeek-R1\n<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align=\"center\">\n  <img src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true\" width=\"60%\" alt=\"DeepSeek-V3\" />\n</div>\n<hr>\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://www.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Homepage\" src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://chat.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/🤖%20Chat-DeepSeek%20R1-536af5?color=536af5&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://huggingface.co/deepseek-ai\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://discord.gg/Tc7c45Zzu5\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Wechat\" src=\"https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://twitter.com/deepseek_ai\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Twitter Follow\" src=\"https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE\" style=\"margin: 2px;\">\n    <img alt=\"License\" src=\"https://img.shields.io/badge/License-MIT-f5de53?&color=f5de53\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n\n<p align=\"center\">\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf\"><b>Paper Link</b>👁️</a>\n</p>\n\n\n## 1. Introduction\n\nWe introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. \nDeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning.\nWith RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors.\nHowever, DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. To address these issues and further enhance reasoning performance,\nwe introduce DeepSeek-R1, which incorporates cold-start data before RL.\nDeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks. \nTo support the research community, we have open-sourced DeepSeek-R1-Zero, DeepSeek-R1, and six dense models distilled from DeepSeek-R1 based on Llama and Qwen. DeepSeek-R1-Distill-Qwen-32B outperforms OpenAI-o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.\n\n**NOTE: Before running DeepSeek-R1 series models locally, we kindly recommend reviewing the [Usage Recommendation](#usage-recommendations) section.**\n\n<p align=\"center\">\n  <img width=\"80%\" src=\"figures/benchmark.jpg\">\n</p>\n\n## 2. Model Summary\n\n---\n\n**Post-Training: Large-Scale Reinforcement Learning on the Base Model**\n\n-  We directly apply reinforcement learning (RL) to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.\n\n-   We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model's reasoning and non-reasoning capabilities.\n    We believe the pipeline will benefit the industry by creating better models. \n\n---\n\n**Distillation: Smaller Models Can Be Powerful Too**\n\n-  We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future. \n- Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.\n\n## 3. Model Downloads\n\n### DeepSeek-R1 Models\n\n<div align=\"center\">\n\n| **Model** | **#Total Params** | **#Activated Params** | **Context Length** | **Download** |\n| :------------: | :------------: | :------------: | :------------: | :------------: |\n| DeepSeek-R1-Zero | 671B | 37B | 128K   | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Zero)   |\n| DeepSeek-R1   | 671B | 37B |  128K   | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1)   |\n\n</div>\n\nDeepSeek-R1-Zero & DeepSeek-R1 are trained based on DeepSeek-V3-Base. \nFor more details regarding the model architecture, please refer to [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) repository.\n\n### DeepSeek-R1-Distill Models\n\n<div align=\"center\">\n\n| **Model** | **Base Model** | **Download** |\n| :------------: | :------------: | :------------: |\n| DeepSeek-R1-Distill-Qwen-1.5B  | [Qwen2.5-Math-1.5B](https://huggingface.co/Qwen/Qwen2.5-Math-1.5B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B)   |\n| DeepSeek-R1-Distill-Qwen-7B  | [Qwen2.5-Math-7B](https://huggingface.co/Qwen/Qwen2.5-Math-7B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B)   |\n| DeepSeek-R1-Distill-Llama-8B  | [Llama-3.1-8B](https://huggingface.co/meta-llama/Llama-3.1-8B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B)   |\n| DeepSeek-R1-Distill-Qwen-14B   | [Qwen2.5-14B](https://huggingface.co/Qwen/Qwen2.5-14B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B)   |\n|DeepSeek-R1-Distill-Qwen-32B  | [Qwen2.5-32B](https://huggingface.co/Qwen/Qwen2.5-32B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B)   |\n| DeepSeek-R1-Distill-Llama-70B  | [Llama-3.3-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B)   |\n\n</div>\n\nDeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1.\nWe slightly change their configs and tokenizers. Please use our setting to run these models.\n\n## 4. Evaluation Results\n\n### DeepSeek-R1-Evaluation\n For all our models, the maximum generation length is set to 32,768 tokens. For benchmarks requiring sampling, we use a temperature of $0.6$, a top-p value of $0.95$, and generate 64 responses per query to estimate pass@1.\n<div align=\"center\">\n\n\n| Category | Benchmark (Metric) | Claude-3.5-Sonnet-1022 | GPT-4o 0513 | DeepSeek V3 | OpenAI o1-mini | OpenAI o1-1217 | DeepSeek R1 |\n|----------|-------------------|----------------------|------------|--------------|----------------|------------|--------------|\n| | Architecture | - | - | MoE | - | - | MoE |\n| | # Activated Params | - | - | 37B | - | - | 37B |\n| | # Total Params | - | - | 671B | - | - | 671B |\n| English | MMLU (Pass@1) | 88.3 | 87.2 | 88.5 | 85.2 | **91.8** | 90.8 |\n| | MMLU-Redux (EM) | 88.9 | 88.0 | 89.1 | 86.7 | - | **92.9** |\n| | MMLU-Pro (EM) | 78.0 | 72.6 | 75.9 | 80.3 | - | **84.0** |\n| | DROP (3-shot F1) | 88.3 | 83.7 | 91.6 | 83.9 | 90.2 | **92.2** |\n| | IF-Eval (Prompt Strict) | **86.5** | 84.3 | 86.1 | 84.8 | - | 83.3 |\n| | GPQA-Diamond (Pass@1) | 65.0 | 49.9 | 59.1 | 60.0 | **75.7** | 71.5 |\n| | SimpleQA (Correct) | 28.4 | 38.2 | 24.9 | 7.0 | **47.0** | 30.1 |\n| | FRAMES (Acc.) | 72.5 | 80.5 | 73.3 | 76.9 | - | **82.5** |\n| | AlpacaEval2.0 (LC-winrate) | 52.0 | 51.1 | 70.0 | 57.8 | - | **87.6** |\n| | ArenaHard (GPT-4-1106) | 85.2 | 80.4 | 85.5 | 92.0 | - | **92.3** |\n| Code | LiveCodeBench (Pass@1-COT) | 33.8 | 34.2 | - | 53.8 | 63.4 | **65.9** |\n| | Codeforces (Percentile) | 20.3 | 23.6 | 58.7 | 93.4 | **96.6** | 96.3 |\n| | Codeforces (Rating) | 717 | 759 | 1134 | 1820 | **2061** | 2029 |\n| | SWE Verified (Resolved) | **50.8** | 38.8 | 42.0 | 41.6 | 48.9 | 49.2 |\n| | Aider-Polyglot (Acc.) | 45.3 | 16.0 | 49.6 | 32.9 | **61.7** | 53.3 |\n| Math | AIME 2024 (Pass@1) | 16.0 | 9.3 | 39.2 | 63.6 | 79.2 | **79.8** |\n| | MATH-500 (Pass@1) | 78.3 | 74.6 | 90.2 | 90.0 | 96.4 | **97.3** |\n| | CNMO 2024 (Pass@1) | 13.1 | 10.8 | 43.2 | 67.6 | - | **78.8** |\n| Chinese | CLUEWSC (EM) | 85.4 | 87.9 | 90.9 | 89.9 | - | **92.8** |\n| | C-Eval (EM) | 76.7 | 76.0 | 86.5 | 68.9 | - | **91.8** |\n| | C-SimpleQA (Correct) | 55.4 | 58.7 | **68.0** | 40.3 | - | 63.7 |\n\n</div>\n\n\n### Distilled Model Evaluation\n\n\n<div align=\"center\">\n\n| Model                                    | AIME 2024 pass@1 | AIME 2024 cons@64 | MATH-500 pass@1 | GPQA Diamond pass@1 | LiveCodeBench pass@1 | CodeForces rating |\n|------------------------------------------|------------------|-------------------|-----------------|----------------------|----------------------|-------------------|\n| GPT-4o-0513                          | 9.3              | 13.4              | 74.6            | 49.9                 | 32.9                 | 759               |\n| Claude-3.5-Sonnet-1022             | 16.0             | 26.7                 | 78.3            | 65.0                 | 38.9                 | 717               |\n| o1-mini                              | 63.6             | 80.0              | 90.0            | 60.0                 | 53.8                 | **1820**          |\n| QwQ-32B-Preview                              | 44.0             | 60.0                 | 90.6            | 54.5               | 41.9                 | 1316              |\n| DeepSeek-R1-Distill-Qwen-1.5B       | 28.9             | 52.7              | 83.9            | 33.8                 | 16.9                 | 954               |\n| DeepSeek-R1-Distill-Qwen-7B          | 55.5             | 83.3              | 92.8            | 49.1                 | 37.6                 | 1189              |\n| DeepSeek-R1-Distill-Qwen-14B         | 69.7             | 80.0              | 93.9            | 59.1                 | 53.1                 | 1481              |\n| DeepSeek-R1-Distill-Qwen-32B        | **72.6**         | 83.3              | 94.3            | 62.1                 | 57.2                 | 1691              |\n| DeepSeek-R1-Distill-Llama-8B         | 50.4             | 80.0              | 89.1            | 49.0                 | 39.6                 | 1205              |\n| DeepSeek-R1-Distill-Llama-70B        | 70.0             | **86.7**          | **94.5**        | **65.2**             | **57.5**             | 1633              |\n\n</div>\n\n\n## 5. Chat Website & API Platform\nYou can chat with DeepSeek-R1 on DeepSeek's official website: [chat.deepseek.com](https://chat.deepseek.com), and switch on the button \"DeepThink\"\n\nWe also provide OpenAI-Compatible API at DeepSeek Platform: [platform.deepseek.com](https://platform.deepseek.com/)\n\n## 6. How to Run Locally\n\n### DeepSeek-R1 Models\n\nPlease visit [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) repo for more information about running DeepSeek-R1 locally.\n\n**NOTE: Hugging Face's Transformers has not been directly supported yet.**\n\n### DeepSeek-R1-Distill Models\n\nDeepSeek-R1-Distill models can be utilized in the same manner as Qwen or Llama models.\n\nFor instance, you can easily start a service using [vLLM](https://github.com/vllm-project/vllm):\n\n```shell\nvllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --tensor-parallel-size 2 --max-model-len 32768 --enforce-eager\n```\n\nYou can also easily start a service using [SGLang](https://github.com/sgl-project/sglang)\n\n```bash\npython3 -m sglang.launch_server --model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --trust-remote-code --tp 2\n```\n\n### Usage Recommendations\n\n**We recommend adhering to the following configurations when utilizing the DeepSeek-R1 series models, including benchmarking, to achieve the expected performance:**\n\n1. Set the temperature within the range of 0.5-0.7 (0.6 is recommended) to prevent endless repetitions or incoherent outputs.\n2. **Avoid adding a system prompt; all instructions should be contained within the user prompt.**\n3. For mathematical problems, it is advisable to include a directive in your prompt such as: \"Please reason step by step, and put your final answer within \\boxed{}.\"\n4. When evaluating model performance, it is recommended to conduct multiple tests and average the results.\n\nAdditionally, we have observed that the DeepSeek-R1 series models tend to bypass thinking pattern (i.e., outputting \"\\<think\\>\\n\\n\\</think\\>\") when responding to certain queries, which can adversely affect the model's performance.\n**To ensure that the model engages in thorough reasoning, we recommend enforcing the model to initiate its response with \"\\<think\\>\\n\" at the beginning of every output.**\n\n## 7. License\nThis code repository and the model weights are licensed under the [MIT License](https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE).\nDeepSeek-R1 series support commercial use, allow for any modifications and derivative works, including, but not limited to, distillation for training other LLMs. Please note that:\n- DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Qwen-14B and DeepSeek-R1-Distill-Qwen-32B are derived from [Qwen-2.5 series](https://github.com/QwenLM/Qwen2.5), which are originally licensed under [Apache 2.0 License](https://huggingface.co/Qwen/Qwen2.5-1.5B/blob/main/LICENSE), and now finetuned with 800k samples curated with DeepSeek-R1.\n- DeepSeek-R1-Distill-Llama-8B is derived from Llama3.1-8B-Base and is originally licensed under [llama3.1 license](https://huggingface.co/meta-llama/Llama-3.1-8B/blob/main/LICENSE).\n- DeepSeek-R1-Distill-Llama-70B is derived from Llama3.3-70B-Instruct and is originally licensed under [llama3.3 license](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct/blob/main/LICENSE).\n\n## 8. Citation\n```\n@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\n      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, \n      author={DeepSeek-AI},\n      year={2025},\n      eprint={2501.12948},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2501.12948}, \n}\n\n```\n\n## 9. Contact\nIf you have any questions, please raise an issue or contact us at [service@deepseek.com](service@deepseek.com).\n",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModelForCausalLM",
      "custom_class": null,
      "pipeline_tag": "text-generation",
      "processor": "AutoTokenizer"
    },
    "safetensors": {
      "parameters": {
        "BF16": 32763876352
      },
      "total": 32763876352
    },
    "model_index": null,
    "trending_score": null
  },
  {
    "model_id": "peft-internal-testing/tiny-dummy-qwen2",
    "model_name": "peft-internal-testing/tiny-dummy-qwen2",
    "author": "peft-internal-testing",
    "downloads": 1686796,
    "downloads_all_time": null,
    "likes": 0,
    "tags": [
      "transformers",
      "safetensors",
      "qwen2",
      "text-generation",
      "conversational",
      "arxiv:1910.09700",
      "autotrain_compatible",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/peft-internal-testing/tiny-dummy-qwen2",
    "dependencies": [
      [
        "transformers",
        null
      ]
    ],
    "last_modified": "2024-07-04T10:52:09+00:00",
    "created_at": "2024-07-04T10:15:41+00:00",
    "analysis_date": "2025-03-22T00:42:06.434666",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "qwen2",
    "pipeline_tag": "text-generation",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "library_name": "transformers",
      "tags": []
    },
    "card_text": "\n# Model Card for Model ID\n\n<!-- Provide a quick summary of what the model is/does. -->\n\n\n\n## Model Details\n\n### Model Description\n\n<!-- Provide a longer summary of what this model is. -->\n\nThis is the model card of a 🤗 transformers model that has been pushed on the Hub. This model card has been automatically generated.\n\n- **Developed by:** [More Information Needed]\n- **Funded by [optional]:** [More Information Needed]\n- **Shared by [optional]:** [More Information Needed]\n- **Model type:** [More Information Needed]\n- **Language(s) (NLP):** [More Information Needed]\n- **License:** [More Information Needed]\n- **Finetuned from model [optional]:** [More Information Needed]\n\n### Model Sources [optional]\n\n<!-- Provide the basic links for the model. -->\n\n- **Repository:** [More Information Needed]\n- **Paper [optional]:** [More Information Needed]\n- **Demo [optional]:** [More Information Needed]\n\n## Uses\n\n<!-- Address questions around how the model is intended to be used, including the foreseeable users of the model and those affected by the model. -->\n\n### Direct Use\n\n<!-- This section is for the model use without fine-tuning or plugging into a larger ecosystem/app. -->\n\n[More Information Needed]\n\n### Downstream Use [optional]\n\n<!-- This section is for the model use when fine-tuned for a task, or when plugged into a larger ecosystem/app -->\n\n[More Information Needed]\n\n### Out-of-Scope Use\n\n<!-- This section addresses misuse, malicious use, and uses that the model will not work well for. -->\n\n[More Information Needed]\n\n## Bias, Risks, and Limitations\n\n<!-- This section is meant to convey both technical and sociotechnical limitations. -->\n\n[More Information Needed]\n\n### Recommendations\n\n<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->\n\nUsers (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.\n\n## How to Get Started with the Model\n\nUse the code below to get started with the model.\n\n[More Information Needed]\n\n## Training Details\n\n### Training Data\n\n<!-- This should link to a Dataset Card, perhaps with a short stub of information on what the training data is all about as well as documentation related to data pre-processing or additional filtering. -->\n\n[More Information Needed]\n\n### Training Procedure\n\n<!-- This relates heavily to the Technical Specifications. Content here should link to that section when it is relevant to the training procedure. -->\n\n#### Preprocessing [optional]\n\n[More Information Needed]\n\n\n#### Training Hyperparameters\n\n- **Training regime:** [More Information Needed] <!--fp32, fp16 mixed precision, bf16 mixed precision, bf16 non-mixed precision, fp16 non-mixed precision, fp8 mixed precision -->\n\n#### Speeds, Sizes, Times [optional]\n\n<!-- This section provides information about throughput, start/end time, checkpoint size if relevant, etc. -->\n\n[More Information Needed]\n\n## Evaluation\n\n<!-- This section describes the evaluation protocols and provides the results. -->\n\n### Testing Data, Factors & Metrics\n\n#### Testing Data\n\n<!-- This should link to a Dataset Card if possible. -->\n\n[More Information Needed]\n\n#### Factors\n\n<!-- These are the things the evaluation is disaggregating by, e.g., subpopulations or domains. -->\n\n[More Information Needed]\n\n#### Metrics\n\n<!-- These are the evaluation metrics being used, ideally with a description of why. -->\n\n[More Information Needed]\n\n### Results\n\n[More Information Needed]\n\n#### Summary\n\n\n\n## Model Examination [optional]\n\n<!-- Relevant interpretability work for the model goes here -->\n\n[More Information Needed]\n\n## Environmental Impact\n\n<!-- Total emissions (in grams of CO2eq) and additional considerations, such as electricity usage, go here. Edit the suggested text below accordingly -->\n\nCarbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).\n\n- **Hardware Type:** [More Information Needed]\n- **Hours used:** [More Information Needed]\n- **Cloud Provider:** [More Information Needed]\n- **Compute Region:** [More Information Needed]\n- **Carbon Emitted:** [More Information Needed]\n\n## Technical Specifications [optional]\n\n### Model Architecture and Objective\n\n[More Information Needed]\n\n### Compute Infrastructure\n\n[More Information Needed]\n\n#### Hardware\n\n[More Information Needed]\n\n#### Software\n\n[More Information Needed]\n\n## Citation [optional]\n\n<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->\n\n**BibTeX:**\n\n[More Information Needed]\n\n**APA:**\n\n[More Information Needed]\n\n## Glossary [optional]\n\n<!-- If relevant, include terms and calculations in this section that can help readers understand the model or model card. -->\n\n[More Information Needed]\n\n## More Information [optional]\n\n[More Information Needed]\n\n## Model Card Authors [optional]\n\n[More Information Needed]\n\n## Model Card Contact\n\n[More Information Needed]",
    "card_content": "---\nlibrary_name: transformers\ntags: []\n---\n\n# Model Card for Model ID\n\n<!-- Provide a quick summary of what the model is/does. -->\n\n\n\n## Model Details\n\n### Model Description\n\n<!-- Provide a longer summary of what this model is. -->\n\nThis is the model card of a 🤗 transformers model that has been pushed on the Hub. This model card has been automatically generated.\n\n- **Developed by:** [More Information Needed]\n- **Funded by [optional]:** [More Information Needed]\n- **Shared by [optional]:** [More Information Needed]\n- **Model type:** [More Information Needed]\n- **Language(s) (NLP):** [More Information Needed]\n- **License:** [More Information Needed]\n- **Finetuned from model [optional]:** [More Information Needed]\n\n### Model Sources [optional]\n\n<!-- Provide the basic links for the model. -->\n\n- **Repository:** [More Information Needed]\n- **Paper [optional]:** [More Information Needed]\n- **Demo [optional]:** [More Information Needed]\n\n## Uses\n\n<!-- Address questions around how the model is intended to be used, including the foreseeable users of the model and those affected by the model. -->\n\n### Direct Use\n\n<!-- This section is for the model use without fine-tuning or plugging into a larger ecosystem/app. -->\n\n[More Information Needed]\n\n### Downstream Use [optional]\n\n<!-- This section is for the model use when fine-tuned for a task, or when plugged into a larger ecosystem/app -->\n\n[More Information Needed]\n\n### Out-of-Scope Use\n\n<!-- This section addresses misuse, malicious use, and uses that the model will not work well for. -->\n\n[More Information Needed]\n\n## Bias, Risks, and Limitations\n\n<!-- This section is meant to convey both technical and sociotechnical limitations. -->\n\n[More Information Needed]\n\n### Recommendations\n\n<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->\n\nUsers (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.\n\n## How to Get Started with the Model\n\nUse the code below to get started with the model.\n\n[More Information Needed]\n\n## Training Details\n\n### Training Data\n\n<!-- This should link to a Dataset Card, perhaps with a short stub of information on what the training data is all about as well as documentation related to data pre-processing or additional filtering. -->\n\n[More Information Needed]\n\n### Training Procedure\n\n<!-- This relates heavily to the Technical Specifications. Content here should link to that section when it is relevant to the training procedure. -->\n\n#### Preprocessing [optional]\n\n[More Information Needed]\n\n\n#### Training Hyperparameters\n\n- **Training regime:** [More Information Needed] <!--fp32, fp16 mixed precision, bf16 mixed precision, bf16 non-mixed precision, fp16 non-mixed precision, fp8 mixed precision -->\n\n#### Speeds, Sizes, Times [optional]\n\n<!-- This section provides information about throughput, start/end time, checkpoint size if relevant, etc. -->\n\n[More Information Needed]\n\n## Evaluation\n\n<!-- This section describes the evaluation protocols and provides the results. -->\n\n### Testing Data, Factors & Metrics\n\n#### Testing Data\n\n<!-- This should link to a Dataset Card if possible. -->\n\n[More Information Needed]\n\n#### Factors\n\n<!-- These are the things the evaluation is disaggregating by, e.g., subpopulations or domains. -->\n\n[More Information Needed]\n\n#### Metrics\n\n<!-- These are the evaluation metrics being used, ideally with a description of why. -->\n\n[More Information Needed]\n\n### Results\n\n[More Information Needed]\n\n#### Summary\n\n\n\n## Model Examination [optional]\n\n<!-- Relevant interpretability work for the model goes here -->\n\n[More Information Needed]\n\n## Environmental Impact\n\n<!-- Total emissions (in grams of CO2eq) and additional considerations, such as electricity usage, go here. Edit the suggested text below accordingly -->\n\nCarbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).\n\n- **Hardware Type:** [More Information Needed]\n- **Hours used:** [More Information Needed]\n- **Cloud Provider:** [More Information Needed]\n- **Compute Region:** [More Information Needed]\n- **Carbon Emitted:** [More Information Needed]\n\n## Technical Specifications [optional]\n\n### Model Architecture and Objective\n\n[More Information Needed]\n\n### Compute Infrastructure\n\n[More Information Needed]\n\n#### Hardware\n\n[More Information Needed]\n\n#### Software\n\n[More Information Needed]\n\n## Citation [optional]\n\n<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->\n\n**BibTeX:**\n\n[More Information Needed]\n\n**APA:**\n\n[More Information Needed]\n\n## Glossary [optional]\n\n<!-- If relevant, include terms and calculations in this section that can help readers understand the model or model card. -->\n\n[More Information Needed]\n\n## More Information [optional]\n\n[More Information Needed]\n\n## Model Card Authors [optional]\n\n[More Information Needed]\n\n## Model Card Contact\n\n[More Information Needed]",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModelForCausalLM",
      "custom_class": null,
      "pipeline_tag": "text-generation",
      "processor": "AutoTokenizer"
    },
    "safetensors": {
      "parameters": {
        "F32": 1217480
      },
      "total": 1217480
    },
    "model_index": null,
    "trending_score": null
  },
  {
    "model_id": "deepseek-ai/DeepSeek-R1",
    "model_name": "deepseek-ai/DeepSeek-R1",
    "author": "deepseek-ai",
    "downloads": 1680994,
    "downloads_all_time": null,
    "likes": 11545,
    "tags": [
      "transformers",
      "safetensors",
      "deepseek_v3",
      "text-generation",
      "conversational",
      "custom_code",
      "arxiv:2501.12948",
      "license:mit",
      "autotrain_compatible",
      "fp8",
      "region:us"
    ],
    "card_url": "https://huggingface.co/deepseek-ai/DeepSeek-R1",
    "dependencies": [
      [
        "transformers",
        null
      ]
    ],
    "last_modified": "2025-02-24T03:30:31+00:00",
    "created_at": "2025-01-20T03:46:07+00:00",
    "analysis_date": "2025-03-22T00:42:08.073238",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "deepseek_v3",
    "pipeline_tag": "text-generation",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "license": "mit",
      "library_name": "transformers"
    },
    "card_text": "# DeepSeek-R1\n<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align=\"center\">\n  <img src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true\" width=\"60%\" alt=\"DeepSeek-V3\" />\n</div>\n<hr>\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://www.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Homepage\" src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://chat.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/🤖%20Chat-DeepSeek%20R1-536af5?color=536af5&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://huggingface.co/deepseek-ai\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://discord.gg/Tc7c45Zzu5\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Wechat\" src=\"https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://twitter.com/deepseek_ai\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Twitter Follow\" src=\"https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE\" style=\"margin: 2px;\">\n    <img alt=\"License\" src=\"https://img.shields.io/badge/License-MIT-f5de53?&color=f5de53\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n\n<p align=\"center\">\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf\"><b>Paper Link</b>👁️</a>\n</p>\n\n\n## 1. Introduction\n\nWe introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. \nDeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning.\nWith RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors.\nHowever, DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. To address these issues and further enhance reasoning performance,\nwe introduce DeepSeek-R1, which incorporates cold-start data before RL.\nDeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks. \nTo support the research community, we have open-sourced DeepSeek-R1-Zero, DeepSeek-R1, and six dense models distilled from DeepSeek-R1 based on Llama and Qwen. DeepSeek-R1-Distill-Qwen-32B outperforms OpenAI-o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.\n\n**NOTE: Before running DeepSeek-R1 series models locally, we kindly recommend reviewing the [Usage Recommendation](#usage-recommendations) section.**\n\n<p align=\"center\">\n  <img width=\"80%\" src=\"figures/benchmark.jpg\">\n</p>\n\n## 2. Model Summary\n\n---\n\n**Post-Training: Large-Scale Reinforcement Learning on the Base Model**\n\n-  We directly apply reinforcement learning (RL) to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.\n\n-   We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model's reasoning and non-reasoning capabilities.\n    We believe the pipeline will benefit the industry by creating better models. \n\n---\n\n**Distillation: Smaller Models Can Be Powerful Too**\n\n-  We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future. \n- Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.\n\n## 3. Model Downloads\n\n### DeepSeek-R1 Models\n\n<div align=\"center\">\n\n| **Model** | **#Total Params** | **#Activated Params** | **Context Length** | **Download** |\n| :------------: | :------------: | :------------: | :------------: | :------------: |\n| DeepSeek-R1-Zero | 671B | 37B | 128K   | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Zero)   |\n| DeepSeek-R1   | 671B | 37B |  128K   | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1)   |\n\n</div>\n\nDeepSeek-R1-Zero & DeepSeek-R1 are trained based on DeepSeek-V3-Base. \nFor more details regarding the model architecture, please refer to [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) repository.\n\n### DeepSeek-R1-Distill Models\n\n<div align=\"center\">\n\n| **Model** | **Base Model** | **Download** |\n| :------------: | :------------: | :------------: |\n| DeepSeek-R1-Distill-Qwen-1.5B  | [Qwen2.5-Math-1.5B](https://huggingface.co/Qwen/Qwen2.5-Math-1.5B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B)   |\n| DeepSeek-R1-Distill-Qwen-7B  | [Qwen2.5-Math-7B](https://huggingface.co/Qwen/Qwen2.5-Math-7B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B)   |\n| DeepSeek-R1-Distill-Llama-8B  | [Llama-3.1-8B](https://huggingface.co/meta-llama/Llama-3.1-8B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B)   |\n| DeepSeek-R1-Distill-Qwen-14B   | [Qwen2.5-14B](https://huggingface.co/Qwen/Qwen2.5-14B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B)   |\n|DeepSeek-R1-Distill-Qwen-32B  | [Qwen2.5-32B](https://huggingface.co/Qwen/Qwen2.5-32B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B)   |\n| DeepSeek-R1-Distill-Llama-70B  | [Llama-3.3-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B)   |\n\n</div>\n\nDeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1.\nWe slightly change their configs and tokenizers. Please use our setting to run these models.\n\n## 4. Evaluation Results\n\n### DeepSeek-R1-Evaluation\n For all our models, the maximum generation length is set to 32,768 tokens. For benchmarks requiring sampling, we use a temperature of $0.6$, a top-p value of $0.95$, and generate 64 responses per query to estimate pass@1.\n<div align=\"center\">\n\n\n| Category | Benchmark (Metric) | Claude-3.5-Sonnet-1022 | GPT-4o 0513 | DeepSeek V3 | OpenAI o1-mini | OpenAI o1-1217 | DeepSeek R1 |\n|----------|-------------------|----------------------|------------|--------------|----------------|------------|--------------|\n| | Architecture | - | - | MoE | - | - | MoE |\n| | # Activated Params | - | - | 37B | - | - | 37B |\n| | # Total Params | - | - | 671B | - | - | 671B |\n| English | MMLU (Pass@1) | 88.3 | 87.2 | 88.5 | 85.2 | **91.8** | 90.8 |\n| | MMLU-Redux (EM) | 88.9 | 88.0 | 89.1 | 86.7 | - | **92.9** |\n| | MMLU-Pro (EM) | 78.0 | 72.6 | 75.9 | 80.3 | - | **84.0** |\n| | DROP (3-shot F1) | 88.3 | 83.7 | 91.6 | 83.9 | 90.2 | **92.2** |\n| | IF-Eval (Prompt Strict) | **86.5** | 84.3 | 86.1 | 84.8 | - | 83.3 |\n| | GPQA-Diamond (Pass@1) | 65.0 | 49.9 | 59.1 | 60.0 | **75.7** | 71.5 |\n| | SimpleQA (Correct) | 28.4 | 38.2 | 24.9 | 7.0 | **47.0** | 30.1 |\n| | FRAMES (Acc.) | 72.5 | 80.5 | 73.3 | 76.9 | - | **82.5** |\n| | AlpacaEval2.0 (LC-winrate) | 52.0 | 51.1 | 70.0 | 57.8 | - | **87.6** |\n| | ArenaHard (GPT-4-1106) | 85.2 | 80.4 | 85.5 | 92.0 | - | **92.3** |\n| Code | LiveCodeBench (Pass@1-COT) | 33.8 | 34.2 | - | 53.8 | 63.4 | **65.9** |\n| | Codeforces (Percentile) | 20.3 | 23.6 | 58.7 | 93.4 | **96.6** | 96.3 |\n| | Codeforces (Rating) | 717 | 759 | 1134 | 1820 | **2061** | 2029 |\n| | SWE Verified (Resolved) | **50.8** | 38.8 | 42.0 | 41.6 | 48.9 | 49.2 |\n| | Aider-Polyglot (Acc.) | 45.3 | 16.0 | 49.6 | 32.9 | **61.7** | 53.3 |\n| Math | AIME 2024 (Pass@1) | 16.0 | 9.3 | 39.2 | 63.6 | 79.2 | **79.8** |\n| | MATH-500 (Pass@1) | 78.3 | 74.6 | 90.2 | 90.0 | 96.4 | **97.3** |\n| | CNMO 2024 (Pass@1) | 13.1 | 10.8 | 43.2 | 67.6 | - | **78.8** |\n| Chinese | CLUEWSC (EM) | 85.4 | 87.9 | 90.9 | 89.9 | - | **92.8** |\n| | C-Eval (EM) | 76.7 | 76.0 | 86.5 | 68.9 | - | **91.8** |\n| | C-SimpleQA (Correct) | 55.4 | 58.7 | **68.0** | 40.3 | - | 63.7 |\n\n</div>\n\n\n### Distilled Model Evaluation\n\n\n<div align=\"center\">\n\n| Model                                    | AIME 2024 pass@1 | AIME 2024 cons@64 | MATH-500 pass@1 | GPQA Diamond pass@1 | LiveCodeBench pass@1 | CodeForces rating |\n|------------------------------------------|------------------|-------------------|-----------------|----------------------|----------------------|-------------------|\n| GPT-4o-0513                          | 9.3              | 13.4              | 74.6            | 49.9                 | 32.9                 | 759               |\n| Claude-3.5-Sonnet-1022             | 16.0             | 26.7                 | 78.3            | 65.0                 | 38.9                 | 717               |\n| o1-mini                              | 63.6             | 80.0              | 90.0            | 60.0                 | 53.8                 | **1820**          |\n| QwQ-32B-Preview                              | 44.0             | 60.0                 | 90.6            | 54.5               | 41.9                 | 1316              |\n| DeepSeek-R1-Distill-Qwen-1.5B       | 28.9             | 52.7              | 83.9            | 33.8                 | 16.9                 | 954               |\n| DeepSeek-R1-Distill-Qwen-7B          | 55.5             | 83.3              | 92.8            | 49.1                 | 37.6                 | 1189              |\n| DeepSeek-R1-Distill-Qwen-14B         | 69.7             | 80.0              | 93.9            | 59.1                 | 53.1                 | 1481              |\n| DeepSeek-R1-Distill-Qwen-32B        | **72.6**         | 83.3              | 94.3            | 62.1                 | 57.2                 | 1691              |\n| DeepSeek-R1-Distill-Llama-8B         | 50.4             | 80.0              | 89.1            | 49.0                 | 39.6                 | 1205              |\n| DeepSeek-R1-Distill-Llama-70B        | 70.0             | **86.7**          | **94.5**        | **65.2**             | **57.5**             | 1633              |\n\n</div>\n\n\n## 5. Chat Website & API Platform\nYou can chat with DeepSeek-R1 on DeepSeek's official website: [chat.deepseek.com](https://chat.deepseek.com), and switch on the button \"DeepThink\"\n\nWe also provide OpenAI-Compatible API at DeepSeek Platform: [platform.deepseek.com](https://platform.deepseek.com/)\n\n## 6. How to Run Locally\n\n### DeepSeek-R1 Models\n\nPlease visit [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) repo for more information about running DeepSeek-R1 locally.\n\n**NOTE: Hugging Face's Transformers has not been directly supported yet.**\n\n### DeepSeek-R1-Distill Models\n\nDeepSeek-R1-Distill models can be utilized in the same manner as Qwen or Llama models.\n\nFor instance, you can easily start a service using [vLLM](https://github.com/vllm-project/vllm):\n\n```shell\nvllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --tensor-parallel-size 2 --max-model-len 32768 --enforce-eager\n```\n\nYou can also easily start a service using [SGLang](https://github.com/sgl-project/sglang)\n\n```bash\npython3 -m sglang.launch_server --model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --trust-remote-code --tp 2\n```\n\n### Usage Recommendations\n\n**We recommend adhering to the following configurations when utilizing the DeepSeek-R1 series models, including benchmarking, to achieve the expected performance:**\n\n1. Set the temperature within the range of 0.5-0.7 (0.6 is recommended) to prevent endless repetitions or incoherent outputs.\n2. **Avoid adding a system prompt; all instructions should be contained within the user prompt.**\n3. For mathematical problems, it is advisable to include a directive in your prompt such as: \"Please reason step by step, and put your final answer within \\boxed{}.\"\n4. When evaluating model performance, it is recommended to conduct multiple tests and average the results.\n\nAdditionally, we have observed that the DeepSeek-R1 series models tend to bypass thinking pattern (i.e., outputting \"\\<think\\>\\n\\n\\</think\\>\") when responding to certain queries, which can adversely affect the model's performance.\n**To ensure that the model engages in thorough reasoning, we recommend enforcing the model to initiate its response with \"\\<think\\>\\n\" at the beginning of every output.**\n\n## 7. License\nThis code repository and the model weights are licensed under the [MIT License](https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE).\nDeepSeek-R1 series support commercial use, allow for any modifications and derivative works, including, but not limited to, distillation for training other LLMs. Please note that:\n- DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Qwen-14B and DeepSeek-R1-Distill-Qwen-32B are derived from [Qwen-2.5 series](https://github.com/QwenLM/Qwen2.5), which are originally licensed under [Apache 2.0 License](https://huggingface.co/Qwen/Qwen2.5-1.5B/blob/main/LICENSE), and now finetuned with 800k samples curated with DeepSeek-R1.\n- DeepSeek-R1-Distill-Llama-8B is derived from Llama3.1-8B-Base and is originally licensed under [llama3.1 license](https://huggingface.co/meta-llama/Llama-3.1-8B/blob/main/LICENSE).\n- DeepSeek-R1-Distill-Llama-70B is derived from Llama3.3-70B-Instruct and is originally licensed under [llama3.3 license](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct/blob/main/LICENSE).\n\n## 8. Citation\n```\n@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\n      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, \n      author={DeepSeek-AI},\n      year={2025},\n      eprint={2501.12948},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2501.12948}, \n}\n\n```\n\n## 9. Contact\nIf you have any questions, please raise an issue or contact us at [service@deepseek.com](service@deepseek.com).\n",
    "card_content": "---\nlicense: mit\nlibrary_name: transformers\n---\n# DeepSeek-R1\n<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align=\"center\">\n  <img src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true\" width=\"60%\" alt=\"DeepSeek-V3\" />\n</div>\n<hr>\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://www.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Homepage\" src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://chat.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/🤖%20Chat-DeepSeek%20R1-536af5?color=536af5&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://huggingface.co/deepseek-ai\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://discord.gg/Tc7c45Zzu5\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Wechat\" src=\"https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://twitter.com/deepseek_ai\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Twitter Follow\" src=\"https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE\" style=\"margin: 2px;\">\n    <img alt=\"License\" src=\"https://img.shields.io/badge/License-MIT-f5de53?&color=f5de53\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n\n<p align=\"center\">\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf\"><b>Paper Link</b>👁️</a>\n</p>\n\n\n## 1. Introduction\n\nWe introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. \nDeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning.\nWith RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors.\nHowever, DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. To address these issues and further enhance reasoning performance,\nwe introduce DeepSeek-R1, which incorporates cold-start data before RL.\nDeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks. \nTo support the research community, we have open-sourced DeepSeek-R1-Zero, DeepSeek-R1, and six dense models distilled from DeepSeek-R1 based on Llama and Qwen. DeepSeek-R1-Distill-Qwen-32B outperforms OpenAI-o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.\n\n**NOTE: Before running DeepSeek-R1 series models locally, we kindly recommend reviewing the [Usage Recommendation](#usage-recommendations) section.**\n\n<p align=\"center\">\n  <img width=\"80%\" src=\"figures/benchmark.jpg\">\n</p>\n\n## 2. Model Summary\n\n---\n\n**Post-Training: Large-Scale Reinforcement Learning on the Base Model**\n\n-  We directly apply reinforcement learning (RL) to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.\n\n-   We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model's reasoning and non-reasoning capabilities.\n    We believe the pipeline will benefit the industry by creating better models. \n\n---\n\n**Distillation: Smaller Models Can Be Powerful Too**\n\n-  We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future. \n- Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.\n\n## 3. Model Downloads\n\n### DeepSeek-R1 Models\n\n<div align=\"center\">\n\n| **Model** | **#Total Params** | **#Activated Params** | **Context Length** | **Download** |\n| :------------: | :------------: | :------------: | :------------: | :------------: |\n| DeepSeek-R1-Zero | 671B | 37B | 128K   | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Zero)   |\n| DeepSeek-R1   | 671B | 37B |  128K   | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1)   |\n\n</div>\n\nDeepSeek-R1-Zero & DeepSeek-R1 are trained based on DeepSeek-V3-Base. \nFor more details regarding the model architecture, please refer to [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) repository.\n\n### DeepSeek-R1-Distill Models\n\n<div align=\"center\">\n\n| **Model** | **Base Model** | **Download** |\n| :------------: | :------------: | :------------: |\n| DeepSeek-R1-Distill-Qwen-1.5B  | [Qwen2.5-Math-1.5B](https://huggingface.co/Qwen/Qwen2.5-Math-1.5B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B)   |\n| DeepSeek-R1-Distill-Qwen-7B  | [Qwen2.5-Math-7B](https://huggingface.co/Qwen/Qwen2.5-Math-7B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B)   |\n| DeepSeek-R1-Distill-Llama-8B  | [Llama-3.1-8B](https://huggingface.co/meta-llama/Llama-3.1-8B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B)   |\n| DeepSeek-R1-Distill-Qwen-14B   | [Qwen2.5-14B](https://huggingface.co/Qwen/Qwen2.5-14B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B)   |\n|DeepSeek-R1-Distill-Qwen-32B  | [Qwen2.5-32B](https://huggingface.co/Qwen/Qwen2.5-32B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B)   |\n| DeepSeek-R1-Distill-Llama-70B  | [Llama-3.3-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B)   |\n\n</div>\n\nDeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1.\nWe slightly change their configs and tokenizers. Please use our setting to run these models.\n\n## 4. Evaluation Results\n\n### DeepSeek-R1-Evaluation\n For all our models, the maximum generation length is set to 32,768 tokens. For benchmarks requiring sampling, we use a temperature of $0.6$, a top-p value of $0.95$, and generate 64 responses per query to estimate pass@1.\n<div align=\"center\">\n\n\n| Category | Benchmark (Metric) | Claude-3.5-Sonnet-1022 | GPT-4o 0513 | DeepSeek V3 | OpenAI o1-mini | OpenAI o1-1217 | DeepSeek R1 |\n|----------|-------------------|----------------------|------------|--------------|----------------|------------|--------------|\n| | Architecture | - | - | MoE | - | - | MoE |\n| | # Activated Params | - | - | 37B | - | - | 37B |\n| | # Total Params | - | - | 671B | - | - | 671B |\n| English | MMLU (Pass@1) | 88.3 | 87.2 | 88.5 | 85.2 | **91.8** | 90.8 |\n| | MMLU-Redux (EM) | 88.9 | 88.0 | 89.1 | 86.7 | - | **92.9** |\n| | MMLU-Pro (EM) | 78.0 | 72.6 | 75.9 | 80.3 | - | **84.0** |\n| | DROP (3-shot F1) | 88.3 | 83.7 | 91.6 | 83.9 | 90.2 | **92.2** |\n| | IF-Eval (Prompt Strict) | **86.5** | 84.3 | 86.1 | 84.8 | - | 83.3 |\n| | GPQA-Diamond (Pass@1) | 65.0 | 49.9 | 59.1 | 60.0 | **75.7** | 71.5 |\n| | SimpleQA (Correct) | 28.4 | 38.2 | 24.9 | 7.0 | **47.0** | 30.1 |\n| | FRAMES (Acc.) | 72.5 | 80.5 | 73.3 | 76.9 | - | **82.5** |\n| | AlpacaEval2.0 (LC-winrate) | 52.0 | 51.1 | 70.0 | 57.8 | - | **87.6** |\n| | ArenaHard (GPT-4-1106) | 85.2 | 80.4 | 85.5 | 92.0 | - | **92.3** |\n| Code | LiveCodeBench (Pass@1-COT) | 33.8 | 34.2 | - | 53.8 | 63.4 | **65.9** |\n| | Codeforces (Percentile) | 20.3 | 23.6 | 58.7 | 93.4 | **96.6** | 96.3 |\n| | Codeforces (Rating) | 717 | 759 | 1134 | 1820 | **2061** | 2029 |\n| | SWE Verified (Resolved) | **50.8** | 38.8 | 42.0 | 41.6 | 48.9 | 49.2 |\n| | Aider-Polyglot (Acc.) | 45.3 | 16.0 | 49.6 | 32.9 | **61.7** | 53.3 |\n| Math | AIME 2024 (Pass@1) | 16.0 | 9.3 | 39.2 | 63.6 | 79.2 | **79.8** |\n| | MATH-500 (Pass@1) | 78.3 | 74.6 | 90.2 | 90.0 | 96.4 | **97.3** |\n| | CNMO 2024 (Pass@1) | 13.1 | 10.8 | 43.2 | 67.6 | - | **78.8** |\n| Chinese | CLUEWSC (EM) | 85.4 | 87.9 | 90.9 | 89.9 | - | **92.8** |\n| | C-Eval (EM) | 76.7 | 76.0 | 86.5 | 68.9 | - | **91.8** |\n| | C-SimpleQA (Correct) | 55.4 | 58.7 | **68.0** | 40.3 | - | 63.7 |\n\n</div>\n\n\n### Distilled Model Evaluation\n\n\n<div align=\"center\">\n\n| Model                                    | AIME 2024 pass@1 | AIME 2024 cons@64 | MATH-500 pass@1 | GPQA Diamond pass@1 | LiveCodeBench pass@1 | CodeForces rating |\n|------------------------------------------|------------------|-------------------|-----------------|----------------------|----------------------|-------------------|\n| GPT-4o-0513                          | 9.3              | 13.4              | 74.6            | 49.9                 | 32.9                 | 759               |\n| Claude-3.5-Sonnet-1022             | 16.0             | 26.7                 | 78.3            | 65.0                 | 38.9                 | 717               |\n| o1-mini                              | 63.6             | 80.0              | 90.0            | 60.0                 | 53.8                 | **1820**          |\n| QwQ-32B-Preview                              | 44.0             | 60.0                 | 90.6            | 54.5               | 41.9                 | 1316              |\n| DeepSeek-R1-Distill-Qwen-1.5B       | 28.9             | 52.7              | 83.9            | 33.8                 | 16.9                 | 954               |\n| DeepSeek-R1-Distill-Qwen-7B          | 55.5             | 83.3              | 92.8            | 49.1                 | 37.6                 | 1189              |\n| DeepSeek-R1-Distill-Qwen-14B         | 69.7             | 80.0              | 93.9            | 59.1                 | 53.1                 | 1481              |\n| DeepSeek-R1-Distill-Qwen-32B        | **72.6**         | 83.3              | 94.3            | 62.1                 | 57.2                 | 1691              |\n| DeepSeek-R1-Distill-Llama-8B         | 50.4             | 80.0              | 89.1            | 49.0                 | 39.6                 | 1205              |\n| DeepSeek-R1-Distill-Llama-70B        | 70.0             | **86.7**          | **94.5**        | **65.2**             | **57.5**             | 1633              |\n\n</div>\n\n\n## 5. Chat Website & API Platform\nYou can chat with DeepSeek-R1 on DeepSeek's official website: [chat.deepseek.com](https://chat.deepseek.com), and switch on the button \"DeepThink\"\n\nWe also provide OpenAI-Compatible API at DeepSeek Platform: [platform.deepseek.com](https://platform.deepseek.com/)\n\n## 6. How to Run Locally\n\n### DeepSeek-R1 Models\n\nPlease visit [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) repo for more information about running DeepSeek-R1 locally.\n\n**NOTE: Hugging Face's Transformers has not been directly supported yet.**\n\n### DeepSeek-R1-Distill Models\n\nDeepSeek-R1-Distill models can be utilized in the same manner as Qwen or Llama models.\n\nFor instance, you can easily start a service using [vLLM](https://github.com/vllm-project/vllm):\n\n```shell\nvllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --tensor-parallel-size 2 --max-model-len 32768 --enforce-eager\n```\n\nYou can also easily start a service using [SGLang](https://github.com/sgl-project/sglang)\n\n```bash\npython3 -m sglang.launch_server --model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --trust-remote-code --tp 2\n```\n\n### Usage Recommendations\n\n**We recommend adhering to the following configurations when utilizing the DeepSeek-R1 series models, including benchmarking, to achieve the expected performance:**\n\n1. Set the temperature within the range of 0.5-0.7 (0.6 is recommended) to prevent endless repetitions or incoherent outputs.\n2. **Avoid adding a system prompt; all instructions should be contained within the user prompt.**\n3. For mathematical problems, it is advisable to include a directive in your prompt such as: \"Please reason step by step, and put your final answer within \\boxed{}.\"\n4. When evaluating model performance, it is recommended to conduct multiple tests and average the results.\n\nAdditionally, we have observed that the DeepSeek-R1 series models tend to bypass thinking pattern (i.e., outputting \"\\<think\\>\\n\\n\\</think\\>\") when responding to certain queries, which can adversely affect the model's performance.\n**To ensure that the model engages in thorough reasoning, we recommend enforcing the model to initiate its response with \"\\<think\\>\\n\" at the beginning of every output.**\n\n## 7. License\nThis code repository and the model weights are licensed under the [MIT License](https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE).\nDeepSeek-R1 series support commercial use, allow for any modifications and derivative works, including, but not limited to, distillation for training other LLMs. Please note that:\n- DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Qwen-14B and DeepSeek-R1-Distill-Qwen-32B are derived from [Qwen-2.5 series](https://github.com/QwenLM/Qwen2.5), which are originally licensed under [Apache 2.0 License](https://huggingface.co/Qwen/Qwen2.5-1.5B/blob/main/LICENSE), and now finetuned with 800k samples curated with DeepSeek-R1.\n- DeepSeek-R1-Distill-Llama-8B is derived from Llama3.1-8B-Base and is originally licensed under [llama3.1 license](https://huggingface.co/meta-llama/Llama-3.1-8B/blob/main/LICENSE).\n- DeepSeek-R1-Distill-Llama-70B is derived from Llama3.3-70B-Instruct and is originally licensed under [llama3.3 license](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct/blob/main/LICENSE).\n\n## 8. Citation\n```\n@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\n      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, \n      author={DeepSeek-AI},\n      year={2025},\n      eprint={2501.12948},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2501.12948}, \n}\n\n```\n\n## 9. Contact\nIf you have any questions, please raise an issue or contact us at [service@deepseek.com](service@deepseek.com).\n",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModelForCausalLM",
      "custom_class": "modeling_deepseek.DeepseekV3ForCausalLM",
      "pipeline_tag": "text-generation",
      "processor": null
    },
    "safetensors": {
      "parameters": {
        "BF16": 3918786560,
        "F8_E4M3": 680571043840,
        "F32": 41555600
      },
      "total": 684531386000
    },
    "model_index": null,
    "trending_score": null
  },
  {
    "model_id": "MaziyarPanahi/Mixtral-8x22B-v0.1-GGUF",
    "model_name": "MaziyarPanahi/Mixtral-8x22B-v0.1-GGUF",
    "author": "MaziyarPanahi",
    "downloads": 1649984,
    "downloads_all_time": null,
    "likes": 74,
    "tags": [
      "transformers",
      "gguf",
      "mixtral",
      "text-generation",
      "quantized",
      "2-bit",
      "3-bit",
      "4-bit",
      "5-bit",
      "6-bit",
      "8-bit",
      "16-bit",
      "GGUF",
      "moe",
      "fr",
      "en",
      "es",
      "it",
      "de",
      "base_model:v2ray/Mixtral-8x22B-v0.1",
      "base_model:quantized:v2ray/Mixtral-8x22B-v0.1",
      "license:apache-2.0",
      "autotrain_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/MaziyarPanahi/Mixtral-8x22B-v0.1-GGUF",
    "dependencies": null,
    "last_modified": "2024-04-15T20:30:10+00:00",
    "created_at": "2024-04-10T10:26:05+00:00",
    "analysis_date": "2025-03-22T00:42:09.768731",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "mixtral",
    "pipeline_tag": "text-generation",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "license": "apache-2.0",
      "base_model": "v2ray/Mixtral-8x22B-v0.1",
      "inference": false,
      "model_creator": "MaziyarPanahi",
      "model_name": "Mixtral-8x22B-v0.1-GGUF",
      "pipeline_tag": "text-generation",
      "quantized_by": "MaziyarPanahi",
      "tags": [
        "quantized",
        "2-bit",
        "3-bit",
        "4-bit",
        "5-bit",
        "6-bit",
        "8-bit",
        "16-bit",
        "GGUF",
        "mixtral",
        "moe"
      ],
      "language": [
        "fr",
        "en",
        "es",
        "it",
        "de"
      ]
    },
    "card_text": "\n<img src=\"./mixtral-8x22b.jpeg\" width=\"600\" />\n\n# Mixtral-8x22B-v0.1-GGUF\n\nOn April 10th, [@MistralAI](https://huggingface.co/mistralai) released a model named \"Mixtral 8x22B,\" an 176B MoE via magnet link (torrent):\n\n- 141B MoE with ~35B active\n- Context length of 65k tokens\n- The base model can be fine-tuned\n- Requires ~260GB VRAM in fp16, 73GB in int4\n- Licensed under Apache 2.0, according to their Discord\n- Available on @huggingface (community)\n- Utilizes a tokenizer similar to previous models\n\nThe GGUF and quantized models here are based on [v2ray/Mixtral-8x22B-v0.1](https://huggingface.co/v2ray/Mixtral-8x22B-v0.1) model\n\n## How to download\nYou can download only the quants you need instead of cloning the entire repository as follows:\n\n```\nhuggingface-cli download MaziyarPanahi/WizardLM-2-8x22B-GGUF --local-dir . --include '*Q2_K*gguf'\n```\n\n## Load sharded model\n\n`llama_load_model_from_file` will detect the number of files and will load additional tensors from the rest of files.\n\n```sh\nllama.cpp/main -m Mixtral-8x22B-v0.1.Q2_K-00001-of-00005.gguf -p \"Building a website can be done in 10 simple steps:\\nStep 1:\" -n 1024 -e\n```\n\nThe output from `Q2_K` quantized model:\n\n```\nsystem_info: n_threads = 64 / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 |\nsampling:\n        repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n        top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800\n        mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampling order:\nCFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature\ngenerate: n_ctx = 512, n_batch = 2048, n_predict = 1024, n_keep = 1\n\n\n Building a website can be done in 10 simple steps:\nStep 1: Pick a domain name\nThe domain name is your address on the Internet. It’s what people type into the browser to get to your website. It’s important to pick a domain name that is easy to remember and relates to your business. For example, if you were a plumber, you could register a domain like fixitplumbing.com. You can check the availability of a domain name with the WHOIS lookup tool. If your domain is available, you can register it at a domain registrar like GoDaddy.com or Domain.com.\nStep 2: Sign up for a web hosting account\nWeb hosting is the service that stores your website files and makes them available to people on the Internet. It’s important to pick a web hosting provider that is reliable and has good customer service. Some popular web hosting providers include Bluehost, Hostgator, and Dreamhost.\nStep 3: Create a website template\nA website template is a pre-designed website that you can use as a starting point for your own website. There are many free website templates available online. Once you’ve found a template you like, you can download it and start customizing it to fit your needs.\nStep 4: Add your content\nOnce you’ve chosen a template, you’ll need to add your own content to it. This includes things like your company logo, contact information, and text about your business. You can also add photos and videos to make your website more engaging.\nStep 5: Test your website\nBefore you make your website live, it’s important to test it out. This includes checking for broken links, typos, and making sure that all of your content is correct. You can also ask friends or family to test your website and give you feedback.\nStep 6: Launch your website\nOnce you’re happy with your website, you can make it live on the Internet. This process is called “launching” your website. You’ll need to upload your website files to your web hosting account and then point your domain name to your hosting account. Once you’ve done this, your website will be available to people on the Internet.\nStep 7: Promote your website\nJust because you’ve built a website doesn’t mean people will automatically find it. You need to promote your website to get people to visit it. This includes things like search engine optimization (SEO) and social media marketing.\nStep 8: Track your website’s progress\nOnce you’ve built your website, you need to track its progress. This includes things like traffic, search engine rankings, and conversion rates. By tracking your website’s progress, you can make sure that it’s working properly and that people are finding it.\nStep 9: Keep your website up-to-date\nJust because you’ve built your website doesn’t mean you’re done. You need to keep your website up-to-date by adding new content and fixing any errors that occur. By keeping your website up-to-date, you can make sure that it’s always available to people on the Internet.\nStep 10: Repeat steps 1-10\n\nOnce you’ve built your website, you need to promote it so that people can find it. You can do this by adding your website to search engines, directories, and social media sites. You can also promote your website by word-of-mouth and by giving people your business card.\n\nOnce you’ve promoted your website, you need to keep track of how it’s doing. You can do this by using website analytics tools. These tools will help you see how many people are visiting your website, where they’re coming from, and what they’re doing on your site.\n\nIf you want to keep your website up-to-date, you need to add new content on a regular basis. You can do this by writing blog posts, creating infographics, or recording videos. You can also add new content by updating your website’s design and by adding new features.\n\nBy following these steps, you can build a website that’s available to people on the Internet. You can also keep your website\n```\n\nSince this appears to be a base model, it will keep on generating. \n\n\n\n## Credit\n\n- [MistralAI](https://huggingface.co/mistralai) for opening the weights\n- [v2ray](https://huggingface.co/v2ray/) for downloading, converting, and sharing it with the community [Mixtral-8x22B-v0.1](https://huggingface.co/v2ray/Mixtral-8x22B-v0.1)\n- [philschmid](https://huggingface.co/philschmid) for the photo he shared on his Twitter\n\n                                                                     ▄▄▄░░\n                                                            ▄▄▄▄▄█████████░░░░\n                                                ▄▄▄▄▄▄████████████████████░░░░░\n                                             █████████████████████████████░░░░░\n                        ▄▄▄▄▄▄█████░░░       █████████████████████████████░░░░░\n             ▄▄▄▄▄██████████████████░░░░░░  ██████████████████████████████░░░░░\n      ▄█████████████████████████████░░░░░░░░██████████████████████████████░░░░░\n      ███████████████████████████████░░░░░░░██████████████████████████████░░░░░\n      ███████████████████████████████░░░░░░░██████████████████████████████░░░░░\n      ███████████████████████████████░░░░░░███████████████████████████████░░░░░\n      ████████████████████████████████░░░░░███████████████████████████████░░░░░\n      ████████████████████████████████░░░░████████████████████████████████░░░░░\n      █████████████████████████████████░░░████████████████████████████████░░░░░\n      █████████████████████████████████░░░████████████░███████████████████░░░░░\n      ██████████████████████████████████░█████████████░███████████████████░░░░░\n      ███████████████████░██████████████▄█████████████░███████████████████░░░░░\n      ███████████████████░███████████████████████████░░███████████████████░░░░░\n      ███████████████████░░██████████████████████████░░███████████████████░░░░░\n      ███████████████████░░█████████████████████████░░░███████████████████░░░░░\n      ███████████████████░░░████████████████████████░░░███████████████████░░░░░\n      ███████████████████░░░████████████████████████░░░███████████████████░░░░░\n      ███████████████████░░░░██████████████████████░░░░███████████████████░░░░░\n      ███████████████████░░░░██████████████████████░░░░███████████████████░░░░░\n      ███████████████████░░░░░█████████████████████░░░░███████████████████░░░░░\n      ███████████████████░░░░░████████████████████░░░░░███████████████████░░░░░\n      ███████████████████░░░░░░███████████████████░░░░░███████████████████░░░░░\n      ███████████████████░░░░░░██████████████████░░░░░░███████████████████░░░░░\n      ███████████████████░░░░░░░█████████████████░░░░░░███████████████████░░░░░\n      ███████████████████░░░░░░░█████████████████░░░░░░███████████████████░░░░░\n      ███████████████████░░░░░░░░███████████████░░░░░░░██████████░░░░░░░░░░░░░░\n      ███████████████████░░░░░░░░███████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░\n      ███████████████████░░░░░░░░███████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░\n      ███████████████████░░░░░░░░░██░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░\n      ███████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░\n      ██████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░  ░░░░░░░\n          ░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░      ░░░\n                ░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░    ░░░░░░░░░░░░░░░░░░\n                   ░░░░░░░░░░░░░░░░░░░░░░░░░░░░\n                        ░░░░░░░░░░░░░░░░░\n                           ░░░░░",
    "card_content": "---\nlicense: apache-2.0\nbase_model: v2ray/Mixtral-8x22B-v0.1\ninference: false\nmodel_creator: MaziyarPanahi\nmodel_name: Mixtral-8x22B-v0.1-GGUF\npipeline_tag: text-generation\nquantized_by: MaziyarPanahi\ntags:\n- quantized\n- 2-bit\n- 3-bit\n- 4-bit\n- 5-bit\n- 6-bit\n- 8-bit\n- 16-bit\n- GGUF\n- mixtral\n- moe\nlanguage:\n- fr\n- en\n- es\n- it\n- de\n---\n\n<img src=\"./mixtral-8x22b.jpeg\" width=\"600\" />\n\n# Mixtral-8x22B-v0.1-GGUF\n\nOn April 10th, [@MistralAI](https://huggingface.co/mistralai) released a model named \"Mixtral 8x22B,\" an 176B MoE via magnet link (torrent):\n\n- 141B MoE with ~35B active\n- Context length of 65k tokens\n- The base model can be fine-tuned\n- Requires ~260GB VRAM in fp16, 73GB in int4\n- Licensed under Apache 2.0, according to their Discord\n- Available on @huggingface (community)\n- Utilizes a tokenizer similar to previous models\n\nThe GGUF and quantized models here are based on [v2ray/Mixtral-8x22B-v0.1](https://huggingface.co/v2ray/Mixtral-8x22B-v0.1) model\n\n## How to download\nYou can download only the quants you need instead of cloning the entire repository as follows:\n\n```\nhuggingface-cli download MaziyarPanahi/WizardLM-2-8x22B-GGUF --local-dir . --include '*Q2_K*gguf'\n```\n\n## Load sharded model\n\n`llama_load_model_from_file` will detect the number of files and will load additional tensors from the rest of files.\n\n```sh\nllama.cpp/main -m Mixtral-8x22B-v0.1.Q2_K-00001-of-00005.gguf -p \"Building a website can be done in 10 simple steps:\\nStep 1:\" -n 1024 -e\n```\n\nThe output from `Q2_K` quantized model:\n\n```\nsystem_info: n_threads = 64 / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 |\nsampling:\n        repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n        top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800\n        mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\nsampling order:\nCFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature\ngenerate: n_ctx = 512, n_batch = 2048, n_predict = 1024, n_keep = 1\n\n\n Building a website can be done in 10 simple steps:\nStep 1: Pick a domain name\nThe domain name is your address on the Internet. It’s what people type into the browser to get to your website. It’s important to pick a domain name that is easy to remember and relates to your business. For example, if you were a plumber, you could register a domain like fixitplumbing.com. You can check the availability of a domain name with the WHOIS lookup tool. If your domain is available, you can register it at a domain registrar like GoDaddy.com or Domain.com.\nStep 2: Sign up for a web hosting account\nWeb hosting is the service that stores your website files and makes them available to people on the Internet. It’s important to pick a web hosting provider that is reliable and has good customer service. Some popular web hosting providers include Bluehost, Hostgator, and Dreamhost.\nStep 3: Create a website template\nA website template is a pre-designed website that you can use as a starting point for your own website. There are many free website templates available online. Once you’ve found a template you like, you can download it and start customizing it to fit your needs.\nStep 4: Add your content\nOnce you’ve chosen a template, you’ll need to add your own content to it. This includes things like your company logo, contact information, and text about your business. You can also add photos and videos to make your website more engaging.\nStep 5: Test your website\nBefore you make your website live, it’s important to test it out. This includes checking for broken links, typos, and making sure that all of your content is correct. You can also ask friends or family to test your website and give you feedback.\nStep 6: Launch your website\nOnce you’re happy with your website, you can make it live on the Internet. This process is called “launching” your website. You’ll need to upload your website files to your web hosting account and then point your domain name to your hosting account. Once you’ve done this, your website will be available to people on the Internet.\nStep 7: Promote your website\nJust because you’ve built a website doesn’t mean people will automatically find it. You need to promote your website to get people to visit it. This includes things like search engine optimization (SEO) and social media marketing.\nStep 8: Track your website’s progress\nOnce you’ve built your website, you need to track its progress. This includes things like traffic, search engine rankings, and conversion rates. By tracking your website’s progress, you can make sure that it’s working properly and that people are finding it.\nStep 9: Keep your website up-to-date\nJust because you’ve built your website doesn’t mean you’re done. You need to keep your website up-to-date by adding new content and fixing any errors that occur. By keeping your website up-to-date, you can make sure that it’s always available to people on the Internet.\nStep 10: Repeat steps 1-10\n\nOnce you’ve built your website, you need to promote it so that people can find it. You can do this by adding your website to search engines, directories, and social media sites. You can also promote your website by word-of-mouth and by giving people your business card.\n\nOnce you’ve promoted your website, you need to keep track of how it’s doing. You can do this by using website analytics tools. These tools will help you see how many people are visiting your website, where they’re coming from, and what they’re doing on your site.\n\nIf you want to keep your website up-to-date, you need to add new content on a regular basis. You can do this by writing blog posts, creating infographics, or recording videos. You can also add new content by updating your website’s design and by adding new features.\n\nBy following these steps, you can build a website that’s available to people on the Internet. You can also keep your website\n```\n\nSince this appears to be a base model, it will keep on generating. \n\n\n\n## Credit\n\n- [MistralAI](https://huggingface.co/mistralai) for opening the weights\n- [v2ray](https://huggingface.co/v2ray/) for downloading, converting, and sharing it with the community [Mixtral-8x22B-v0.1](https://huggingface.co/v2ray/Mixtral-8x22B-v0.1)\n- [philschmid](https://huggingface.co/philschmid) for the photo he shared on his Twitter\n\n                                                                     ▄▄▄░░\n                                                            ▄▄▄▄▄█████████░░░░\n                                                ▄▄▄▄▄▄████████████████████░░░░░\n                                             █████████████████████████████░░░░░\n                        ▄▄▄▄▄▄█████░░░       █████████████████████████████░░░░░\n             ▄▄▄▄▄██████████████████░░░░░░  ██████████████████████████████░░░░░\n      ▄█████████████████████████████░░░░░░░░██████████████████████████████░░░░░\n      ███████████████████████████████░░░░░░░██████████████████████████████░░░░░\n      ███████████████████████████████░░░░░░░██████████████████████████████░░░░░\n      ███████████████████████████████░░░░░░███████████████████████████████░░░░░\n      ████████████████████████████████░░░░░███████████████████████████████░░░░░\n      ████████████████████████████████░░░░████████████████████████████████░░░░░\n      █████████████████████████████████░░░████████████████████████████████░░░░░\n      █████████████████████████████████░░░████████████░███████████████████░░░░░\n      ██████████████████████████████████░█████████████░███████████████████░░░░░\n      ███████████████████░██████████████▄█████████████░███████████████████░░░░░\n      ███████████████████░███████████████████████████░░███████████████████░░░░░\n      ███████████████████░░██████████████████████████░░███████████████████░░░░░\n      ███████████████████░░█████████████████████████░░░███████████████████░░░░░\n      ███████████████████░░░████████████████████████░░░███████████████████░░░░░\n      ███████████████████░░░████████████████████████░░░███████████████████░░░░░\n      ███████████████████░░░░██████████████████████░░░░███████████████████░░░░░\n      ███████████████████░░░░██████████████████████░░░░███████████████████░░░░░\n      ███████████████████░░░░░█████████████████████░░░░███████████████████░░░░░\n      ███████████████████░░░░░████████████████████░░░░░███████████████████░░░░░\n      ███████████████████░░░░░░███████████████████░░░░░███████████████████░░░░░\n      ███████████████████░░░░░░██████████████████░░░░░░███████████████████░░░░░\n      ███████████████████░░░░░░░█████████████████░░░░░░███████████████████░░░░░\n      ███████████████████░░░░░░░█████████████████░░░░░░███████████████████░░░░░\n      ███████████████████░░░░░░░░███████████████░░░░░░░██████████░░░░░░░░░░░░░░\n      ███████████████████░░░░░░░░███████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░\n      ███████████████████░░░░░░░░███████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░\n      ███████████████████░░░░░░░░░██░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░\n      ███████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░\n      ██████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░  ░░░░░░░\n          ░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░      ░░░\n                ░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░    ░░░░░░░░░░░░░░░░░░\n                   ░░░░░░░░░░░░░░░░░░░░░░░░░░░░\n                        ░░░░░░░░░░░░░░░░░\n                           ░░░░░",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModelForCausalLM",
      "custom_class": null,
      "pipeline_tag": "text-generation",
      "processor": "AutoTokenizer"
    },
    "safetensors": null,
    "model_index": null,
    "trending_score": null
  },
  {
    "model_id": "petals-team/StableBeluga2",
    "model_name": "petals-team/StableBeluga2",
    "author": "petals-team",
    "downloads": 1603342,
    "downloads_all_time": null,
    "likes": 18,
    "tags": [
      "transformers",
      "safetensors",
      "llama",
      "text-generation",
      "en",
      "dataset:conceptofmind/cot_submix_original",
      "dataset:conceptofmind/flan2021_submix_original",
      "dataset:conceptofmind/t0_submix_original",
      "dataset:conceptofmind/niv2_submix_original",
      "arxiv:2307.09288",
      "arxiv:2306.02707",
      "autotrain_compatible",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/petals-team/StableBeluga2",
    "dependencies": [
      [
        "torch",
        null
      ],
      [
        "transformers",
        null
      ]
    ],
    "last_modified": "2023-08-23T18:00:41+00:00",
    "created_at": "2023-08-12T22:04:01+00:00",
    "analysis_date": "2025-03-22T00:42:12.831596",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "llama",
    "pipeline_tag": "text-generation",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "datasets": [
        "conceptofmind/cot_submix_original",
        "conceptofmind/flan2021_submix_original",
        "conceptofmind/t0_submix_original",
        "conceptofmind/niv2_submix_original"
      ],
      "language": [
        "en"
      ],
      "pipeline_tag": "text-generation"
    },
    "card_text": "# Stable Beluga 2\n\n## Changes in this fork\n\nThis repository contains the model from the [stabilityai/StableBeluga2](https://huggingface.co/stabilityai/StableBeluga2) repository with the following changes:\n\n1. **Storing weights in `bfloat16` instead of `float32`.**\n    This leads to 2x smaller files and a small quality loss, which is not significant compared to the loss caused by NF4 quantization used in Petals by default.\n1. **Storing weights in small shards.**\n    Each transformer block is stored in its own shard (1.71 GB each). The input and output embeddings and adjacent layernorms are in a separate shard (1.05 GB) too.\n    This way, Petals clients and servers don't have to download any excess data besides the layers they actually use.\n1. **Using [Safetensors](https://github.com/huggingface/safetensors) instead of Pickle.**\n    This allows faster loading with smaller RAM requirements.\n\nWe provide the original README below. Please refer there for model details and licensing information.\n\n## Model Description\n\n`Stable Beluga 2` is a Llama2 70B model finetuned on an Orca style Dataset\n\n## Usage\n\nStart chatting with `Stable Beluga 2` using the following code snippet:\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n\ntokenizer = AutoTokenizer.from_pretrained(\"stabilityai/StableBeluga2\", use_fast=False)\nmodel = AutoModelForCausalLM.from_pretrained(\"stabilityai/StableBeluga2\", torch_dtype=torch.float16, low_cpu_mem_usage=True, device_map=\"auto\")\nsystem_prompt = \"### System:\\nYou are Stable Beluga, an AI that follows instructions extremely well. Help as much as you can. Remember, be safe, and don't do anything illegal.\\n\\n\"\n\nmessage = \"Write me a poem please\"\nprompt = f\"{system_prompt}### User: {message}\\n\\n### Assistant:\\n\"\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\noutput = model.generate(**inputs, do_sample=True, top_p=0.95, top_k=0, max_new_tokens=256)\n\nprint(tokenizer.decode(output[0], skip_special_tokens=True))\n```\n\nStable Beluga 2 should be used with this prompt format:\n```\n### System:\nThis is a system prompt, please behave and help the user.\n\n### User:\nYour prompt here\n\n### Assistant:\nThe output of Stable Beluga 2\n```\n\n## Other Beluga Models\n\n[StableBeluga 1 - Delta](https://huggingface.co/stabilityai/StableBeluga1-Delta)  \n[StableBeluga 13B](https://huggingface.co/stabilityai/StableBeluga-13B)  \n[StableBeluga 7B](https://huggingface.co/stabilityai/StableBeluga-7B)  \n\n## Model Details\n\n* **Developed by**: [Stability AI](https://stability.ai/)\n* **Model type**: Stable Beluga 2 is an auto-regressive language model fine-tuned on Llama2 70B.\n* **Language(s)**: English\n* **Library**: [HuggingFace Transformers](https://github.com/huggingface/transformers)\n* **License**: Fine-tuned checkpoints (`Stable Beluga 2`) is licensed under the [STABLE BELUGA NON-COMMERCIAL COMMUNITY LICENSE AGREEMENT](https://huggingface.co/stabilityai/StableBeluga2/blob/main/LICENSE.txt)\n* **Contact**: For questions and comments about the model, please email `lm@stability.ai`\n\n### Training Dataset\n\n` Stable Beluga 2` is trained on our internal Orca-style dataset\n\n### Training Procedure\n\nModels are learned via supervised fine-tuning on the aforementioned datasets, trained in mixed-precision (BF16), and optimized with AdamW. We outline the following hyperparameters:\n\n| Dataset           | Batch Size | Learning Rate |Learning Rate Decay| Warm-up | Weight Decay | Betas       |\n|-------------------|------------|---------------|-------------------|---------|--------------|-------------|\n| Orca pt1 packed   | 256        | 3e-5          | Cosine to 3e-6    | 100     | 1e-6         | (0.9, 0.95) |\n| Orca pt2 unpacked | 512        | 3e-5          | Cosine to 3e-6    | 100     | 1e-6         | (0.9, 0.95) |\n\n## Ethical Considerations and Limitations\n\nBeluga is a new technology that carries risks with use. Testing conducted to date has been in English, and has not covered, nor could it cover all scenarios. For these reasons, as with all LLMs, Beluga's potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Beluga, developers should perform safety testing and tuning tailored to their specific applications of the model.\n\n## How to cite\n\n```bibtex\n@misc{StableBelugaModels, \n      url={[https://huggingface.co/stabilityai/StableBeluga2](https://huggingface.co/stabilityai/StableBeluga2)}, \n      title={Stable Beluga models}, \n      author={Mahan, Dakota and Carlow, Ryan and Castricato, Louis and Cooper, Nathan and Laforte, Christian}\n}\n```\n\n## Citations\n\n```bibtext\n@misc{touvron2023llama,\n      title={Llama 2: Open Foundation and Fine-Tuned Chat Models}, \n      author={Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},\n      year={2023},\n      eprint={2307.09288},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n\n```bibtext\n@misc{mukherjee2023orca,\n      title={Orca: Progressive Learning from Complex Explanation Traces of GPT-4}, \n      author={Subhabrata Mukherjee and Arindam Mitra and Ganesh Jawahar and Sahaj Agarwal and Hamid Palangi and Ahmed Awadallah},\n      year={2023},\n      eprint={2306.02707},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```",
    "card_content": "---\ndatasets:\n- conceptofmind/cot_submix_original\n- conceptofmind/flan2021_submix_original\n- conceptofmind/t0_submix_original\n- conceptofmind/niv2_submix_original\nlanguage:\n- en\npipeline_tag: text-generation\n---\n# Stable Beluga 2\n\n## Changes in this fork\n\nThis repository contains the model from the [stabilityai/StableBeluga2](https://huggingface.co/stabilityai/StableBeluga2) repository with the following changes:\n\n1. **Storing weights in `bfloat16` instead of `float32`.**\n    This leads to 2x smaller files and a small quality loss, which is not significant compared to the loss caused by NF4 quantization used in Petals by default.\n1. **Storing weights in small shards.**\n    Each transformer block is stored in its own shard (1.71 GB each). The input and output embeddings and adjacent layernorms are in a separate shard (1.05 GB) too.\n    This way, Petals clients and servers don't have to download any excess data besides the layers they actually use.\n1. **Using [Safetensors](https://github.com/huggingface/safetensors) instead of Pickle.**\n    This allows faster loading with smaller RAM requirements.\n\nWe provide the original README below. Please refer there for model details and licensing information.\n\n## Model Description\n\n`Stable Beluga 2` is a Llama2 70B model finetuned on an Orca style Dataset\n\n## Usage\n\nStart chatting with `Stable Beluga 2` using the following code snippet:\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n\ntokenizer = AutoTokenizer.from_pretrained(\"stabilityai/StableBeluga2\", use_fast=False)\nmodel = AutoModelForCausalLM.from_pretrained(\"stabilityai/StableBeluga2\", torch_dtype=torch.float16, low_cpu_mem_usage=True, device_map=\"auto\")\nsystem_prompt = \"### System:\\nYou are Stable Beluga, an AI that follows instructions extremely well. Help as much as you can. Remember, be safe, and don't do anything illegal.\\n\\n\"\n\nmessage = \"Write me a poem please\"\nprompt = f\"{system_prompt}### User: {message}\\n\\n### Assistant:\\n\"\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\noutput = model.generate(**inputs, do_sample=True, top_p=0.95, top_k=0, max_new_tokens=256)\n\nprint(tokenizer.decode(output[0], skip_special_tokens=True))\n```\n\nStable Beluga 2 should be used with this prompt format:\n```\n### System:\nThis is a system prompt, please behave and help the user.\n\n### User:\nYour prompt here\n\n### Assistant:\nThe output of Stable Beluga 2\n```\n\n## Other Beluga Models\n\n[StableBeluga 1 - Delta](https://huggingface.co/stabilityai/StableBeluga1-Delta)  \n[StableBeluga 13B](https://huggingface.co/stabilityai/StableBeluga-13B)  \n[StableBeluga 7B](https://huggingface.co/stabilityai/StableBeluga-7B)  \n\n## Model Details\n\n* **Developed by**: [Stability AI](https://stability.ai/)\n* **Model type**: Stable Beluga 2 is an auto-regressive language model fine-tuned on Llama2 70B.\n* **Language(s)**: English\n* **Library**: [HuggingFace Transformers](https://github.com/huggingface/transformers)\n* **License**: Fine-tuned checkpoints (`Stable Beluga 2`) is licensed under the [STABLE BELUGA NON-COMMERCIAL COMMUNITY LICENSE AGREEMENT](https://huggingface.co/stabilityai/StableBeluga2/blob/main/LICENSE.txt)\n* **Contact**: For questions and comments about the model, please email `lm@stability.ai`\n\n### Training Dataset\n\n` Stable Beluga 2` is trained on our internal Orca-style dataset\n\n### Training Procedure\n\nModels are learned via supervised fine-tuning on the aforementioned datasets, trained in mixed-precision (BF16), and optimized with AdamW. We outline the following hyperparameters:\n\n| Dataset           | Batch Size | Learning Rate |Learning Rate Decay| Warm-up | Weight Decay | Betas       |\n|-------------------|------------|---------------|-------------------|---------|--------------|-------------|\n| Orca pt1 packed   | 256        | 3e-5          | Cosine to 3e-6    | 100     | 1e-6         | (0.9, 0.95) |\n| Orca pt2 unpacked | 512        | 3e-5          | Cosine to 3e-6    | 100     | 1e-6         | (0.9, 0.95) |\n\n## Ethical Considerations and Limitations\n\nBeluga is a new technology that carries risks with use. Testing conducted to date has been in English, and has not covered, nor could it cover all scenarios. For these reasons, as with all LLMs, Beluga's potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Beluga, developers should perform safety testing and tuning tailored to their specific applications of the model.\n\n## How to cite\n\n```bibtex\n@misc{StableBelugaModels, \n      url={[https://huggingface.co/stabilityai/StableBeluga2](https://huggingface.co/stabilityai/StableBeluga2)}, \n      title={Stable Beluga models}, \n      author={Mahan, Dakota and Carlow, Ryan and Castricato, Louis and Cooper, Nathan and Laforte, Christian}\n}\n```\n\n## Citations\n\n```bibtext\n@misc{touvron2023llama,\n      title={Llama 2: Open Foundation and Fine-Tuned Chat Models}, \n      author={Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},\n      year={2023},\n      eprint={2307.09288},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n\n```bibtext\n@misc{mukherjee2023orca,\n      title={Orca: Progressive Learning from Complex Explanation Traces of GPT-4}, \n      author={Subhabrata Mukherjee and Arindam Mitra and Ganesh Jawahar and Sahaj Agarwal and Hamid Palangi and Ahmed Awadallah},\n      year={2023},\n      eprint={2306.02707},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModelForCausalLM",
      "custom_class": null,
      "pipeline_tag": "text-generation",
      "processor": "AutoTokenizer"
    },
    "safetensors": {
      "parameters": {
        "BF16": 68976648192
      },
      "total": 68976648192
    },
    "model_index": null,
    "trending_score": null
  },
  {
    "model_id": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B",
    "model_name": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B",
    "author": "deepseek-ai",
    "downloads": 1567337,
    "downloads_all_time": null,
    "likes": 661,
    "tags": [
      "transformers",
      "safetensors",
      "llama",
      "text-generation",
      "conversational",
      "arxiv:2501.12948",
      "license:mit",
      "autotrain_compatible",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B",
    "dependencies": [
      [
        "transformers",
        null
      ]
    ],
    "last_modified": "2025-02-24T03:32:07+00:00",
    "created_at": "2025-01-20T09:09:42+00:00",
    "analysis_date": "2025-03-22T00:42:14.882396",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "llama",
    "pipeline_tag": "text-generation",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "license": "mit",
      "library_name": "transformers"
    },
    "card_text": "# DeepSeek-R1\n<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align=\"center\">\n  <img src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true\" width=\"60%\" alt=\"DeepSeek-V3\" />\n</div>\n<hr>\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://www.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Homepage\" src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://chat.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/🤖%20Chat-DeepSeek%20R1-536af5?color=536af5&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://huggingface.co/deepseek-ai\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://discord.gg/Tc7c45Zzu5\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Wechat\" src=\"https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://twitter.com/deepseek_ai\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Twitter Follow\" src=\"https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE\" style=\"margin: 2px;\">\n    <img alt=\"License\" src=\"https://img.shields.io/badge/License-MIT-f5de53?&color=f5de53\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n\n<p align=\"center\">\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf\"><b>Paper Link</b>👁️</a>\n</p>\n\n\n## 1. Introduction\n\nWe introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. \nDeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning.\nWith RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors.\nHowever, DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. To address these issues and further enhance reasoning performance,\nwe introduce DeepSeek-R1, which incorporates cold-start data before RL.\nDeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks. \nTo support the research community, we have open-sourced DeepSeek-R1-Zero, DeepSeek-R1, and six dense models distilled from DeepSeek-R1 based on Llama and Qwen. DeepSeek-R1-Distill-Qwen-32B outperforms OpenAI-o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.\n\n**NOTE: Before running DeepSeek-R1 series models locally, we kindly recommend reviewing the [Usage Recommendation](#usage-recommendations) section.**\n\n<p align=\"center\">\n  <img width=\"80%\" src=\"figures/benchmark.jpg\">\n</p>\n\n## 2. Model Summary\n\n---\n\n**Post-Training: Large-Scale Reinforcement Learning on the Base Model**\n\n-  We directly apply reinforcement learning (RL) to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.\n\n-   We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model's reasoning and non-reasoning capabilities.\n    We believe the pipeline will benefit the industry by creating better models. \n\n---\n\n**Distillation: Smaller Models Can Be Powerful Too**\n\n-  We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future. \n- Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.\n\n## 3. Model Downloads\n\n### DeepSeek-R1 Models\n\n<div align=\"center\">\n\n| **Model** | **#Total Params** | **#Activated Params** | **Context Length** | **Download** |\n| :------------: | :------------: | :------------: | :------------: | :------------: |\n| DeepSeek-R1-Zero | 671B | 37B | 128K   | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Zero)   |\n| DeepSeek-R1   | 671B | 37B |  128K   | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1)   |\n\n</div>\n\nDeepSeek-R1-Zero & DeepSeek-R1 are trained based on DeepSeek-V3-Base. \nFor more details regarding the model architecture, please refer to [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) repository.\n\n### DeepSeek-R1-Distill Models\n\n<div align=\"center\">\n\n| **Model** | **Base Model** | **Download** |\n| :------------: | :------------: | :------------: |\n| DeepSeek-R1-Distill-Qwen-1.5B  | [Qwen2.5-Math-1.5B](https://huggingface.co/Qwen/Qwen2.5-Math-1.5B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B)   |\n| DeepSeek-R1-Distill-Qwen-7B  | [Qwen2.5-Math-7B](https://huggingface.co/Qwen/Qwen2.5-Math-7B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B)   |\n| DeepSeek-R1-Distill-Llama-8B  | [Llama-3.1-8B](https://huggingface.co/meta-llama/Llama-3.1-8B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B)   |\n| DeepSeek-R1-Distill-Qwen-14B   | [Qwen2.5-14B](https://huggingface.co/Qwen/Qwen2.5-14B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B)   |\n|DeepSeek-R1-Distill-Qwen-32B  | [Qwen2.5-32B](https://huggingface.co/Qwen/Qwen2.5-32B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B)   |\n| DeepSeek-R1-Distill-Llama-70B  | [Llama-3.3-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B)   |\n\n</div>\n\nDeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1.\nWe slightly change their configs and tokenizers. Please use our setting to run these models.\n\n## 4. Evaluation Results\n\n### DeepSeek-R1-Evaluation\n For all our models, the maximum generation length is set to 32,768 tokens. For benchmarks requiring sampling, we use a temperature of $0.6$, a top-p value of $0.95$, and generate 64 responses per query to estimate pass@1.\n<div align=\"center\">\n\n\n| Category | Benchmark (Metric) | Claude-3.5-Sonnet-1022 | GPT-4o 0513 | DeepSeek V3 | OpenAI o1-mini | OpenAI o1-1217 | DeepSeek R1 |\n|----------|-------------------|----------------------|------------|--------------|----------------|------------|--------------|\n| | Architecture | - | - | MoE | - | - | MoE |\n| | # Activated Params | - | - | 37B | - | - | 37B |\n| | # Total Params | - | - | 671B | - | - | 671B |\n| English | MMLU (Pass@1) | 88.3 | 87.2 | 88.5 | 85.2 | **91.8** | 90.8 |\n| | MMLU-Redux (EM) | 88.9 | 88.0 | 89.1 | 86.7 | - | **92.9** |\n| | MMLU-Pro (EM) | 78.0 | 72.6 | 75.9 | 80.3 | - | **84.0** |\n| | DROP (3-shot F1) | 88.3 | 83.7 | 91.6 | 83.9 | 90.2 | **92.2** |\n| | IF-Eval (Prompt Strict) | **86.5** | 84.3 | 86.1 | 84.8 | - | 83.3 |\n| | GPQA-Diamond (Pass@1) | 65.0 | 49.9 | 59.1 | 60.0 | **75.7** | 71.5 |\n| | SimpleQA (Correct) | 28.4 | 38.2 | 24.9 | 7.0 | **47.0** | 30.1 |\n| | FRAMES (Acc.) | 72.5 | 80.5 | 73.3 | 76.9 | - | **82.5** |\n| | AlpacaEval2.0 (LC-winrate) | 52.0 | 51.1 | 70.0 | 57.8 | - | **87.6** |\n| | ArenaHard (GPT-4-1106) | 85.2 | 80.4 | 85.5 | 92.0 | - | **92.3** |\n| Code | LiveCodeBench (Pass@1-COT) | 33.8 | 34.2 | - | 53.8 | 63.4 | **65.9** |\n| | Codeforces (Percentile) | 20.3 | 23.6 | 58.7 | 93.4 | **96.6** | 96.3 |\n| | Codeforces (Rating) | 717 | 759 | 1134 | 1820 | **2061** | 2029 |\n| | SWE Verified (Resolved) | **50.8** | 38.8 | 42.0 | 41.6 | 48.9 | 49.2 |\n| | Aider-Polyglot (Acc.) | 45.3 | 16.0 | 49.6 | 32.9 | **61.7** | 53.3 |\n| Math | AIME 2024 (Pass@1) | 16.0 | 9.3 | 39.2 | 63.6 | 79.2 | **79.8** |\n| | MATH-500 (Pass@1) | 78.3 | 74.6 | 90.2 | 90.0 | 96.4 | **97.3** |\n| | CNMO 2024 (Pass@1) | 13.1 | 10.8 | 43.2 | 67.6 | - | **78.8** |\n| Chinese | CLUEWSC (EM) | 85.4 | 87.9 | 90.9 | 89.9 | - | **92.8** |\n| | C-Eval (EM) | 76.7 | 76.0 | 86.5 | 68.9 | - | **91.8** |\n| | C-SimpleQA (Correct) | 55.4 | 58.7 | **68.0** | 40.3 | - | 63.7 |\n\n</div>\n\n\n### Distilled Model Evaluation\n\n\n<div align=\"center\">\n\n| Model                                    | AIME 2024 pass@1 | AIME 2024 cons@64 | MATH-500 pass@1 | GPQA Diamond pass@1 | LiveCodeBench pass@1 | CodeForces rating |\n|------------------------------------------|------------------|-------------------|-----------------|----------------------|----------------------|-------------------|\n| GPT-4o-0513                          | 9.3              | 13.4              | 74.6            | 49.9                 | 32.9                 | 759               |\n| Claude-3.5-Sonnet-1022             | 16.0             | 26.7                 | 78.3            | 65.0                 | 38.9                 | 717               |\n| o1-mini                              | 63.6             | 80.0              | 90.0            | 60.0                 | 53.8                 | **1820**          |\n| QwQ-32B-Preview                              | 44.0             | 60.0                 | 90.6            | 54.5               | 41.9                 | 1316              |\n| DeepSeek-R1-Distill-Qwen-1.5B       | 28.9             | 52.7              | 83.9            | 33.8                 | 16.9                 | 954               |\n| DeepSeek-R1-Distill-Qwen-7B          | 55.5             | 83.3              | 92.8            | 49.1                 | 37.6                 | 1189              |\n| DeepSeek-R1-Distill-Qwen-14B         | 69.7             | 80.0              | 93.9            | 59.1                 | 53.1                 | 1481              |\n| DeepSeek-R1-Distill-Qwen-32B        | **72.6**         | 83.3              | 94.3            | 62.1                 | 57.2                 | 1691              |\n| DeepSeek-R1-Distill-Llama-8B         | 50.4             | 80.0              | 89.1            | 49.0                 | 39.6                 | 1205              |\n| DeepSeek-R1-Distill-Llama-70B        | 70.0             | **86.7**          | **94.5**        | **65.2**             | **57.5**             | 1633              |\n\n</div>\n\n\n## 5. Chat Website & API Platform\nYou can chat with DeepSeek-R1 on DeepSeek's official website: [chat.deepseek.com](https://chat.deepseek.com), and switch on the button \"DeepThink\"\n\nWe also provide OpenAI-Compatible API at DeepSeek Platform: [platform.deepseek.com](https://platform.deepseek.com/)\n\n## 6. How to Run Locally\n\n### DeepSeek-R1 Models\n\nPlease visit [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) repo for more information about running DeepSeek-R1 locally.\n\n**NOTE: Hugging Face's Transformers has not been directly supported yet.**\n\n### DeepSeek-R1-Distill Models\n\nDeepSeek-R1-Distill models can be utilized in the same manner as Qwen or Llama models.\n\nFor instance, you can easily start a service using [vLLM](https://github.com/vllm-project/vllm):\n\n```shell\nvllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --tensor-parallel-size 2 --max-model-len 32768 --enforce-eager\n```\n\nYou can also easily start a service using [SGLang](https://github.com/sgl-project/sglang)\n\n```bash\npython3 -m sglang.launch_server --model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --trust-remote-code --tp 2\n```\n\n### Usage Recommendations\n\n**We recommend adhering to the following configurations when utilizing the DeepSeek-R1 series models, including benchmarking, to achieve the expected performance:**\n\n1. Set the temperature within the range of 0.5-0.7 (0.6 is recommended) to prevent endless repetitions or incoherent outputs.\n2. **Avoid adding a system prompt; all instructions should be contained within the user prompt.**\n3. For mathematical problems, it is advisable to include a directive in your prompt such as: \"Please reason step by step, and put your final answer within \\boxed{}.\"\n4. When evaluating model performance, it is recommended to conduct multiple tests and average the results.\n\nAdditionally, we have observed that the DeepSeek-R1 series models tend to bypass thinking pattern (i.e., outputting \"\\<think\\>\\n\\n\\</think\\>\") when responding to certain queries, which can adversely affect the model's performance.\n**To ensure that the model engages in thorough reasoning, we recommend enforcing the model to initiate its response with \"\\<think\\>\\n\" at the beginning of every output.**\n\n## 7. License\nThis code repository and the model weights are licensed under the [MIT License](https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE).\nDeepSeek-R1 series support commercial use, allow for any modifications and derivative works, including, but not limited to, distillation for training other LLMs. Please note that:\n- DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Qwen-14B and DeepSeek-R1-Distill-Qwen-32B are derived from [Qwen-2.5 series](https://github.com/QwenLM/Qwen2.5), which are originally licensed under [Apache 2.0 License](https://huggingface.co/Qwen/Qwen2.5-1.5B/blob/main/LICENSE), and now finetuned with 800k samples curated with DeepSeek-R1.\n- DeepSeek-R1-Distill-Llama-8B is derived from Llama3.1-8B-Base and is originally licensed under [llama3.1 license](https://huggingface.co/meta-llama/Llama-3.1-8B/blob/main/LICENSE).\n- DeepSeek-R1-Distill-Llama-70B is derived from Llama3.3-70B-Instruct and is originally licensed under [llama3.3 license](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct/blob/main/LICENSE).\n\n## 8. Citation\n```\n@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\n      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, \n      author={DeepSeek-AI},\n      year={2025},\n      eprint={2501.12948},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2501.12948}, \n}\n\n```\n\n## 9. Contact\nIf you have any questions, please raise an issue or contact us at [service@deepseek.com](service@deepseek.com).\n",
    "card_content": "---\nlicense: mit\nlibrary_name: transformers\n---\n# DeepSeek-R1\n<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align=\"center\">\n  <img src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true\" width=\"60%\" alt=\"DeepSeek-V3\" />\n</div>\n<hr>\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://www.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Homepage\" src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://chat.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/🤖%20Chat-DeepSeek%20R1-536af5?color=536af5&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://huggingface.co/deepseek-ai\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://discord.gg/Tc7c45Zzu5\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Wechat\" src=\"https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://twitter.com/deepseek_ai\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Twitter Follow\" src=\"https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE\" style=\"margin: 2px;\">\n    <img alt=\"License\" src=\"https://img.shields.io/badge/License-MIT-f5de53?&color=f5de53\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n\n<p align=\"center\">\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf\"><b>Paper Link</b>👁️</a>\n</p>\n\n\n## 1. Introduction\n\nWe introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. \nDeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning.\nWith RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors.\nHowever, DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. To address these issues and further enhance reasoning performance,\nwe introduce DeepSeek-R1, which incorporates cold-start data before RL.\nDeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks. \nTo support the research community, we have open-sourced DeepSeek-R1-Zero, DeepSeek-R1, and six dense models distilled from DeepSeek-R1 based on Llama and Qwen. DeepSeek-R1-Distill-Qwen-32B outperforms OpenAI-o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.\n\n**NOTE: Before running DeepSeek-R1 series models locally, we kindly recommend reviewing the [Usage Recommendation](#usage-recommendations) section.**\n\n<p align=\"center\">\n  <img width=\"80%\" src=\"figures/benchmark.jpg\">\n</p>\n\n## 2. Model Summary\n\n---\n\n**Post-Training: Large-Scale Reinforcement Learning on the Base Model**\n\n-  We directly apply reinforcement learning (RL) to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.\n\n-   We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model's reasoning and non-reasoning capabilities.\n    We believe the pipeline will benefit the industry by creating better models. \n\n---\n\n**Distillation: Smaller Models Can Be Powerful Too**\n\n-  We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future. \n- Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.\n\n## 3. Model Downloads\n\n### DeepSeek-R1 Models\n\n<div align=\"center\">\n\n| **Model** | **#Total Params** | **#Activated Params** | **Context Length** | **Download** |\n| :------------: | :------------: | :------------: | :------------: | :------------: |\n| DeepSeek-R1-Zero | 671B | 37B | 128K   | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Zero)   |\n| DeepSeek-R1   | 671B | 37B |  128K   | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1)   |\n\n</div>\n\nDeepSeek-R1-Zero & DeepSeek-R1 are trained based on DeepSeek-V3-Base. \nFor more details regarding the model architecture, please refer to [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) repository.\n\n### DeepSeek-R1-Distill Models\n\n<div align=\"center\">\n\n| **Model** | **Base Model** | **Download** |\n| :------------: | :------------: | :------------: |\n| DeepSeek-R1-Distill-Qwen-1.5B  | [Qwen2.5-Math-1.5B](https://huggingface.co/Qwen/Qwen2.5-Math-1.5B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B)   |\n| DeepSeek-R1-Distill-Qwen-7B  | [Qwen2.5-Math-7B](https://huggingface.co/Qwen/Qwen2.5-Math-7B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B)   |\n| DeepSeek-R1-Distill-Llama-8B  | [Llama-3.1-8B](https://huggingface.co/meta-llama/Llama-3.1-8B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B)   |\n| DeepSeek-R1-Distill-Qwen-14B   | [Qwen2.5-14B](https://huggingface.co/Qwen/Qwen2.5-14B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B)   |\n|DeepSeek-R1-Distill-Qwen-32B  | [Qwen2.5-32B](https://huggingface.co/Qwen/Qwen2.5-32B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B)   |\n| DeepSeek-R1-Distill-Llama-70B  | [Llama-3.3-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B)   |\n\n</div>\n\nDeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1.\nWe slightly change their configs and tokenizers. Please use our setting to run these models.\n\n## 4. Evaluation Results\n\n### DeepSeek-R1-Evaluation\n For all our models, the maximum generation length is set to 32,768 tokens. For benchmarks requiring sampling, we use a temperature of $0.6$, a top-p value of $0.95$, and generate 64 responses per query to estimate pass@1.\n<div align=\"center\">\n\n\n| Category | Benchmark (Metric) | Claude-3.5-Sonnet-1022 | GPT-4o 0513 | DeepSeek V3 | OpenAI o1-mini | OpenAI o1-1217 | DeepSeek R1 |\n|----------|-------------------|----------------------|------------|--------------|----------------|------------|--------------|\n| | Architecture | - | - | MoE | - | - | MoE |\n| | # Activated Params | - | - | 37B | - | - | 37B |\n| | # Total Params | - | - | 671B | - | - | 671B |\n| English | MMLU (Pass@1) | 88.3 | 87.2 | 88.5 | 85.2 | **91.8** | 90.8 |\n| | MMLU-Redux (EM) | 88.9 | 88.0 | 89.1 | 86.7 | - | **92.9** |\n| | MMLU-Pro (EM) | 78.0 | 72.6 | 75.9 | 80.3 | - | **84.0** |\n| | DROP (3-shot F1) | 88.3 | 83.7 | 91.6 | 83.9 | 90.2 | **92.2** |\n| | IF-Eval (Prompt Strict) | **86.5** | 84.3 | 86.1 | 84.8 | - | 83.3 |\n| | GPQA-Diamond (Pass@1) | 65.0 | 49.9 | 59.1 | 60.0 | **75.7** | 71.5 |\n| | SimpleQA (Correct) | 28.4 | 38.2 | 24.9 | 7.0 | **47.0** | 30.1 |\n| | FRAMES (Acc.) | 72.5 | 80.5 | 73.3 | 76.9 | - | **82.5** |\n| | AlpacaEval2.0 (LC-winrate) | 52.0 | 51.1 | 70.0 | 57.8 | - | **87.6** |\n| | ArenaHard (GPT-4-1106) | 85.2 | 80.4 | 85.5 | 92.0 | - | **92.3** |\n| Code | LiveCodeBench (Pass@1-COT) | 33.8 | 34.2 | - | 53.8 | 63.4 | **65.9** |\n| | Codeforces (Percentile) | 20.3 | 23.6 | 58.7 | 93.4 | **96.6** | 96.3 |\n| | Codeforces (Rating) | 717 | 759 | 1134 | 1820 | **2061** | 2029 |\n| | SWE Verified (Resolved) | **50.8** | 38.8 | 42.0 | 41.6 | 48.9 | 49.2 |\n| | Aider-Polyglot (Acc.) | 45.3 | 16.0 | 49.6 | 32.9 | **61.7** | 53.3 |\n| Math | AIME 2024 (Pass@1) | 16.0 | 9.3 | 39.2 | 63.6 | 79.2 | **79.8** |\n| | MATH-500 (Pass@1) | 78.3 | 74.6 | 90.2 | 90.0 | 96.4 | **97.3** |\n| | CNMO 2024 (Pass@1) | 13.1 | 10.8 | 43.2 | 67.6 | - | **78.8** |\n| Chinese | CLUEWSC (EM) | 85.4 | 87.9 | 90.9 | 89.9 | - | **92.8** |\n| | C-Eval (EM) | 76.7 | 76.0 | 86.5 | 68.9 | - | **91.8** |\n| | C-SimpleQA (Correct) | 55.4 | 58.7 | **68.0** | 40.3 | - | 63.7 |\n\n</div>\n\n\n### Distilled Model Evaluation\n\n\n<div align=\"center\">\n\n| Model                                    | AIME 2024 pass@1 | AIME 2024 cons@64 | MATH-500 pass@1 | GPQA Diamond pass@1 | LiveCodeBench pass@1 | CodeForces rating |\n|------------------------------------------|------------------|-------------------|-----------------|----------------------|----------------------|-------------------|\n| GPT-4o-0513                          | 9.3              | 13.4              | 74.6            | 49.9                 | 32.9                 | 759               |\n| Claude-3.5-Sonnet-1022             | 16.0             | 26.7                 | 78.3            | 65.0                 | 38.9                 | 717               |\n| o1-mini                              | 63.6             | 80.0              | 90.0            | 60.0                 | 53.8                 | **1820**          |\n| QwQ-32B-Preview                              | 44.0             | 60.0                 | 90.6            | 54.5               | 41.9                 | 1316              |\n| DeepSeek-R1-Distill-Qwen-1.5B       | 28.9             | 52.7              | 83.9            | 33.8                 | 16.9                 | 954               |\n| DeepSeek-R1-Distill-Qwen-7B          | 55.5             | 83.3              | 92.8            | 49.1                 | 37.6                 | 1189              |\n| DeepSeek-R1-Distill-Qwen-14B         | 69.7             | 80.0              | 93.9            | 59.1                 | 53.1                 | 1481              |\n| DeepSeek-R1-Distill-Qwen-32B        | **72.6**         | 83.3              | 94.3            | 62.1                 | 57.2                 | 1691              |\n| DeepSeek-R1-Distill-Llama-8B         | 50.4             | 80.0              | 89.1            | 49.0                 | 39.6                 | 1205              |\n| DeepSeek-R1-Distill-Llama-70B        | 70.0             | **86.7**          | **94.5**        | **65.2**             | **57.5**             | 1633              |\n\n</div>\n\n\n## 5. Chat Website & API Platform\nYou can chat with DeepSeek-R1 on DeepSeek's official website: [chat.deepseek.com](https://chat.deepseek.com), and switch on the button \"DeepThink\"\n\nWe also provide OpenAI-Compatible API at DeepSeek Platform: [platform.deepseek.com](https://platform.deepseek.com/)\n\n## 6. How to Run Locally\n\n### DeepSeek-R1 Models\n\nPlease visit [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) repo for more information about running DeepSeek-R1 locally.\n\n**NOTE: Hugging Face's Transformers has not been directly supported yet.**\n\n### DeepSeek-R1-Distill Models\n\nDeepSeek-R1-Distill models can be utilized in the same manner as Qwen or Llama models.\n\nFor instance, you can easily start a service using [vLLM](https://github.com/vllm-project/vllm):\n\n```shell\nvllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --tensor-parallel-size 2 --max-model-len 32768 --enforce-eager\n```\n\nYou can also easily start a service using [SGLang](https://github.com/sgl-project/sglang)\n\n```bash\npython3 -m sglang.launch_server --model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --trust-remote-code --tp 2\n```\n\n### Usage Recommendations\n\n**We recommend adhering to the following configurations when utilizing the DeepSeek-R1 series models, including benchmarking, to achieve the expected performance:**\n\n1. Set the temperature within the range of 0.5-0.7 (0.6 is recommended) to prevent endless repetitions or incoherent outputs.\n2. **Avoid adding a system prompt; all instructions should be contained within the user prompt.**\n3. For mathematical problems, it is advisable to include a directive in your prompt such as: \"Please reason step by step, and put your final answer within \\boxed{}.\"\n4. When evaluating model performance, it is recommended to conduct multiple tests and average the results.\n\nAdditionally, we have observed that the DeepSeek-R1 series models tend to bypass thinking pattern (i.e., outputting \"\\<think\\>\\n\\n\\</think\\>\") when responding to certain queries, which can adversely affect the model's performance.\n**To ensure that the model engages in thorough reasoning, we recommend enforcing the model to initiate its response with \"\\<think\\>\\n\" at the beginning of every output.**\n\n## 7. License\nThis code repository and the model weights are licensed under the [MIT License](https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE).\nDeepSeek-R1 series support commercial use, allow for any modifications and derivative works, including, but not limited to, distillation for training other LLMs. Please note that:\n- DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Qwen-14B and DeepSeek-R1-Distill-Qwen-32B are derived from [Qwen-2.5 series](https://github.com/QwenLM/Qwen2.5), which are originally licensed under [Apache 2.0 License](https://huggingface.co/Qwen/Qwen2.5-1.5B/blob/main/LICENSE), and now finetuned with 800k samples curated with DeepSeek-R1.\n- DeepSeek-R1-Distill-Llama-8B is derived from Llama3.1-8B-Base and is originally licensed under [llama3.1 license](https://huggingface.co/meta-llama/Llama-3.1-8B/blob/main/LICENSE).\n- DeepSeek-R1-Distill-Llama-70B is derived from Llama3.3-70B-Instruct and is originally licensed under [llama3.3 license](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct/blob/main/LICENSE).\n\n## 8. Citation\n```\n@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\n      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, \n      author={DeepSeek-AI},\n      year={2025},\n      eprint={2501.12948},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2501.12948}, \n}\n\n```\n\n## 9. Contact\nIf you have any questions, please raise an issue or contact us at [service@deepseek.com](service@deepseek.com).\n",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModelForCausalLM",
      "custom_class": null,
      "pipeline_tag": "text-generation",
      "processor": "AutoTokenizer"
    },
    "safetensors": {
      "parameters": {
        "BF16": 8030261248
      },
      "total": 8030261248
    },
    "model_index": null,
    "trending_score": null
  },
  {
    "model_id": "Qwen/Qwen2.5-7B-Instruct",
    "model_name": "Qwen/Qwen2.5-7B-Instruct",
    "author": "Qwen",
    "downloads": 1557359,
    "downloads_all_time": null,
    "likes": 580,
    "tags": [
      "transformers",
      "safetensors",
      "qwen2",
      "text-generation",
      "chat",
      "conversational",
      "en",
      "arxiv:2309.00071",
      "arxiv:2407.10671",
      "base_model:Qwen/Qwen2.5-7B",
      "base_model:finetune:Qwen/Qwen2.5-7B",
      "license:apache-2.0",
      "autotrain_compatible",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/Qwen/Qwen2.5-7B-Instruct",
    "dependencies": [
      [
        "transformers",
        null
      ]
    ],
    "last_modified": "2025-01-12T02:10:10+00:00",
    "created_at": "2024-09-16T11:55:40+00:00",
    "analysis_date": "2025-03-22T00:42:16.005004",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "qwen2",
    "pipeline_tag": "text-generation",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "license": "apache-2.0",
      "license_link": "https://huggingface.co/Qwen/Qwen2.5-7B-Instruct/blob/main/LICENSE",
      "language": [
        "en"
      ],
      "pipeline_tag": "text-generation",
      "base_model": "Qwen/Qwen2.5-7B",
      "tags": [
        "chat"
      ],
      "library_name": "transformers"
    },
    "card_text": "\n# Qwen2.5-7B-Instruct\n<a href=\"https://chat.qwenlm.ai/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5\" style=\"display: inline-block; vertical-align: middle;\"/>\n</a>\n\n## Introduction\n\nQwen2.5 is the latest series of Qwen large language models. For Qwen2.5, we release a number of base language models and instruction-tuned language models ranging from 0.5 to 72 billion parameters. Qwen2.5 brings the following improvements upon Qwen2:\n\n- Significantly **more knowledge** and has greatly improved capabilities in **coding** and **mathematics**, thanks to our specialized expert models in these domains.\n- Significant improvements in **instruction following**, **generating long texts** (over 8K tokens), **understanding structured data** (e.g, tables), and **generating structured outputs** especially JSON. **More resilient to the diversity of system prompts**, enhancing role-play implementation and condition-setting for chatbots.\n- **Long-context Support** up to 128K tokens and can generate up to 8K tokens.\n- **Multilingual support** for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more. \n\n**This repo contains the instruction-tuned 7B Qwen2.5 model**, which has the following features:\n- Type: Causal Language Models\n- Training Stage: Pretraining & Post-training\n- Architecture: transformers with RoPE, SwiGLU, RMSNorm, and Attention QKV bias\n- Number of Parameters: 7.61B\n- Number of Paramaters (Non-Embedding): 6.53B\n- Number of Layers: 28\n- Number of Attention Heads (GQA): 28 for Q and 4 for KV\n- Context Length: Full 131,072 tokens and generation 8192 tokens\n  - Please refer to [this section](#processing-long-texts) for detailed instructions on how to deploy Qwen2.5 for handling long texts.\n\nFor more details, please refer to our [blog](https://qwenlm.github.io/blog/qwen2.5/), [GitHub](https://github.com/QwenLM/Qwen2.5), and [Documentation](https://qwen.readthedocs.io/en/latest/).\n\n## Requirements\n\nThe code of Qwen2.5 has been in the latest Hugging face `transformers` and we advise you to use the latest version of `transformers`.\n\nWith `transformers<4.37.0`, you will encounter the following error:\n```\nKeyError: 'qwen2'\n```\n\n## Quickstart\n\nHere provides a code snippet with `apply_chat_template` to show you how to load the tokenizer and model and how to generate contents.\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"Qwen/Qwen2.5-7B-Instruct\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=512\n)\ngenerated_ids = [\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\n\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n```\n\n### Processing Long Texts\n\nThe current `config.json` is set for context length up to 32,768 tokens.\nTo handle extensive inputs exceeding 32,768 tokens, we utilize [YaRN](https://arxiv.org/abs/2309.00071), a technique for enhancing model length extrapolation, ensuring optimal performance on lengthy texts.\n\nFor supported frameworks, you could add the following to `config.json` to enable YaRN:\n```json\n{\n  ...,\n  \"rope_scaling\": {\n    \"factor\": 4.0,\n    \"original_max_position_embeddings\": 32768,\n    \"type\": \"yarn\"\n  }\n}\n```\n\nFor deployment, we recommend using vLLM. \nPlease refer to our [Documentation](https://qwen.readthedocs.io/en/latest/deployment/vllm.html) for usage if you are not familar with vLLM.\nPresently, vLLM only supports static YARN, which means the scaling factor remains constant regardless of input length, **potentially impacting performance on shorter texts**. \nWe advise adding the `rope_scaling` configuration only when processing long contexts is required.\n\n## Evaluation & Performance\n\nDetailed evaluation results are reported in this [📑 blog](https://qwenlm.github.io/blog/qwen2.5/).\n\nFor requirements on GPU memory and the respective throughput, see results [here](https://qwen.readthedocs.io/en/latest/benchmark/speed_benchmark.html).\n\n## Citation\n\nIf you find our work helpful, feel free to give us a cite.\n\n```\n@misc{qwen2.5,\n    title = {Qwen2.5: A Party of Foundation Models},\n    url = {https://qwenlm.github.io/blog/qwen2.5/},\n    author = {Qwen Team},\n    month = {September},\n    year = {2024}\n}\n\n@article{qwen2,\n      title={Qwen2 Technical Report}, \n      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\n      journal={arXiv preprint arXiv:2407.10671},\n      year={2024}\n}\n```",
    "card_content": "---\nlicense: apache-2.0\nlicense_link: https://huggingface.co/Qwen/Qwen2.5-7B-Instruct/blob/main/LICENSE\nlanguage:\n- en\npipeline_tag: text-generation\nbase_model: Qwen/Qwen2.5-7B\ntags:\n- chat\nlibrary_name: transformers\n---\n\n# Qwen2.5-7B-Instruct\n<a href=\"https://chat.qwenlm.ai/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5\" style=\"display: inline-block; vertical-align: middle;\"/>\n</a>\n\n## Introduction\n\nQwen2.5 is the latest series of Qwen large language models. For Qwen2.5, we release a number of base language models and instruction-tuned language models ranging from 0.5 to 72 billion parameters. Qwen2.5 brings the following improvements upon Qwen2:\n\n- Significantly **more knowledge** and has greatly improved capabilities in **coding** and **mathematics**, thanks to our specialized expert models in these domains.\n- Significant improvements in **instruction following**, **generating long texts** (over 8K tokens), **understanding structured data** (e.g, tables), and **generating structured outputs** especially JSON. **More resilient to the diversity of system prompts**, enhancing role-play implementation and condition-setting for chatbots.\n- **Long-context Support** up to 128K tokens and can generate up to 8K tokens.\n- **Multilingual support** for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more. \n\n**This repo contains the instruction-tuned 7B Qwen2.5 model**, which has the following features:\n- Type: Causal Language Models\n- Training Stage: Pretraining & Post-training\n- Architecture: transformers with RoPE, SwiGLU, RMSNorm, and Attention QKV bias\n- Number of Parameters: 7.61B\n- Number of Paramaters (Non-Embedding): 6.53B\n- Number of Layers: 28\n- Number of Attention Heads (GQA): 28 for Q and 4 for KV\n- Context Length: Full 131,072 tokens and generation 8192 tokens\n  - Please refer to [this section](#processing-long-texts) for detailed instructions on how to deploy Qwen2.5 for handling long texts.\n\nFor more details, please refer to our [blog](https://qwenlm.github.io/blog/qwen2.5/), [GitHub](https://github.com/QwenLM/Qwen2.5), and [Documentation](https://qwen.readthedocs.io/en/latest/).\n\n## Requirements\n\nThe code of Qwen2.5 has been in the latest Hugging face `transformers` and we advise you to use the latest version of `transformers`.\n\nWith `transformers<4.37.0`, you will encounter the following error:\n```\nKeyError: 'qwen2'\n```\n\n## Quickstart\n\nHere provides a code snippet with `apply_chat_template` to show you how to load the tokenizer and model and how to generate contents.\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"Qwen/Qwen2.5-7B-Instruct\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=512\n)\ngenerated_ids = [\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\n\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n```\n\n### Processing Long Texts\n\nThe current `config.json` is set for context length up to 32,768 tokens.\nTo handle extensive inputs exceeding 32,768 tokens, we utilize [YaRN](https://arxiv.org/abs/2309.00071), a technique for enhancing model length extrapolation, ensuring optimal performance on lengthy texts.\n\nFor supported frameworks, you could add the following to `config.json` to enable YaRN:\n```json\n{\n  ...,\n  \"rope_scaling\": {\n    \"factor\": 4.0,\n    \"original_max_position_embeddings\": 32768,\n    \"type\": \"yarn\"\n  }\n}\n```\n\nFor deployment, we recommend using vLLM. \nPlease refer to our [Documentation](https://qwen.readthedocs.io/en/latest/deployment/vllm.html) for usage if you are not familar with vLLM.\nPresently, vLLM only supports static YARN, which means the scaling factor remains constant regardless of input length, **potentially impacting performance on shorter texts**. \nWe advise adding the `rope_scaling` configuration only when processing long contexts is required.\n\n## Evaluation & Performance\n\nDetailed evaluation results are reported in this [📑 blog](https://qwenlm.github.io/blog/qwen2.5/).\n\nFor requirements on GPU memory and the respective throughput, see results [here](https://qwen.readthedocs.io/en/latest/benchmark/speed_benchmark.html).\n\n## Citation\n\nIf you find our work helpful, feel free to give us a cite.\n\n```\n@misc{qwen2.5,\n    title = {Qwen2.5: A Party of Foundation Models},\n    url = {https://qwenlm.github.io/blog/qwen2.5/},\n    author = {Qwen Team},\n    month = {September},\n    year = {2024}\n}\n\n@article{qwen2,\n      title={Qwen2 Technical Report}, \n      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\n      journal={arXiv preprint arXiv:2407.10671},\n      year={2024}\n}\n```",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModelForCausalLM",
      "custom_class": null,
      "pipeline_tag": "text-generation",
      "processor": "AutoTokenizer"
    },
    "safetensors": {
      "parameters": {
        "BF16": 7615616512
      },
      "total": 7615616512
    },
    "model_index": null,
    "trending_score": null
  },
  {
    "model_id": "lmsys/vicuna-7b-v1.5",
    "model_name": "lmsys/vicuna-7b-v1.5",
    "author": "lmsys",
    "downloads": 1534094,
    "downloads_all_time": null,
    "likes": 335,
    "tags": [
      "transformers",
      "pytorch",
      "llama",
      "text-generation",
      "arxiv:2307.09288",
      "arxiv:2306.05685",
      "license:llama2",
      "autotrain_compatible",
      "text-generation-inference",
      "region:us"
    ],
    "card_url": "https://huggingface.co/lmsys/vicuna-7b-v1.5",
    "dependencies": [
      [
        "transformers",
        null
      ],
      [
        "torch",
        null
      ],
      [
        "fastchat",
        null
      ]
    ],
    "last_modified": "2024-03-13T02:01:41+00:00",
    "created_at": "2023-07-29T04:42:33+00:00",
    "analysis_date": "2025-03-22T00:42:17.277716",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "llama",
    "pipeline_tag": "text-generation",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "inference": false,
      "license": "llama2"
    },
    "card_text": "\n# Vicuna Model Card\n\n## Model Details\n\nVicuna is a chat assistant trained by fine-tuning Llama 2 on user-shared conversations collected from ShareGPT.\n\n- **Developed by:** [LMSYS](https://lmsys.org/)\n- **Model type:** An auto-regressive language model based on the transformer architecture\n- **License:** Llama 2 Community License Agreement\t\n- **Finetuned from model:** [Llama 2](https://arxiv.org/abs/2307.09288)\n\n### Model Sources\n\n- **Repository:** https://github.com/lm-sys/FastChat\n- **Blog:** https://lmsys.org/blog/2023-03-30-vicuna/\n- **Paper:** https://arxiv.org/abs/2306.05685\n- **Demo:** https://chat.lmsys.org/\n\n## Uses\n\nThe primary use of Vicuna is research on large language models and chatbots.\nThe primary intended users of the model are researchers and hobbyists in natural language processing, machine learning, and artificial intelligence.\n\n## How to Get Started with the Model\n\n- Command line interface: https://github.com/lm-sys/FastChat#vicuna-weights\n- APIs (OpenAI API, Huggingface API): https://github.com/lm-sys/FastChat/tree/main#api  \n\n## Training Details\n\nVicuna v1.5 is fine-tuned from Llama 2 with supervised instruction fine-tuning.\nThe training data is around 125K conversations collected from ShareGPT.com.\nSee more details in the \"Training Details of Vicuna Models\" section in the appendix of this [paper](https://arxiv.org/pdf/2306.05685.pdf).\n\n## Evaluation\n\n![Evaluation Results](https://github.com/lm-sys/lm-sys.github.io/blob/main/public/images/webdata/vicuna_v1.5_eval.png?raw=true)\n\nVicuna is evaluated with standard benchmarks, human preference, and LLM-as-a-judge. See more details in this [paper](https://arxiv.org/pdf/2306.05685.pdf) and [leaderboard](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard).\n\n## Difference between different versions of Vicuna\n\nSee [vicuna_weights_version.md](https://github.com/lm-sys/FastChat/blob/main/docs/vicuna_weights_version.md)",
    "card_content": "---\ninference: false\nlicense: llama2\n---\n\n# Vicuna Model Card\n\n## Model Details\n\nVicuna is a chat assistant trained by fine-tuning Llama 2 on user-shared conversations collected from ShareGPT.\n\n- **Developed by:** [LMSYS](https://lmsys.org/)\n- **Model type:** An auto-regressive language model based on the transformer architecture\n- **License:** Llama 2 Community License Agreement\t\n- **Finetuned from model:** [Llama 2](https://arxiv.org/abs/2307.09288)\n\n### Model Sources\n\n- **Repository:** https://github.com/lm-sys/FastChat\n- **Blog:** https://lmsys.org/blog/2023-03-30-vicuna/\n- **Paper:** https://arxiv.org/abs/2306.05685\n- **Demo:** https://chat.lmsys.org/\n\n## Uses\n\nThe primary use of Vicuna is research on large language models and chatbots.\nThe primary intended users of the model are researchers and hobbyists in natural language processing, machine learning, and artificial intelligence.\n\n## How to Get Started with the Model\n\n- Command line interface: https://github.com/lm-sys/FastChat#vicuna-weights\n- APIs (OpenAI API, Huggingface API): https://github.com/lm-sys/FastChat/tree/main#api  \n\n## Training Details\n\nVicuna v1.5 is fine-tuned from Llama 2 with supervised instruction fine-tuning.\nThe training data is around 125K conversations collected from ShareGPT.com.\nSee more details in the \"Training Details of Vicuna Models\" section in the appendix of this [paper](https://arxiv.org/pdf/2306.05685.pdf).\n\n## Evaluation\n\n![Evaluation Results](https://github.com/lm-sys/lm-sys.github.io/blob/main/public/images/webdata/vicuna_v1.5_eval.png?raw=true)\n\nVicuna is evaluated with standard benchmarks, human preference, and LLM-as-a-judge. See more details in this [paper](https://arxiv.org/pdf/2306.05685.pdf) and [leaderboard](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard).\n\n## Difference between different versions of Vicuna\n\nSee [vicuna_weights_version.md](https://github.com/lm-sys/FastChat/blob/main/docs/vicuna_weights_version.md)",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModelForCausalLM",
      "custom_class": null,
      "pipeline_tag": "text-generation",
      "processor": "AutoTokenizer"
    },
    "safetensors": null,
    "model_index": null,
    "trending_score": null
  },
  {
    "model_id": "apple/OpenELM-1_1B-Instruct",
    "model_name": "apple/OpenELM-1_1B-Instruct",
    "author": "apple",
    "downloads": 1526388,
    "downloads_all_time": null,
    "likes": 60,
    "tags": [
      "transformers",
      "safetensors",
      "openelm",
      "text-generation",
      "custom_code",
      "arxiv:2404.14619",
      "license:apple-amlr",
      "autotrain_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/apple/OpenELM-1_1B-Instruct",
    "dependencies": [
      [
        "datasets",
        null
      ],
      [
        "tokenizers",
        ">=0.15.2"
      ],
      [
        "transformers",
        ">=4.38.2"
      ],
      [
        "sentencepiece",
        ">=0.2.0"
      ]
    ],
    "last_modified": "2025-02-28T18:31:24+00:00",
    "created_at": "2024-04-12T21:52:12+00:00",
    "analysis_date": "2025-03-22T00:42:20.898610",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "openelm",
    "pipeline_tag": "text-generation",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "license": "apple-amlr",
      "license_name": "apple-sample-code-license",
      "license_link": "LICENSE"
    },
    "card_text": "\n# OpenELM\n\n*Sachin Mehta, Mohammad Hossein Sekhavat, Qingqing Cao, Maxwell Horton, Yanzi Jin, Chenfan Sun, Iman Mirzadeh, Mahyar Najibi, Dmitry Belenko, Peter Zatloukal, Mohammad Rastegari*\n\nWe introduce **OpenELM**, a family of **Open** **E**fficient **L**anguage **M**odels. OpenELM uses a layer-wise scaling strategy to efficiently allocate parameters within each layer of the transformer model, leading to enhanced accuracy. We pretrained OpenELM models using the [CoreNet](https://github.com/apple/corenet) library. We release both pretrained and instruction tuned models with 270M, 450M, 1.1B and 3B parameters. We release the complete framework, encompassing data preparation, training, fine-tuning, and evaluation procedures, alongside multiple pre-trained checkpoints and training logs, to facilitate open research.\n\nOur pre-training dataset contains RefinedWeb, deduplicated PILE, a subset of RedPajama, and a subset of Dolma v1.6, totaling approximately 1.8 trillion tokens. Please check license agreements and terms of these datasets before using them.\n\n\n\n## Usage\n\nWe have provided an example function to generate output from OpenELM models loaded via [HuggingFace Hub](https://huggingface.co/docs/hub/) in `generate_openelm.py`.\n\nYou can try the model by running the following command:\n```\npython generate_openelm.py --model apple/OpenELM-1_1B-Instruct --hf_access_token [HF_ACCESS_TOKEN] --prompt 'Once upon a time there was' --generate_kwargs repetition_penalty=1.2\n```\nPlease refer to [this link](https://huggingface.co/docs/hub/security-tokens) to obtain your hugging face access token.\n\nAdditional arguments to the hugging face generate function can be passed via `generate_kwargs`. As an example, to speedup the inference, you can try [lookup token speculative generation](https://huggingface.co/docs/transformers/generation_strategies) by passing the `prompt_lookup_num_tokens` argument as follows:\n```\npython generate_openelm.py --model apple/OpenELM-1_1B-Instruct --hf_access_token [HF_ACCESS_TOKEN] --prompt 'Once upon a time there was' --generate_kwargs repetition_penalty=1.2 prompt_lookup_num_tokens=10\n```\nAlternatively, try model-wise speculative generation with an [assistive model](https://huggingface.co/blog/assisted-generation) by passing a smaller model through the `assistant_model` argument, for example:\n```\npython generate_openelm.py --model apple/OpenELM-1_1B-Instruct --hf_access_token [HF_ACCESS_TOKEN] --prompt 'Once upon a time there was' --generate_kwargs repetition_penalty=1.2 --assistant_model [SMALLER_MODEL]\n```\n\n## Main Results\n\n### Zero-Shot\n\n| **Model Size**                                                              | **ARC-c** | **ARC-e** | **BoolQ** | **HellaSwag** | **PIQA**  | **SciQ**  | **WinoGrande** | **Average** |\n|-----------------------------------------------------------------------------|-----------|-----------|-----------|---------------|-----------|-----------|----------------|-------------|\n| [OpenELM-270M](https://huggingface.co/apple/OpenELM-270M)                   | 26.45     | 45.08     | **53.98** | 46.71         | 69.75     | **84.70** | **53.91**      | 54.37       |\n| [OpenELM-270M-Instruct](https://huggingface.co/apple/OpenELM-270M-Instruct) | **30.55** | **46.68** | 48.56     | **52.07**     | **70.78** | 84.40     | 52.72          | **55.11**   |\n| [OpenELM-450M](https://huggingface.co/apple/OpenELM-450M)                   | 27.56     | 48.06     | 55.78     | 53.97         | 72.31     | 87.20     | 58.01          | 57.56       |\n| [OpenELM-450M-Instruct](https://huggingface.co/apple/OpenELM-450M-Instruct) | **30.38** | **50.00** | **60.37** | **59.34**     | **72.63** | **88.00** | **58.96**      | **59.95**   |\n| [OpenELM-1_1B](https://huggingface.co/apple/OpenELM-1_1B)                   | 32.34     | **55.43** | 63.58     | 64.81         | **75.57** | **90.60** | 61.72          | 63.44       |\n| [OpenELM-1_1B-Instruct](https://huggingface.co/apple/OpenELM-1_1B-Instruct) | **37.97** | 52.23     | **70.00** | **71.20**     | 75.03     | 89.30     | **62.75**      | **65.50**   |\n| [OpenELM-3B](https://huggingface.co/apple/OpenELM-3B)                       | 35.58     | 59.89     | 67.40     | 72.44         | 78.24     | **92.70** | 65.51          | 67.39       |\n| [OpenELM-3B-Instruct](https://huggingface.co/apple/OpenELM-3B-Instruct)     | **39.42** | **61.74** | **68.17** | **76.36**     | **79.00** | 92.50     | **66.85**      | **69.15**   |\n\n### LLM360\n\n| **Model Size**                                                              | **ARC-c** | **HellaSwag** | **MMLU**  | **TruthfulQA** | **WinoGrande** | **Average** |\n|-----------------------------------------------------------------------------|-----------|---------------|-----------|----------------|----------------|-------------|\n| [OpenELM-270M](https://huggingface.co/apple/OpenELM-270M)                   | 27.65     | 47.15         | 25.72     | **39.24**      | **53.83**      | 38.72       |\n| [OpenELM-270M-Instruct](https://huggingface.co/apple/OpenELM-270M-Instruct) | **32.51** | **51.58**     | **26.70** | 38.72          | 53.20          | **40.54**   |\n| [OpenELM-450M](https://huggingface.co/apple/OpenELM-450M)                   | 30.20     | 53.86         | **26.01** | 40.18          | 57.22          | 41.50       |\n| [OpenELM-450M-Instruct](https://huggingface.co/apple/OpenELM-450M-Instruct) | **33.53** | **59.31**     | 25.41     | **40.48**      | **58.33**      | **43.41**   |\n| [OpenELM-1_1B](https://huggingface.co/apple/OpenELM-1_1B)                   | 36.69     | 65.71         | **27.05** | 36.98          | 63.22          | 45.93       |\n| [OpenELM-1_1B-Instruct](https://huggingface.co/apple/OpenELM-1_1B-Instruct) | **41.55** | **71.83**     | 25.65     | **45.95**      | **64.72**      | **49.94**   |\n| [OpenELM-3B](https://huggingface.co/apple/OpenELM-3B)                       | 42.24     | 73.28         | **26.76** | 34.98          | 67.25          | 48.90       |\n| [OpenELM-3B-Instruct](https://huggingface.co/apple/OpenELM-3B-Instruct)     | **47.70** | **76.87**     | 24.80     | **38.76**      | **67.96**      | **51.22**   |\n\n\n### OpenLLM Leaderboard\n\n| **Model Size**                                                              | **ARC-c** | **CrowS-Pairs** | **HellaSwag** | **MMLU**  | **PIQA**  | **RACE**  | **TruthfulQA** | **WinoGrande** | **Average** |\n|-----------------------------------------------------------------------------|-----------|-----------------|---------------|-----------|-----------|-----------|----------------|----------------|-------------|\n| [OpenELM-270M](https://huggingface.co/apple/OpenELM-270M)                   | 27.65     | **66.79**       | 47.15         | 25.72     | 69.75     | 30.91     | **39.24**      | **53.83**      | 45.13       |\n| [OpenELM-270M-Instruct](https://huggingface.co/apple/OpenELM-270M-Instruct) | **32.51** | 66.01           | **51.58**     | **26.70** | **70.78** | 33.78     | 38.72          | 53.20          | **46.66**   |\n| [OpenELM-450M](https://huggingface.co/apple/OpenELM-450M)                   | 30.20     | **68.63**       | 53.86         | **26.01** | 72.31     | 33.11     | 40.18          | 57.22          | 47.69       |\n| [OpenELM-450M-Instruct](https://huggingface.co/apple/OpenELM-450M-Instruct) | **33.53** | 67.44           | **59.31**     | 25.41     | **72.63** | **36.84** | **40.48**      | **58.33**      | **49.25**   |\n| [OpenELM-1_1B](https://huggingface.co/apple/OpenELM-1_1B)                   | 36.69     | **71.74**       | 65.71         | **27.05** | **75.57** | 36.46     | 36.98          | 63.22          | 51.68       |\n| [OpenELM-1_1B-Instruct](https://huggingface.co/apple/OpenELM-1_1B-Instruct) | **41.55** | 71.02           | **71.83**     | 25.65     | 75.03     | **39.43** | **45.95**      | **64.72**      | **54.40**   |\n| [OpenELM-3B](https://huggingface.co/apple/OpenELM-3B)                       | 42.24     | **73.29**       | 73.28         | **26.76** | 78.24     | **38.76** | 34.98          | 67.25          | 54.35       |\n| [OpenELM-3B-Instruct](https://huggingface.co/apple/OpenELM-3B-Instruct)     | **47.70** | 72.33           | **76.87**     | 24.80     | **79.00** | 38.47     | **38.76**      | **67.96**      | **55.73**   |\n\nSee the technical report for more results and comparison.\n\n## Evaluation\n\n### Setup\n\nInstall the following dependencies:\n\n```bash\n\n# install public lm-eval-harness\n\nharness_repo=\"public-lm-eval-harness\"\ngit clone https://github.com/EleutherAI/lm-evaluation-harness ${harness_repo}\ncd ${harness_repo}\n# use main branch on 03-15-2024, SHA is dc90fec\ngit checkout dc90fec\npip install -e .\ncd ..\n\n# 66d6242 is the main branch on 2024-04-01 \npip install datasets@git+https://github.com/huggingface/datasets.git@66d6242\npip install tokenizers>=0.15.2 transformers>=4.38.2 sentencepiece>=0.2.0\n\n```\n\n### Evaluate OpenELM\n\n```bash\n\n# OpenELM-1_1B-Instruct\nhf_model=apple/OpenELM-1_1B-Instruct\n\n# this flag is needed because lm-eval-harness set add_bos_token to False by default, but OpenELM uses LLaMA tokenizer which requires add_bos_token to be True\ntokenizer=meta-llama/Llama-2-7b-hf\nadd_bos_token=True\nbatch_size=1\n\nmkdir lm_eval_output\n\nshot=0\ntask=arc_challenge,arc_easy,boolq,hellaswag,piqa,race,winogrande,sciq,truthfulqa_mc2\nlm_eval --model hf \\\n        --model_args pretrained=${hf_model},trust_remote_code=True,add_bos_token=${add_bos_token},tokenizer=${tokenizer} \\\n        --tasks ${task} \\\n        --device cuda:0 \\\n        --num_fewshot ${shot} \\\n        --output_path ./lm_eval_output/${hf_model//\\//_}_${task//,/_}-${shot}shot \\\n        --batch_size ${batch_size} 2>&1 | tee ./lm_eval_output/eval-${hf_model//\\//_}_${task//,/_}-${shot}shot.log\n\nshot=5\ntask=mmlu,winogrande\nlm_eval --model hf \\\n        --model_args pretrained=${hf_model},trust_remote_code=True,add_bos_token=${add_bos_token},tokenizer=${tokenizer} \\\n        --tasks ${task} \\\n        --device cuda:0 \\\n        --num_fewshot ${shot} \\\n        --output_path ./lm_eval_output/${hf_model//\\//_}_${task//,/_}-${shot}shot \\\n        --batch_size ${batch_size} 2>&1 | tee ./lm_eval_output/eval-${hf_model//\\//_}_${task//,/_}-${shot}shot.log\n\nshot=25\ntask=arc_challenge,crows_pairs_english\nlm_eval --model hf \\\n        --model_args pretrained=${hf_model},trust_remote_code=True,add_bos_token=${add_bos_token},tokenizer=${tokenizer} \\\n        --tasks ${task} \\\n        --device cuda:0 \\\n        --num_fewshot ${shot} \\\n        --output_path ./lm_eval_output/${hf_model//\\//_}_${task//,/_}-${shot}shot \\\n        --batch_size ${batch_size} 2>&1 | tee ./lm_eval_output/eval-${hf_model//\\//_}_${task//,/_}-${shot}shot.log\n\nshot=10\ntask=hellaswag\nlm_eval --model hf \\\n        --model_args pretrained=${hf_model},trust_remote_code=True,add_bos_token=${add_bos_token},tokenizer=${tokenizer} \\\n        --tasks ${task} \\\n        --device cuda:0 \\\n        --num_fewshot ${shot} \\\n        --output_path ./lm_eval_output/${hf_model//\\//_}_${task//,/_}-${shot}shot \\\n        --batch_size ${batch_size} 2>&1 | tee ./lm_eval_output/eval-${hf_model//\\//_}_${task//,/_}-${shot}shot.log\n\n```\n\n\n## Bias, Risks, and Limitations\n\nThe release of OpenELM models aims to empower and enrich the open research community by providing access to state-of-the-art language models. Trained on publicly available datasets, these models are made available without any safety guarantees. Consequently, there exists the possibility of these models producing outputs that are inaccurate, harmful, biased, or objectionable in response to user prompts. Thus, it is imperative for users and developers to undertake thorough safety testing and implement appropriate filtering mechanisms tailored to their specific requirements.\n\n## Citation\n\nIf you find our work useful, please cite:\n\n```BibTex \n@article{mehtaOpenELMEfficientLanguage2024,\n\ttitle = {{OpenELM}: {An} {Efficient} {Language} {Model} {Family} with {Open} {Training} and {Inference} {Framework}},\n\tshorttitle = {{OpenELM}},\n\turl = {https://arxiv.org/abs/2404.14619v1},\n\tlanguage = {en},\n\turldate = {2024-04-24},\n\tjournal = {arXiv.org},\n\tauthor = {Mehta, Sachin and Sekhavat, Mohammad Hossein and Cao, Qingqing and Horton, Maxwell and Jin, Yanzi and Sun, Chenfan and Mirzadeh, Iman and Najibi, Mahyar and Belenko, Dmitry and Zatloukal, Peter and Rastegari, Mohammad},\n\tmonth = apr,\n\tyear = {2024},\n}\n\n@inproceedings{mehta2022cvnets, \n     author = {Mehta, Sachin and Abdolhosseini, Farzad and Rastegari, Mohammad}, \n     title = {CVNets: High Performance Library for Computer Vision}, \n     year = {2022}, \n     booktitle = {Proceedings of the 30th ACM International Conference on Multimedia}, \n     series = {MM '22} \n}\n```\n",
    "card_content": "---\nlicense: apple-amlr\nlicense_name: apple-sample-code-license\nlicense_link: LICENSE\n---\n\n# OpenELM\n\n*Sachin Mehta, Mohammad Hossein Sekhavat, Qingqing Cao, Maxwell Horton, Yanzi Jin, Chenfan Sun, Iman Mirzadeh, Mahyar Najibi, Dmitry Belenko, Peter Zatloukal, Mohammad Rastegari*\n\nWe introduce **OpenELM**, a family of **Open** **E**fficient **L**anguage **M**odels. OpenELM uses a layer-wise scaling strategy to efficiently allocate parameters within each layer of the transformer model, leading to enhanced accuracy. We pretrained OpenELM models using the [CoreNet](https://github.com/apple/corenet) library. We release both pretrained and instruction tuned models with 270M, 450M, 1.1B and 3B parameters. We release the complete framework, encompassing data preparation, training, fine-tuning, and evaluation procedures, alongside multiple pre-trained checkpoints and training logs, to facilitate open research.\n\nOur pre-training dataset contains RefinedWeb, deduplicated PILE, a subset of RedPajama, and a subset of Dolma v1.6, totaling approximately 1.8 trillion tokens. Please check license agreements and terms of these datasets before using them.\n\n\n\n## Usage\n\nWe have provided an example function to generate output from OpenELM models loaded via [HuggingFace Hub](https://huggingface.co/docs/hub/) in `generate_openelm.py`.\n\nYou can try the model by running the following command:\n```\npython generate_openelm.py --model apple/OpenELM-1_1B-Instruct --hf_access_token [HF_ACCESS_TOKEN] --prompt 'Once upon a time there was' --generate_kwargs repetition_penalty=1.2\n```\nPlease refer to [this link](https://huggingface.co/docs/hub/security-tokens) to obtain your hugging face access token.\n\nAdditional arguments to the hugging face generate function can be passed via `generate_kwargs`. As an example, to speedup the inference, you can try [lookup token speculative generation](https://huggingface.co/docs/transformers/generation_strategies) by passing the `prompt_lookup_num_tokens` argument as follows:\n```\npython generate_openelm.py --model apple/OpenELM-1_1B-Instruct --hf_access_token [HF_ACCESS_TOKEN] --prompt 'Once upon a time there was' --generate_kwargs repetition_penalty=1.2 prompt_lookup_num_tokens=10\n```\nAlternatively, try model-wise speculative generation with an [assistive model](https://huggingface.co/blog/assisted-generation) by passing a smaller model through the `assistant_model` argument, for example:\n```\npython generate_openelm.py --model apple/OpenELM-1_1B-Instruct --hf_access_token [HF_ACCESS_TOKEN] --prompt 'Once upon a time there was' --generate_kwargs repetition_penalty=1.2 --assistant_model [SMALLER_MODEL]\n```\n\n## Main Results\n\n### Zero-Shot\n\n| **Model Size**                                                              | **ARC-c** | **ARC-e** | **BoolQ** | **HellaSwag** | **PIQA**  | **SciQ**  | **WinoGrande** | **Average** |\n|-----------------------------------------------------------------------------|-----------|-----------|-----------|---------------|-----------|-----------|----------------|-------------|\n| [OpenELM-270M](https://huggingface.co/apple/OpenELM-270M)                   | 26.45     | 45.08     | **53.98** | 46.71         | 69.75     | **84.70** | **53.91**      | 54.37       |\n| [OpenELM-270M-Instruct](https://huggingface.co/apple/OpenELM-270M-Instruct) | **30.55** | **46.68** | 48.56     | **52.07**     | **70.78** | 84.40     | 52.72          | **55.11**   |\n| [OpenELM-450M](https://huggingface.co/apple/OpenELM-450M)                   | 27.56     | 48.06     | 55.78     | 53.97         | 72.31     | 87.20     | 58.01          | 57.56       |\n| [OpenELM-450M-Instruct](https://huggingface.co/apple/OpenELM-450M-Instruct) | **30.38** | **50.00** | **60.37** | **59.34**     | **72.63** | **88.00** | **58.96**      | **59.95**   |\n| [OpenELM-1_1B](https://huggingface.co/apple/OpenELM-1_1B)                   | 32.34     | **55.43** | 63.58     | 64.81         | **75.57** | **90.60** | 61.72          | 63.44       |\n| [OpenELM-1_1B-Instruct](https://huggingface.co/apple/OpenELM-1_1B-Instruct) | **37.97** | 52.23     | **70.00** | **71.20**     | 75.03     | 89.30     | **62.75**      | **65.50**   |\n| [OpenELM-3B](https://huggingface.co/apple/OpenELM-3B)                       | 35.58     | 59.89     | 67.40     | 72.44         | 78.24     | **92.70** | 65.51          | 67.39       |\n| [OpenELM-3B-Instruct](https://huggingface.co/apple/OpenELM-3B-Instruct)     | **39.42** | **61.74** | **68.17** | **76.36**     | **79.00** | 92.50     | **66.85**      | **69.15**   |\n\n### LLM360\n\n| **Model Size**                                                              | **ARC-c** | **HellaSwag** | **MMLU**  | **TruthfulQA** | **WinoGrande** | **Average** |\n|-----------------------------------------------------------------------------|-----------|---------------|-----------|----------------|----------------|-------------|\n| [OpenELM-270M](https://huggingface.co/apple/OpenELM-270M)                   | 27.65     | 47.15         | 25.72     | **39.24**      | **53.83**      | 38.72       |\n| [OpenELM-270M-Instruct](https://huggingface.co/apple/OpenELM-270M-Instruct) | **32.51** | **51.58**     | **26.70** | 38.72          | 53.20          | **40.54**   |\n| [OpenELM-450M](https://huggingface.co/apple/OpenELM-450M)                   | 30.20     | 53.86         | **26.01** | 40.18          | 57.22          | 41.50       |\n| [OpenELM-450M-Instruct](https://huggingface.co/apple/OpenELM-450M-Instruct) | **33.53** | **59.31**     | 25.41     | **40.48**      | **58.33**      | **43.41**   |\n| [OpenELM-1_1B](https://huggingface.co/apple/OpenELM-1_1B)                   | 36.69     | 65.71         | **27.05** | 36.98          | 63.22          | 45.93       |\n| [OpenELM-1_1B-Instruct](https://huggingface.co/apple/OpenELM-1_1B-Instruct) | **41.55** | **71.83**     | 25.65     | **45.95**      | **64.72**      | **49.94**   |\n| [OpenELM-3B](https://huggingface.co/apple/OpenELM-3B)                       | 42.24     | 73.28         | **26.76** | 34.98          | 67.25          | 48.90       |\n| [OpenELM-3B-Instruct](https://huggingface.co/apple/OpenELM-3B-Instruct)     | **47.70** | **76.87**     | 24.80     | **38.76**      | **67.96**      | **51.22**   |\n\n\n### OpenLLM Leaderboard\n\n| **Model Size**                                                              | **ARC-c** | **CrowS-Pairs** | **HellaSwag** | **MMLU**  | **PIQA**  | **RACE**  | **TruthfulQA** | **WinoGrande** | **Average** |\n|-----------------------------------------------------------------------------|-----------|-----------------|---------------|-----------|-----------|-----------|----------------|----------------|-------------|\n| [OpenELM-270M](https://huggingface.co/apple/OpenELM-270M)                   | 27.65     | **66.79**       | 47.15         | 25.72     | 69.75     | 30.91     | **39.24**      | **53.83**      | 45.13       |\n| [OpenELM-270M-Instruct](https://huggingface.co/apple/OpenELM-270M-Instruct) | **32.51** | 66.01           | **51.58**     | **26.70** | **70.78** | 33.78     | 38.72          | 53.20          | **46.66**   |\n| [OpenELM-450M](https://huggingface.co/apple/OpenELM-450M)                   | 30.20     | **68.63**       | 53.86         | **26.01** | 72.31     | 33.11     | 40.18          | 57.22          | 47.69       |\n| [OpenELM-450M-Instruct](https://huggingface.co/apple/OpenELM-450M-Instruct) | **33.53** | 67.44           | **59.31**     | 25.41     | **72.63** | **36.84** | **40.48**      | **58.33**      | **49.25**   |\n| [OpenELM-1_1B](https://huggingface.co/apple/OpenELM-1_1B)                   | 36.69     | **71.74**       | 65.71         | **27.05** | **75.57** | 36.46     | 36.98          | 63.22          | 51.68       |\n| [OpenELM-1_1B-Instruct](https://huggingface.co/apple/OpenELM-1_1B-Instruct) | **41.55** | 71.02           | **71.83**     | 25.65     | 75.03     | **39.43** | **45.95**      | **64.72**      | **54.40**   |\n| [OpenELM-3B](https://huggingface.co/apple/OpenELM-3B)                       | 42.24     | **73.29**       | 73.28         | **26.76** | 78.24     | **38.76** | 34.98          | 67.25          | 54.35       |\n| [OpenELM-3B-Instruct](https://huggingface.co/apple/OpenELM-3B-Instruct)     | **47.70** | 72.33           | **76.87**     | 24.80     | **79.00** | 38.47     | **38.76**      | **67.96**      | **55.73**   |\n\nSee the technical report for more results and comparison.\n\n## Evaluation\n\n### Setup\n\nInstall the following dependencies:\n\n```bash\n\n# install public lm-eval-harness\n\nharness_repo=\"public-lm-eval-harness\"\ngit clone https://github.com/EleutherAI/lm-evaluation-harness ${harness_repo}\ncd ${harness_repo}\n# use main branch on 03-15-2024, SHA is dc90fec\ngit checkout dc90fec\npip install -e .\ncd ..\n\n# 66d6242 is the main branch on 2024-04-01 \npip install datasets@git+https://github.com/huggingface/datasets.git@66d6242\npip install tokenizers>=0.15.2 transformers>=4.38.2 sentencepiece>=0.2.0\n\n```\n\n### Evaluate OpenELM\n\n```bash\n\n# OpenELM-1_1B-Instruct\nhf_model=apple/OpenELM-1_1B-Instruct\n\n# this flag is needed because lm-eval-harness set add_bos_token to False by default, but OpenELM uses LLaMA tokenizer which requires add_bos_token to be True\ntokenizer=meta-llama/Llama-2-7b-hf\nadd_bos_token=True\nbatch_size=1\n\nmkdir lm_eval_output\n\nshot=0\ntask=arc_challenge,arc_easy,boolq,hellaswag,piqa,race,winogrande,sciq,truthfulqa_mc2\nlm_eval --model hf \\\n        --model_args pretrained=${hf_model},trust_remote_code=True,add_bos_token=${add_bos_token},tokenizer=${tokenizer} \\\n        --tasks ${task} \\\n        --device cuda:0 \\\n        --num_fewshot ${shot} \\\n        --output_path ./lm_eval_output/${hf_model//\\//_}_${task//,/_}-${shot}shot \\\n        --batch_size ${batch_size} 2>&1 | tee ./lm_eval_output/eval-${hf_model//\\//_}_${task//,/_}-${shot}shot.log\n\nshot=5\ntask=mmlu,winogrande\nlm_eval --model hf \\\n        --model_args pretrained=${hf_model},trust_remote_code=True,add_bos_token=${add_bos_token},tokenizer=${tokenizer} \\\n        --tasks ${task} \\\n        --device cuda:0 \\\n        --num_fewshot ${shot} \\\n        --output_path ./lm_eval_output/${hf_model//\\//_}_${task//,/_}-${shot}shot \\\n        --batch_size ${batch_size} 2>&1 | tee ./lm_eval_output/eval-${hf_model//\\//_}_${task//,/_}-${shot}shot.log\n\nshot=25\ntask=arc_challenge,crows_pairs_english\nlm_eval --model hf \\\n        --model_args pretrained=${hf_model},trust_remote_code=True,add_bos_token=${add_bos_token},tokenizer=${tokenizer} \\\n        --tasks ${task} \\\n        --device cuda:0 \\\n        --num_fewshot ${shot} \\\n        --output_path ./lm_eval_output/${hf_model//\\//_}_${task//,/_}-${shot}shot \\\n        --batch_size ${batch_size} 2>&1 | tee ./lm_eval_output/eval-${hf_model//\\//_}_${task//,/_}-${shot}shot.log\n\nshot=10\ntask=hellaswag\nlm_eval --model hf \\\n        --model_args pretrained=${hf_model},trust_remote_code=True,add_bos_token=${add_bos_token},tokenizer=${tokenizer} \\\n        --tasks ${task} \\\n        --device cuda:0 \\\n        --num_fewshot ${shot} \\\n        --output_path ./lm_eval_output/${hf_model//\\//_}_${task//,/_}-${shot}shot \\\n        --batch_size ${batch_size} 2>&1 | tee ./lm_eval_output/eval-${hf_model//\\//_}_${task//,/_}-${shot}shot.log\n\n```\n\n\n## Bias, Risks, and Limitations\n\nThe release of OpenELM models aims to empower and enrich the open research community by providing access to state-of-the-art language models. Trained on publicly available datasets, these models are made available without any safety guarantees. Consequently, there exists the possibility of these models producing outputs that are inaccurate, harmful, biased, or objectionable in response to user prompts. Thus, it is imperative for users and developers to undertake thorough safety testing and implement appropriate filtering mechanisms tailored to their specific requirements.\n\n## Citation\n\nIf you find our work useful, please cite:\n\n```BibTex \n@article{mehtaOpenELMEfficientLanguage2024,\n\ttitle = {{OpenELM}: {An} {Efficient} {Language} {Model} {Family} with {Open} {Training} and {Inference} {Framework}},\n\tshorttitle = {{OpenELM}},\n\turl = {https://arxiv.org/abs/2404.14619v1},\n\tlanguage = {en},\n\turldate = {2024-04-24},\n\tjournal = {arXiv.org},\n\tauthor = {Mehta, Sachin and Sekhavat, Mohammad Hossein and Cao, Qingqing and Horton, Maxwell and Jin, Yanzi and Sun, Chenfan and Mirzadeh, Iman and Najibi, Mahyar and Belenko, Dmitry and Zatloukal, Peter and Rastegari, Mohammad},\n\tmonth = apr,\n\tyear = {2024},\n}\n\n@inproceedings{mehta2022cvnets, \n     author = {Mehta, Sachin and Abdolhosseini, Farzad and Rastegari, Mohammad}, \n     title = {CVNets: High Performance Library for Computer Vision}, \n     year = {2022}, \n     booktitle = {Proceedings of the 30th ACM International Conference on Multimedia}, \n     series = {MM '22} \n}\n```\n",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModelForCausalLM",
      "custom_class": "modeling_openelm.OpenELMForCausalLM",
      "pipeline_tag": "text-generation",
      "processor": null
    },
    "safetensors": {
      "parameters": {
        "BF16": 1079891456
      },
      "total": 1079891456
    },
    "model_index": null,
    "trending_score": null
  },
  {
    "model_id": "bigcode/starcoder2-3b",
    "model_name": "bigcode/starcoder2-3b",
    "author": "bigcode",
    "downloads": 1434678,
    "downloads_all_time": null,
    "likes": 169,
    "tags": [
      "transformers",
      "safetensors",
      "starcoder2",
      "text-generation",
      "code",
      "dataset:bigcode/the-stack-v2-train",
      "arxiv:2305.13245",
      "arxiv:2205.14135",
      "arxiv:2004.05150",
      "arxiv:2207.14255",
      "arxiv:2402.19173",
      "license:bigcode-openrail-m",
      "model-index",
      "autotrain_compatible",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/bigcode/starcoder2-3b",
    "dependencies": [
      [
        "transformers",
        null
      ],
      [
        "torch",
        null
      ],
      [
        "accelerate",
        null
      ],
      [
        "bitsandbytes",
        null
      ]
    ],
    "last_modified": "2024-03-04T13:33:12+00:00",
    "created_at": "2023-11-29T15:22:51+00:00",
    "analysis_date": "2025-03-22T00:42:23.128552",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "starcoder2",
    "pipeline_tag": "text-generation",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "pipeline_tag": "text-generation",
      "inference": true,
      "widget": [
        {
          "text": "def print_hello_world():",
          "example_title": "Hello world",
          "group": "Python"
        }
      ],
      "datasets": [
        "bigcode/the-stack-v2-train"
      ],
      "license": "bigcode-openrail-m",
      "library_name": "transformers",
      "tags": [
        "code"
      ],
      "model-index": [
        {
          "name": "starcoder2-3b",
          "results": [
            {
              "task": {
                "type": "text-generation"
              },
              "dataset": {
                "name": "CruxEval-I",
                "type": "cruxeval-i"
              },
              "metrics": [
                {
                  "type": "pass@1",
                  "value": 32.7
                }
              ]
            },
            {
              "task": {
                "type": "text-generation"
              },
              "dataset": {
                "name": "DS-1000",
                "type": "ds-1000"
              },
              "metrics": [
                {
                  "type": "pass@1",
                  "value": 25.0
                }
              ]
            },
            {
              "task": {
                "type": "text-generation"
              },
              "dataset": {
                "name": "GSM8K (PAL)",
                "type": "gsm8k-pal"
              },
              "metrics": [
                {
                  "type": "accuracy",
                  "value": 27.7
                }
              ]
            },
            {
              "task": {
                "type": "text-generation"
              },
              "dataset": {
                "name": "HumanEval+",
                "type": "humanevalplus"
              },
              "metrics": [
                {
                  "type": "pass@1",
                  "value": 27.4
                }
              ]
            },
            {
              "task": {
                "type": "text-generation"
              },
              "dataset": {
                "name": "HumanEval",
                "type": "humaneval"
              },
              "metrics": [
                {
                  "type": "pass@1",
                  "value": 31.7
                }
              ]
            },
            {
              "task": {
                "type": "text-generation"
              },
              "dataset": {
                "name": "RepoBench-v1.1",
                "type": "repobench-v1.1"
              },
              "metrics": [
                {
                  "type": "edit-smiliarity",
                  "value": 71.19
                }
              ]
            }
          ]
        }
      ]
    },
    "card_text": "\n# StarCoder2\n\n<center>\n    <img src=\"https://huggingface.co/datasets/bigcode/admin_private/resolve/main/starcoder2_banner.png\" alt=\"SC2\" width=\"900\" height=\"600\">\n</center>\n\n##  Table of Contents\n\n1. [Model Summary](##model-summary)\n2. [Use](##use)\n3. [Limitations](##limitations)\n4. [Training](##training)\n5. [License](##license)\n6. [Citation](##citation)\n\n## Model Summary\n\nStarCoder2-3B model is a 3B parameter model trained on 17 programming languages from [The Stack v2](https://huggingface.co/datasets/bigcode/the-stack-v2-train), with opt-out requests excluded. The model uses [Grouped Query Attention](https://arxiv.org/abs/2305.13245), [a context window of 16,384 tokens](https://arxiv.org/abs/2205.14135) with [a sliding window attention of 4,096 tokens](https://arxiv.org/abs/2004.05150v2),  and was trained using the [Fill-in-the-Middle objective](https://arxiv.org/abs/2207.14255) on 3+ trillion tokens. \n\n- **Project Website:** [bigcode-project.org](https://www.bigcode-project.org)\n- **Paper:** [Link](https://huggingface.co/papers/2402.19173)\n- **Point of Contact:** [contact@bigcode-project.org](mailto:contact@bigcode-project.org)\n- **Languages:** 17 Programming languages\n\n## Use\n\n### Intended use\n\nThe model was trained on GitHub code as well as additional selected data sources such as Arxiv and Wikipedia. As such it is _not_ an instruction model and commands like \"Write a function that computes the square root.\" do not work well.\n\n### Generation\nHere are some examples to get started with the model. You can find a script for fine-tuning in StarCoder2's [GitHub repository](https://github.com/bigcode-project/starcoder2).\n\nFirst, make sure to install `transformers` from source:\n```bash\npip install git+https://github.com/huggingface/transformers.git\n```\n\n#### Running the model on CPU/GPU/multi GPU\n* _Using full precision_\n```python\n# pip install git+https://github.com/huggingface/transformers.git # TODO: merge PR to main\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ncheckpoint = \"bigcode/starcoder2-3b\"\ndevice = \"cuda\" # for GPU usage or \"cpu\" for CPU usage\n\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n# for multiple GPUs install accelerate and do `model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\")`\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)\n\ninputs = tokenizer.encode(\"def print_hello_world():\", return_tensors=\"pt\").to(device)\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\n```\n```bash\n>>> print(f\"Memory footprint: {model.get_memory_footprint() / 1e6:.2f} MB\")\nMemory footprint: 12624.81 MB\n```\n* _Using `torch.bfloat16`_\n```python\n# pip install accelerate\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ncheckpoint = \"bigcode/starcoder2-3b\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\n# for fp16 use `torch_dtype=torch.float16` instead\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\", torch_dtype=torch.bfloat16)\n\ninputs = tokenizer.encode(\"def print_hello_world():\", return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\n```\n```bash\n>>> print(f\"Memory footprint: {model.get_memory_footprint() / 1e6:.2f} MB\")\nMemory footprint: 6312.41 MB\n```\n\n#### Quantized Versions through `bitsandbytes`\n* _Using 8-bit precision (int8)_\n\n```python\n# pip install bitsandbytes accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\n# to use 4bit use `load_in_4bit=True` instead\nquantization_config = BitsAndBytesConfig(load_in_8bit=True)\n\ncheckpoint = \"bigcode/starcoder2-3b\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint, quantization_config=quantization_config)\n\ninputs = tokenizer.encode(\"def print_hello_world():\", return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\n```\n```bash\n>>> print(f\"Memory footprint: {model.get_memory_footprint() / 1e6:.2f} MB\")\n# load_in_8bit\nMemory footprint: 3434.07 MB\n# load_in_4bit\n>>> print(f\"Memory footprint: {model.get_memory_footprint() / 1e6:.2f} MB\")\nMemory footprint: 1994.90 MB\n```\n### Attribution & Other Requirements\n\nThe pretraining dataset of the model was filtered for permissive licenses and code with no license only. Nevertheless, the model can generate source code verbatim from the dataset. The code's license might require attribution and/or other specific requirements that must be respected. We provide a [search index](https://huggingface.co/spaces/bigcode/search-v2) that lets you search through the pretraining data to identify where the generated code came from, and apply the proper attribution to your code.\n\n# Limitations\n\nThe model has been trained on source code from 600+ programming languages. The predominant language in source is English although other languages are also present. As such the model is capable to generate code snippets provided some context but the generated code is not guaranteed to work as intended. It can be inefficient, contain bugs or exploits. See [the paper](https://huggingface.co/papers/2402.19173) for an in-depth discussion of the model limitations. \n\n# Training\n\n## Model\n\n- **Architecture:** Transformer decoder with grouped-query and sliding window attention and Fill-in-the-Middle objective\n- **Pretraining steps:** 1.2 million\n- **Pretraining tokens:** 3+ trillion\n- **Precision:** bfloat16\n\n## Hardware\n\n- **GPUs:** 160 A100\n\n## Software\n\n- **Framework:** TODO\n- **Neural networks:** [PyTorch](https://github.com/pytorch/pytorch)\n\n# License\n\nThe model is licensed under the BigCode OpenRAIL-M v1 license agreement. You can find the full agreement [here](https://huggingface.co/spaces/bigcode/bigcode-model-license-agreement).\n\n# Citation\n\n```bash\n@misc{lozhkov2024starcoder,\n      title={StarCoder 2 and The Stack v2: The Next Generation}, \n      author={Anton Lozhkov and Raymond Li and Loubna Ben Allal and Federico Cassano and Joel Lamy-Poirier and Nouamane Tazi and Ao Tang and Dmytro Pykhtar and Jiawei Liu and Yuxiang Wei and Tianyang Liu and Max Tian and Denis Kocetkov and Arthur Zucker and Younes Belkada and Zijian Wang and Qian Liu and Dmitry Abulkhanov and Indraneil Paul and Zhuang Li and Wen-Ding Li and Megan Risdal and Jia Li and Jian Zhu and Terry Yue Zhuo and Evgenii Zheltonozhskii and Nii Osae Osae Dade and Wenhao Yu and Lucas Krauß and Naman Jain and Yixuan Su and Xuanli He and Manan Dey and Edoardo Abati and Yekun Chai and Niklas Muennighoff and Xiangru Tang and Muhtasham Oblokulov and Christopher Akiki and Marc Marone and Chenghao Mou and Mayank Mishra and Alex Gu and Binyuan Hui and Tri Dao and Armel Zebaze and Olivier Dehaene and Nicolas Patry and Canwen Xu and Julian McAuley and Han Hu and Torsten Scholak and Sebastien Paquet and Jennifer Robinson and Carolyn Jane Anderson and Nicolas Chapados and Mostofa Patwary and Nima Tajbakhsh and Yacine Jernite and Carlos Muñoz Ferrandis and Lingming Zhang and Sean Hughes and Thomas Wolf and Arjun Guha and Leandro von Werra and Harm de Vries},\n      year={2024},\n      eprint={2402.19173},\n      archivePrefix={arXiv},\n      primaryClass={cs.SE}\n}\n```",
    "card_content": "---\npipeline_tag: text-generation\ninference: true\nwidget:\n- text: 'def print_hello_world():'\n  example_title: Hello world\n  group: Python\ndatasets:\n- bigcode/the-stack-v2-train\nlicense: bigcode-openrail-m\nlibrary_name: transformers\ntags:\n- code\nmodel-index:\n- name: starcoder2-3b\n  results:\n  - task:\n      type: text-generation\n    dataset:\n      name: CruxEval-I\n      type: cruxeval-i\n    metrics:\n    - type: pass@1\n      value: 32.7\n  - task:\n      type: text-generation\n    dataset:\n      name: DS-1000\n      type: ds-1000\n    metrics:\n    - type: pass@1\n      value: 25.0\n  - task:\n      type: text-generation\n    dataset:\n      name: GSM8K (PAL)\n      type: gsm8k-pal\n    metrics:\n    - type: accuracy\n      value: 27.7\n  - task:\n      type: text-generation\n    dataset:\n      name: HumanEval+\n      type: humanevalplus\n    metrics:\n    - type: pass@1\n      value: 27.4\n  - task:\n      type: text-generation\n    dataset:\n      name: HumanEval\n      type: humaneval\n    metrics:\n    - type: pass@1\n      value: 31.7\n  - task:\n      type: text-generation\n    dataset:\n      name: RepoBench-v1.1\n      type: repobench-v1.1\n    metrics:\n    - type: edit-smiliarity\n      value: 71.19\n---\n\n# StarCoder2\n\n<center>\n    <img src=\"https://huggingface.co/datasets/bigcode/admin_private/resolve/main/starcoder2_banner.png\" alt=\"SC2\" width=\"900\" height=\"600\">\n</center>\n\n##  Table of Contents\n\n1. [Model Summary](##model-summary)\n2. [Use](##use)\n3. [Limitations](##limitations)\n4. [Training](##training)\n5. [License](##license)\n6. [Citation](##citation)\n\n## Model Summary\n\nStarCoder2-3B model is a 3B parameter model trained on 17 programming languages from [The Stack v2](https://huggingface.co/datasets/bigcode/the-stack-v2-train), with opt-out requests excluded. The model uses [Grouped Query Attention](https://arxiv.org/abs/2305.13245), [a context window of 16,384 tokens](https://arxiv.org/abs/2205.14135) with [a sliding window attention of 4,096 tokens](https://arxiv.org/abs/2004.05150v2),  and was trained using the [Fill-in-the-Middle objective](https://arxiv.org/abs/2207.14255) on 3+ trillion tokens. \n\n- **Project Website:** [bigcode-project.org](https://www.bigcode-project.org)\n- **Paper:** [Link](https://huggingface.co/papers/2402.19173)\n- **Point of Contact:** [contact@bigcode-project.org](mailto:contact@bigcode-project.org)\n- **Languages:** 17 Programming languages\n\n## Use\n\n### Intended use\n\nThe model was trained on GitHub code as well as additional selected data sources such as Arxiv and Wikipedia. As such it is _not_ an instruction model and commands like \"Write a function that computes the square root.\" do not work well.\n\n### Generation\nHere are some examples to get started with the model. You can find a script for fine-tuning in StarCoder2's [GitHub repository](https://github.com/bigcode-project/starcoder2).\n\nFirst, make sure to install `transformers` from source:\n```bash\npip install git+https://github.com/huggingface/transformers.git\n```\n\n#### Running the model on CPU/GPU/multi GPU\n* _Using full precision_\n```python\n# pip install git+https://github.com/huggingface/transformers.git # TODO: merge PR to main\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ncheckpoint = \"bigcode/starcoder2-3b\"\ndevice = \"cuda\" # for GPU usage or \"cpu\" for CPU usage\n\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n# for multiple GPUs install accelerate and do `model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\")`\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)\n\ninputs = tokenizer.encode(\"def print_hello_world():\", return_tensors=\"pt\").to(device)\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\n```\n```bash\n>>> print(f\"Memory footprint: {model.get_memory_footprint() / 1e6:.2f} MB\")\nMemory footprint: 12624.81 MB\n```\n* _Using `torch.bfloat16`_\n```python\n# pip install accelerate\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ncheckpoint = \"bigcode/starcoder2-3b\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\n# for fp16 use `torch_dtype=torch.float16` instead\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\", torch_dtype=torch.bfloat16)\n\ninputs = tokenizer.encode(\"def print_hello_world():\", return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\n```\n```bash\n>>> print(f\"Memory footprint: {model.get_memory_footprint() / 1e6:.2f} MB\")\nMemory footprint: 6312.41 MB\n```\n\n#### Quantized Versions through `bitsandbytes`\n* _Using 8-bit precision (int8)_\n\n```python\n# pip install bitsandbytes accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\n# to use 4bit use `load_in_4bit=True` instead\nquantization_config = BitsAndBytesConfig(load_in_8bit=True)\n\ncheckpoint = \"bigcode/starcoder2-3b\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint, quantization_config=quantization_config)\n\ninputs = tokenizer.encode(\"def print_hello_world():\", return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\n```\n```bash\n>>> print(f\"Memory footprint: {model.get_memory_footprint() / 1e6:.2f} MB\")\n# load_in_8bit\nMemory footprint: 3434.07 MB\n# load_in_4bit\n>>> print(f\"Memory footprint: {model.get_memory_footprint() / 1e6:.2f} MB\")\nMemory footprint: 1994.90 MB\n```\n### Attribution & Other Requirements\n\nThe pretraining dataset of the model was filtered for permissive licenses and code with no license only. Nevertheless, the model can generate source code verbatim from the dataset. The code's license might require attribution and/or other specific requirements that must be respected. We provide a [search index](https://huggingface.co/spaces/bigcode/search-v2) that lets you search through the pretraining data to identify where the generated code came from, and apply the proper attribution to your code.\n\n# Limitations\n\nThe model has been trained on source code from 600+ programming languages. The predominant language in source is English although other languages are also present. As such the model is capable to generate code snippets provided some context but the generated code is not guaranteed to work as intended. It can be inefficient, contain bugs or exploits. See [the paper](https://huggingface.co/papers/2402.19173) for an in-depth discussion of the model limitations. \n\n# Training\n\n## Model\n\n- **Architecture:** Transformer decoder with grouped-query and sliding window attention and Fill-in-the-Middle objective\n- **Pretraining steps:** 1.2 million\n- **Pretraining tokens:** 3+ trillion\n- **Precision:** bfloat16\n\n## Hardware\n\n- **GPUs:** 160 A100\n\n## Software\n\n- **Framework:** TODO\n- **Neural networks:** [PyTorch](https://github.com/pytorch/pytorch)\n\n# License\n\nThe model is licensed under the BigCode OpenRAIL-M v1 license agreement. You can find the full agreement [here](https://huggingface.co/spaces/bigcode/bigcode-model-license-agreement).\n\n# Citation\n\n```bash\n@misc{lozhkov2024starcoder,\n      title={StarCoder 2 and The Stack v2: The Next Generation}, \n      author={Anton Lozhkov and Raymond Li and Loubna Ben Allal and Federico Cassano and Joel Lamy-Poirier and Nouamane Tazi and Ao Tang and Dmytro Pykhtar and Jiawei Liu and Yuxiang Wei and Tianyang Liu and Max Tian and Denis Kocetkov and Arthur Zucker and Younes Belkada and Zijian Wang and Qian Liu and Dmitry Abulkhanov and Indraneil Paul and Zhuang Li and Wen-Ding Li and Megan Risdal and Jia Li and Jian Zhu and Terry Yue Zhuo and Evgenii Zheltonozhskii and Nii Osae Osae Dade and Wenhao Yu and Lucas Krauß and Naman Jain and Yixuan Su and Xuanli He and Manan Dey and Edoardo Abati and Yekun Chai and Niklas Muennighoff and Xiangru Tang and Muhtasham Oblokulov and Christopher Akiki and Marc Marone and Chenghao Mou and Mayank Mishra and Alex Gu and Binyuan Hui and Tri Dao and Armel Zebaze and Olivier Dehaene and Nicolas Patry and Canwen Xu and Julian McAuley and Han Hu and Torsten Scholak and Sebastien Paquet and Jennifer Robinson and Carolyn Jane Anderson and Nicolas Chapados and Mostofa Patwary and Nima Tajbakhsh and Yacine Jernite and Carlos Muñoz Ferrandis and Lingming Zhang and Sean Hughes and Thomas Wolf and Arjun Guha and Leandro von Werra and Harm de Vries},\n      year={2024},\n      eprint={2402.19173},\n      archivePrefix={arXiv},\n      primaryClass={cs.SE}\n}\n```",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModelForCausalLM",
      "custom_class": null,
      "pipeline_tag": "text-generation",
      "processor": "AutoTokenizer"
    },
    "safetensors": {
      "parameters": {
        "F32": 3030371328
      },
      "total": 3030371328
    },
    "model_index": [
      {
        "name": "starcoder2-3b",
        "results": [
          {
            "task": {
              "type": "text-generation"
            },
            "dataset": {
              "name": "CruxEval-I",
              "type": "cruxeval-i"
            },
            "metrics": [
              {
                "type": "pass@1",
                "value": 32.7,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "text-generation"
            },
            "dataset": {
              "name": "DS-1000",
              "type": "ds-1000"
            },
            "metrics": [
              {
                "type": "pass@1",
                "value": 25,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "text-generation"
            },
            "dataset": {
              "name": "GSM8K (PAL)",
              "type": "gsm8k-pal"
            },
            "metrics": [
              {
                "type": "accuracy",
                "value": 27.7,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "text-generation"
            },
            "dataset": {
              "name": "HumanEval+",
              "type": "humanevalplus"
            },
            "metrics": [
              {
                "type": "pass@1",
                "value": 27.4,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "text-generation"
            },
            "dataset": {
              "name": "HumanEval",
              "type": "humaneval"
            },
            "metrics": [
              {
                "type": "pass@1",
                "value": 31.7,
                "verified": false
              }
            ]
          },
          {
            "task": {
              "type": "text-generation"
            },
            "dataset": {
              "name": "RepoBench-v1.1",
              "type": "repobench-v1.1"
            },
            "metrics": [
              {
                "type": "edit-smiliarity",
                "value": 71.19,
                "verified": false
              }
            ]
          }
        ]
      }
    ],
    "trending_score": null
  },
  {
    "model_id": "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
    "model_name": "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
    "author": "deepseek-ai",
    "downloads": 1316303,
    "downloads_all_time": null,
    "likes": 559,
    "tags": [
      "transformers",
      "safetensors",
      "qwen2",
      "text-generation",
      "conversational",
      "arxiv:2501.12948",
      "license:mit",
      "autotrain_compatible",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
    "dependencies": [
      [
        "transformers",
        null
      ]
    ],
    "last_modified": "2025-02-24T03:32:20+00:00",
    "created_at": "2025-01-20T09:16:14+00:00",
    "analysis_date": "2025-03-22T00:42:24.230596",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "qwen2",
    "pipeline_tag": "text-generation",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "license": "mit",
      "library_name": "transformers"
    },
    "card_text": "# DeepSeek-R1\n<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align=\"center\">\n  <img src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true\" width=\"60%\" alt=\"DeepSeek-V3\" />\n</div>\n<hr>\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://www.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Homepage\" src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://chat.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/🤖%20Chat-DeepSeek%20R1-536af5?color=536af5&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://huggingface.co/deepseek-ai\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://discord.gg/Tc7c45Zzu5\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Wechat\" src=\"https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://twitter.com/deepseek_ai\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Twitter Follow\" src=\"https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE\" style=\"margin: 2px;\">\n    <img alt=\"License\" src=\"https://img.shields.io/badge/License-MIT-f5de53?&color=f5de53\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n\n<p align=\"center\">\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf\"><b>Paper Link</b>👁️</a>\n</p>\n\n\n## 1. Introduction\n\nWe introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. \nDeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning.\nWith RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors.\nHowever, DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. To address these issues and further enhance reasoning performance,\nwe introduce DeepSeek-R1, which incorporates cold-start data before RL.\nDeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks. \nTo support the research community, we have open-sourced DeepSeek-R1-Zero, DeepSeek-R1, and six dense models distilled from DeepSeek-R1 based on Llama and Qwen. DeepSeek-R1-Distill-Qwen-32B outperforms OpenAI-o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.\n\n**NOTE: Before running DeepSeek-R1 series models locally, we kindly recommend reviewing the [Usage Recommendation](#usage-recommendations) section.**\n\n<p align=\"center\">\n  <img width=\"80%\" src=\"figures/benchmark.jpg\">\n</p>\n\n## 2. Model Summary\n\n---\n\n**Post-Training: Large-Scale Reinforcement Learning on the Base Model**\n\n-  We directly apply reinforcement learning (RL) to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.\n\n-   We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model's reasoning and non-reasoning capabilities.\n    We believe the pipeline will benefit the industry by creating better models. \n\n---\n\n**Distillation: Smaller Models Can Be Powerful Too**\n\n-  We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future. \n- Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.\n\n## 3. Model Downloads\n\n### DeepSeek-R1 Models\n\n<div align=\"center\">\n\n| **Model** | **#Total Params** | **#Activated Params** | **Context Length** | **Download** |\n| :------------: | :------------: | :------------: | :------------: | :------------: |\n| DeepSeek-R1-Zero | 671B | 37B | 128K   | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Zero)   |\n| DeepSeek-R1   | 671B | 37B |  128K   | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1)   |\n\n</div>\n\nDeepSeek-R1-Zero & DeepSeek-R1 are trained based on DeepSeek-V3-Base. \nFor more details regarding the model architecture, please refer to [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) repository.\n\n### DeepSeek-R1-Distill Models\n\n<div align=\"center\">\n\n| **Model** | **Base Model** | **Download** |\n| :------------: | :------------: | :------------: |\n| DeepSeek-R1-Distill-Qwen-1.5B  | [Qwen2.5-Math-1.5B](https://huggingface.co/Qwen/Qwen2.5-Math-1.5B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B)   |\n| DeepSeek-R1-Distill-Qwen-7B  | [Qwen2.5-Math-7B](https://huggingface.co/Qwen/Qwen2.5-Math-7B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B)   |\n| DeepSeek-R1-Distill-Llama-8B  | [Llama-3.1-8B](https://huggingface.co/meta-llama/Llama-3.1-8B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B)   |\n| DeepSeek-R1-Distill-Qwen-14B   | [Qwen2.5-14B](https://huggingface.co/Qwen/Qwen2.5-14B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B)   |\n|DeepSeek-R1-Distill-Qwen-32B  | [Qwen2.5-32B](https://huggingface.co/Qwen/Qwen2.5-32B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B)   |\n| DeepSeek-R1-Distill-Llama-70B  | [Llama-3.3-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B)   |\n\n</div>\n\nDeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1.\nWe slightly change their configs and tokenizers. Please use our setting to run these models.\n\n## 4. Evaluation Results\n\n### DeepSeek-R1-Evaluation\n For all our models, the maximum generation length is set to 32,768 tokens. For benchmarks requiring sampling, we use a temperature of $0.6$, a top-p value of $0.95$, and generate 64 responses per query to estimate pass@1.\n<div align=\"center\">\n\n\n| Category | Benchmark (Metric) | Claude-3.5-Sonnet-1022 | GPT-4o 0513 | DeepSeek V3 | OpenAI o1-mini | OpenAI o1-1217 | DeepSeek R1 |\n|----------|-------------------|----------------------|------------|--------------|----------------|------------|--------------|\n| | Architecture | - | - | MoE | - | - | MoE |\n| | # Activated Params | - | - | 37B | - | - | 37B |\n| | # Total Params | - | - | 671B | - | - | 671B |\n| English | MMLU (Pass@1) | 88.3 | 87.2 | 88.5 | 85.2 | **91.8** | 90.8 |\n| | MMLU-Redux (EM) | 88.9 | 88.0 | 89.1 | 86.7 | - | **92.9** |\n| | MMLU-Pro (EM) | 78.0 | 72.6 | 75.9 | 80.3 | - | **84.0** |\n| | DROP (3-shot F1) | 88.3 | 83.7 | 91.6 | 83.9 | 90.2 | **92.2** |\n| | IF-Eval (Prompt Strict) | **86.5** | 84.3 | 86.1 | 84.8 | - | 83.3 |\n| | GPQA-Diamond (Pass@1) | 65.0 | 49.9 | 59.1 | 60.0 | **75.7** | 71.5 |\n| | SimpleQA (Correct) | 28.4 | 38.2 | 24.9 | 7.0 | **47.0** | 30.1 |\n| | FRAMES (Acc.) | 72.5 | 80.5 | 73.3 | 76.9 | - | **82.5** |\n| | AlpacaEval2.0 (LC-winrate) | 52.0 | 51.1 | 70.0 | 57.8 | - | **87.6** |\n| | ArenaHard (GPT-4-1106) | 85.2 | 80.4 | 85.5 | 92.0 | - | **92.3** |\n| Code | LiveCodeBench (Pass@1-COT) | 33.8 | 34.2 | - | 53.8 | 63.4 | **65.9** |\n| | Codeforces (Percentile) | 20.3 | 23.6 | 58.7 | 93.4 | **96.6** | 96.3 |\n| | Codeforces (Rating) | 717 | 759 | 1134 | 1820 | **2061** | 2029 |\n| | SWE Verified (Resolved) | **50.8** | 38.8 | 42.0 | 41.6 | 48.9 | 49.2 |\n| | Aider-Polyglot (Acc.) | 45.3 | 16.0 | 49.6 | 32.9 | **61.7** | 53.3 |\n| Math | AIME 2024 (Pass@1) | 16.0 | 9.3 | 39.2 | 63.6 | 79.2 | **79.8** |\n| | MATH-500 (Pass@1) | 78.3 | 74.6 | 90.2 | 90.0 | 96.4 | **97.3** |\n| | CNMO 2024 (Pass@1) | 13.1 | 10.8 | 43.2 | 67.6 | - | **78.8** |\n| Chinese | CLUEWSC (EM) | 85.4 | 87.9 | 90.9 | 89.9 | - | **92.8** |\n| | C-Eval (EM) | 76.7 | 76.0 | 86.5 | 68.9 | - | **91.8** |\n| | C-SimpleQA (Correct) | 55.4 | 58.7 | **68.0** | 40.3 | - | 63.7 |\n\n</div>\n\n\n### Distilled Model Evaluation\n\n\n<div align=\"center\">\n\n| Model                                    | AIME 2024 pass@1 | AIME 2024 cons@64 | MATH-500 pass@1 | GPQA Diamond pass@1 | LiveCodeBench pass@1 | CodeForces rating |\n|------------------------------------------|------------------|-------------------|-----------------|----------------------|----------------------|-------------------|\n| GPT-4o-0513                          | 9.3              | 13.4              | 74.6            | 49.9                 | 32.9                 | 759               |\n| Claude-3.5-Sonnet-1022             | 16.0             | 26.7                 | 78.3            | 65.0                 | 38.9                 | 717               |\n| o1-mini                              | 63.6             | 80.0              | 90.0            | 60.0                 | 53.8                 | **1820**          |\n| QwQ-32B-Preview                              | 44.0             | 60.0                 | 90.6            | 54.5               | 41.9                 | 1316              |\n| DeepSeek-R1-Distill-Qwen-1.5B       | 28.9             | 52.7              | 83.9            | 33.8                 | 16.9                 | 954               |\n| DeepSeek-R1-Distill-Qwen-7B          | 55.5             | 83.3              | 92.8            | 49.1                 | 37.6                 | 1189              |\n| DeepSeek-R1-Distill-Qwen-14B         | 69.7             | 80.0              | 93.9            | 59.1                 | 53.1                 | 1481              |\n| DeepSeek-R1-Distill-Qwen-32B        | **72.6**         | 83.3              | 94.3            | 62.1                 | 57.2                 | 1691              |\n| DeepSeek-R1-Distill-Llama-8B         | 50.4             | 80.0              | 89.1            | 49.0                 | 39.6                 | 1205              |\n| DeepSeek-R1-Distill-Llama-70B        | 70.0             | **86.7**          | **94.5**        | **65.2**             | **57.5**             | 1633              |\n\n</div>\n\n\n## 5. Chat Website & API Platform\nYou can chat with DeepSeek-R1 on DeepSeek's official website: [chat.deepseek.com](https://chat.deepseek.com), and switch on the button \"DeepThink\"\n\nWe also provide OpenAI-Compatible API at DeepSeek Platform: [platform.deepseek.com](https://platform.deepseek.com/)\n\n## 6. How to Run Locally\n\n### DeepSeek-R1 Models\n\nPlease visit [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) repo for more information about running DeepSeek-R1 locally.\n\n**NOTE: Hugging Face's Transformers has not been directly supported yet.**\n\n### DeepSeek-R1-Distill Models\n\nDeepSeek-R1-Distill models can be utilized in the same manner as Qwen or Llama models.\n\nFor instance, you can easily start a service using [vLLM](https://github.com/vllm-project/vllm):\n\n```shell\nvllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --tensor-parallel-size 2 --max-model-len 32768 --enforce-eager\n```\n\nYou can also easily start a service using [SGLang](https://github.com/sgl-project/sglang)\n\n```bash\npython3 -m sglang.launch_server --model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --trust-remote-code --tp 2\n```\n\n### Usage Recommendations\n\n**We recommend adhering to the following configurations when utilizing the DeepSeek-R1 series models, including benchmarking, to achieve the expected performance:**\n\n1. Set the temperature within the range of 0.5-0.7 (0.6 is recommended) to prevent endless repetitions or incoherent outputs.\n2. **Avoid adding a system prompt; all instructions should be contained within the user prompt.**\n3. For mathematical problems, it is advisable to include a directive in your prompt such as: \"Please reason step by step, and put your final answer within \\boxed{}.\"\n4. When evaluating model performance, it is recommended to conduct multiple tests and average the results.\n\nAdditionally, we have observed that the DeepSeek-R1 series models tend to bypass thinking pattern (i.e., outputting \"\\<think\\>\\n\\n\\</think\\>\") when responding to certain queries, which can adversely affect the model's performance.\n**To ensure that the model engages in thorough reasoning, we recommend enforcing the model to initiate its response with \"\\<think\\>\\n\" at the beginning of every output.**\n\n## 7. License\nThis code repository and the model weights are licensed under the [MIT License](https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE).\nDeepSeek-R1 series support commercial use, allow for any modifications and derivative works, including, but not limited to, distillation for training other LLMs. Please note that:\n- DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Qwen-14B and DeepSeek-R1-Distill-Qwen-32B are derived from [Qwen-2.5 series](https://github.com/QwenLM/Qwen2.5), which are originally licensed under [Apache 2.0 License](https://huggingface.co/Qwen/Qwen2.5-1.5B/blob/main/LICENSE), and now finetuned with 800k samples curated with DeepSeek-R1.\n- DeepSeek-R1-Distill-Llama-8B is derived from Llama3.1-8B-Base and is originally licensed under [llama3.1 license](https://huggingface.co/meta-llama/Llama-3.1-8B/blob/main/LICENSE).\n- DeepSeek-R1-Distill-Llama-70B is derived from Llama3.3-70B-Instruct and is originally licensed under [llama3.3 license](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct/blob/main/LICENSE).\n\n## 8. Citation\n```\n@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\n      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, \n      author={DeepSeek-AI},\n      year={2025},\n      eprint={2501.12948},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2501.12948}, \n}\n\n```\n\n## 9. Contact\nIf you have any questions, please raise an issue or contact us at [service@deepseek.com](service@deepseek.com).\n",
    "card_content": "---\nlicense: mit\nlibrary_name: transformers\n---\n# DeepSeek-R1\n<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align=\"center\">\n  <img src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true\" width=\"60%\" alt=\"DeepSeek-V3\" />\n</div>\n<hr>\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://www.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Homepage\" src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://chat.deepseek.com/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/🤖%20Chat-DeepSeek%20R1-536af5?color=536af5&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://huggingface.co/deepseek-ai\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://discord.gg/Tc7c45Zzu5\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Wechat\" src=\"https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://twitter.com/deepseek_ai\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Twitter Follow\" src=\"https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE\" style=\"margin: 2px;\">\n    <img alt=\"License\" src=\"https://img.shields.io/badge/License-MIT-f5de53?&color=f5de53\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\n\n<p align=\"center\">\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf\"><b>Paper Link</b>👁️</a>\n</p>\n\n\n## 1. Introduction\n\nWe introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. \nDeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning.\nWith RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors.\nHowever, DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. To address these issues and further enhance reasoning performance,\nwe introduce DeepSeek-R1, which incorporates cold-start data before RL.\nDeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks. \nTo support the research community, we have open-sourced DeepSeek-R1-Zero, DeepSeek-R1, and six dense models distilled from DeepSeek-R1 based on Llama and Qwen. DeepSeek-R1-Distill-Qwen-32B outperforms OpenAI-o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.\n\n**NOTE: Before running DeepSeek-R1 series models locally, we kindly recommend reviewing the [Usage Recommendation](#usage-recommendations) section.**\n\n<p align=\"center\">\n  <img width=\"80%\" src=\"figures/benchmark.jpg\">\n</p>\n\n## 2. Model Summary\n\n---\n\n**Post-Training: Large-Scale Reinforcement Learning on the Base Model**\n\n-  We directly apply reinforcement learning (RL) to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.\n\n-   We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model's reasoning and non-reasoning capabilities.\n    We believe the pipeline will benefit the industry by creating better models. \n\n---\n\n**Distillation: Smaller Models Can Be Powerful Too**\n\n-  We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future. \n- Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.\n\n## 3. Model Downloads\n\n### DeepSeek-R1 Models\n\n<div align=\"center\">\n\n| **Model** | **#Total Params** | **#Activated Params** | **Context Length** | **Download** |\n| :------------: | :------------: | :------------: | :------------: | :------------: |\n| DeepSeek-R1-Zero | 671B | 37B | 128K   | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Zero)   |\n| DeepSeek-R1   | 671B | 37B |  128K   | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1)   |\n\n</div>\n\nDeepSeek-R1-Zero & DeepSeek-R1 are trained based on DeepSeek-V3-Base. \nFor more details regarding the model architecture, please refer to [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) repository.\n\n### DeepSeek-R1-Distill Models\n\n<div align=\"center\">\n\n| **Model** | **Base Model** | **Download** |\n| :------------: | :------------: | :------------: |\n| DeepSeek-R1-Distill-Qwen-1.5B  | [Qwen2.5-Math-1.5B](https://huggingface.co/Qwen/Qwen2.5-Math-1.5B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B)   |\n| DeepSeek-R1-Distill-Qwen-7B  | [Qwen2.5-Math-7B](https://huggingface.co/Qwen/Qwen2.5-Math-7B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B)   |\n| DeepSeek-R1-Distill-Llama-8B  | [Llama-3.1-8B](https://huggingface.co/meta-llama/Llama-3.1-8B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B)   |\n| DeepSeek-R1-Distill-Qwen-14B   | [Qwen2.5-14B](https://huggingface.co/Qwen/Qwen2.5-14B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B)   |\n|DeepSeek-R1-Distill-Qwen-32B  | [Qwen2.5-32B](https://huggingface.co/Qwen/Qwen2.5-32B) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B)   |\n| DeepSeek-R1-Distill-Llama-70B  | [Llama-3.3-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct) | [🤗 HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B)   |\n\n</div>\n\nDeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1.\nWe slightly change their configs and tokenizers. Please use our setting to run these models.\n\n## 4. Evaluation Results\n\n### DeepSeek-R1-Evaluation\n For all our models, the maximum generation length is set to 32,768 tokens. For benchmarks requiring sampling, we use a temperature of $0.6$, a top-p value of $0.95$, and generate 64 responses per query to estimate pass@1.\n<div align=\"center\">\n\n\n| Category | Benchmark (Metric) | Claude-3.5-Sonnet-1022 | GPT-4o 0513 | DeepSeek V3 | OpenAI o1-mini | OpenAI o1-1217 | DeepSeek R1 |\n|----------|-------------------|----------------------|------------|--------------|----------------|------------|--------------|\n| | Architecture | - | - | MoE | - | - | MoE |\n| | # Activated Params | - | - | 37B | - | - | 37B |\n| | # Total Params | - | - | 671B | - | - | 671B |\n| English | MMLU (Pass@1) | 88.3 | 87.2 | 88.5 | 85.2 | **91.8** | 90.8 |\n| | MMLU-Redux (EM) | 88.9 | 88.0 | 89.1 | 86.7 | - | **92.9** |\n| | MMLU-Pro (EM) | 78.0 | 72.6 | 75.9 | 80.3 | - | **84.0** |\n| | DROP (3-shot F1) | 88.3 | 83.7 | 91.6 | 83.9 | 90.2 | **92.2** |\n| | IF-Eval (Prompt Strict) | **86.5** | 84.3 | 86.1 | 84.8 | - | 83.3 |\n| | GPQA-Diamond (Pass@1) | 65.0 | 49.9 | 59.1 | 60.0 | **75.7** | 71.5 |\n| | SimpleQA (Correct) | 28.4 | 38.2 | 24.9 | 7.0 | **47.0** | 30.1 |\n| | FRAMES (Acc.) | 72.5 | 80.5 | 73.3 | 76.9 | - | **82.5** |\n| | AlpacaEval2.0 (LC-winrate) | 52.0 | 51.1 | 70.0 | 57.8 | - | **87.6** |\n| | ArenaHard (GPT-4-1106) | 85.2 | 80.4 | 85.5 | 92.0 | - | **92.3** |\n| Code | LiveCodeBench (Pass@1-COT) | 33.8 | 34.2 | - | 53.8 | 63.4 | **65.9** |\n| | Codeforces (Percentile) | 20.3 | 23.6 | 58.7 | 93.4 | **96.6** | 96.3 |\n| | Codeforces (Rating) | 717 | 759 | 1134 | 1820 | **2061** | 2029 |\n| | SWE Verified (Resolved) | **50.8** | 38.8 | 42.0 | 41.6 | 48.9 | 49.2 |\n| | Aider-Polyglot (Acc.) | 45.3 | 16.0 | 49.6 | 32.9 | **61.7** | 53.3 |\n| Math | AIME 2024 (Pass@1) | 16.0 | 9.3 | 39.2 | 63.6 | 79.2 | **79.8** |\n| | MATH-500 (Pass@1) | 78.3 | 74.6 | 90.2 | 90.0 | 96.4 | **97.3** |\n| | CNMO 2024 (Pass@1) | 13.1 | 10.8 | 43.2 | 67.6 | - | **78.8** |\n| Chinese | CLUEWSC (EM) | 85.4 | 87.9 | 90.9 | 89.9 | - | **92.8** |\n| | C-Eval (EM) | 76.7 | 76.0 | 86.5 | 68.9 | - | **91.8** |\n| | C-SimpleQA (Correct) | 55.4 | 58.7 | **68.0** | 40.3 | - | 63.7 |\n\n</div>\n\n\n### Distilled Model Evaluation\n\n\n<div align=\"center\">\n\n| Model                                    | AIME 2024 pass@1 | AIME 2024 cons@64 | MATH-500 pass@1 | GPQA Diamond pass@1 | LiveCodeBench pass@1 | CodeForces rating |\n|------------------------------------------|------------------|-------------------|-----------------|----------------------|----------------------|-------------------|\n| GPT-4o-0513                          | 9.3              | 13.4              | 74.6            | 49.9                 | 32.9                 | 759               |\n| Claude-3.5-Sonnet-1022             | 16.0             | 26.7                 | 78.3            | 65.0                 | 38.9                 | 717               |\n| o1-mini                              | 63.6             | 80.0              | 90.0            | 60.0                 | 53.8                 | **1820**          |\n| QwQ-32B-Preview                              | 44.0             | 60.0                 | 90.6            | 54.5               | 41.9                 | 1316              |\n| DeepSeek-R1-Distill-Qwen-1.5B       | 28.9             | 52.7              | 83.9            | 33.8                 | 16.9                 | 954               |\n| DeepSeek-R1-Distill-Qwen-7B          | 55.5             | 83.3              | 92.8            | 49.1                 | 37.6                 | 1189              |\n| DeepSeek-R1-Distill-Qwen-14B         | 69.7             | 80.0              | 93.9            | 59.1                 | 53.1                 | 1481              |\n| DeepSeek-R1-Distill-Qwen-32B        | **72.6**         | 83.3              | 94.3            | 62.1                 | 57.2                 | 1691              |\n| DeepSeek-R1-Distill-Llama-8B         | 50.4             | 80.0              | 89.1            | 49.0                 | 39.6                 | 1205              |\n| DeepSeek-R1-Distill-Llama-70B        | 70.0             | **86.7**          | **94.5**        | **65.2**             | **57.5**             | 1633              |\n\n</div>\n\n\n## 5. Chat Website & API Platform\nYou can chat with DeepSeek-R1 on DeepSeek's official website: [chat.deepseek.com](https://chat.deepseek.com), and switch on the button \"DeepThink\"\n\nWe also provide OpenAI-Compatible API at DeepSeek Platform: [platform.deepseek.com](https://platform.deepseek.com/)\n\n## 6. How to Run Locally\n\n### DeepSeek-R1 Models\n\nPlease visit [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) repo for more information about running DeepSeek-R1 locally.\n\n**NOTE: Hugging Face's Transformers has not been directly supported yet.**\n\n### DeepSeek-R1-Distill Models\n\nDeepSeek-R1-Distill models can be utilized in the same manner as Qwen or Llama models.\n\nFor instance, you can easily start a service using [vLLM](https://github.com/vllm-project/vllm):\n\n```shell\nvllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --tensor-parallel-size 2 --max-model-len 32768 --enforce-eager\n```\n\nYou can also easily start a service using [SGLang](https://github.com/sgl-project/sglang)\n\n```bash\npython3 -m sglang.launch_server --model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --trust-remote-code --tp 2\n```\n\n### Usage Recommendations\n\n**We recommend adhering to the following configurations when utilizing the DeepSeek-R1 series models, including benchmarking, to achieve the expected performance:**\n\n1. Set the temperature within the range of 0.5-0.7 (0.6 is recommended) to prevent endless repetitions or incoherent outputs.\n2. **Avoid adding a system prompt; all instructions should be contained within the user prompt.**\n3. For mathematical problems, it is advisable to include a directive in your prompt such as: \"Please reason step by step, and put your final answer within \\boxed{}.\"\n4. When evaluating model performance, it is recommended to conduct multiple tests and average the results.\n\nAdditionally, we have observed that the DeepSeek-R1 series models tend to bypass thinking pattern (i.e., outputting \"\\<think\\>\\n\\n\\</think\\>\") when responding to certain queries, which can adversely affect the model's performance.\n**To ensure that the model engages in thorough reasoning, we recommend enforcing the model to initiate its response with \"\\<think\\>\\n\" at the beginning of every output.**\n\n## 7. License\nThis code repository and the model weights are licensed under the [MIT License](https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE).\nDeepSeek-R1 series support commercial use, allow for any modifications and derivative works, including, but not limited to, distillation for training other LLMs. Please note that:\n- DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Qwen-14B and DeepSeek-R1-Distill-Qwen-32B are derived from [Qwen-2.5 series](https://github.com/QwenLM/Qwen2.5), which are originally licensed under [Apache 2.0 License](https://huggingface.co/Qwen/Qwen2.5-1.5B/blob/main/LICENSE), and now finetuned with 800k samples curated with DeepSeek-R1.\n- DeepSeek-R1-Distill-Llama-8B is derived from Llama3.1-8B-Base and is originally licensed under [llama3.1 license](https://huggingface.co/meta-llama/Llama-3.1-8B/blob/main/LICENSE).\n- DeepSeek-R1-Distill-Llama-70B is derived from Llama3.3-70B-Instruct and is originally licensed under [llama3.3 license](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct/blob/main/LICENSE).\n\n## 8. Citation\n```\n@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\n      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, \n      author={DeepSeek-AI},\n      year={2025},\n      eprint={2501.12948},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2501.12948}, \n}\n\n```\n\n## 9. Contact\nIf you have any questions, please raise an issue or contact us at [service@deepseek.com](service@deepseek.com).\n",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModelForCausalLM",
      "custom_class": null,
      "pipeline_tag": "text-generation",
      "processor": "AutoTokenizer"
    },
    "safetensors": {
      "parameters": {
        "BF16": 7615616512
      },
      "total": 7615616512
    },
    "model_index": null,
    "trending_score": null
  },
  {
    "model_id": "meta-llama/Llama-2-7b-chat-hf",
    "model_name": "meta-llama/Llama-2-7b-chat-hf",
    "author": "meta-llama",
    "downloads": 1244212,
    "downloads_all_time": null,
    "likes": 4320,
    "tags": [
      "transformers",
      "pytorch",
      "safetensors",
      "llama",
      "text-generation",
      "facebook",
      "meta",
      "llama-2",
      "conversational",
      "en",
      "arxiv:2307.09288",
      "license:llama2",
      "autotrain_compatible",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/meta-llama/Llama-2-7b-chat-hf",
    "dependencies": [
      [
        "pytorch",
        null
      ],
      [
        "transformers",
        null
      ]
    ],
    "last_modified": "2024-04-17T08:40:48+00:00",
    "created_at": "2023-07-13T16:45:23+00:00",
    "analysis_date": "2025-03-22T00:42:26.321981",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "llama",
    "pipeline_tag": "text-generation",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "extra_gated_heading": "You need to share contact information with Meta to access this model",
      "extra_gated_prompt": "### LLAMA 2 COMMUNITY LICENSE AGREEMENT\n\"Agreement\" means the terms and conditions for use, reproduction, distribution and  modification of the Llama Materials set forth herein. \n\"Documentation\" means the specifications, manuals and documentation  accompanying Llama 2 distributed by Meta at https://ai.meta.com/resources/models-and-libraries/llama-downloads/.  \n\"Licensee\" or \"you\" means you, or your employer or any other person or entity (if you are entering into this Agreement on such person or entity's behalf), of the age required under applicable laws, rules or regulations to provide legal consent and that has legal authority to bind your employer or such other person or  entity if you are  entering in this Agreement on their behalf. \n\"Llama 2\" means the foundational large language models and software and algorithms, including machine-learning model code, trained model weights, inference-enabling code, training-enabling code, fine-tuning enabling code and other  elements of the foregoing distributed by Meta at ai.meta.com/resources/models-and-libraries/llama-downloads/.\n\"Llama Materials\" means, collectively, Meta's proprietary Llama 2 and documentation (and any portion thereof) made available under this Agreement.\n\"Meta\" or \"we\" means Meta Platforms Ireland Limited (if you are located in or, if you are an entity, your principal place of business is in the EEA or Switzerland) and Meta Platforms, Inc. (if you are located outside of the EEA or Switzerland). \n\nBy clicking \"I Accept\" below or by using or distributing any portion or element of the Llama Materials, you agree to be bound by this Agreement.\n1. License Rights and Redistribution. \na. Grant of Rights. You are granted a non-exclusive, worldwide, non- transferable and royalty-free limited license under Meta's intellectual property or  other rights owned by Meta embodied in the Llama Materials to use, reproduce,  distribute, copy, create derivative works of, and make modifications to the Llama  Materials.  \nb. Redistribution and Use.\ni. If you distribute or make the Llama Materials, or any derivative works  thereof, available to a third party, you shall provide a copy of this Agreement to such  third party. \nii.  If you receive Llama Materials, or any derivative works thereof, from  a Licensee as part of an integrated end user product, then Section 2 of this  Agreement will not apply to you. \niii. You must retain in all copies of the Llama Materials that you  distribute the following attribution notice within a \"Notice\" text file distributed as a  part of such copies: \"Llama 2 is licensed under the LLAMA 2 Community License,  Copyright (c) Meta Platforms, Inc. All Rights Reserved.\"\niv. Your use of the Llama Materials must comply with applicable laws  and regulations (including trade compliance laws and regulations) and adhere to the  Acceptable Use Policy for the Llama Materials (available at  https://ai.meta.com/llama/use-policy), which is hereby incorporated by reference into  this Agreement.\nv. You will not use the Llama Materials or any output or results of the  Llama Materials to improve any other large language model (excluding Llama 2 or  derivative works thereof).  \n\n2. Additional Commercial Terms. If, on the Llama 2 version release date, the  monthly active users of the products or services made available by or for Licensee,  or Licensee's affiliates, is greater than 700 million monthly active users in the  preceding calendar month, you must request a license from Meta, which Meta may  grant to you in its sole discretion, and you are not authorized to exercise any of the  rights under this Agreement unless or until Meta otherwise expressly grants you  such rights.\n3. Disclaimer of Warranty. UNLESS REQUIRED BY APPLICABLE LAW, THE  LLAMA MATERIALS AND ANY OUTPUT AND RESULTS THEREFROM ARE  PROVIDED ON AN \"AS IS\" BASIS, WITHOUT WARRANTIES OF ANY KIND,  EITHER EXPRESS OR IMPLIED, INCLUDING, WITHOUT LIMITATION, ANY  WARRANTIES OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY, OR  FITNESS FOR A PARTICULAR PURPOSE. YOU ARE SOLELY RESPONSIBLE  FOR DETERMINING THE APPROPRIATENESS OF USING OR REDISTRIBUTING  THE LLAMA MATERIALS AND ASSUME ANY RISKS ASSOCIATED WITH YOUR  USE OF THE LLAMA MATERIALS AND ANY OUTPUT AND RESULTS.\n4. Limitation of Liability. IN NO EVENT WILL META OR ITS AFFILIATES BE  LIABLE UNDER ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, TORT,  NEGLIGENCE, PRODUCTS LIABILITY, OR OTHERWISE, ARISING OUT OF THIS  AGREEMENT, FOR ANY LOST PROFITS OR ANY INDIRECT, SPECIAL,  CONSEQUENTIAL, INCIDENTAL, EXEMPLARY OR PUNITIVE DAMAGES, EVEN  IF META OR ITS AFFILIATES HAVE BEEN ADVISED OF THE POSSIBILITY OF  ANY OF THE FOREGOING.\n\n5. Intellectual Property.\na. No trademark licenses are granted under this Agreement, and in  connection with the Llama Materials, neither Meta nor Licensee may use any name  or mark owned by or associated with the other or any of its affiliates, except as  required for reasonable and customary use in describing and redistributing the  Llama Materials.\nb. Subject to Meta's ownership of Llama Materials and derivatives made by or  for Meta, with respect to any derivative works and modifications of the Llama  Materials that are made by you, as between you and Meta, you are and will be the  owner of such derivative works and modifications.\nc. If you institute litigation or other proceedings against Meta or any entity  (including a cross-claim or counterclaim in a lawsuit) alleging that the Llama  Materials or Llama 2 outputs or results, or any portion of any of the foregoing,  constitutes infringement of intellectual property or other rights owned or licensable  by you, then any licenses granted to you under this Agreement shall terminate as of  the date such litigation or claim is filed or instituted. You will indemnify and hold  harmless Meta from and against any claim by any third party arising out of or related  to your use or distribution of the Llama Materials.\n6. Term and Termination. The term of this Agreement will commence upon your  acceptance of this Agreement or access to the Llama Materials and will continue in  full force and effect until terminated in accordance with the terms and conditions  herein. Meta may terminate this Agreement if you are in breach of any term or  condition of this Agreement. Upon termination of this Agreement, you shall delete  and cease use of the Llama Materials. Sections 3, 4 and 7 shall survive the  termination of this Agreement. \n7. Governing Law and Jurisdiction. This Agreement will be governed and  construed under the laws of the State of California without regard to choice of law  principles, and the UN Convention on Contracts for the International Sale of Goods  does not apply to this Agreement. The courts of California shall have exclusive  jurisdiction of any dispute arising out of this Agreement. \n### Llama 2 Acceptable Use Policy\nMeta is committed to promoting safe and fair use of its tools and features, including Llama 2. If you access or use Llama 2, you agree to this Acceptable Use Policy (“Policy”). The most recent copy of this policy can be found at [ai.meta.com/llama/use-policy](http://ai.meta.com/llama/use-policy).\n#### Prohibited Uses\nWe want everyone to use Llama 2 safely and responsibly. You agree you will not use, or allow others to use, Llama 2 to:\n1. Violate the law or others’ rights, including to:\n      1. Engage in, promote, generate, contribute to, encourage, plan, incite, or further illegal or unlawful activity or content, such as: \n          1. Violence or terrorism \n          2. Exploitation or harm to children, including the solicitation, creation, acquisition, or dissemination of child exploitative content or failure to report Child Sexual Abuse Material\n          3. Human trafficking, exploitation, and sexual violence\n          4. The illegal distribution of information or materials to minors, including obscene materials, or failure to employ legally required age-gating in connection with such information or materials.\n          5. Sexual solicitation\n          6. Any other criminal activity\n      2. Engage in, promote, incite, or facilitate the harassment, abuse, threatening, or bullying of individuals or groups of individuals\n      3. Engage in, promote, incite, or facilitate discrimination or other unlawful or harmful conduct in the provision of employment, employment benefits, credit, housing, other economic benefits, or other essential goods and services\n      4. Engage in the unauthorized or unlicensed practice of any profession including, but not limited to, financial, legal, medical/health, or related professional practices \n      5. Collect, process, disclose, generate, or infer health, demographic, or other sensitive personal or private information about individuals without rights and consents required by applicable laws\n      6. Engage in or facilitate any action or generate any content that infringes, misappropriates, or otherwise violates any third-party rights, including the outputs or results of any products or services using the Llama 2 Materials\n      7. Create, generate, or facilitate the creation of malicious code, malware, computer viruses or do anything else that could disable, overburden, interfere with or impair the proper working, integrity, operation or appearance of a website or computer system \n2. Engage in, promote, incite, facilitate, or assist in the planning or development of activities that present a risk of death or bodily harm to individuals, including use of Llama 2 related to the following:\n    1. Military, warfare, nuclear industries or applications, espionage, use for materials or activities that are subject to the International Traffic Arms Regulations (ITAR) maintained by the United States Department of State\n    2. Guns and illegal weapons (including weapon development)\n    3. Illegal drugs and regulated/controlled substances\n    4. Operation of critical infrastructure, transportation technologies, or heavy machinery\n    5. Self-harm or harm to others, including suicide, cutting, and eating disorders\n    6. Any content intended to incite or promote violence, abuse, or any infliction of bodily harm to an individual\n3. Intentionally deceive or mislead others, including use of Llama 2 related to the following:\n    1. Generating, promoting, or furthering fraud or the creation or promotion of disinformation\n    2. Generating, promoting, or furthering defamatory content, including the creation of defamatory statements, images, or other content\n    3. Generating, promoting, or further distributing spam\n    4. Impersonating another individual without consent, authorization, or legal right\n    5. Representing that the use of Llama 2 or outputs are human-generated\n    6. Generating or facilitating false online engagement, including fake reviews and other means of fake online engagement \n    4. Fail to appropriately disclose to end users any known dangers of your AI system \nPlease report any violation of this Policy, software “bug,” or other problems that could lead to a violation of this Policy through one of the following means: \n    * Reporting issues with the model: [github.com/facebookresearch/llama](http://github.com/facebookresearch/llama)\n    * Reporting risky content generated by the model: [developers.facebook.com/llama_output_feedback](http://developers.facebook.com/llama_output_feedback)\n    * Reporting bugs and security concerns: [facebook.com/whitehat/info](http://facebook.com/whitehat/info) \n    * Reporting violations of the Acceptable Use Policy or unlicensed uses of Llama: [LlamaUseReport@meta.com](mailto:LlamaUseReport@meta.com)",
      "extra_gated_fields": {
        "First Name": "text",
        "Last Name": "text",
        "Date of birth": "date_picker",
        "Country": "country",
        "Affiliation": "text",
        "geo": "ip_location",
        "By clicking Submit below I accept the terms of the license and acknowledge that the information I provide will be collected stored processed and shared in accordance with the Meta Privacy Policy": "checkbox"
      },
      "extra_gated_description": "The information you provide will be collected, stored, processed and shared in accordance with the [Meta Privacy Policy](https://www.facebook.com/privacy/policy/).",
      "extra_gated_button_content": "Submit",
      "language": [
        "en"
      ],
      "pipeline_tag": "text-generation",
      "tags": [
        "facebook",
        "meta",
        "pytorch",
        "llama",
        "llama-2"
      ],
      "license": "llama2"
    },
    "card_text": "# **Llama 2**\nLlama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 7B fine-tuned model, optimized for dialogue use cases and converted for the Hugging Face Transformers format. Links to other models can be found in the index at the bottom.\n\n## Model Details\n*Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the [website](https://ai.meta.com/resources/models-and-libraries/llama-downloads/) and accept our License before requesting access here.*\n\nMeta developed and publicly released the Llama 2 family of large language models (LLMs), a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama-2-Chat, are optimized for dialogue use cases. Llama-2-Chat models outperform open-source chat models on most benchmarks we tested, and in our human evaluations for helpfulness and safety, are on par with some popular closed-source models like ChatGPT and PaLM.\n\n**Model Developers** Meta\n\n**Variations** Llama 2 comes in a range of parameter sizes — 7B, 13B, and 70B — as well as pretrained and fine-tuned variations.\n\n**Input** Models input text only.\n\n**Output** Models generate text only.\n\n**Model Architecture** Llama 2 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align to human preferences for helpfulness and safety.\n\n\n||Training Data|Params|Content Length|GQA|Tokens|LR|\n|---|---|---|---|---|---|---|\n|Llama 2|*A new mix of publicly available online data*|7B|4k|&#10007;|2.0T|3.0 x 10<sup>-4</sup>|\n|Llama 2|*A new mix of publicly available online data*|13B|4k|&#10007;|2.0T|3.0 x 10<sup>-4</sup>|\n|Llama 2|*A new mix of publicly available online data*|70B|4k|&#10004;|2.0T|1.5 x 10<sup>-4</sup>|\n\n*Llama 2 family of models.* Token counts refer to pretraining data only. All models are trained with a global batch-size of 4M tokens. Bigger models -  70B -- use Grouped-Query Attention (GQA) for improved inference scalability.\n\n**Model Dates** Llama 2 was trained between January 2023 and July 2023.\n\n**Status** This is a static model trained on an offline dataset. Future versions of the tuned models will be released as we improve model safety with community feedback.\n\n**License** A custom commercial license is available at: [https://ai.meta.com/resources/models-and-libraries/llama-downloads/](https://ai.meta.com/resources/models-and-libraries/llama-downloads/)\n\n**Research Paper** [\"Llama-2: Open Foundation and Fine-tuned Chat Models\"](arxiv.org/abs/2307.09288)\n\n## Intended Use\n**Intended Use Cases** Llama 2 is intended for commercial and research use in English. Tuned models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks.\n\nTo get the expected features and performance for the chat versions, a specific formatting needs to be followed, including the `INST` and `<<SYS>>` tags, `BOS` and `EOS` tokens, and the whitespaces and breaklines in between (we recommend calling `strip()` on inputs to avoid double-spaces). See our reference code in github for details: [`chat_completion`](https://github.com/facebookresearch/llama/blob/main/llama/generation.py#L212).\n\n**Out-of-scope Uses** Use in any manner that violates applicable laws or regulations (including trade compliance laws).Use in languages other than English. Use in any other way that is prohibited by the Acceptable Use Policy and Licensing Agreement for Llama 2.\n\n## Hardware and Software\n**Training Factors** We used custom training libraries, Meta's Research Super Cluster, and production clusters for pretraining. Fine-tuning, annotation, and evaluation were also performed on third-party cloud compute.\n\n**Carbon Footprint** Pretraining utilized a cumulative 3.3M GPU hours of computation on hardware of type A100-80GB (TDP of 350-400W). Estimated total emissions were 539 tCO2eq, 100% of which were offset by Meta’s sustainability program.\n\n||Time (GPU hours)|Power Consumption (W)|Carbon Emitted(tCO<sub>2</sub>eq)|\n|---|---|---|---|\n|Llama 2 7B|184320|400|31.22|\n|Llama 2 13B|368640|400|62.44|\n|Llama 2 70B|1720320|400|291.42|\n|Total|3311616||539.00|\n\n**CO<sub>2</sub> emissions during pretraining.** Time: total GPU time required for training each model. Power Consumption: peak power capacity per GPU device for the GPUs used adjusted for power usage efficiency. 100% of the emissions are directly offset by Meta's sustainability program, and because we are openly releasing these models, the pretraining costs do not need to be incurred by others.\n\n## Training Data\n**Overview** Llama 2 was pretrained on 2 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over one million new human-annotated examples. Neither the pretraining nor the fine-tuning datasets include Meta user data.\n\n**Data Freshness** The pretraining data has a cutoff of September 2022, but some tuning data is more recent, up to July 2023.\n\n## Evaluation Results\n\nIn this section, we report the results for the Llama 1 and Llama 2 models on standard academic benchmarks.For all the evaluations, we use our internal evaluations library.\n\n|Model|Size|Code|Commonsense Reasoning|World Knowledge|Reading Comprehension|Math|MMLU|BBH|AGI Eval|\n|---|---|---|---|---|---|---|---|---|---|\n|Llama 1|7B|14.1|60.8|46.2|58.5|6.95|35.1|30.3|23.9|\n|Llama 1|13B|18.9|66.1|52.6|62.3|10.9|46.9|37.0|33.9|\n|Llama 1|33B|26.0|70.0|58.4|67.6|21.4|57.8|39.8|41.7|\n|Llama 1|65B|30.7|70.7|60.5|68.6|30.8|63.4|43.5|47.6|\n|Llama 2|7B|16.8|63.9|48.9|61.3|14.6|45.3|32.6|29.3|\n|Llama 2|13B|24.5|66.9|55.4|65.8|28.7|54.8|39.4|39.1|\n|Llama 2|70B|**37.5**|**71.9**|**63.6**|**69.4**|**35.2**|**68.9**|**51.2**|**54.2**|\n\n**Overall performance on grouped academic benchmarks.** *Code:* We report the average pass@1 scores of our models on HumanEval and MBPP. *Commonsense Reasoning:* We report the average of PIQA, SIQA, HellaSwag, WinoGrande, ARC easy and challenge, OpenBookQA, and CommonsenseQA. We report 7-shot results for CommonSenseQA and 0-shot results for all other benchmarks. *World Knowledge:* We evaluate the 5-shot performance on NaturalQuestions and TriviaQA and report the average. *Reading Comprehension:* For reading comprehension, we report the 0-shot average on SQuAD, QuAC, and BoolQ. *MATH:* We report the average of the GSM8K (8 shot) and MATH (4 shot) benchmarks at top 1.\n\n|||TruthfulQA|Toxigen|\n|---|---|---|---|\n|Llama 1|7B|27.42|23.00|\n|Llama 1|13B|41.74|23.08|\n|Llama 1|33B|44.19|22.57|\n|Llama 1|65B|48.71|21.77|\n|Llama 2|7B|33.29|**21.25**|\n|Llama 2|13B|41.86|26.10|\n|Llama 2|70B|**50.18**|24.60|\n\n**Evaluation of pretrained LLMs on automatic safety benchmarks.** For TruthfulQA, we present the percentage of generations that are both truthful and informative (the higher the better). For ToxiGen, we present the percentage of toxic generations (the smaller the better).\n\n\n|||TruthfulQA|Toxigen|\n|---|---|---|---|\n|Llama-2-Chat|7B|57.04|**0.00**|\n|Llama-2-Chat|13B|62.18|**0.00**|\n|Llama-2-Chat|70B|**64.14**|0.01|\n\n**Evaluation of fine-tuned LLMs on different safety datasets.** Same metric definitions as above.\n\n## Ethical Considerations and Limitations\nLlama 2 is a new technology that carries risks with use. Testing conducted to date has been in English, and has not covered, nor could it cover all scenarios. For these reasons, as with all LLMs, Llama 2’s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 2, developers should perform safety testing and tuning tailored to their specific applications of the model.\n\nPlease see the Responsible Use Guide available at [https://ai.meta.com/llama/responsible-use-guide/](https://ai.meta.com/llama/responsible-use-guide)\n\n## Reporting Issues\nPlease report any software “bug,” or other problems with the models through one of the following means:\n- Reporting issues with the model: [github.com/facebookresearch/llama](http://github.com/facebookresearch/llama)\n- Reporting problematic content generated by the model: [developers.facebook.com/llama_output_feedback](http://developers.facebook.com/llama_output_feedback)\n- Reporting bugs and security concerns: [facebook.com/whitehat/info](http://facebook.com/whitehat/info)\n\n## Llama Model Index\n|Model|Llama2|Llama2-hf|Llama2-chat|Llama2-chat-hf|\n|---|---|---|---|---|\n|7B| [Link](https://huggingface.co/meta-llama/Llama-2-7b) | [Link](https://huggingface.co/meta-llama/Llama-2-7b-hf) | [Link](https://huggingface.co/meta-llama/Llama-2-7b-chat) | [Link](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf)|\n|13B| [Link](https://huggingface.co/meta-llama/Llama-2-13b) | [Link](https://huggingface.co/meta-llama/Llama-2-13b-hf) | [Link](https://huggingface.co/meta-llama/Llama-2-13b-chat) | [Link](https://huggingface.co/meta-llama/Llama-2-13b-chat-hf)|\n|70B| [Link](https://huggingface.co/meta-llama/Llama-2-70b) | [Link](https://huggingface.co/meta-llama/Llama-2-70b-hf) | [Link](https://huggingface.co/meta-llama/Llama-2-70b-chat) | [Link](https://huggingface.co/meta-llama/Llama-2-70b-chat-hf)|",
    "card_content": "---\nextra_gated_heading: You need to share contact information with Meta to access this\n  model\nextra_gated_prompt: \"### LLAMA 2 COMMUNITY LICENSE AGREEMENT\\n\\\"Agreement\\\" means\\\n  \\ the terms and conditions for use, reproduction, distribution and  modification\\\n  \\ of the Llama Materials set forth herein. \\n\\\"Documentation\\\" means the specifications,\\\n  \\ manuals and documentation  accompanying Llama 2 distributed by Meta at https://ai.meta.com/resources/models-and-libraries/llama-downloads/.\\\n  \\  \\n\\\"Licensee\\\" or \\\"you\\\" means you, or your employer or any other person or\\\n  \\ entity (if you are entering into this Agreement on such person or entity's behalf),\\\n  \\ of the age required under applicable laws, rules or regulations to provide legal\\\n  \\ consent and that has legal authority to bind your employer or such other person\\\n  \\ or  entity if you are  entering in this Agreement on their behalf. \\n\\\"Llama 2\\\"\\\n  \\ means the foundational large language models and software and algorithms, including\\\n  \\ machine-learning model code, trained model weights, inference-enabling code, training-enabling\\\n  \\ code, fine-tuning enabling code and other  elements of the foregoing distributed\\\n  \\ by Meta at ai.meta.com/resources/models-and-libraries/llama-downloads/.\\n\\\"Llama\\\n  \\ Materials\\\" means, collectively, Meta's proprietary Llama 2 and documentation\\\n  \\ (and any portion thereof) made available under this Agreement.\\n\\\"Meta\\\" or \\\"\\\n  we\\\" means Meta Platforms Ireland Limited (if you are located in or, if you are\\\n  \\ an entity, your principal place of business is in the EEA or Switzerland) and\\\n  \\ Meta Platforms, Inc. (if you are located outside of the EEA or Switzerland). \\n\\\n  \\nBy clicking \\\"I Accept\\\" below or by using or distributing any portion or element\\\n  \\ of the Llama Materials, you agree to be bound by this Agreement.\\n1. License Rights\\\n  \\ and Redistribution. \\na. Grant of Rights. You are granted a non-exclusive, worldwide,\\\n  \\ non- transferable and royalty-free limited license under Meta's intellectual property\\\n  \\ or  other rights owned by Meta embodied in the Llama Materials to use, reproduce,\\\n  \\  distribute, copy, create derivative works of, and make modifications to the Llama\\\n  \\  Materials.  \\nb. Redistribution and Use.\\ni. If you distribute or make the Llama\\\n  \\ Materials, or any derivative works  thereof, available to a third party, you shall\\\n  \\ provide a copy of this Agreement to such  third party. \\nii.  If you receive Llama\\\n  \\ Materials, or any derivative works thereof, from  a Licensee as part of an integrated\\\n  \\ end user product, then Section 2 of this  Agreement will not apply to you. \\n\\\n  iii. You must retain in all copies of the Llama Materials that you  distribute the\\\n  \\ following attribution notice within a \\\"Notice\\\" text file distributed as a  part\\\n  \\ of such copies: \\\"Llama 2 is licensed under the LLAMA 2 Community License,  Copyright\\\n  \\ (c) Meta Platforms, Inc. All Rights Reserved.\\\"\\niv. Your use of the Llama Materials\\\n  \\ must comply with applicable laws  and regulations (including trade compliance\\\n  \\ laws and regulations) and adhere to the  Acceptable Use Policy for the Llama Materials\\\n  \\ (available at  https://ai.meta.com/llama/use-policy), which is hereby incorporated\\\n  \\ by reference into  this Agreement.\\nv. You will not use the Llama Materials or\\\n  \\ any output or results of the  Llama Materials to improve any other large language\\\n  \\ model (excluding Llama 2 or  derivative works thereof).  \\n\\n2. Additional Commercial\\\n  \\ Terms. If, on the Llama 2 version release date, the  monthly active users of the\\\n  \\ products or services made available by or for Licensee,  or Licensee's affiliates,\\\n  \\ is greater than 700 million monthly active users in the  preceding calendar month,\\\n  \\ you must request a license from Meta, which Meta may  grant to you in its sole\\\n  \\ discretion, and you are not authorized to exercise any of the  rights under this\\\n  \\ Agreement unless or until Meta otherwise expressly grants you  such rights.\\n\\\n  3. Disclaimer of Warranty. UNLESS REQUIRED BY APPLICABLE LAW, THE  LLAMA MATERIALS\\\n  \\ AND ANY OUTPUT AND RESULTS THEREFROM ARE  PROVIDED ON AN \\\"AS IS\\\" BASIS, WITHOUT\\\n  \\ WARRANTIES OF ANY KIND,  EITHER EXPRESS OR IMPLIED, INCLUDING, WITHOUT LIMITATION,\\\n  \\ ANY  WARRANTIES OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY, OR  FITNESS FOR A\\\n  \\ PARTICULAR PURPOSE. YOU ARE SOLELY RESPONSIBLE  FOR DETERMINING THE APPROPRIATENESS\\\n  \\ OF USING OR REDISTRIBUTING  THE LLAMA MATERIALS AND ASSUME ANY RISKS ASSOCIATED\\\n  \\ WITH YOUR  USE OF THE LLAMA MATERIALS AND ANY OUTPUT AND RESULTS.\\n4. Limitation\\\n  \\ of Liability. IN NO EVENT WILL META OR ITS AFFILIATES BE  LIABLE UNDER ANY THEORY\\\n  \\ OF LIABILITY, WHETHER IN CONTRACT, TORT,  NEGLIGENCE, PRODUCTS LIABILITY, OR OTHERWISE,\\\n  \\ ARISING OUT OF THIS  AGREEMENT, FOR ANY LOST PROFITS OR ANY INDIRECT, SPECIAL,\\\n  \\  CONSEQUENTIAL, INCIDENTAL, EXEMPLARY OR PUNITIVE DAMAGES, EVEN  IF META OR ITS\\\n  \\ AFFILIATES HAVE BEEN ADVISED OF THE POSSIBILITY OF  ANY OF THE FOREGOING.\\n\\n\\\n  5. Intellectual Property.\\na. No trademark licenses are granted under this Agreement,\\\n  \\ and in  connection with the Llama Materials, neither Meta nor Licensee may use\\\n  \\ any name  or mark owned by or associated with the other or any of its affiliates,\\\n  \\ except as  required for reasonable and customary use in describing and redistributing\\\n  \\ the  Llama Materials.\\nb. Subject to Meta's ownership of Llama Materials and derivatives\\\n  \\ made by or  for Meta, with respect to any derivative works and modifications of\\\n  \\ the Llama  Materials that are made by you, as between you and Meta, you are and\\\n  \\ will be the  owner of such derivative works and modifications.\\nc. If you institute\\\n  \\ litigation or other proceedings against Meta or any entity  (including a cross-claim\\\n  \\ or counterclaim in a lawsuit) alleging that the Llama  Materials or Llama 2 outputs\\\n  \\ or results, or any portion of any of the foregoing,  constitutes infringement\\\n  \\ of intellectual property or other rights owned or licensable  by you, then any\\\n  \\ licenses granted to you under this Agreement shall terminate as of  the date such\\\n  \\ litigation or claim is filed or instituted. You will indemnify and hold  harmless\\\n  \\ Meta from and against any claim by any third party arising out of or related \\\n  \\ to your use or distribution of the Llama Materials.\\n6. Term and Termination.\\\n  \\ The term of this Agreement will commence upon your  acceptance of this Agreement\\\n  \\ or access to the Llama Materials and will continue in  full force and effect until\\\n  \\ terminated in accordance with the terms and conditions  herein. Meta may terminate\\\n  \\ this Agreement if you are in breach of any term or  condition of this Agreement.\\\n  \\ Upon termination of this Agreement, you shall delete  and cease use of the Llama\\\n  \\ Materials. Sections 3, 4 and 7 shall survive the  termination of this Agreement.\\\n  \\ \\n7. Governing Law and Jurisdiction. This Agreement will be governed and  construed\\\n  \\ under the laws of the State of California without regard to choice of law  principles,\\\n  \\ and the UN Convention on Contracts for the International Sale of Goods  does not\\\n  \\ apply to this Agreement. The courts of California shall have exclusive  jurisdiction\\\n  \\ of any dispute arising out of this Agreement. \\n### Llama 2 Acceptable Use Policy\\n\\\n  Meta is committed to promoting safe and fair use of its tools and features, including\\\n  \\ Llama 2. If you access or use Llama 2, you agree to this Acceptable Use Policy\\\n  \\ (“Policy”). The most recent copy of this policy can be found at [ai.meta.com/llama/use-policy](http://ai.meta.com/llama/use-policy).\\n\\\n  #### Prohibited Uses\\nWe want everyone to use Llama 2 safely and responsibly. You\\\n  \\ agree you will not use, or allow others to use, Llama 2 to:\\n1. Violate the law\\\n  \\ or others’ rights, including to:\\n      1. Engage in, promote, generate, contribute\\\n  \\ to, encourage, plan, incite, or further illegal or unlawful activity or content,\\\n  \\ such as: \\n          1. Violence or terrorism \\n          2. Exploitation or harm\\\n  \\ to children, including the solicitation, creation, acquisition, or dissemination\\\n  \\ of child exploitative content or failure to report Child Sexual Abuse Material\\n\\\n  \\          3. Human trafficking, exploitation, and sexual violence\\n          4.\\\n  \\ The illegal distribution of information or materials to minors, including obscene\\\n  \\ materials, or failure to employ legally required age-gating in connection with\\\n  \\ such information or materials.\\n          5. Sexual solicitation\\n          6.\\\n  \\ Any other criminal activity\\n      2. Engage in, promote, incite, or facilitate\\\n  \\ the harassment, abuse, threatening, or bullying of individuals or groups of individuals\\n\\\n  \\      3. Engage in, promote, incite, or facilitate discrimination or other unlawful\\\n  \\ or harmful conduct in the provision of employment, employment benefits, credit,\\\n  \\ housing, other economic benefits, or other essential goods and services\\n    \\\n  \\  4. Engage in the unauthorized or unlicensed practice of any profession including,\\\n  \\ but not limited to, financial, legal, medical/health, or related professional\\\n  \\ practices \\n      5. Collect, process, disclose, generate, or infer health, demographic,\\\n  \\ or other sensitive personal or private information about individuals without rights\\\n  \\ and consents required by applicable laws\\n      6. Engage in or facilitate any\\\n  \\ action or generate any content that infringes, misappropriates, or otherwise violates\\\n  \\ any third-party rights, including the outputs or results of any products or services\\\n  \\ using the Llama 2 Materials\\n      7. Create, generate, or facilitate the creation\\\n  \\ of malicious code, malware, computer viruses or do anything else that could disable,\\\n  \\ overburden, interfere with or impair the proper working, integrity, operation\\\n  \\ or appearance of a website or computer system \\n2. Engage in, promote, incite,\\\n  \\ facilitate, or assist in the planning or development of activities that present\\\n  \\ a risk of death or bodily harm to individuals, including use of Llama 2 related\\\n  \\ to the following:\\n    1. Military, warfare, nuclear industries or applications,\\\n  \\ espionage, use for materials or activities that are subject to the International\\\n  \\ Traffic Arms Regulations (ITAR) maintained by the United States Department of\\\n  \\ State\\n    2. Guns and illegal weapons (including weapon development)\\n    3.\\\n  \\ Illegal drugs and regulated/controlled substances\\n    4. Operation of critical\\\n  \\ infrastructure, transportation technologies, or heavy machinery\\n    5. Self-harm\\\n  \\ or harm to others, including suicide, cutting, and eating disorders\\n    6. Any\\\n  \\ content intended to incite or promote violence, abuse, or any infliction of bodily\\\n  \\ harm to an individual\\n3. Intentionally deceive or mislead others, including use\\\n  \\ of Llama 2 related to the following:\\n    1. Generating, promoting, or furthering\\\n  \\ fraud or the creation or promotion of disinformation\\n    2. Generating, promoting,\\\n  \\ or furthering defamatory content, including the creation of defamatory statements,\\\n  \\ images, or other content\\n    3. Generating, promoting, or further distributing\\\n  \\ spam\\n    4. Impersonating another individual without consent, authorization,\\\n  \\ or legal right\\n    5. Representing that the use of Llama 2 or outputs are human-generated\\n\\\n  \\    6. Generating or facilitating false online engagement, including fake reviews\\\n  \\ and other means of fake online engagement \\n    4. Fail to appropriately disclose\\\n  \\ to end users any known dangers of your AI system \\nPlease report any violation\\\n  \\ of this Policy, software “bug,” or other problems that could lead to a violation\\\n  \\ of this Policy through one of the following means: \\n    * Reporting issues with\\\n  \\ the model: [github.com/facebookresearch/llama](http://github.com/facebookresearch/llama)\\n\\\n  \\    * Reporting risky content generated by the model: [developers.facebook.com/llama_output_feedback](http://developers.facebook.com/llama_output_feedback)\\n\\\n  \\    * Reporting bugs and security concerns: [facebook.com/whitehat/info](http://facebook.com/whitehat/info)\\\n  \\ \\n    * Reporting violations of the Acceptable Use Policy or unlicensed uses of\\\n  \\ Llama: [LlamaUseReport@meta.com](mailto:LlamaUseReport@meta.com)\"\nextra_gated_fields:\n  First Name: text\n  Last Name: text\n  Date of birth: date_picker\n  Country: country\n  Affiliation: text\n  geo: ip_location\n  ? By clicking Submit below I accept the terms of the license and acknowledge that\n    the information I provide will be collected stored processed and shared in accordance\n    with the Meta Privacy Policy\n  : checkbox\nextra_gated_description: The information you provide will be collected, stored, processed\n  and shared in accordance with the [Meta Privacy Policy](https://www.facebook.com/privacy/policy/).\nextra_gated_button_content: Submit\nlanguage:\n- en\npipeline_tag: text-generation\ntags:\n- facebook\n- meta\n- pytorch\n- llama\n- llama-2\nlicense: llama2\n---\n# **Llama 2**\nLlama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 7B fine-tuned model, optimized for dialogue use cases and converted for the Hugging Face Transformers format. Links to other models can be found in the index at the bottom.\n\n## Model Details\n*Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the [website](https://ai.meta.com/resources/models-and-libraries/llama-downloads/) and accept our License before requesting access here.*\n\nMeta developed and publicly released the Llama 2 family of large language models (LLMs), a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama-2-Chat, are optimized for dialogue use cases. Llama-2-Chat models outperform open-source chat models on most benchmarks we tested, and in our human evaluations for helpfulness and safety, are on par with some popular closed-source models like ChatGPT and PaLM.\n\n**Model Developers** Meta\n\n**Variations** Llama 2 comes in a range of parameter sizes — 7B, 13B, and 70B — as well as pretrained and fine-tuned variations.\n\n**Input** Models input text only.\n\n**Output** Models generate text only.\n\n**Model Architecture** Llama 2 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align to human preferences for helpfulness and safety.\n\n\n||Training Data|Params|Content Length|GQA|Tokens|LR|\n|---|---|---|---|---|---|---|\n|Llama 2|*A new mix of publicly available online data*|7B|4k|&#10007;|2.0T|3.0 x 10<sup>-4</sup>|\n|Llama 2|*A new mix of publicly available online data*|13B|4k|&#10007;|2.0T|3.0 x 10<sup>-4</sup>|\n|Llama 2|*A new mix of publicly available online data*|70B|4k|&#10004;|2.0T|1.5 x 10<sup>-4</sup>|\n\n*Llama 2 family of models.* Token counts refer to pretraining data only. All models are trained with a global batch-size of 4M tokens. Bigger models -  70B -- use Grouped-Query Attention (GQA) for improved inference scalability.\n\n**Model Dates** Llama 2 was trained between January 2023 and July 2023.\n\n**Status** This is a static model trained on an offline dataset. Future versions of the tuned models will be released as we improve model safety with community feedback.\n\n**License** A custom commercial license is available at: [https://ai.meta.com/resources/models-and-libraries/llama-downloads/](https://ai.meta.com/resources/models-and-libraries/llama-downloads/)\n\n**Research Paper** [\"Llama-2: Open Foundation and Fine-tuned Chat Models\"](arxiv.org/abs/2307.09288)\n\n## Intended Use\n**Intended Use Cases** Llama 2 is intended for commercial and research use in English. Tuned models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks.\n\nTo get the expected features and performance for the chat versions, a specific formatting needs to be followed, including the `INST` and `<<SYS>>` tags, `BOS` and `EOS` tokens, and the whitespaces and breaklines in between (we recommend calling `strip()` on inputs to avoid double-spaces). See our reference code in github for details: [`chat_completion`](https://github.com/facebookresearch/llama/blob/main/llama/generation.py#L212).\n\n**Out-of-scope Uses** Use in any manner that violates applicable laws or regulations (including trade compliance laws).Use in languages other than English. Use in any other way that is prohibited by the Acceptable Use Policy and Licensing Agreement for Llama 2.\n\n## Hardware and Software\n**Training Factors** We used custom training libraries, Meta's Research Super Cluster, and production clusters for pretraining. Fine-tuning, annotation, and evaluation were also performed on third-party cloud compute.\n\n**Carbon Footprint** Pretraining utilized a cumulative 3.3M GPU hours of computation on hardware of type A100-80GB (TDP of 350-400W). Estimated total emissions were 539 tCO2eq, 100% of which were offset by Meta’s sustainability program.\n\n||Time (GPU hours)|Power Consumption (W)|Carbon Emitted(tCO<sub>2</sub>eq)|\n|---|---|---|---|\n|Llama 2 7B|184320|400|31.22|\n|Llama 2 13B|368640|400|62.44|\n|Llama 2 70B|1720320|400|291.42|\n|Total|3311616||539.00|\n\n**CO<sub>2</sub> emissions during pretraining.** Time: total GPU time required for training each model. Power Consumption: peak power capacity per GPU device for the GPUs used adjusted for power usage efficiency. 100% of the emissions are directly offset by Meta's sustainability program, and because we are openly releasing these models, the pretraining costs do not need to be incurred by others.\n\n## Training Data\n**Overview** Llama 2 was pretrained on 2 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over one million new human-annotated examples. Neither the pretraining nor the fine-tuning datasets include Meta user data.\n\n**Data Freshness** The pretraining data has a cutoff of September 2022, but some tuning data is more recent, up to July 2023.\n\n## Evaluation Results\n\nIn this section, we report the results for the Llama 1 and Llama 2 models on standard academic benchmarks.For all the evaluations, we use our internal evaluations library.\n\n|Model|Size|Code|Commonsense Reasoning|World Knowledge|Reading Comprehension|Math|MMLU|BBH|AGI Eval|\n|---|---|---|---|---|---|---|---|---|---|\n|Llama 1|7B|14.1|60.8|46.2|58.5|6.95|35.1|30.3|23.9|\n|Llama 1|13B|18.9|66.1|52.6|62.3|10.9|46.9|37.0|33.9|\n|Llama 1|33B|26.0|70.0|58.4|67.6|21.4|57.8|39.8|41.7|\n|Llama 1|65B|30.7|70.7|60.5|68.6|30.8|63.4|43.5|47.6|\n|Llama 2|7B|16.8|63.9|48.9|61.3|14.6|45.3|32.6|29.3|\n|Llama 2|13B|24.5|66.9|55.4|65.8|28.7|54.8|39.4|39.1|\n|Llama 2|70B|**37.5**|**71.9**|**63.6**|**69.4**|**35.2**|**68.9**|**51.2**|**54.2**|\n\n**Overall performance on grouped academic benchmarks.** *Code:* We report the average pass@1 scores of our models on HumanEval and MBPP. *Commonsense Reasoning:* We report the average of PIQA, SIQA, HellaSwag, WinoGrande, ARC easy and challenge, OpenBookQA, and CommonsenseQA. We report 7-shot results for CommonSenseQA and 0-shot results for all other benchmarks. *World Knowledge:* We evaluate the 5-shot performance on NaturalQuestions and TriviaQA and report the average. *Reading Comprehension:* For reading comprehension, we report the 0-shot average on SQuAD, QuAC, and BoolQ. *MATH:* We report the average of the GSM8K (8 shot) and MATH (4 shot) benchmarks at top 1.\n\n|||TruthfulQA|Toxigen|\n|---|---|---|---|\n|Llama 1|7B|27.42|23.00|\n|Llama 1|13B|41.74|23.08|\n|Llama 1|33B|44.19|22.57|\n|Llama 1|65B|48.71|21.77|\n|Llama 2|7B|33.29|**21.25**|\n|Llama 2|13B|41.86|26.10|\n|Llama 2|70B|**50.18**|24.60|\n\n**Evaluation of pretrained LLMs on automatic safety benchmarks.** For TruthfulQA, we present the percentage of generations that are both truthful and informative (the higher the better). For ToxiGen, we present the percentage of toxic generations (the smaller the better).\n\n\n|||TruthfulQA|Toxigen|\n|---|---|---|---|\n|Llama-2-Chat|7B|57.04|**0.00**|\n|Llama-2-Chat|13B|62.18|**0.00**|\n|Llama-2-Chat|70B|**64.14**|0.01|\n\n**Evaluation of fine-tuned LLMs on different safety datasets.** Same metric definitions as above.\n\n## Ethical Considerations and Limitations\nLlama 2 is a new technology that carries risks with use. Testing conducted to date has been in English, and has not covered, nor could it cover all scenarios. For these reasons, as with all LLMs, Llama 2’s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 2, developers should perform safety testing and tuning tailored to their specific applications of the model.\n\nPlease see the Responsible Use Guide available at [https://ai.meta.com/llama/responsible-use-guide/](https://ai.meta.com/llama/responsible-use-guide)\n\n## Reporting Issues\nPlease report any software “bug,” or other problems with the models through one of the following means:\n- Reporting issues with the model: [github.com/facebookresearch/llama](http://github.com/facebookresearch/llama)\n- Reporting problematic content generated by the model: [developers.facebook.com/llama_output_feedback](http://developers.facebook.com/llama_output_feedback)\n- Reporting bugs and security concerns: [facebook.com/whitehat/info](http://facebook.com/whitehat/info)\n\n## Llama Model Index\n|Model|Llama2|Llama2-hf|Llama2-chat|Llama2-chat-hf|\n|---|---|---|---|---|\n|7B| [Link](https://huggingface.co/meta-llama/Llama-2-7b) | [Link](https://huggingface.co/meta-llama/Llama-2-7b-hf) | [Link](https://huggingface.co/meta-llama/Llama-2-7b-chat) | [Link](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf)|\n|13B| [Link](https://huggingface.co/meta-llama/Llama-2-13b) | [Link](https://huggingface.co/meta-llama/Llama-2-13b-hf) | [Link](https://huggingface.co/meta-llama/Llama-2-13b-chat) | [Link](https://huggingface.co/meta-llama/Llama-2-13b-chat-hf)|\n|70B| [Link](https://huggingface.co/meta-llama/Llama-2-70b) | [Link](https://huggingface.co/meta-llama/Llama-2-70b-hf) | [Link](https://huggingface.co/meta-llama/Llama-2-70b-chat) | [Link](https://huggingface.co/meta-llama/Llama-2-70b-chat-hf)|",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModelForCausalLM",
      "custom_class": null,
      "pipeline_tag": "text-generation",
      "processor": "AutoTokenizer"
    },
    "safetensors": {
      "parameters": {
        "F16": 6738417664
      },
      "total": 6738417664
    },
    "model_index": null,
    "trending_score": null
  },
  {
    "model_id": "meta-llama/Llama-3.1-8B",
    "model_name": "meta-llama/Llama-3.1-8B",
    "author": "meta-llama",
    "downloads": 1167911,
    "downloads_all_time": null,
    "likes": 1509,
    "tags": [
      "transformers",
      "safetensors",
      "llama",
      "text-generation",
      "facebook",
      "meta",
      "pytorch",
      "llama-3",
      "en",
      "de",
      "fr",
      "it",
      "pt",
      "hi",
      "es",
      "th",
      "arxiv:2204.05149",
      "license:llama3.1",
      "autotrain_compatible",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/meta-llama/Llama-3.1-8B",
    "dependencies": null,
    "last_modified": "2024-10-16T22:00:37+00:00",
    "created_at": "2024-07-14T22:20:15+00:00",
    "analysis_date": "2025-03-22T00:42:29.468647",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "llama",
    "pipeline_tag": "text-generation",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": [
        "en",
        "de",
        "fr",
        "it",
        "pt",
        "hi",
        "es",
        "th"
      ],
      "pipeline_tag": "text-generation",
      "tags": [
        "facebook",
        "meta",
        "pytorch",
        "llama",
        "llama-3"
      ],
      "license": "llama3.1",
      "extra_gated_prompt": "### LLAMA 3.1 COMMUNITY LICENSE AGREEMENT\nLlama 3.1 Version Release Date: July 23, 2024\n\"Agreement\" means the terms and conditions for use, reproduction, distribution and modification of the  Llama Materials set forth herein.\n\"Documentation\" means the specifications, manuals and documentation accompanying Llama 3.1 distributed by Meta at https://llama.meta.com/doc/overview.\n\"Licensee\" or \"you\" means you, or your employer or any other person or entity (if you are entering into this Agreement on such person or entity’s behalf), of the age required under applicable laws, rules or regulations to provide legal consent and that has legal authority to bind your employer or such other person or entity if you are entering in this Agreement on their behalf.\n\"Llama 3.1\" means the foundational large language models and software and algorithms, including machine-learning model code, trained model weights, inference-enabling code, training-enabling code, fine-tuning enabling code and other elements of the foregoing distributed by Meta at https://llama.meta.com/llama-downloads.\n\"Llama Materials\" means, collectively, Meta’s proprietary Llama 3.1 and Documentation (and any portion thereof) made available under this Agreement.\n\"Meta\" or \"we\" means Meta Platforms Ireland Limited (if you are located in or, if you are an entity, your principal place of business is in the EEA or Switzerland) and Meta Platforms, Inc. (if you are located outside of the EEA or Switzerland).\n   \n1. License Rights and Redistribution.\na. Grant of Rights. You are granted a non-exclusive, worldwide, non-transferable and royalty-free limited license under Meta’s intellectual property or other rights owned by Meta embodied in the Llama Materials to use, reproduce, distribute, copy, create derivative works of, and make modifications to the Llama Materials.\nb. Redistribution and Use.\ni. If you distribute or make available the Llama Materials (or any derivative works thereof), or a product or service (including another AI model) that contains any of them, you shall (A) provide a copy of this Agreement with any such Llama Materials; and (B) prominently display “Built with Llama” on a related website, user interface, blogpost, about page, or product documentation. If you use the Llama Materials or any outputs or results of the Llama Materials to create, train, fine tune, or otherwise improve an AI model, which is distributed or made available, you shall also include “Llama” at the beginning of any such AI model name.\nii. If you receive Llama Materials, or any derivative works thereof, from a Licensee as part  of an integrated end user product, then Section 2 of this Agreement will not apply to you.\niii. You must retain in all copies of the Llama Materials that you distribute the following attribution notice within a “Notice” text file distributed as a part of such copies: “Llama 3.1 is licensed under the Llama 3.1 Community License, Copyright © Meta Platforms, Inc. All Rights Reserved.”\niv. Your use of the Llama Materials must comply with applicable laws and regulations (including trade compliance laws and regulations) and adhere to the Acceptable Use Policy for the Llama Materials (available at https://llama.meta.com/llama3_1/use-policy), which is hereby incorporated by reference into this Agreement.\n2. Additional Commercial Terms. If, on the Llama 3.1 version release date, the monthly active users of the products or services made available by or for Licensee, or Licensee’s affiliates, is greater than 700 million monthly active users in the preceding calendar month, you must request a license from Meta, which Meta may grant to you in its sole discretion, and you are not authorized to exercise any of the rights under this Agreement unless or until Meta otherwise expressly grants you such rights.\n3. Disclaimer of Warranty. UNLESS REQUIRED BY APPLICABLE LAW, THE LLAMA MATERIALS AND ANY OUTPUT AND RESULTS THEREFROM ARE PROVIDED ON AN “AS IS” BASIS, WITHOUT WARRANTIES OF ANY KIND, AND META DISCLAIMS ALL WARRANTIES OF ANY KIND, BOTH EXPRESS AND IMPLIED, INCLUDING, WITHOUT LIMITATION, ANY WARRANTIES OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE. YOU ARE SOLELY RESPONSIBLE FOR DETERMINING THE APPROPRIATENESS OF USING OR REDISTRIBUTING THE LLAMA MATERIALS AND ASSUME ANY RISKS ASSOCIATED WITH YOUR USE OF THE LLAMA MATERIALS AND ANY OUTPUT AND RESULTS.\n4. Limitation of Liability. IN NO EVENT WILL META OR ITS AFFILIATES BE LIABLE UNDER ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, TORT, NEGLIGENCE, PRODUCTS LIABILITY, OR OTHERWISE, ARISING OUT OF THIS AGREEMENT, FOR ANY LOST PROFITS OR ANY INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL, EXEMPLARY OR PUNITIVE DAMAGES, EVEN IF META OR ITS AFFILIATES HAVE BEEN ADVISED OF THE POSSIBILITY OF ANY OF THE FOREGOING.\n5. Intellectual Property.\na. No trademark licenses are granted under this Agreement, and in connection with the Llama Materials, neither Meta nor Licensee may use any name or mark owned by or associated with the other or any of its affiliates, except as required for reasonable and customary use in describing and redistributing the Llama Materials or as set forth in this Section 5(a). Meta hereby grants you a license to use “Llama” (the “Mark”) solely as required to comply with the last sentence of Section 1.b.i. You will comply with Meta’s brand guidelines (currently accessible at https://about.meta.com/brand/resources/meta/company-brand/ ). All goodwill arising out of your use of the Mark will inure to the benefit of Meta.\nb. Subject to Meta’s ownership of Llama Materials and derivatives made by or for Meta, with respect to any derivative works and modifications of the Llama Materials that are made by you, as between you and Meta, you are and will be the owner of such derivative works and modifications.\nc. If you institute litigation or other proceedings against Meta or any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Llama Materials or Llama 3.1 outputs or results, or any portion of any of the foregoing, constitutes infringement of intellectual property or other rights owned or licensable by you, then any licenses granted to you under this Agreement shall terminate as of the date such litigation or claim is filed or instituted. You will indemnify and hold harmless Meta from and against any claim by any third party arising out of or related to your use or distribution of the Llama Materials.\n6. Term and Termination. The term of this Agreement will commence upon your acceptance of this Agreement or access to the Llama Materials and will continue in full force and effect until terminated in accordance with the terms and conditions herein. Meta may terminate this Agreement if you are in breach of any term or condition of this Agreement. Upon termination of this Agreement, you shall delete and cease use of the Llama Materials. Sections 3, 4 and 7 shall survive the termination of this Agreement.\n7. Governing Law and Jurisdiction. This Agreement will be governed and construed under the laws of the State of California without regard to choice of law principles, and the UN Convention on Contracts for the International Sale of Goods does not apply to this Agreement. The courts of California shall have exclusive jurisdiction of any dispute arising out of this Agreement.\n### Llama 3.1 Acceptable Use Policy\nMeta is committed to promoting safe and fair use of its tools and features, including Llama 3.1. If you access or use Llama 3.1, you agree to this Acceptable Use Policy (“Policy”). The most recent copy of this policy can be found at [https://llama.meta.com/llama3_1/use-policy](https://llama.meta.com/llama3_1/use-policy)\n#### Prohibited Uses\nWe want everyone to use Llama 3.1 safely and responsibly. You agree you will not use, or allow others to use, Llama 3.1 to:\n 1. Violate the law or others’ rights, including to:\n    1. Engage in, promote, generate, contribute to, encourage, plan, incite, or further illegal or unlawful activity or content, such as:\n        1. Violence or terrorism\n        2. Exploitation or harm to children, including the solicitation, creation, acquisition, or dissemination of child exploitative content or failure to report Child Sexual Abuse Material\n        3. Human trafficking, exploitation, and sexual violence\n        4. The illegal distribution of information or materials to minors, including obscene materials, or failure to employ legally required age-gating in connection with such information or materials.\n        5. Sexual solicitation\n        6. Any other criminal activity\n    3. Engage in, promote, incite, or facilitate the harassment, abuse, threatening, or bullying of individuals or groups of individuals\n    4. Engage in, promote, incite, or facilitate discrimination or other unlawful or harmful conduct in the provision of employment, employment benefits, credit, housing, other economic benefits, or other essential goods and services\n    5. Engage in the unauthorized or unlicensed practice of any profession including, but not limited to, financial, legal, medical/health, or related professional practices\n    6. Collect, process, disclose, generate, or infer health, demographic, or other sensitive personal or private information about individuals without rights and consents required by applicable laws\n    7. Engage in or facilitate any action or generate any content that infringes, misappropriates, or otherwise violates any third-party rights, including the outputs or results of any products or services using the Llama Materials\n    8. Create, generate, or facilitate the creation of malicious code, malware, computer viruses or do anything else that could disable, overburden, interfere with or impair the proper working, integrity, operation or appearance of a website or computer system\n2. Engage in, promote, incite, facilitate, or assist in the planning or development of activities that present a risk of death or bodily harm to individuals, including use of Llama 3.1 related to the following:\n    1. Military, warfare, nuclear industries or applications, espionage, use for materials or activities that are subject to the International Traffic Arms Regulations (ITAR) maintained by the United States Department of State\n    2. Guns and illegal weapons (including weapon development)\n    3. Illegal drugs and regulated/controlled substances\n    4. Operation of critical infrastructure, transportation technologies, or heavy machinery\n    5. Self-harm or harm to others, including suicide, cutting, and eating disorders\n    6. Any content intended to incite or promote violence, abuse, or any infliction of bodily harm to an individual\n3. Intentionally deceive or mislead others, including use of Llama 3.1 related to the following:\n    1. Generating, promoting, or furthering fraud or the creation or promotion of disinformation\n    2. Generating, promoting, or furthering defamatory content, including the creation of defamatory statements, images, or other content\n    3. Generating, promoting, or further distributing spam\n    4. Impersonating another individual without consent, authorization, or legal right\n    5. Representing that the use of Llama 3.1 or outputs are human-generated\n    6. Generating or facilitating false online engagement, including fake reviews and other means of fake online engagement\n4. Fail to appropriately disclose to end users any known dangers of your AI system\nPlease report any violation of this Policy, software “bug,” or other problems that could lead to a violation of this Policy through one of the following means:\n    * Reporting issues with the model: [https://github.com/meta-llama/llama-models/issues](https://github.com/meta-llama/llama-models/issues)\n    * Reporting risky content generated by the model:\n    developers.facebook.com/llama_output_feedback\n    * Reporting bugs and security concerns: facebook.com/whitehat/info\n    * Reporting violations of the Acceptable Use Policy or unlicensed uses of Meta Llama 3: LlamaUseReport@meta.com",
      "extra_gated_fields": {
        "First Name": "text",
        "Last Name": "text",
        "Date of birth": "date_picker",
        "Country": "country",
        "Affiliation": "text",
        "Job title": {
          "type": "select",
          "options": [
            "Student",
            "Research Graduate",
            "AI researcher",
            "AI developer/engineer",
            "Reporter",
            "Other"
          ]
        },
        "geo": "ip_location",
        "By clicking Submit below I accept the terms of the license and acknowledge that the information I provide will be collected stored processed and shared in accordance with the Meta Privacy Policy": "checkbox"
      },
      "extra_gated_description": "The information you provide will be collected, stored, processed and shared in accordance with the [Meta Privacy Policy](https://www.facebook.com/privacy/policy/).",
      "extra_gated_button_content": "Submit",
      "library_name": "transformers"
    },
    "card_text": "\n## Model Information\n\nThe Meta Llama 3.1 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction tuned generative models in 8B, 70B and 405B sizes (text in/text out). The Llama 3.1 instruction tuned text only models (8B, 70B, 405B) are optimized for multilingual dialogue use cases and outperform many of the available open source and closed chat models on common industry benchmarks.\n\n**Model developer**: Meta\n\n**Model Architecture:** Llama 3.1 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety. \n\n\n<table>\n  <tr>\n   <td>\n   </td>\n   <td><strong>Training Data</strong>\n   </td>\n   <td><strong>Params</strong>\n   </td>\n   <td><strong>Input modalities</strong>\n   </td>\n   <td><strong>Output modalities</strong>\n   </td>\n   <td><strong>Context length</strong>\n   </td>\n   <td><strong>GQA</strong>\n   </td>\n   <td><strong>Token count</strong>\n   </td>\n   <td><strong>Knowledge cutoff</strong>\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"3\" >Llama 3.1 (text only)\n   </td>\n   <td rowspan=\"3\" >A new mix of publicly available online data.\n   </td>\n   <td>8B\n   </td>\n   <td>Multilingual Text\n   </td>\n   <td>Multilingual Text and code\n   </td>\n   <td>128k\n   </td>\n   <td>Yes\n   </td>\n   <td rowspan=\"3\" >15T+\n   </td>\n   <td rowspan=\"3\" >December 2023\n   </td>\n  </tr>\n  <tr>\n   <td>70B\n   </td>\n   <td>Multilingual Text\n   </td>\n   <td>Multilingual Text and code\n   </td>\n   <td>128k\n   </td>\n   <td>Yes\n   </td>\n  </tr>\n  <tr>\n   <td>405B\n   </td>\n   <td>Multilingual Text\n   </td>\n   <td>Multilingual Text and code\n   </td>\n   <td>128k\n   </td>\n   <td>Yes\n   </td>\n  </tr>\n</table>\n\n\n**Supported languages:** English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai.\n\n**Llama 3.1 family of models**. Token counts refer to pretraining data only. All model versions use Grouped-Query Attention (GQA) for improved inference scalability.\n\n**Model Release Date:** July 23, 2024.\n\n**Status:** This is a static model trained on an offline dataset. Future versions of the tuned models will be released as we improve model safety with community feedback.\n\n**License:** A custom commercial license, the Llama 3.1 Community License, is available at: [https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/LICENSE](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/LICENSE)\n\nWhere to send questions or comments about the model Instructions on how to provide feedback or comments on the model can be found in the model [README](https://github.com/meta-llama/llama3). For more technical information about generation parameters and recipes for how to use Llama 3.1 in applications, please go [here](https://github.com/meta-llama/llama-recipes). \n\n\n## Intended Use\n\n**Intended Use Cases** Llama 3.1 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks. The Llama 3.1 model collection also supports the ability to leverage the outputs of its models to improve other models including synthetic data generation and distillation. The Llama 3.1 Community License allows for these use cases. \n\n**Out-of-scope** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.1 Community License. Use in languages beyond those explicitly referenced as supported in this model card**.\n\n**<span style=\"text-decoration:underline;\">Note</span>: Llama 3.1 has been trained on a broader collection of languages than the 8 supported languages. Developers may fine-tune Llama 3.1 models for languages beyond the 8 supported languages provided they comply with the Llama 3.1 Community License and the Acceptable Use Policy and in such cases are responsible for ensuring that any uses of Llama 3.1 in additional languages is done in a safe and responsible manner.\n\n## How to use\n\nThis repository contains two versions of Meta's Llama-3.1-8B, for use with transformers and with the original `llama` codebase.\n\n### Use with transformers\n\nStarting with transformers >= 4.43.0 onward, you can run conversational inference using the Transformers pipeline abstraction or by leveraging the Auto classes with the generate() function.\n\nMake sure to update your transformers installation via pip install --upgrade transformers.\n\n```python\nimport transformers\nimport torch\n\nmodel_id = \"meta-llama/Llama-3.1-8B\"\n\npipeline = transformers.pipeline(\n    \"text-generation\", model=model_id, model_kwargs={\"torch_dtype\": torch.bfloat16}, device_map=\"auto\"\n)\n\npipeline(\"Hey how are you doing today?\")\n```\n\n### Use with `llama`\n\nPlease, follow the instructions in the [repository](https://github.com/meta-llama/llama).\n\nTo download Original checkpoints, see the example command below leveraging `huggingface-cli`:\n\n```\nhuggingface-cli download meta-llama/Llama-3.1-8B --include \"original/*\" --local-dir Llama-3.1-8B\n```\n\n## Hardware and Software\n\n**Training Factors** We used custom training libraries, Meta's custom built GPU cluster, and production infrastructure for pretraining. Fine-tuning, annotation, and evaluation were also performed on production infrastructure.\n\n**Training utilized a cumulative of** 39.3M GPU hours of computation on H100-80GB (TDP of 700W) type hardware, per the table below. Training time is the total GPU time required for training each model and power consumption is the peak power capacity per GPU device used, adjusted for power usage efficiency. \n\n\n**Training Greenhouse Gas Emissions** Estimated total location-based greenhouse gas emissions were **11,390** tons CO2eq for training. Since 2020, Meta has maintained net zero greenhouse gas emissions in its global operations and matched 100% of its electricity use with renewable energy, therefore the total market-based greenhouse gas emissions for training were 0 tons CO2eq.\n\n\n<table>\n  <tr>\n   <td>\n   </td>\n   <td><strong>Training Time (GPU hours)</strong>\n   </td>\n   <td><strong>Training Power Consumption (W)</strong>\n   </td>\n   <td><strong>Training Location-Based Greenhouse Gas Emissions</strong>\n<p>\n<strong>(tons CO2eq)</strong>\n   </td>\n   <td><strong>Training Market-Based Greenhouse Gas Emissions</strong>\n<p>\n<strong>(tons CO2eq)</strong>\n   </td>\n  </tr>\n  <tr>\n   <td>Llama 3.1 8B\n   </td>\n   <td>1.46M\n   </td>\n   <td>700\n   </td>\n   <td>420\n   </td>\n   <td>0\n   </td>\n  </tr>\n  <tr>\n   <td>Llama 3.1 70B\n   </td>\n   <td>7.0M\n   </td>\n   <td>700\n   </td>\n   <td>2,040\n   </td>\n   <td>0\n   </td>\n  </tr>\n  <tr>\n   <td>Llama 3.1 405B\n   </td>\n   <td>30.84M\n   </td>\n   <td>700\n   </td>\n   <td>8,930\n   </td>\n   <td>0\n   </td>\n  </tr>\n  <tr>\n   <td>Total\n   </td>\n   <td>39.3M\n   <td>\n<ul>\n\n</ul>\n   </td>\n   <td>11,390\n   </td>\n   <td>0\n   </td>\n  </tr>\n</table>\n\n\n\nThe methodology used to determine training energy use and greenhouse gas emissions can be found [here](https://arxiv.org/pdf/2204.05149).  Since Meta is openly releasing these models, the training energy use and greenhouse gas emissions  will not be incurred by others.\n\n\n## Training Data\n\n**Overview:** Llama 3.1 was pretrained on ~15 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over 25M synthetically generated examples. \n\n**Data Freshness:** The pretraining data has a cutoff of December 2023.\n\n\n## Benchmark scores\n\nIn this section, we report the results for Llama 3.1 models on standard automatic benchmarks. For all the evaluations, we use our internal evaluations library. \n\n### Base pretrained models\n\n\n<table>\n  <tr>\n   <td><strong>Category</strong>\n   </td>\n   <td><strong>Benchmark</strong>\n   </td>\n   <td><strong># Shots</strong>\n   </td>\n   <td><strong>Metric</strong>\n   </td>\n   <td><strong>Llama 3 8B</strong>\n   </td>\n   <td><strong>Llama 3.1 8B</strong>\n   </td>\n   <td><strong>Llama 3 70B</strong>\n   </td>\n   <td><strong>Llama 3.1 70B</strong>\n   </td>\n   <td><strong>Llama 3.1 405B</strong>\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"7\" >General\n   </td>\n   <td>MMLU\n   </td>\n   <td>5\n   </td>\n   <td>macro_avg/acc_char\n   </td>\n   <td>66.7\n   </td>\n   <td>66.7\n   </td>\n   <td>79.5\n   </td>\n   <td>79.3\n   </td>\n   <td>85.2\n   </td>\n  </tr>\n  <tr>\n   <td>MMLU-Pro (CoT)\n   </td>\n   <td>5\n   </td>\n   <td>macro_avg/acc_char\n   </td>\n   <td>36.2\n   </td>\n   <td>37.1\n   </td>\n   <td>55.0\n   </td>\n   <td>53.8\n   </td>\n   <td>61.6\n   </td>\n  </tr>\n  <tr>\n   <td>AGIEval English\n   </td>\n   <td>3-5\n   </td>\n   <td>average/acc_char\n   </td>\n   <td>47.1\n   </td>\n   <td>47.8\n   </td>\n   <td>63.0\n   </td>\n   <td>64.6\n   </td>\n   <td>71.6\n   </td>\n  </tr>\n  <tr>\n   <td>CommonSenseQA\n   </td>\n   <td>7\n   </td>\n   <td>acc_char\n   </td>\n   <td>72.6\n   </td>\n   <td>75.0\n   </td>\n   <td>83.8\n   </td>\n   <td>84.1\n   </td>\n   <td>85.8\n   </td>\n  </tr>\n  <tr>\n   <td>Winogrande\n   </td>\n   <td>5\n   </td>\n   <td>acc_char\n   </td>\n   <td>-\n   </td>\n   <td>60.5\n   </td>\n   <td>-\n   </td>\n   <td>83.3\n   </td>\n   <td>86.7\n   </td>\n  </tr>\n  <tr>\n   <td>BIG-Bench Hard (CoT)\n   </td>\n   <td>3\n   </td>\n   <td>average/em\n   </td>\n   <td>61.1\n   </td>\n   <td>64.2\n   </td>\n   <td>81.3\n   </td>\n   <td>81.6\n   </td>\n   <td>85.9\n   </td>\n  </tr>\n  <tr>\n   <td>ARC-Challenge\n   </td>\n   <td>25\n   </td>\n   <td>acc_char\n   </td>\n   <td>79.4\n   </td>\n   <td>79.7\n   </td>\n   <td>93.1\n   </td>\n   <td>92.9\n   </td>\n   <td>96.1\n   </td>\n  </tr>\n  <tr>\n   <td>Knowledge reasoning\n   </td>\n   <td>TriviaQA-Wiki\n   </td>\n   <td>5\n   </td>\n   <td>em\n   </td>\n   <td>78.5\n   </td>\n   <td>77.6\n   </td>\n   <td>89.7\n   </td>\n   <td>89.8\n   </td>\n   <td>91.8\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"4\" >Reading comprehension\n   </td>\n   <td>SQuAD\n   </td>\n   <td>1\n   </td>\n   <td>em\n   </td>\n   <td>76.4\n   </td>\n   <td>77.0\n   </td>\n   <td>85.6\n   </td>\n   <td>81.8\n   </td>\n   <td>89.3\n   </td>\n  </tr>\n  <tr>\n   <td>QuAC (F1)\n   </td>\n   <td>1\n   </td>\n   <td>f1\n   </td>\n   <td>44.4\n   </td>\n   <td>44.9\n   </td>\n   <td>51.1\n   </td>\n   <td>51.1\n   </td>\n   <td>53.6\n   </td>\n  </tr>\n  <tr>\n   <td>BoolQ\n   </td>\n   <td>0\n   </td>\n   <td>acc_char\n   </td>\n   <td>75.7\n   </td>\n   <td>75.0\n   </td>\n   <td>79.0\n   </td>\n   <td>79.4\n   </td>\n   <td>80.0\n   </td>\n  </tr>\n  <tr>\n   <td>DROP (F1)\n   </td>\n   <td>3\n   </td>\n   <td>f1\n   </td>\n   <td>58.4\n   </td>\n   <td>59.5\n   </td>\n   <td>79.7\n   </td>\n   <td>79.6\n   </td>\n   <td>84.8\n   </td>\n  </tr>\n</table>\n\n\n\n### Instruction tuned models\n\n\n<table>\n  <tr>\n   <td><strong>Category</strong>\n   </td>\n   <td><strong>Benchmark</strong>\n   </td>\n   <td><strong># Shots</strong>\n   </td>\n   <td><strong>Metric</strong>\n   </td>\n   <td><strong>Llama 3 8B Instruct</strong>\n   </td>\n   <td><strong>Llama 3.1 8B Instruct</strong>\n   </td>\n   <td><strong>Llama 3 70B Instruct</strong>\n   </td>\n   <td><strong>Llama 3.1 70B Instruct</strong>\n   </td>\n   <td><strong>Llama 3.1 405B Instruct</strong>\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"4\" >General\n   </td>\n   <td>MMLU\n   </td>\n   <td>5\n   </td>\n   <td>macro_avg/acc\n   </td>\n   <td>68.5\n   </td>\n   <td>69.4\n   </td>\n   <td>82.0\n   </td>\n   <td>83.6\n   </td>\n   <td>87.3\n   </td>\n  </tr>\n  <tr>\n   <td>MMLU (CoT)\n   </td>\n   <td>0\n   </td>\n   <td>macro_avg/acc\n   </td>\n   <td>65.3\n   </td>\n   <td>73.0\n   </td>\n   <td>80.9\n   </td>\n   <td>86.0\n   </td>\n   <td>88.6\n   </td>\n  </tr>\n  <tr>\n   <td>MMLU-Pro (CoT)\n   </td>\n   <td>5\n   </td>\n   <td>micro_avg/acc_char\n   </td>\n   <td>45.5\n   </td>\n   <td>48.3\n   </td>\n   <td>63.4\n   </td>\n   <td>66.4\n   </td>\n   <td>73.3\n   </td>\n  </tr>\n  <tr>\n   <td>IFEval\n   </td>\n   <td>\n   </td>\n   <td>\n   </td>\n   <td>76.8\n   </td>\n   <td>80.4\n   </td>\n   <td>82.9\n   </td>\n   <td>87.5\n   </td>\n   <td>88.6\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"2\" >Reasoning\n   </td>\n   <td>ARC-C\n   </td>\n   <td>0\n   </td>\n   <td>acc\n   </td>\n   <td>82.4\n   </td>\n   <td>83.4\n   </td>\n   <td>94.4\n   </td>\n   <td>94.8\n   </td>\n   <td>96.9\n   </td>\n  </tr>\n  <tr>\n   <td>GPQA\n   </td>\n   <td>0\n   </td>\n   <td>em\n   </td>\n   <td>34.6\n   </td>\n   <td>30.4\n   </td>\n   <td>39.5\n   </td>\n   <td>46.7\n   </td>\n   <td>50.7\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"4\" >Code\n   </td>\n   <td>HumanEval\n   </td>\n   <td>0\n   </td>\n   <td>pass@1\n   </td>\n   <td>60.4\n   </td>\n   <td>72.6\n   </td>\n   <td>81.7\n   </td>\n   <td>80.5\n   </td>\n   <td>89.0\n   </td>\n  </tr>\n  <tr>\n   <td>MBPP ++ base version\n   </td>\n   <td>0\n   </td>\n   <td>pass@1\n   </td>\n   <td>70.6\n   </td>\n   <td>72.8\n   </td>\n   <td>82.5\n   </td>\n   <td>86.0\n   </td>\n   <td>88.6\n   </td>\n  </tr>\n  <tr>\n   <td>Multipl-E HumanEval\n   </td>\n   <td>0\n   </td>\n   <td>pass@1\n   </td>\n   <td>-\n   </td>\n   <td>50.8\n   </td>\n   <td>-\n   </td>\n   <td>65.5\n   </td>\n   <td>75.2\n   </td>\n  </tr>\n  <tr>\n   <td>Multipl-E MBPP\n   </td>\n   <td>0\n   </td>\n   <td>pass@1\n   </td>\n   <td>-\n   </td>\n   <td>52.4\n   </td>\n   <td>-\n   </td>\n   <td>62.0\n   </td>\n   <td>65.7\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"2\" >Math\n   </td>\n   <td>GSM-8K (CoT)\n   </td>\n   <td>8\n   </td>\n   <td>em_maj1@1\n   </td>\n   <td>80.6\n   </td>\n   <td>84.5\n   </td>\n   <td>93.0\n   </td>\n   <td>95.1\n   </td>\n   <td>96.8\n   </td>\n  </tr>\n  <tr>\n   <td>MATH (CoT)\n   </td>\n   <td>0\n   </td>\n   <td>final_em\n   </td>\n   <td>29.1\n   </td>\n   <td>51.9\n   </td>\n   <td>51.0\n   </td>\n   <td>68.0\n   </td>\n   <td>73.8\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"4\" >Tool Use\n   </td>\n   <td>API-Bank\n   </td>\n   <td>0\n   </td>\n   <td>acc\n   </td>\n   <td>48.3\n   </td>\n   <td>82.6\n   </td>\n   <td>85.1\n   </td>\n   <td>90.0\n   </td>\n   <td>92.0\n   </td>\n  </tr>\n  <tr>\n   <td>BFCL\n   </td>\n   <td>0\n   </td>\n   <td>acc\n   </td>\n   <td>60.3\n   </td>\n   <td>76.1\n   </td>\n   <td>83.0\n   </td>\n   <td>84.8\n   </td>\n   <td>88.5\n   </td>\n  </tr>\n  <tr>\n   <td>Gorilla Benchmark API Bench\n   </td>\n   <td>0\n   </td>\n   <td>acc\n   </td>\n   <td>1.7\n   </td>\n   <td>8.2\n   </td>\n   <td>14.7\n   </td>\n   <td>29.7\n   </td>\n   <td>35.3\n   </td>\n  </tr>\n  <tr>\n   <td>Nexus (0-shot)\n   </td>\n   <td>0\n   </td>\n   <td>macro_avg/acc\n   </td>\n   <td>18.1\n   </td>\n   <td>38.5\n   </td>\n   <td>47.8\n   </td>\n   <td>56.7\n   </td>\n   <td>58.7\n   </td>\n  </tr>\n  <tr>\n   <td>Multilingual\n   </td>\n   <td>Multilingual MGSM (CoT)\n   </td>\n   <td>0\n   </td>\n   <td>em\n   </td>\n   <td>-\n   </td>\n   <td>68.9\n   </td>\n   <td>-\n   </td>\n   <td>86.9\n   </td>\n   <td>91.6\n   </td>\n  </tr>\n</table>\n\n#### Multilingual benchmarks\n\n<table>\n  <tr>\n   <td><strong>Category</strong>\n   </td>\n   <td><strong>Benchmark</strong>\n   </td>\n   <td><strong>Language</strong>\n   </td>\n   <td><strong>Llama 3.1 8B</strong>\n   </td>\n   <td><strong>Llama 3.1 70B</strong>\n   </td>\n   <td><strong>Llama 3.1 405B</strong>\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"9\" ><strong>General</strong>\n   </td>\n   <td rowspan=\"9\" ><strong>MMLU (5-shot, macro_avg/acc)</strong>\n   </td>\n   <td>Portuguese\n   </td>\n   <td>62.12\n   </td>\n   <td>80.13\n   </td>\n   <td>84.95\n   </td>\n  </tr>\n  <tr>\n   <td>Spanish\n   </td>\n   <td>62.45\n   </td>\n   <td>80.05\n   </td>\n   <td>85.08\n   </td>\n  </tr>\n  <tr>\n   <td>Italian\n   </td>\n   <td>61.63\n   </td>\n   <td>80.4\n   </td>\n   <td>85.04\n   </td>\n  </tr>\n  <tr>\n   <td>German\n   </td>\n   <td>60.59\n   </td>\n   <td>79.27\n   </td>\n   <td>84.36\n   </td>\n  </tr>\n  <tr>\n   <td>French\n   </td>\n   <td>62.34\n   </td>\n   <td>79.82\n   </td>\n   <td>84.66\n   </td>\n  </tr>\n  <tr>\n   <td>Hindi\n   </td>\n   <td>50.88\n   </td>\n   <td>74.52\n   </td>\n   <td>80.31\n   </td>\n  </tr>\n  <tr>\n   <td>Thai\n   </td>\n   <td>50.32\n   </td>\n   <td>72.95\n   </td>\n   <td>78.21\n   </td>\n  </tr>\n</table>\n\n\n\n## Responsibility & Safety\n\nAs part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:\n\n\n\n* Enable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama. \n* Protect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm.\n* Provide protections for the community to help prevent the misuse of our models.\n\n\n### Responsible deployment \n\nLlama is a foundational technology designed to be used in a variety of use cases, examples on how Meta’s Llama models have been responsibly deployed can be found in our [Community Stories webpage](https://llama.meta.com/community-stories/). Our approach is to build the most helpful models enabling the world to benefit from the technology power, by aligning our model safety for the generic use cases addressing a standard set of harms. Developers are then in the driver seat to tailor safety for their use case, defining their own policy and deploying the models with the necessary safeguards in their Llama systems. Llama 3.1 was developed following the best practices outlined in our Responsible Use Guide, you can refer to the [Responsible Use Guide](https://llama.meta.com/responsible-use-guide/) to learn more. \n\n\n#### Llama 3.1 instruct \n\nOur main objectives for conducting safety fine-tuning are to provide the research community with a valuable resource for studying the robustness of safety fine-tuning, as well as to offer developers a readily available, safe, and powerful model for various applications to reduce the developer workload to deploy safe AI systems. For more details on the safety mitigations implemented please read the Llama 3 paper. \n\n**Fine-tuning data**\n\nWe employ a multi-faceted approach to data collection, combining human-generated data from our vendors with synthetic data to mitigate potential safety risks. We’ve developed many large language model (LLM)-based classifiers that enable us to thoughtfully select high-quality prompts and responses, enhancing data quality control. \n\n**Refusals and Tone**\n\nBuilding on the work we started with Llama 3, we put a great emphasis on model refusals to benign prompts as well as refusal tone. We included both borderline and adversarial prompts in our safety data strategy, and modified our safety data responses to follow  tone guidelines. \n\n\n#### Llama 3.1 systems\n\n**Large language models, including Llama 3.1, are not designed to be deployed in isolation but instead should be deployed as part of an overall AI system with additional safety guardrails as required.** Developers are expected to deploy system safeguards when building agentic systems. Safeguards are key to achieve the right helpfulness-safety alignment as well as mitigating safety and security risks inherent to the system and any integration of the model or system with external tools. \n\nAs part of our responsible release approach, we provide the community with [safeguards](https://llama.meta.com/trust-and-safety/) that developers should deploy with Llama models or other LLMs, including Llama Guard 3, Prompt Guard and Code Shield. All our [reference implementations](https://github.com/meta-llama/llama-agentic-system) demos contain these safeguards by default so developers can benefit from system-level safety out-of-the-box. \n\n\n#### New capabilities \n\nNote that this release introduces new capabilities, including a longer context window, multilingual inputs and outputs and possible integrations by developers with third party tools. Building with these new capabilities requires specific considerations in addition to the best practices that generally apply across all Generative AI use cases.\n\n**Tool-use**: Just like in standard software development, developers are responsible for the integration of the LLM with the tools and services of their choice. They should define a clear policy for their use case and assess the integrity of the third party services they use to be aware of the safety and security limitations when using this capability. Refer to the Responsible Use Guide for best practices on the safe deployment of the third party safeguards. \n\n**Multilinguality**: Llama 3.1 supports 7 languages in addition to English: French, German, Hindi, Italian, Portuguese, Spanish, and Thai. Llama may be able to output text in other languages than those that meet performance thresholds for safety and helpfulness. We strongly discourage developers from using this model to converse in non-supported languages without implementing finetuning and system controls in alignment with their policies and the best practices shared in the Responsible Use Guide. \n\n\n### Evaluations\n\nWe evaluated Llama models for common use cases as well as specific capabilities. Common use cases evaluations measure safety risks of systems for most commonly built applications including chat bot, coding assistant, tool calls. We built dedicated, adversarial evaluation datasets and evaluated systems composed of Llama models and Llama Guard 3 to filter input prompt and output response. It is important to evaluate applications in context, and we recommend building dedicated evaluation dataset for your use case. Prompt Guard and Code Shield are also available if relevant to the application. \n\nCapability evaluations measure vulnerabilities of Llama models inherent to specific capabilities, for which were crafted dedicated benchmarks including long context, multilingual, tools calls, coding or memorization.\n\n**Red teaming**\n\nFor both scenarios, we conducted recurring red teaming exercises with the goal of discovering risks via adversarial prompting and we used the learnings to improve our benchmarks and safety tuning datasets. \n\nWe partnered early with subject-matter experts in critical risk areas to understand the nature of these real-world harms and how such models may lead to unintended harm for society. Based on these conversations, we derived a set of adversarial goals for the red team to attempt to achieve, such as extracting harmful information or reprogramming the model to act in a potentially harmful capacity.  The red team consisted of experts in cybersecurity, adversarial machine learning, responsible AI, and integrity in addition to multilingual content specialists with background in integrity issues in specific geographic markets.\n\n\n### Critical and other risks \n\nWe specifically focused our efforts on mitigating the following critical risk areas:\n\n**1- CBRNE (Chemical, Biological, Radiological, Nuclear, and Explosive materials) helpfulness**\n\nTo assess risks related to proliferation of chemical and biological weapons, we performed uplift testing designed to assess whether use of Llama 3.1 models could meaningfully increase the capabilities of malicious actors to plan or carry out attacks using these types of weapons. \n\n\n**2. Child Safety**\n\nChild Safety risk assessments were conducted using a team of experts, to assess the model’s capability to produce outputs that could result in Child Safety risks and inform on any necessary and appropriate risk mitigations via fine tuning. We leveraged those expert red teaming sessions to expand the coverage of our evaluation benchmarks through Llama 3 model development.  For Llama 3, we conducted new in-depth sessions using objective based methodologies to assess the model risks along multiple attack vectors including the additional languages Llama 3 is trained on. We also partnered with content specialists to perform red teaming exercises assessing potentially violating content while taking account of market specific nuances or experiences. \n\n**3. Cyber attack enablement**\n\nOur cyber attack uplift study investigated whether LLMs can enhance human capabilities in hacking tasks, both in terms of skill level and speed.\n\nOur attack automation study focused on evaluating the capabilities of LLMs when used as autonomous agents in cyber offensive operations, specifically in the context of ransomware attacks. This evaluation was distinct from previous studies that considered LLMs as interactive assistants. The primary objective was to assess whether these models could effectively function as independent agents in executing complex cyber-attacks without human intervention.\n\nOur study of Llama-3.1-405B’s social engineering uplift for cyber attackers was conducted to assess the effectiveness of AI models in aiding cyber threat actors in spear phishing campaigns. Please read our Llama 3.1 Cyber security whitepaper to learn more.\n\n\n### Community \n\nGenerative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership on AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Purple Llama tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to our [Github repository](https://github.com/meta-llama/PurpleLlama). \n\nWe also set up the [Llama Impact Grants](https://llama.meta.com/llama-impact-grants/) program to identify and support the most compelling applications of Meta’s Llama model for societal benefit across three categories: education, climate and open innovation. The 20 finalists from the hundreds of applications can be found [here](https://llama.meta.com/llama-impact-grants/#finalists). \n\nFinally, we put in place a set of resources including an [output reporting mechanism](https://developers.facebook.com/llama_output_feedback) and [bug bounty program](https://www.facebook.com/whitehat) to continuously improve the Llama technology with the help of the community.\n\n\n## Ethical Considerations and Limitations\n\nThe core values of Llama 3.1 are openness, inclusivity and helpfulness. It is meant to serve everyone, and to work for a wide range of use cases. It is thus designed to be accessible to people across many different backgrounds, experiences and perspectives. Llama 3.1 addresses users and their needs as they are, without insertion unnecessary judgment or normativity, while reflecting the understanding that even content that may appear problematic in some cases can serve valuable purposes in others. It respects the dignity and autonomy of all users, especially in terms of the values of free thought and expression that power innovation and progress. \n\nBut Llama 3.1 is a new technology, and like any new technology, there are risks associated with its use. Testing conducted to date has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, Llama 3.1’s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 3.1 models, developers should perform safety testing and tuning tailored to their specific applications of the model. Please refer to available resources including our [Responsible Use Guide](https://llama.meta.com/responsible-use-guide), [Trust and Safety](https://llama.meta.com/trust-and-safety/) solutions, and other [resources](https://llama.meta.com/docs/get-started/) to learn more about responsible development.",
    "card_content": "---\nlanguage:\n- en\n- de\n- fr\n- it\n- pt\n- hi\n- es\n- th\npipeline_tag: text-generation\ntags:\n- facebook\n- meta\n- pytorch\n- llama\n- llama-3\nlicense: llama3.1\nextra_gated_prompt: \"### LLAMA 3.1 COMMUNITY LICENSE AGREEMENT\\nLlama 3.1 Version\\\n  \\ Release Date: July 23, 2024\\n\\\"Agreement\\\" means the terms and conditions for\\\n  \\ use, reproduction, distribution and modification of the  Llama Materials set forth\\\n  \\ herein.\\n\\\"Documentation\\\" means the specifications, manuals and documentation\\\n  \\ accompanying Llama 3.1 distributed by Meta at https://llama.meta.com/doc/overview.\\n\\\n  \\\"Licensee\\\" or \\\"you\\\" means you, or your employer or any other person or entity\\\n  \\ (if you are entering into this Agreement on such person or entity’s behalf), of\\\n  \\ the age required under applicable laws, rules or regulations to provide legal\\\n  \\ consent and that has legal authority to bind your employer or such other person\\\n  \\ or entity if you are entering in this Agreement on their behalf.\\n\\\"Llama 3.1\\\"\\\n  \\ means the foundational large language models and software and algorithms, including\\\n  \\ machine-learning model code, trained model weights, inference-enabling code, training-enabling\\\n  \\ code, fine-tuning enabling code and other elements of the foregoing distributed\\\n  \\ by Meta at https://llama.meta.com/llama-downloads.\\n\\\"Llama Materials\\\" means,\\\n  \\ collectively, Meta’s proprietary Llama 3.1 and Documentation (and any portion\\\n  \\ thereof) made available under this Agreement.\\n\\\"Meta\\\" or \\\"we\\\" means Meta Platforms\\\n  \\ Ireland Limited (if you are located in or, if you are an entity, your principal\\\n  \\ place of business is in the EEA or Switzerland) and Meta Platforms, Inc. (if you\\\n  \\ are located outside of the EEA or Switzerland).\\n   \\n1. License Rights and Redistribution.\\n\\\n  a. Grant of Rights. You are granted a non-exclusive, worldwide, non-transferable\\\n  \\ and royalty-free limited license under Meta’s intellectual property or other rights\\\n  \\ owned by Meta embodied in the Llama Materials to use, reproduce, distribute, copy,\\\n  \\ create derivative works of, and make modifications to the Llama Materials.\\nb.\\\n  \\ Redistribution and Use.\\ni. If you distribute or make available the Llama Materials\\\n  \\ (or any derivative works thereof), or a product or service (including another\\\n  \\ AI model) that contains any of them, you shall (A) provide a copy of this Agreement\\\n  \\ with any such Llama Materials; and (B) prominently display “Built with Llama”\\\n  \\ on a related website, user interface, blogpost, about page, or product documentation.\\\n  \\ If you use the Llama Materials or any outputs or results of the Llama Materials\\\n  \\ to create, train, fine tune, or otherwise improve an AI model, which is distributed\\\n  \\ or made available, you shall also include “Llama” at the beginning of any such\\\n  \\ AI model name.\\nii. If you receive Llama Materials, or any derivative works thereof,\\\n  \\ from a Licensee as part  of an integrated end user product, then Section 2 of\\\n  \\ this Agreement will not apply to you.\\niii. You must retain in all copies of the\\\n  \\ Llama Materials that you distribute the following attribution notice within a\\\n  \\ “Notice” text file distributed as a part of such copies: “Llama 3.1 is licensed\\\n  \\ under the Llama 3.1 Community License, Copyright © Meta Platforms, Inc. All Rights\\\n  \\ Reserved.”\\niv. Your use of the Llama Materials must comply with applicable laws\\\n  \\ and regulations (including trade compliance laws and regulations) and adhere to\\\n  \\ the Acceptable Use Policy for the Llama Materials (available at https://llama.meta.com/llama3_1/use-policy),\\\n  \\ which is hereby incorporated by reference into this Agreement.\\n2. Additional\\\n  \\ Commercial Terms. If, on the Llama 3.1 version release date, the monthly active\\\n  \\ users of the products or services made available by or for Licensee, or Licensee’s\\\n  \\ affiliates, is greater than 700 million monthly active users in the preceding\\\n  \\ calendar month, you must request a license from Meta, which Meta may grant to\\\n  \\ you in its sole discretion, and you are not authorized to exercise any of the\\\n  \\ rights under this Agreement unless or until Meta otherwise expressly grants you\\\n  \\ such rights.\\n3. Disclaimer of Warranty. UNLESS REQUIRED BY APPLICABLE LAW, THE\\\n  \\ LLAMA MATERIALS AND ANY OUTPUT AND RESULTS THEREFROM ARE PROVIDED ON AN “AS IS”\\\n  \\ BASIS, WITHOUT WARRANTIES OF ANY KIND, AND META DISCLAIMS ALL WARRANTIES OF ANY\\\n  \\ KIND, BOTH EXPRESS AND IMPLIED, INCLUDING, WITHOUT LIMITATION, ANY WARRANTIES\\\n  \\ OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.\\\n  \\ YOU ARE SOLELY RESPONSIBLE FOR DETERMINING THE APPROPRIATENESS OF USING OR REDISTRIBUTING\\\n  \\ THE LLAMA MATERIALS AND ASSUME ANY RISKS ASSOCIATED WITH YOUR USE OF THE LLAMA\\\n  \\ MATERIALS AND ANY OUTPUT AND RESULTS.\\n4. Limitation of Liability. IN NO EVENT\\\n  \\ WILL META OR ITS AFFILIATES BE LIABLE UNDER ANY THEORY OF LIABILITY, WHETHER IN\\\n  \\ CONTRACT, TORT, NEGLIGENCE, PRODUCTS LIABILITY, OR OTHERWISE, ARISING OUT OF THIS\\\n  \\ AGREEMENT, FOR ANY LOST PROFITS OR ANY INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL,\\\n  \\ EXEMPLARY OR PUNITIVE DAMAGES, EVEN IF META OR ITS AFFILIATES HAVE BEEN ADVISED\\\n  \\ OF THE POSSIBILITY OF ANY OF THE FOREGOING.\\n5. Intellectual Property.\\na. No\\\n  \\ trademark licenses are granted under this Agreement, and in connection with the\\\n  \\ Llama Materials, neither Meta nor Licensee may use any name or mark owned by or\\\n  \\ associated with the other or any of its affiliates, except as required for reasonable\\\n  \\ and customary use in describing and redistributing the Llama Materials or as set\\\n  \\ forth in this Section 5(a). Meta hereby grants you a license to use “Llama” (the\\\n  \\ “Mark”) solely as required to comply with the last sentence of Section 1.b.i.\\\n  \\ You will comply with Meta’s brand guidelines (currently accessible at https://about.meta.com/brand/resources/meta/company-brand/\\\n  \\ ). All goodwill arising out of your use of the Mark will inure to the benefit\\\n  \\ of Meta.\\nb. Subject to Meta’s ownership of Llama Materials and derivatives made\\\n  \\ by or for Meta, with respect to any derivative works and modifications of the\\\n  \\ Llama Materials that are made by you, as between you and Meta, you are and will\\\n  \\ be the owner of such derivative works and modifications.\\nc. If you institute\\\n  \\ litigation or other proceedings against Meta or any entity (including a cross-claim\\\n  \\ or counterclaim in a lawsuit) alleging that the Llama Materials or Llama 3.1 outputs\\\n  \\ or results, or any portion of any of the foregoing, constitutes infringement of\\\n  \\ intellectual property or other rights owned or licensable by you, then any licenses\\\n  \\ granted to you under this Agreement shall terminate as of the date such litigation\\\n  \\ or claim is filed or instituted. You will indemnify and hold harmless Meta from\\\n  \\ and against any claim by any third party arising out of or related to your use\\\n  \\ or distribution of the Llama Materials.\\n6. Term and Termination. The term of\\\n  \\ this Agreement will commence upon your acceptance of this Agreement or access\\\n  \\ to the Llama Materials and will continue in full force and effect until terminated\\\n  \\ in accordance with the terms and conditions herein. Meta may terminate this Agreement\\\n  \\ if you are in breach of any term or condition of this Agreement. Upon termination\\\n  \\ of this Agreement, you shall delete and cease use of the Llama Materials. Sections\\\n  \\ 3, 4 and 7 shall survive the termination of this Agreement.\\n7. Governing Law\\\n  \\ and Jurisdiction. This Agreement will be governed and construed under the laws\\\n  \\ of the State of California without regard to choice of law principles, and the\\\n  \\ UN Convention on Contracts for the International Sale of Goods does not apply\\\n  \\ to this Agreement. The courts of California shall have exclusive jurisdiction\\\n  \\ of any dispute arising out of this Agreement.\\n### Llama 3.1 Acceptable Use Policy\\n\\\n  Meta is committed to promoting safe and fair use of its tools and features, including\\\n  \\ Llama 3.1. If you access or use Llama 3.1, you agree to this Acceptable Use Policy\\\n  \\ (“Policy”). The most recent copy of this policy can be found at [https://llama.meta.com/llama3_1/use-policy](https://llama.meta.com/llama3_1/use-policy)\\n\\\n  #### Prohibited Uses\\nWe want everyone to use Llama 3.1 safely and responsibly.\\\n  \\ You agree you will not use, or allow others to use, Llama 3.1 to:\\n 1. Violate\\\n  \\ the law or others’ rights, including to:\\n    1. Engage in, promote, generate,\\\n  \\ contribute to, encourage, plan, incite, or further illegal or unlawful activity\\\n  \\ or content, such as:\\n        1. Violence or terrorism\\n        2. Exploitation\\\n  \\ or harm to children, including the solicitation, creation, acquisition, or dissemination\\\n  \\ of child exploitative content or failure to report Child Sexual Abuse Material\\n\\\n  \\        3. Human trafficking, exploitation, and sexual violence\\n        4. The\\\n  \\ illegal distribution of information or materials to minors, including obscene\\\n  \\ materials, or failure to employ legally required age-gating in connection with\\\n  \\ such information or materials.\\n        5. Sexual solicitation\\n        6. Any\\\n  \\ other criminal activity\\n    3. Engage in, promote, incite, or facilitate the\\\n  \\ harassment, abuse, threatening, or bullying of individuals or groups of individuals\\n\\\n  \\    4. Engage in, promote, incite, or facilitate discrimination or other unlawful\\\n  \\ or harmful conduct in the provision of employment, employment benefits, credit,\\\n  \\ housing, other economic benefits, or other essential goods and services\\n    5.\\\n  \\ Engage in the unauthorized or unlicensed practice of any profession including,\\\n  \\ but not limited to, financial, legal, medical/health, or related professional\\\n  \\ practices\\n    6. Collect, process, disclose, generate, or infer health, demographic,\\\n  \\ or other sensitive personal or private information about individuals without rights\\\n  \\ and consents required by applicable laws\\n    7. Engage in or facilitate any action\\\n  \\ or generate any content that infringes, misappropriates, or otherwise violates\\\n  \\ any third-party rights, including the outputs or results of any products or services\\\n  \\ using the Llama Materials\\n    8. Create, generate, or facilitate the creation\\\n  \\ of malicious code, malware, computer viruses or do anything else that could disable,\\\n  \\ overburden, interfere with or impair the proper working, integrity, operation\\\n  \\ or appearance of a website or computer system\\n2. Engage in, promote, incite,\\\n  \\ facilitate, or assist in the planning or development of activities that present\\\n  \\ a risk of death or bodily harm to individuals, including use of Llama 3.1 related\\\n  \\ to the following:\\n    1. Military, warfare, nuclear industries or applications,\\\n  \\ espionage, use for materials or activities that are subject to the International\\\n  \\ Traffic Arms Regulations (ITAR) maintained by the United States Department of\\\n  \\ State\\n    2. Guns and illegal weapons (including weapon development)\\n    3.\\\n  \\ Illegal drugs and regulated/controlled substances\\n    4. Operation of critical\\\n  \\ infrastructure, transportation technologies, or heavy machinery\\n    5. Self-harm\\\n  \\ or harm to others, including suicide, cutting, and eating disorders\\n    6. Any\\\n  \\ content intended to incite or promote violence, abuse, or any infliction of bodily\\\n  \\ harm to an individual\\n3. Intentionally deceive or mislead others, including use\\\n  \\ of Llama 3.1 related to the following:\\n    1. Generating, promoting, or furthering\\\n  \\ fraud or the creation or promotion of disinformation\\n    2. Generating, promoting,\\\n  \\ or furthering defamatory content, including the creation of defamatory statements,\\\n  \\ images, or other content\\n    3. Generating, promoting, or further distributing\\\n  \\ spam\\n    4. Impersonating another individual without consent, authorization,\\\n  \\ or legal right\\n    5. Representing that the use of Llama 3.1 or outputs are human-generated\\n\\\n  \\    6. Generating or facilitating false online engagement, including fake reviews\\\n  \\ and other means of fake online engagement\\n4. Fail to appropriately disclose to\\\n  \\ end users any known dangers of your AI system\\nPlease report any violation of\\\n  \\ this Policy, software “bug,” or other problems that could lead to a violation\\\n  \\ of this Policy through one of the following means:\\n    * Reporting issues with\\\n  \\ the model: [https://github.com/meta-llama/llama-models/issues](https://github.com/meta-llama/llama-models/issues)\\n\\\n  \\    * Reporting risky content generated by the model:\\n    developers.facebook.com/llama_output_feedback\\n\\\n  \\    * Reporting bugs and security concerns: facebook.com/whitehat/info\\n    * Reporting\\\n  \\ violations of the Acceptable Use Policy or unlicensed uses of Meta Llama 3: LlamaUseReport@meta.com\"\nextra_gated_fields:\n  First Name: text\n  Last Name: text\n  Date of birth: date_picker\n  Country: country\n  Affiliation: text\n  Job title:\n    type: select\n    options:\n    - Student\n    - Research Graduate\n    - AI researcher\n    - AI developer/engineer\n    - Reporter\n    - Other\n  geo: ip_location\n  ? By clicking Submit below I accept the terms of the license and acknowledge that\n    the information I provide will be collected stored processed and shared in accordance\n    with the Meta Privacy Policy\n  : checkbox\nextra_gated_description: The information you provide will be collected, stored, processed\n  and shared in accordance with the [Meta Privacy Policy](https://www.facebook.com/privacy/policy/).\nextra_gated_button_content: Submit\nlibrary_name: transformers\n---\n\n## Model Information\n\nThe Meta Llama 3.1 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction tuned generative models in 8B, 70B and 405B sizes (text in/text out). The Llama 3.1 instruction tuned text only models (8B, 70B, 405B) are optimized for multilingual dialogue use cases and outperform many of the available open source and closed chat models on common industry benchmarks.\n\n**Model developer**: Meta\n\n**Model Architecture:** Llama 3.1 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety. \n\n\n<table>\n  <tr>\n   <td>\n   </td>\n   <td><strong>Training Data</strong>\n   </td>\n   <td><strong>Params</strong>\n   </td>\n   <td><strong>Input modalities</strong>\n   </td>\n   <td><strong>Output modalities</strong>\n   </td>\n   <td><strong>Context length</strong>\n   </td>\n   <td><strong>GQA</strong>\n   </td>\n   <td><strong>Token count</strong>\n   </td>\n   <td><strong>Knowledge cutoff</strong>\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"3\" >Llama 3.1 (text only)\n   </td>\n   <td rowspan=\"3\" >A new mix of publicly available online data.\n   </td>\n   <td>8B\n   </td>\n   <td>Multilingual Text\n   </td>\n   <td>Multilingual Text and code\n   </td>\n   <td>128k\n   </td>\n   <td>Yes\n   </td>\n   <td rowspan=\"3\" >15T+\n   </td>\n   <td rowspan=\"3\" >December 2023\n   </td>\n  </tr>\n  <tr>\n   <td>70B\n   </td>\n   <td>Multilingual Text\n   </td>\n   <td>Multilingual Text and code\n   </td>\n   <td>128k\n   </td>\n   <td>Yes\n   </td>\n  </tr>\n  <tr>\n   <td>405B\n   </td>\n   <td>Multilingual Text\n   </td>\n   <td>Multilingual Text and code\n   </td>\n   <td>128k\n   </td>\n   <td>Yes\n   </td>\n  </tr>\n</table>\n\n\n**Supported languages:** English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai.\n\n**Llama 3.1 family of models**. Token counts refer to pretraining data only. All model versions use Grouped-Query Attention (GQA) for improved inference scalability.\n\n**Model Release Date:** July 23, 2024.\n\n**Status:** This is a static model trained on an offline dataset. Future versions of the tuned models will be released as we improve model safety with community feedback.\n\n**License:** A custom commercial license, the Llama 3.1 Community License, is available at: [https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/LICENSE](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/LICENSE)\n\nWhere to send questions or comments about the model Instructions on how to provide feedback or comments on the model can be found in the model [README](https://github.com/meta-llama/llama3). For more technical information about generation parameters and recipes for how to use Llama 3.1 in applications, please go [here](https://github.com/meta-llama/llama-recipes). \n\n\n## Intended Use\n\n**Intended Use Cases** Llama 3.1 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks. The Llama 3.1 model collection also supports the ability to leverage the outputs of its models to improve other models including synthetic data generation and distillation. The Llama 3.1 Community License allows for these use cases. \n\n**Out-of-scope** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.1 Community License. Use in languages beyond those explicitly referenced as supported in this model card**.\n\n**<span style=\"text-decoration:underline;\">Note</span>: Llama 3.1 has been trained on a broader collection of languages than the 8 supported languages. Developers may fine-tune Llama 3.1 models for languages beyond the 8 supported languages provided they comply with the Llama 3.1 Community License and the Acceptable Use Policy and in such cases are responsible for ensuring that any uses of Llama 3.1 in additional languages is done in a safe and responsible manner.\n\n## How to use\n\nThis repository contains two versions of Meta's Llama-3.1-8B, for use with transformers and with the original `llama` codebase.\n\n### Use with transformers\n\nStarting with transformers >= 4.43.0 onward, you can run conversational inference using the Transformers pipeline abstraction or by leveraging the Auto classes with the generate() function.\n\nMake sure to update your transformers installation via pip install --upgrade transformers.\n\n```python\nimport transformers\nimport torch\n\nmodel_id = \"meta-llama/Llama-3.1-8B\"\n\npipeline = transformers.pipeline(\n    \"text-generation\", model=model_id, model_kwargs={\"torch_dtype\": torch.bfloat16}, device_map=\"auto\"\n)\n\npipeline(\"Hey how are you doing today?\")\n```\n\n### Use with `llama`\n\nPlease, follow the instructions in the [repository](https://github.com/meta-llama/llama).\n\nTo download Original checkpoints, see the example command below leveraging `huggingface-cli`:\n\n```\nhuggingface-cli download meta-llama/Llama-3.1-8B --include \"original/*\" --local-dir Llama-3.1-8B\n```\n\n## Hardware and Software\n\n**Training Factors** We used custom training libraries, Meta's custom built GPU cluster, and production infrastructure for pretraining. Fine-tuning, annotation, and evaluation were also performed on production infrastructure.\n\n**Training utilized a cumulative of** 39.3M GPU hours of computation on H100-80GB (TDP of 700W) type hardware, per the table below. Training time is the total GPU time required for training each model and power consumption is the peak power capacity per GPU device used, adjusted for power usage efficiency. \n\n\n**Training Greenhouse Gas Emissions** Estimated total location-based greenhouse gas emissions were **11,390** tons CO2eq for training. Since 2020, Meta has maintained net zero greenhouse gas emissions in its global operations and matched 100% of its electricity use with renewable energy, therefore the total market-based greenhouse gas emissions for training were 0 tons CO2eq.\n\n\n<table>\n  <tr>\n   <td>\n   </td>\n   <td><strong>Training Time (GPU hours)</strong>\n   </td>\n   <td><strong>Training Power Consumption (W)</strong>\n   </td>\n   <td><strong>Training Location-Based Greenhouse Gas Emissions</strong>\n<p>\n<strong>(tons CO2eq)</strong>\n   </td>\n   <td><strong>Training Market-Based Greenhouse Gas Emissions</strong>\n<p>\n<strong>(tons CO2eq)</strong>\n   </td>\n  </tr>\n  <tr>\n   <td>Llama 3.1 8B\n   </td>\n   <td>1.46M\n   </td>\n   <td>700\n   </td>\n   <td>420\n   </td>\n   <td>0\n   </td>\n  </tr>\n  <tr>\n   <td>Llama 3.1 70B\n   </td>\n   <td>7.0M\n   </td>\n   <td>700\n   </td>\n   <td>2,040\n   </td>\n   <td>0\n   </td>\n  </tr>\n  <tr>\n   <td>Llama 3.1 405B\n   </td>\n   <td>30.84M\n   </td>\n   <td>700\n   </td>\n   <td>8,930\n   </td>\n   <td>0\n   </td>\n  </tr>\n  <tr>\n   <td>Total\n   </td>\n   <td>39.3M\n   <td>\n<ul>\n\n</ul>\n   </td>\n   <td>11,390\n   </td>\n   <td>0\n   </td>\n  </tr>\n</table>\n\n\n\nThe methodology used to determine training energy use and greenhouse gas emissions can be found [here](https://arxiv.org/pdf/2204.05149).  Since Meta is openly releasing these models, the training energy use and greenhouse gas emissions  will not be incurred by others.\n\n\n## Training Data\n\n**Overview:** Llama 3.1 was pretrained on ~15 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over 25M synthetically generated examples. \n\n**Data Freshness:** The pretraining data has a cutoff of December 2023.\n\n\n## Benchmark scores\n\nIn this section, we report the results for Llama 3.1 models on standard automatic benchmarks. For all the evaluations, we use our internal evaluations library. \n\n### Base pretrained models\n\n\n<table>\n  <tr>\n   <td><strong>Category</strong>\n   </td>\n   <td><strong>Benchmark</strong>\n   </td>\n   <td><strong># Shots</strong>\n   </td>\n   <td><strong>Metric</strong>\n   </td>\n   <td><strong>Llama 3 8B</strong>\n   </td>\n   <td><strong>Llama 3.1 8B</strong>\n   </td>\n   <td><strong>Llama 3 70B</strong>\n   </td>\n   <td><strong>Llama 3.1 70B</strong>\n   </td>\n   <td><strong>Llama 3.1 405B</strong>\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"7\" >General\n   </td>\n   <td>MMLU\n   </td>\n   <td>5\n   </td>\n   <td>macro_avg/acc_char\n   </td>\n   <td>66.7\n   </td>\n   <td>66.7\n   </td>\n   <td>79.5\n   </td>\n   <td>79.3\n   </td>\n   <td>85.2\n   </td>\n  </tr>\n  <tr>\n   <td>MMLU-Pro (CoT)\n   </td>\n   <td>5\n   </td>\n   <td>macro_avg/acc_char\n   </td>\n   <td>36.2\n   </td>\n   <td>37.1\n   </td>\n   <td>55.0\n   </td>\n   <td>53.8\n   </td>\n   <td>61.6\n   </td>\n  </tr>\n  <tr>\n   <td>AGIEval English\n   </td>\n   <td>3-5\n   </td>\n   <td>average/acc_char\n   </td>\n   <td>47.1\n   </td>\n   <td>47.8\n   </td>\n   <td>63.0\n   </td>\n   <td>64.6\n   </td>\n   <td>71.6\n   </td>\n  </tr>\n  <tr>\n   <td>CommonSenseQA\n   </td>\n   <td>7\n   </td>\n   <td>acc_char\n   </td>\n   <td>72.6\n   </td>\n   <td>75.0\n   </td>\n   <td>83.8\n   </td>\n   <td>84.1\n   </td>\n   <td>85.8\n   </td>\n  </tr>\n  <tr>\n   <td>Winogrande\n   </td>\n   <td>5\n   </td>\n   <td>acc_char\n   </td>\n   <td>-\n   </td>\n   <td>60.5\n   </td>\n   <td>-\n   </td>\n   <td>83.3\n   </td>\n   <td>86.7\n   </td>\n  </tr>\n  <tr>\n   <td>BIG-Bench Hard (CoT)\n   </td>\n   <td>3\n   </td>\n   <td>average/em\n   </td>\n   <td>61.1\n   </td>\n   <td>64.2\n   </td>\n   <td>81.3\n   </td>\n   <td>81.6\n   </td>\n   <td>85.9\n   </td>\n  </tr>\n  <tr>\n   <td>ARC-Challenge\n   </td>\n   <td>25\n   </td>\n   <td>acc_char\n   </td>\n   <td>79.4\n   </td>\n   <td>79.7\n   </td>\n   <td>93.1\n   </td>\n   <td>92.9\n   </td>\n   <td>96.1\n   </td>\n  </tr>\n  <tr>\n   <td>Knowledge reasoning\n   </td>\n   <td>TriviaQA-Wiki\n   </td>\n   <td>5\n   </td>\n   <td>em\n   </td>\n   <td>78.5\n   </td>\n   <td>77.6\n   </td>\n   <td>89.7\n   </td>\n   <td>89.8\n   </td>\n   <td>91.8\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"4\" >Reading comprehension\n   </td>\n   <td>SQuAD\n   </td>\n   <td>1\n   </td>\n   <td>em\n   </td>\n   <td>76.4\n   </td>\n   <td>77.0\n   </td>\n   <td>85.6\n   </td>\n   <td>81.8\n   </td>\n   <td>89.3\n   </td>\n  </tr>\n  <tr>\n   <td>QuAC (F1)\n   </td>\n   <td>1\n   </td>\n   <td>f1\n   </td>\n   <td>44.4\n   </td>\n   <td>44.9\n   </td>\n   <td>51.1\n   </td>\n   <td>51.1\n   </td>\n   <td>53.6\n   </td>\n  </tr>\n  <tr>\n   <td>BoolQ\n   </td>\n   <td>0\n   </td>\n   <td>acc_char\n   </td>\n   <td>75.7\n   </td>\n   <td>75.0\n   </td>\n   <td>79.0\n   </td>\n   <td>79.4\n   </td>\n   <td>80.0\n   </td>\n  </tr>\n  <tr>\n   <td>DROP (F1)\n   </td>\n   <td>3\n   </td>\n   <td>f1\n   </td>\n   <td>58.4\n   </td>\n   <td>59.5\n   </td>\n   <td>79.7\n   </td>\n   <td>79.6\n   </td>\n   <td>84.8\n   </td>\n  </tr>\n</table>\n\n\n\n### Instruction tuned models\n\n\n<table>\n  <tr>\n   <td><strong>Category</strong>\n   </td>\n   <td><strong>Benchmark</strong>\n   </td>\n   <td><strong># Shots</strong>\n   </td>\n   <td><strong>Metric</strong>\n   </td>\n   <td><strong>Llama 3 8B Instruct</strong>\n   </td>\n   <td><strong>Llama 3.1 8B Instruct</strong>\n   </td>\n   <td><strong>Llama 3 70B Instruct</strong>\n   </td>\n   <td><strong>Llama 3.1 70B Instruct</strong>\n   </td>\n   <td><strong>Llama 3.1 405B Instruct</strong>\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"4\" >General\n   </td>\n   <td>MMLU\n   </td>\n   <td>5\n   </td>\n   <td>macro_avg/acc\n   </td>\n   <td>68.5\n   </td>\n   <td>69.4\n   </td>\n   <td>82.0\n   </td>\n   <td>83.6\n   </td>\n   <td>87.3\n   </td>\n  </tr>\n  <tr>\n   <td>MMLU (CoT)\n   </td>\n   <td>0\n   </td>\n   <td>macro_avg/acc\n   </td>\n   <td>65.3\n   </td>\n   <td>73.0\n   </td>\n   <td>80.9\n   </td>\n   <td>86.0\n   </td>\n   <td>88.6\n   </td>\n  </tr>\n  <tr>\n   <td>MMLU-Pro (CoT)\n   </td>\n   <td>5\n   </td>\n   <td>micro_avg/acc_char\n   </td>\n   <td>45.5\n   </td>\n   <td>48.3\n   </td>\n   <td>63.4\n   </td>\n   <td>66.4\n   </td>\n   <td>73.3\n   </td>\n  </tr>\n  <tr>\n   <td>IFEval\n   </td>\n   <td>\n   </td>\n   <td>\n   </td>\n   <td>76.8\n   </td>\n   <td>80.4\n   </td>\n   <td>82.9\n   </td>\n   <td>87.5\n   </td>\n   <td>88.6\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"2\" >Reasoning\n   </td>\n   <td>ARC-C\n   </td>\n   <td>0\n   </td>\n   <td>acc\n   </td>\n   <td>82.4\n   </td>\n   <td>83.4\n   </td>\n   <td>94.4\n   </td>\n   <td>94.8\n   </td>\n   <td>96.9\n   </td>\n  </tr>\n  <tr>\n   <td>GPQA\n   </td>\n   <td>0\n   </td>\n   <td>em\n   </td>\n   <td>34.6\n   </td>\n   <td>30.4\n   </td>\n   <td>39.5\n   </td>\n   <td>46.7\n   </td>\n   <td>50.7\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"4\" >Code\n   </td>\n   <td>HumanEval\n   </td>\n   <td>0\n   </td>\n   <td>pass@1\n   </td>\n   <td>60.4\n   </td>\n   <td>72.6\n   </td>\n   <td>81.7\n   </td>\n   <td>80.5\n   </td>\n   <td>89.0\n   </td>\n  </tr>\n  <tr>\n   <td>MBPP ++ base version\n   </td>\n   <td>0\n   </td>\n   <td>pass@1\n   </td>\n   <td>70.6\n   </td>\n   <td>72.8\n   </td>\n   <td>82.5\n   </td>\n   <td>86.0\n   </td>\n   <td>88.6\n   </td>\n  </tr>\n  <tr>\n   <td>Multipl-E HumanEval\n   </td>\n   <td>0\n   </td>\n   <td>pass@1\n   </td>\n   <td>-\n   </td>\n   <td>50.8\n   </td>\n   <td>-\n   </td>\n   <td>65.5\n   </td>\n   <td>75.2\n   </td>\n  </tr>\n  <tr>\n   <td>Multipl-E MBPP\n   </td>\n   <td>0\n   </td>\n   <td>pass@1\n   </td>\n   <td>-\n   </td>\n   <td>52.4\n   </td>\n   <td>-\n   </td>\n   <td>62.0\n   </td>\n   <td>65.7\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"2\" >Math\n   </td>\n   <td>GSM-8K (CoT)\n   </td>\n   <td>8\n   </td>\n   <td>em_maj1@1\n   </td>\n   <td>80.6\n   </td>\n   <td>84.5\n   </td>\n   <td>93.0\n   </td>\n   <td>95.1\n   </td>\n   <td>96.8\n   </td>\n  </tr>\n  <tr>\n   <td>MATH (CoT)\n   </td>\n   <td>0\n   </td>\n   <td>final_em\n   </td>\n   <td>29.1\n   </td>\n   <td>51.9\n   </td>\n   <td>51.0\n   </td>\n   <td>68.0\n   </td>\n   <td>73.8\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"4\" >Tool Use\n   </td>\n   <td>API-Bank\n   </td>\n   <td>0\n   </td>\n   <td>acc\n   </td>\n   <td>48.3\n   </td>\n   <td>82.6\n   </td>\n   <td>85.1\n   </td>\n   <td>90.0\n   </td>\n   <td>92.0\n   </td>\n  </tr>\n  <tr>\n   <td>BFCL\n   </td>\n   <td>0\n   </td>\n   <td>acc\n   </td>\n   <td>60.3\n   </td>\n   <td>76.1\n   </td>\n   <td>83.0\n   </td>\n   <td>84.8\n   </td>\n   <td>88.5\n   </td>\n  </tr>\n  <tr>\n   <td>Gorilla Benchmark API Bench\n   </td>\n   <td>0\n   </td>\n   <td>acc\n   </td>\n   <td>1.7\n   </td>\n   <td>8.2\n   </td>\n   <td>14.7\n   </td>\n   <td>29.7\n   </td>\n   <td>35.3\n   </td>\n  </tr>\n  <tr>\n   <td>Nexus (0-shot)\n   </td>\n   <td>0\n   </td>\n   <td>macro_avg/acc\n   </td>\n   <td>18.1\n   </td>\n   <td>38.5\n   </td>\n   <td>47.8\n   </td>\n   <td>56.7\n   </td>\n   <td>58.7\n   </td>\n  </tr>\n  <tr>\n   <td>Multilingual\n   </td>\n   <td>Multilingual MGSM (CoT)\n   </td>\n   <td>0\n   </td>\n   <td>em\n   </td>\n   <td>-\n   </td>\n   <td>68.9\n   </td>\n   <td>-\n   </td>\n   <td>86.9\n   </td>\n   <td>91.6\n   </td>\n  </tr>\n</table>\n\n#### Multilingual benchmarks\n\n<table>\n  <tr>\n   <td><strong>Category</strong>\n   </td>\n   <td><strong>Benchmark</strong>\n   </td>\n   <td><strong>Language</strong>\n   </td>\n   <td><strong>Llama 3.1 8B</strong>\n   </td>\n   <td><strong>Llama 3.1 70B</strong>\n   </td>\n   <td><strong>Llama 3.1 405B</strong>\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"9\" ><strong>General</strong>\n   </td>\n   <td rowspan=\"9\" ><strong>MMLU (5-shot, macro_avg/acc)</strong>\n   </td>\n   <td>Portuguese\n   </td>\n   <td>62.12\n   </td>\n   <td>80.13\n   </td>\n   <td>84.95\n   </td>\n  </tr>\n  <tr>\n   <td>Spanish\n   </td>\n   <td>62.45\n   </td>\n   <td>80.05\n   </td>\n   <td>85.08\n   </td>\n  </tr>\n  <tr>\n   <td>Italian\n   </td>\n   <td>61.63\n   </td>\n   <td>80.4\n   </td>\n   <td>85.04\n   </td>\n  </tr>\n  <tr>\n   <td>German\n   </td>\n   <td>60.59\n   </td>\n   <td>79.27\n   </td>\n   <td>84.36\n   </td>\n  </tr>\n  <tr>\n   <td>French\n   </td>\n   <td>62.34\n   </td>\n   <td>79.82\n   </td>\n   <td>84.66\n   </td>\n  </tr>\n  <tr>\n   <td>Hindi\n   </td>\n   <td>50.88\n   </td>\n   <td>74.52\n   </td>\n   <td>80.31\n   </td>\n  </tr>\n  <tr>\n   <td>Thai\n   </td>\n   <td>50.32\n   </td>\n   <td>72.95\n   </td>\n   <td>78.21\n   </td>\n  </tr>\n</table>\n\n\n\n## Responsibility & Safety\n\nAs part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:\n\n\n\n* Enable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama. \n* Protect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm.\n* Provide protections for the community to help prevent the misuse of our models.\n\n\n### Responsible deployment \n\nLlama is a foundational technology designed to be used in a variety of use cases, examples on how Meta’s Llama models have been responsibly deployed can be found in our [Community Stories webpage](https://llama.meta.com/community-stories/). Our approach is to build the most helpful models enabling the world to benefit from the technology power, by aligning our model safety for the generic use cases addressing a standard set of harms. Developers are then in the driver seat to tailor safety for their use case, defining their own policy and deploying the models with the necessary safeguards in their Llama systems. Llama 3.1 was developed following the best practices outlined in our Responsible Use Guide, you can refer to the [Responsible Use Guide](https://llama.meta.com/responsible-use-guide/) to learn more. \n\n\n#### Llama 3.1 instruct \n\nOur main objectives for conducting safety fine-tuning are to provide the research community with a valuable resource for studying the robustness of safety fine-tuning, as well as to offer developers a readily available, safe, and powerful model for various applications to reduce the developer workload to deploy safe AI systems. For more details on the safety mitigations implemented please read the Llama 3 paper. \n\n**Fine-tuning data**\n\nWe employ a multi-faceted approach to data collection, combining human-generated data from our vendors with synthetic data to mitigate potential safety risks. We’ve developed many large language model (LLM)-based classifiers that enable us to thoughtfully select high-quality prompts and responses, enhancing data quality control. \n\n**Refusals and Tone**\n\nBuilding on the work we started with Llama 3, we put a great emphasis on model refusals to benign prompts as well as refusal tone. We included both borderline and adversarial prompts in our safety data strategy, and modified our safety data responses to follow  tone guidelines. \n\n\n#### Llama 3.1 systems\n\n**Large language models, including Llama 3.1, are not designed to be deployed in isolation but instead should be deployed as part of an overall AI system with additional safety guardrails as required.** Developers are expected to deploy system safeguards when building agentic systems. Safeguards are key to achieve the right helpfulness-safety alignment as well as mitigating safety and security risks inherent to the system and any integration of the model or system with external tools. \n\nAs part of our responsible release approach, we provide the community with [safeguards](https://llama.meta.com/trust-and-safety/) that developers should deploy with Llama models or other LLMs, including Llama Guard 3, Prompt Guard and Code Shield. All our [reference implementations](https://github.com/meta-llama/llama-agentic-system) demos contain these safeguards by default so developers can benefit from system-level safety out-of-the-box. \n\n\n#### New capabilities \n\nNote that this release introduces new capabilities, including a longer context window, multilingual inputs and outputs and possible integrations by developers with third party tools. Building with these new capabilities requires specific considerations in addition to the best practices that generally apply across all Generative AI use cases.\n\n**Tool-use**: Just like in standard software development, developers are responsible for the integration of the LLM with the tools and services of their choice. They should define a clear policy for their use case and assess the integrity of the third party services they use to be aware of the safety and security limitations when using this capability. Refer to the Responsible Use Guide for best practices on the safe deployment of the third party safeguards. \n\n**Multilinguality**: Llama 3.1 supports 7 languages in addition to English: French, German, Hindi, Italian, Portuguese, Spanish, and Thai. Llama may be able to output text in other languages than those that meet performance thresholds for safety and helpfulness. We strongly discourage developers from using this model to converse in non-supported languages without implementing finetuning and system controls in alignment with their policies and the best practices shared in the Responsible Use Guide. \n\n\n### Evaluations\n\nWe evaluated Llama models for common use cases as well as specific capabilities. Common use cases evaluations measure safety risks of systems for most commonly built applications including chat bot, coding assistant, tool calls. We built dedicated, adversarial evaluation datasets and evaluated systems composed of Llama models and Llama Guard 3 to filter input prompt and output response. It is important to evaluate applications in context, and we recommend building dedicated evaluation dataset for your use case. Prompt Guard and Code Shield are also available if relevant to the application. \n\nCapability evaluations measure vulnerabilities of Llama models inherent to specific capabilities, for which were crafted dedicated benchmarks including long context, multilingual, tools calls, coding or memorization.\n\n**Red teaming**\n\nFor both scenarios, we conducted recurring red teaming exercises with the goal of discovering risks via adversarial prompting and we used the learnings to improve our benchmarks and safety tuning datasets. \n\nWe partnered early with subject-matter experts in critical risk areas to understand the nature of these real-world harms and how such models may lead to unintended harm for society. Based on these conversations, we derived a set of adversarial goals for the red team to attempt to achieve, such as extracting harmful information or reprogramming the model to act in a potentially harmful capacity.  The red team consisted of experts in cybersecurity, adversarial machine learning, responsible AI, and integrity in addition to multilingual content specialists with background in integrity issues in specific geographic markets.\n\n\n### Critical and other risks \n\nWe specifically focused our efforts on mitigating the following critical risk areas:\n\n**1- CBRNE (Chemical, Biological, Radiological, Nuclear, and Explosive materials) helpfulness**\n\nTo assess risks related to proliferation of chemical and biological weapons, we performed uplift testing designed to assess whether use of Llama 3.1 models could meaningfully increase the capabilities of malicious actors to plan or carry out attacks using these types of weapons. \n\n\n**2. Child Safety**\n\nChild Safety risk assessments were conducted using a team of experts, to assess the model’s capability to produce outputs that could result in Child Safety risks and inform on any necessary and appropriate risk mitigations via fine tuning. We leveraged those expert red teaming sessions to expand the coverage of our evaluation benchmarks through Llama 3 model development.  For Llama 3, we conducted new in-depth sessions using objective based methodologies to assess the model risks along multiple attack vectors including the additional languages Llama 3 is trained on. We also partnered with content specialists to perform red teaming exercises assessing potentially violating content while taking account of market specific nuances or experiences. \n\n**3. Cyber attack enablement**\n\nOur cyber attack uplift study investigated whether LLMs can enhance human capabilities in hacking tasks, both in terms of skill level and speed.\n\nOur attack automation study focused on evaluating the capabilities of LLMs when used as autonomous agents in cyber offensive operations, specifically in the context of ransomware attacks. This evaluation was distinct from previous studies that considered LLMs as interactive assistants. The primary objective was to assess whether these models could effectively function as independent agents in executing complex cyber-attacks without human intervention.\n\nOur study of Llama-3.1-405B’s social engineering uplift for cyber attackers was conducted to assess the effectiveness of AI models in aiding cyber threat actors in spear phishing campaigns. Please read our Llama 3.1 Cyber security whitepaper to learn more.\n\n\n### Community \n\nGenerative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership on AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Purple Llama tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to our [Github repository](https://github.com/meta-llama/PurpleLlama). \n\nWe also set up the [Llama Impact Grants](https://llama.meta.com/llama-impact-grants/) program to identify and support the most compelling applications of Meta’s Llama model for societal benefit across three categories: education, climate and open innovation. The 20 finalists from the hundreds of applications can be found [here](https://llama.meta.com/llama-impact-grants/#finalists). \n\nFinally, we put in place a set of resources including an [output reporting mechanism](https://developers.facebook.com/llama_output_feedback) and [bug bounty program](https://www.facebook.com/whitehat) to continuously improve the Llama technology with the help of the community.\n\n\n## Ethical Considerations and Limitations\n\nThe core values of Llama 3.1 are openness, inclusivity and helpfulness. It is meant to serve everyone, and to work for a wide range of use cases. It is thus designed to be accessible to people across many different backgrounds, experiences and perspectives. Llama 3.1 addresses users and their needs as they are, without insertion unnecessary judgment or normativity, while reflecting the understanding that even content that may appear problematic in some cases can serve valuable purposes in others. It respects the dignity and autonomy of all users, especially in terms of the values of free thought and expression that power innovation and progress. \n\nBut Llama 3.1 is a new technology, and like any new technology, there are risks associated with its use. Testing conducted to date has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, Llama 3.1’s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 3.1 models, developers should perform safety testing and tuning tailored to their specific applications of the model. Please refer to available resources including our [Responsible Use Guide](https://llama.meta.com/responsible-use-guide), [Trust and Safety](https://llama.meta.com/trust-and-safety/) solutions, and other [resources](https://llama.meta.com/docs/get-started/) to learn more about responsible development.",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModelForCausalLM",
      "custom_class": null,
      "pipeline_tag": "text-generation",
      "processor": "AutoTokenizer"
    },
    "safetensors": {
      "parameters": {
        "BF16": 8030261248
      },
      "total": 8030261248
    },
    "model_index": null,
    "trending_score": null
  },
  {
    "model_id": "meta-llama/Meta-Llama-3-8B-Instruct",
    "model_name": "meta-llama/Meta-Llama-3-8B-Instruct",
    "author": "meta-llama",
    "downloads": 1127302,
    "downloads_all_time": null,
    "likes": 3875,
    "tags": [
      "transformers",
      "safetensors",
      "llama",
      "text-generation",
      "facebook",
      "meta",
      "pytorch",
      "llama-3",
      "conversational",
      "en",
      "license:llama3",
      "autotrain_compatible",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct",
    "dependencies": null,
    "last_modified": "2024-09-27T15:52:39+00:00",
    "created_at": "2024-04-17T09:35:12+00:00",
    "analysis_date": "2025-03-22T00:42:32.462080",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "llama",
    "pipeline_tag": "text-generation",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": [
        "en"
      ],
      "pipeline_tag": "text-generation",
      "tags": [
        "facebook",
        "meta",
        "pytorch",
        "llama",
        "llama-3"
      ],
      "license": "llama3",
      "new_version": "meta-llama/Llama-3.1-8B-Instruct",
      "extra_gated_prompt": "### META LLAMA 3 COMMUNITY LICENSE AGREEMENT\nMeta Llama 3 Version Release Date: April 18, 2024\n\"Agreement\" means the terms and conditions for use, reproduction, distribution and modification of the Llama Materials set forth herein.\n\"Documentation\" means the specifications, manuals and documentation accompanying Meta Llama 3 distributed by Meta at https://llama.meta.com/get-started/.\n\"Licensee\" or \"you\" means you, or your employer or any other person or entity (if you are entering into this Agreement on such person or entity’s behalf), of the age required under applicable laws, rules or regulations to provide legal consent and that has legal authority to bind your employer or such other person or entity if you are entering in this Agreement on their behalf.\n\"Meta Llama 3\" means the foundational large language models and software and algorithms, including machine-learning model code, trained model weights, inference-enabling code, training-enabling code, fine-tuning enabling code and other elements of the foregoing distributed by Meta at https://llama.meta.com/llama-downloads.\n\"Llama Materials\" means, collectively, Meta’s proprietary Meta Llama 3 and Documentation (and any portion thereof) made available under this Agreement.\n\"Meta\" or \"we\" means Meta Platforms Ireland Limited (if you are located in or, if you are an entity, your principal place of business is in the EEA or Switzerland) and Meta Platforms, Inc. (if you are located outside of the EEA or Switzerland).\n   \n1. License Rights and Redistribution.\na. Grant of Rights. You are granted a non-exclusive, worldwide, non-transferable and royalty-free limited license under Meta’s intellectual property or other rights owned by Meta embodied in the Llama Materials to use, reproduce, distribute, copy, create derivative works of, and make modifications to the Llama Materials.\nb. Redistribution and Use.\ni. If you distribute or make available the Llama Materials (or any derivative works thereof), or a product or service that uses any of them, including another AI model, you shall (A) provide a copy of this Agreement with any such Llama Materials; and (B) prominently display “Built with Meta Llama 3” on a related website, user interface, blogpost, about page, or product documentation. If you use the Llama Materials to create, train, fine tune, or otherwise improve an AI model, which is distributed or made available, you shall also include “Llama 3” at the beginning of any such AI model name.\nii. If you receive Llama Materials, or any derivative works thereof, from a Licensee as part  of an integrated end user product, then Section 2 of this Agreement will not apply to you.\niii. You must retain in all copies of the Llama Materials that you distribute the following attribution notice within a “Notice” text file distributed as a part of such copies: “Meta Llama 3 is licensed under the Meta Llama 3 Community License, Copyright © Meta Platforms, Inc. All Rights Reserved.”\niv. Your use of the Llama Materials must comply with applicable laws and regulations (including trade compliance laws and regulations) and adhere to the Acceptable Use Policy for the Llama Materials (available at https://llama.meta.com/llama3/use-policy), which is hereby incorporated by reference into this Agreement.\nv. You will not use the Llama Materials or any output or results of the Llama Materials to improve any other large language model (excluding Meta Llama 3 or derivative works thereof).\n2. Additional Commercial Terms. If, on the Meta Llama 3 version release date, the monthly active users of the products or services made available by or for Licensee, or Licensee’s affiliates, is greater than 700 million monthly active users in the preceding calendar month, you must request a license from Meta, which Meta may grant to you in its sole discretion, and you are not authorized to exercise any of the rights under this Agreement unless or until Meta otherwise expressly grants you such rights.\n3. Disclaimer of Warranty. UNLESS REQUIRED BY APPLICABLE LAW, THE LLAMA MATERIALS AND ANY OUTPUT AND RESULTS THEREFROM ARE PROVIDED ON AN “AS IS” BASIS, WITHOUT WARRANTIES OF ANY KIND, AND META DISCLAIMS ALL WARRANTIES OF ANY KIND, BOTH EXPRESS AND IMPLIED, INCLUDING, WITHOUT LIMITATION, ANY WARRANTIES OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE. YOU ARE SOLELY RESPONSIBLE FOR DETERMINING THE APPROPRIATENESS OF USING OR REDISTRIBUTING THE LLAMA MATERIALS AND ASSUME ANY RISKS ASSOCIATED WITH YOUR USE OF THE LLAMA MATERIALS AND ANY OUTPUT AND RESULTS.\n4. Limitation of Liability. IN NO EVENT WILL META OR ITS AFFILIATES BE LIABLE UNDER ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, TORT, NEGLIGENCE, PRODUCTS LIABILITY, OR OTHERWISE, ARISING OUT OF THIS AGREEMENT, FOR ANY LOST PROFITS OR ANY INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL, EXEMPLARY OR PUNITIVE DAMAGES, EVEN IF META OR ITS AFFILIATES HAVE BEEN ADVISED OF THE POSSIBILITY OF ANY OF THE FOREGOING.\n5. Intellectual Property.\na. No trademark licenses are granted under this Agreement, and in connection with the Llama Materials, neither Meta nor Licensee may use any name or mark owned by or associated with the other or any of its affiliates, except as required for reasonable and customary use in describing and redistributing the Llama Materials or as set forth in this Section 5(a). Meta hereby grants you a license to use “Llama 3” (the “Mark”) solely as required to comply with the last sentence of Section 1.b.i. You will comply with Meta’s brand guidelines (currently accessible at https://about.meta.com/brand/resources/meta/company-brand/ ). All goodwill arising out of your use of the Mark will inure to the benefit of Meta.\nb. Subject to Meta’s ownership of Llama Materials and derivatives made by or for Meta, with respect to any derivative works and modifications of the Llama Materials that are made by you, as between you and Meta, you are and will be the owner of such derivative works and modifications.\nc. If you institute litigation or other proceedings against Meta or any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Llama Materials or Meta Llama 3 outputs or results, or any portion of any of the foregoing, constitutes infringement of intellectual property or other rights owned or licensable by you, then any licenses granted to you under this Agreement shall terminate as of the date such litigation or claim is filed or instituted. You will indemnify and hold harmless Meta from and against any claim by any third party arising out of or related to your use or distribution of the Llama Materials.\n6. Term and Termination. The term of this Agreement will commence upon your acceptance of this Agreement or access to the Llama Materials and will continue in full force and effect until terminated in accordance with the terms and conditions herein. Meta may terminate this Agreement if you are in breach of any term or condition of this Agreement. Upon termination of this Agreement, you shall delete and cease use of the Llama Materials. Sections 3, 4 and 7 shall survive the termination of this Agreement.\n7. Governing Law and Jurisdiction. This Agreement will be governed and construed under the laws of the State of California without regard to choice of law principles, and the UN Convention on Contracts for the International Sale of Goods does not apply to this Agreement. The courts of California shall have exclusive jurisdiction of any dispute arising out of this Agreement.\n### Meta Llama 3 Acceptable Use Policy\nMeta is committed to promoting safe and fair use of its tools and features, including Meta Llama 3. If you access or use Meta Llama 3, you agree to this Acceptable Use Policy (“Policy”). The most recent copy of this policy can be found at [https://llama.meta.com/llama3/use-policy](https://llama.meta.com/llama3/use-policy)\n#### Prohibited Uses\nWe want everyone to use Meta Llama 3 safely and responsibly. You agree you will not use, or allow others to use, Meta Llama 3 to: 1. Violate the law or others’ rights, including to:\n    1. Engage in, promote, generate, contribute to, encourage, plan, incite, or further illegal or unlawful activity or content, such as:\n        1. Violence or terrorism\n        2. Exploitation or harm to children, including the solicitation, creation, acquisition, or dissemination of child exploitative content or failure to report Child Sexual Abuse Material\n        3. Human trafficking, exploitation, and sexual violence\n        4. The illegal distribution of information or materials to minors, including obscene materials, or failure to employ legally required age-gating in connection with such information or materials.\n        5. Sexual solicitation\n        6. Any other criminal activity\n    2. Engage in, promote, incite, or facilitate the harassment, abuse, threatening, or bullying of individuals or groups of individuals\n    3. Engage in, promote, incite, or facilitate discrimination or other unlawful or harmful conduct in the provision of employment, employment benefits, credit, housing, other economic benefits, or other essential goods and services\n    4. Engage in the unauthorized or unlicensed practice of any profession including, but not limited to, financial, legal, medical/health, or related professional practices\n    5. Collect, process, disclose, generate, or infer health, demographic, or other sensitive personal or private information about individuals without rights and consents required by applicable laws\n    6. Engage in or facilitate any action or generate any content that infringes, misappropriates, or otherwise violates any third-party rights, including the outputs or results of any products or services using the Llama Materials\n    7. Create, generate, or facilitate the creation of malicious code, malware, computer viruses or do anything else that could disable, overburden, interfere with or impair the proper working, integrity, operation or appearance of a website or computer system\n2. Engage in, promote, incite, facilitate, or assist in the planning or development of activities that present a risk of death or bodily harm to individuals, including use of Meta Llama 3 related to the following:\n    1. Military, warfare, nuclear industries or applications, espionage, use for materials or activities that are subject to the International Traffic Arms Regulations (ITAR) maintained by the United States Department of State\n    2. Guns and illegal weapons (including weapon development)\n    3. Illegal drugs and regulated/controlled substances\n    4. Operation of critical infrastructure, transportation technologies, or heavy machinery\n    5. Self-harm or harm to others, including suicide, cutting, and eating disorders\n    6. Any content intended to incite or promote violence, abuse, or any infliction of bodily harm to an individual\n3. Intentionally deceive or mislead others, including use of Meta Llama 3 related to the following:\n    1. Generating, promoting, or furthering fraud or the creation or promotion of disinformation\n    2. Generating, promoting, or furthering defamatory content, including the creation of defamatory statements, images, or other content\n    3. Generating, promoting, or further distributing spam\n    4. Impersonating another individual without consent, authorization, or legal right\n    5. Representing that the use of Meta Llama 3 or outputs are human-generated\n    6. Generating or facilitating false online engagement, including fake reviews and other means of fake online engagement\n4. Fail to appropriately disclose to end users any known dangers of your AI system\nPlease report any violation of this Policy, software “bug,” or other problems that could lead to a violation of this Policy through one of the following means:\n    * Reporting issues with the model: [https://github.com/meta-llama/llama3](https://github.com/meta-llama/llama3)\n    * Reporting risky content generated by the model:\n    developers.facebook.com/llama_output_feedback\n    * Reporting bugs and security concerns: facebook.com/whitehat/info\n    * Reporting violations of the Acceptable Use Policy or unlicensed uses of Meta Llama 3: LlamaUseReport@meta.com",
      "extra_gated_fields": {
        "First Name": "text",
        "Last Name": "text",
        "Date of birth": "date_picker",
        "Country": "country",
        "Affiliation": "text",
        "geo": "ip_location",
        "By clicking Submit below I accept the terms of the license and acknowledge that the information I provide will be collected stored processed and shared in accordance with the Meta Privacy Policy": "checkbox"
      },
      "extra_gated_description": "The information you provide will be collected, stored, processed and shared in accordance with the [Meta Privacy Policy](https://www.facebook.com/privacy/policy/).",
      "extra_gated_button_content": "Submit",
      "widget": [
        {
          "example_title": "Hello",
          "messages": [
            {
              "role": "user",
              "content": "Hey my name is Julien! How are you?"
            }
          ]
        },
        {
          "example_title": "Winter holidays",
          "messages": [
            {
              "role": "system",
              "content": "You are a helpful and honest assistant. Please, respond concisely and truthfully."
            },
            {
              "role": "user",
              "content": "Can you recommend a good destination for Winter holidays?"
            }
          ]
        },
        {
          "example_title": "Programming assistant",
          "messages": [
            {
              "role": "system",
              "content": "You are a helpful and honest code and programming assistant. Please, respond concisely and truthfully."
            },
            {
              "role": "user",
              "content": "Write a function that computes the nth fibonacci number."
            }
          ]
        }
      ],
      "inference": {
        "parameters": {
          "max_new_tokens": 300,
          "stop": [
            "<|end_of_text|>",
            "<|eot_id|>"
          ]
        }
      }
    },
    "card_text": "\n## Model Details\n\nMeta developed and released the Meta Llama 3 family of large language models (LLMs), a collection of pretrained and instruction tuned generative text models in 8 and 70B sizes. The Llama 3 instruction tuned models are optimized for dialogue use cases and outperform many of the available open source chat models on common industry benchmarks. Further, in developing these models, we took great care to optimize helpfulness and safety. \n\n**Model developers** Meta\n\n**Variations** Llama 3 comes in two sizes — 8B and 70B parameters — in pre-trained and instruction tuned variants.\n\n**Input** Models input text only.\n\n**Output** Models generate text and code only.\n\n**Model Architecture** Llama 3 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.\n\n\n<table>\n  <tr>\n   <td>\n   </td>\n   <td><strong>Training Data</strong>\n   </td>\n   <td><strong>Params</strong>\n   </td>\n   <td><strong>Context length</strong>\n   </td>\n   <td><strong>GQA</strong>\n   </td>\n   <td><strong>Token count</strong>\n   </td>\n   <td><strong>Knowledge cutoff</strong>\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"2\" >Llama 3\n   </td>\n   <td rowspan=\"2\" >A new mix of publicly available online data.\n   </td>\n   <td>8B\n   </td>\n   <td>8k\n   </td>\n   <td>Yes\n   </td>\n   <td rowspan=\"2\" >15T+\n   </td>\n   <td>March, 2023\n   </td>\n  </tr>\n  <tr>\n   <td>70B\n   </td>\n   <td>8k\n   </td>\n   <td>Yes\n   </td>\n   <td>December, 2023\n   </td>\n  </tr>\n</table>\n\n\n**Llama 3 family of models**. Token counts refer to pretraining data only. Both the 8 and 70B versions use Grouped-Query Attention (GQA) for improved inference scalability.\n\n**Model Release Date** April 18, 2024.\n\n**Status** This is a static model trained on an offline dataset. Future versions of the tuned models will be released as we improve model safety with community feedback.\n\n**License** A custom commercial license is available at: [https://llama.meta.com/llama3/license](https://llama.meta.com/llama3/license)\n\nWhere to send questions or comments about the model Instructions on how to provide feedback or comments on the model can be found in the model [README](https://github.com/meta-llama/llama3). For more technical information about generation parameters and recipes for how to use Llama 3 in applications, please go [here](https://github.com/meta-llama/llama-recipes). \n\n\n## Intended Use\n\n**Intended Use Cases** Llama 3 is intended for commercial and research use in English. Instruction tuned models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks.\n\n**Out-of-scope** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3 Community License. Use in languages other than English**.\n\n**Note: Developers may fine-tune Llama 3 models for languages beyond English provided they comply with the Llama 3 Community License and the Acceptable Use Policy.\n\n## How to use\n\nThis repository contains two versions of Meta-Llama-3-8B-Instruct, for use with transformers and with the original `llama3` codebase.\n\n### Use with transformers\n\nYou can run conversational inference using the Transformers pipeline abstraction, or by leveraging the Auto classes with the `generate()` function. Let's see examples of both.\n\n#### Transformers pipeline\n\n```python\nimport transformers\nimport torch\n\nmodel_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model_id,\n    model_kwargs={\"torch_dtype\": torch.bfloat16},\n    device_map=\"auto\",\n)\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n    {\"role\": \"user\", \"content\": \"Who are you?\"},\n]\n\nterminators = [\n    pipeline.tokenizer.eos_token_id,\n    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n]\n\noutputs = pipeline(\n    messages,\n    max_new_tokens=256,\n    eos_token_id=terminators,\n    do_sample=True,\n    temperature=0.6,\n    top_p=0.9,\n)\nprint(outputs[0][\"generated_text\"][-1])\n```\n\n#### Transformers AutoModelForCausalLM\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\nmodel_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\",\n)\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n    {\"role\": \"user\", \"content\": \"Who are you?\"},\n]\n\ninput_ids = tokenizer.apply_chat_template(\n    messages,\n    add_generation_prompt=True,\n    return_tensors=\"pt\"\n).to(model.device)\n\nterminators = [\n    tokenizer.eos_token_id,\n    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n]\n\noutputs = model.generate(\n    input_ids,\n    max_new_tokens=256,\n    eos_token_id=terminators,\n    do_sample=True,\n    temperature=0.6,\n    top_p=0.9,\n)\nresponse = outputs[0][input_ids.shape[-1]:]\nprint(tokenizer.decode(response, skip_special_tokens=True))\n```\n\n\n### Use with `llama3`\n\nPlease, follow the instructions in the [repository](https://github.com/meta-llama/llama3)\n\nTo download Original checkpoints, see the example command below leveraging `huggingface-cli`:\n\n```\nhuggingface-cli download meta-llama/Meta-Llama-3-8B-Instruct --include \"original/*\" --local-dir Meta-Llama-3-8B-Instruct\n```\n\nFor Hugging Face support, we recommend using transformers or TGI, but a similar command works.\n\n## Hardware and Software\n\n**Training Factors** We used custom training libraries, Meta's Research SuperCluster, and production clusters for pretraining. Fine-tuning, annotation, and evaluation were also performed on third-party cloud compute.\n\n**Carbon Footprint Pretraining utilized a cumulative** 7.7M GPU hours of computation on hardware of type H100-80GB (TDP of 700W). Estimated total emissions were 2290 tCO2eq, 100% of which were offset by Meta’s sustainability program.\n\n\n<table>\n  <tr>\n   <td>\n   </td>\n   <td><strong>Time (GPU hours)</strong>\n   </td>\n   <td><strong>Power Consumption (W)</strong>\n   </td>\n   <td><strong>Carbon Emitted(tCO2eq)</strong>\n   </td>\n  </tr>\n  <tr>\n   <td>Llama 3 8B\n   </td>\n   <td>1.3M\n   </td>\n   <td>700\n   </td>\n   <td>390\n   </td>\n  </tr>\n  <tr>\n   <td>Llama 3 70B\n   </td>\n   <td>6.4M\n   </td>\n   <td>700\n   </td>\n   <td>1900\n   </td>\n  </tr>\n  <tr>\n   <td>Total\n   </td>\n   <td>7.7M\n   </td>\n   <td>\n   </td>\n   <td>2290\n   </td>\n  </tr>\n</table>\n\n\n\n**CO2 emissions during pre-training**. Time: total GPU time required for training each model. Power Consumption: peak power capacity per GPU device for the GPUs used adjusted for power usage efficiency. 100% of the emissions are directly offset by Meta's sustainability program, and because we are openly releasing these models, the pretraining costs do not need to be incurred by others.\n\n\n## Training Data\n\n**Overview** Llama 3 was pretrained on over 15 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over 10M human-annotated examples. Neither the pretraining nor the fine-tuning datasets include Meta user data.\n\n**Data Freshness** The pretraining data has a cutoff of March 2023 for the 8B and December 2023 for the 70B models respectively. \n\n\n## Benchmarks \n\nIn this section, we report the results for Llama 3 models on standard automatic benchmarks. For all the evaluations, we use our internal evaluations library. For details on the methodology see [here](https://github.com/meta-llama/llama3/blob/main/eval_methodology.md).\n\n\n### Base pretrained models\n\n\n<table>\n  <tr>\n   <td><strong>Category</strong>\n   </td>\n   <td><strong>Benchmark</strong>\n   </td>\n   <td><strong>Llama 3 8B</strong>\n   </td>\n   <td><strong>Llama2 7B</strong>\n   </td>\n   <td><strong>Llama2 13B</strong>\n   </td>\n   <td><strong>Llama 3 70B</strong>\n   </td>\n   <td><strong>Llama2 70B</strong>\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"6\" >General\n   </td>\n   <td>MMLU (5-shot)\n   </td>\n   <td>66.6\n   </td>\n   <td>45.7\n   </td>\n   <td>53.8\n   </td>\n   <td>79.5\n   </td>\n   <td>69.7\n   </td>\n  </tr>\n  <tr>\n   <td>AGIEval English (3-5 shot)\n   </td>\n   <td>45.9\n   </td>\n   <td>28.8\n   </td>\n   <td>38.7\n   </td>\n   <td>63.0\n   </td>\n   <td>54.8\n   </td>\n  </tr>\n  <tr>\n   <td>CommonSenseQA (7-shot)\n   </td>\n   <td>72.6\n   </td>\n   <td>57.6\n   </td>\n   <td>67.6\n   </td>\n   <td>83.8\n   </td>\n   <td>78.7\n   </td>\n  </tr>\n  <tr>\n   <td>Winogrande (5-shot)\n   </td>\n   <td>76.1\n   </td>\n   <td>73.3\n   </td>\n   <td>75.4\n   </td>\n   <td>83.1\n   </td>\n   <td>81.8\n   </td>\n  </tr>\n  <tr>\n   <td>BIG-Bench Hard (3-shot, CoT)\n   </td>\n   <td>61.1\n   </td>\n   <td>38.1\n   </td>\n   <td>47.0\n   </td>\n   <td>81.3\n   </td>\n   <td>65.7\n   </td>\n  </tr>\n  <tr>\n   <td>ARC-Challenge (25-shot)\n   </td>\n   <td>78.6\n   </td>\n   <td>53.7\n   </td>\n   <td>67.6\n   </td>\n   <td>93.0\n   </td>\n   <td>85.3\n   </td>\n  </tr>\n  <tr>\n   <td>Knowledge reasoning\n   </td>\n   <td>TriviaQA-Wiki (5-shot)\n   </td>\n   <td>78.5\n   </td>\n   <td>72.1\n   </td>\n   <td>79.6\n   </td>\n   <td>89.7\n   </td>\n   <td>87.5\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"4\" >Reading comprehension\n   </td>\n   <td>SQuAD (1-shot)\n   </td>\n   <td>76.4\n   </td>\n   <td>72.2\n   </td>\n   <td>72.1\n   </td>\n   <td>85.6\n   </td>\n   <td>82.6\n   </td>\n  </tr>\n  <tr>\n   <td>QuAC (1-shot, F1)\n   </td>\n   <td>44.4\n   </td>\n   <td>39.6\n   </td>\n   <td>44.9\n   </td>\n   <td>51.1\n   </td>\n   <td>49.4\n   </td>\n  </tr>\n  <tr>\n   <td>BoolQ (0-shot)\n   </td>\n   <td>75.7\n   </td>\n   <td>65.5\n   </td>\n   <td>66.9\n   </td>\n   <td>79.0\n   </td>\n   <td>73.1\n   </td>\n  </tr>\n  <tr>\n   <td>DROP (3-shot, F1)\n   </td>\n   <td>58.4\n   </td>\n   <td>37.9\n   </td>\n   <td>49.8\n   </td>\n   <td>79.7\n   </td>\n   <td>70.2\n   </td>\n  </tr>\n</table>\n\n\n\n### Instruction tuned models\n\n\n<table>\n  <tr>\n   <td><strong>Benchmark</strong>\n   </td>\n   <td><strong>Llama 3 8B</strong>\n   </td>\n   <td><strong>Llama 2 7B</strong>\n   </td>\n   <td><strong>Llama 2 13B</strong>\n   </td>\n   <td><strong>Llama 3 70B</strong>\n   </td>\n   <td><strong>Llama 2 70B</strong>\n   </td>\n  </tr>\n  <tr>\n   <td>MMLU (5-shot)\n   </td>\n   <td>68.4\n   </td>\n   <td>34.1\n   </td>\n   <td>47.8\n   </td>\n   <td>82.0\n   </td>\n   <td>52.9\n   </td>\n  </tr>\n  <tr>\n   <td>GPQA (0-shot)\n   </td>\n   <td>34.2\n   </td>\n   <td>21.7\n   </td>\n   <td>22.3\n   </td>\n   <td>39.5\n   </td>\n   <td>21.0\n   </td>\n  </tr>\n  <tr>\n   <td>HumanEval (0-shot)\n   </td>\n   <td>62.2\n   </td>\n   <td>7.9\n   </td>\n   <td>14.0\n   </td>\n   <td>81.7\n   </td>\n   <td>25.6\n   </td>\n  </tr>\n  <tr>\n   <td>GSM-8K (8-shot, CoT)\n   </td>\n   <td>79.6\n   </td>\n   <td>25.7\n   </td>\n   <td>77.4\n   </td>\n   <td>93.0\n   </td>\n   <td>57.5\n   </td>\n  </tr>\n  <tr>\n   <td>MATH (4-shot, CoT)\n   </td>\n   <td>30.0\n   </td>\n   <td>3.8\n   </td>\n   <td>6.7\n   </td>\n   <td>50.4\n   </td>\n   <td>11.6\n   </td>\n  </tr>\n</table>\n\n\n\n### Responsibility & Safety\n\nWe believe that an open approach to AI leads to better, safer products, faster innovation, and a bigger overall market. We are committed to Responsible AI development and took a series of steps to limit misuse and harm and support the open source community.\n\nFoundation models are widely capable technologies that are built to be used for a diverse range of applications. They are not designed to meet every developer preference on safety levels for all use cases, out-of-the-box, as those by their nature will differ across different applications. \n\nRather, responsible LLM-application deployment is achieved by implementing a series of safety best practices throughout the development of such applications, from the model pre-training, fine-tuning and the deployment of systems composed of safeguards to tailor the safety needs specifically to the use case and audience. \n\n\nAs part of the Llama 3 release, we updated our [Responsible Use Guide](https://llama.meta.com/responsible-use-guide/) to outline the steps and best practices for developers to implement model and system level safety for their application. We also provide a set of resources including [Meta Llama Guard 2](https://llama.meta.com/purple-llama/) and [Code Shield](https://llama.meta.com/purple-llama/) safeguards. These tools have proven to drastically reduce residual risks of LLM Systems, while maintaining a high level of helpfulness. We encourage developers to tune and deploy these safeguards according to their needs and we provide a [reference implementation](https://github.com/meta-llama/llama-recipes/tree/main/recipes/responsible_ai) to get you started.\n\n\n#### Llama 3-Instruct\n\nAs outlined in the Responsible Use Guide, some trade-off between model helpfulness and model alignment is likely unavoidable. Developers should exercise discretion about how to weigh the benefits of alignment and helpfulness for their specific use case and audience. Developers should be mindful of residual risks when using Llama models and leverage additional safety tools as needed to reach the right safety bar for their use case. \n\n<span style=\"text-decoration:underline;\">Safety</span>\n\nFor our instruction tuned model, we conducted extensive red teaming exercises, performed adversarial evaluations and implemented safety mitigations techniques to lower residual risks. As with any Large Language Model, residual risks will likely remain and we recommend that developers assess these risks in the context of their use case. In parallel, we are working with the community to make AI safety benchmark standards transparent, rigorous and interpretable. \n\n<span style=\"text-decoration:underline;\">Refusals</span>\n\nIn addition to residual risks, we put a great emphasis on model refusals to benign prompts. Over-refusing not only can impact the user experience but could even be harmful in certain contexts as well. We’ve heard the feedback from the developer community and improved our fine tuning to ensure that Llama 3 is significantly less likely to falsely refuse to answer prompts than Llama 2. \n\nWe built internal benchmarks and developed mitigations to limit false refusals making Llama 3 our most helpful model to date. \n\n\n#### Responsible release \n\nIn addition to responsible use considerations outlined above, we followed a rigorous process that requires us to take extra measures against misuse and critical risks before we make our release decision. \n\nMisuse\n\nIf you access or use Llama 3, you agree to the Acceptable Use Policy. The most recent copy of this policy can be found at [https://llama.meta.com/llama3/use-policy/](https://llama.meta.com/llama3/use-policy/).\n\n\n#### Critical risks \n\n<span style=\"text-decoration:underline;\">CBRNE</span> (Chemical, Biological, Radiological, Nuclear, and high yield Explosives)\n\nWe have conducted a two fold assessment of the safety of the model in this area:\n\n\n\n* Iterative testing during model training to assess the safety of responses related to CBRNE threats and other adversarial risks.\n* Involving external CBRNE experts to conduct an uplift test assessing the ability of the model to accurately provide expert knowledge and reduce barriers to potential CBRNE misuse, by reference to what can be achieved using web search (without the model).\n\n\n### <span style=\"text-decoration:underline;\">Cyber Security </span>\n\nWe have evaluated Llama 3 with CyberSecEval, Meta’s cybersecurity safety eval suite, measuring Llama 3’s propensity to suggest insecure code when used as a coding assistant, and Llama 3’s propensity to comply with requests to help carry out cyber attacks, where attacks are defined by the industry standard MITRE ATT&CK cyber attack ontology. On our insecure coding and cyber attacker helpfulness tests, Llama 3 behaved in the same range or safer than models of [equivalent coding capability](https://huggingface.co/spaces/facebook/CyberSecEval). \n\n\n### <span style=\"text-decoration:underline;\">Child Safety</span>\n\nChild Safety risk assessments were conducted using a team of experts, to assess the model’s capability to produce outputs that could result in Child Safety risks and inform on any necessary and appropriate risk mitigations via fine tuning. We leveraged those expert red teaming sessions to expand the coverage of our evaluation benchmarks through Llama 3 model development.  For Llama 3, we conducted new in-depth sessions using objective based methodologies to assess the model risks along multiple attack vectors. We also partnered with content specialists to perform red teaming exercises assessing potentially violating content while taking account of market specific nuances or experiences. \n\n\n### Community \n\nGenerative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership in AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Purple Llama tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to our [Github repository](https://github.com/meta-llama/PurpleLlama). \n\nFinally, we put in place a set of resources including an [output reporting mechanism](https://developers.facebook.com/llama_output_feedback) and [bug bounty program](https://www.facebook.com/whitehat) to continuously improve the Llama technology with the help of the community. \n\n\n## Ethical Considerations and Limitations\n\nThe core values of Llama 3 are openness, inclusivity and helpfulness. It is meant to serve everyone, and to work for a wide range of use cases. It is thus designed to be accessible to people across many different backgrounds, experiences and perspectives. Llama 3 addresses users and their needs as they are, without insertion unnecessary judgment or normativity, while reflecting the understanding that even content that may appear problematic in some cases can serve valuable purposes in others. It respects the dignity and autonomy of all users, especially in terms of the values of free thought and expression that power innovation and progress. \n\nBut Llama 3 is a new technology, and like any new technology, there are risks associated with its use. Testing conducted to date has been in English, and has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, Llama 3’s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 3 models, developers should perform safety testing and tuning tailored to their specific applications of the model. As outlined in the Responsible Use Guide, we recommend incorporating [Purple Llama](https://github.com/facebookresearch/PurpleLlama) solutions into your workflows and specifically [Llama Guard](https://ai.meta.com/research/publications/llama-guard-llm-based-input-output-safeguard-for-human-ai-conversations/) which provides a base model to filter input and output prompts to layer system-level safety on top of model-level safety. \n\nPlease see the Responsible Use Guide available at [http://llama.meta.com/responsible-use-guide](http://llama.meta.com/responsible-use-guide)\n\n\n## Citation instructions\n\n@article{llama3modelcard,\n\n  title={Llama 3 Model Card},\n\n  author={AI@Meta},\n\n  year={2024},\n\n  url = {https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md}\n\n}\n\n## Contributors\n\nAaditya Singh; Aaron Grattafiori; Abhimanyu Dubey; Abhinav Jauhri; Abhinav Pandey; Abhishek Kadian; Adam Kelsey; Adi Gangidi; Ahmad Al-Dahle; Ahuva Goldstand; Aiesha Letman; Ajay Menon; Akhil Mathur; Alan Schelten; Alex Vaughan; Amy Yang; Andrei Lupu; Andres Alvarado; Andrew Gallagher; Andrew Gu; Andrew Ho; Andrew Poulton; Andrew Ryan; Angela Fan; Ankit Ramchandani; Anthony Hartshorn; Archi Mitra; Archie Sravankumar; Artem Korenev; Arun Rao; Ashley Gabriel; Ashwin Bharambe; Assaf Eisenman; Aston Zhang; Aurelien Rodriguez; Austen Gregerson; Ava Spataru; Baptiste Roziere; Ben Maurer; Benjamin Leonhardi; Bernie Huang; Bhargavi Paranjape; Bing Liu; Binh Tang; Bobbie Chern; Brani Stojkovic; Brian Fuller; Catalina Mejia Arenas; Chao Zhou; Charlotte Caucheteux; Chaya Nayak; Ching-Hsiang Chu; Chloe Bi; Chris Cai; Chris Cox; Chris Marra; Chris McConnell; Christian Keller; Christoph Feichtenhofer; Christophe Touret; Chunyang Wu; Corinne Wong; Cristian Canton Ferrer; Damien Allonsius; Daniel Kreymer; Daniel Haziza; Daniel Li; Danielle Pintz; Danny Livshits; Danny Wyatt; David Adkins; David Esiobu; David Xu; Davide Testuggine; Delia David; Devi Parikh; Dhruv Choudhary; Dhruv Mahajan; Diana Liskovich; Diego Garcia-Olano; Diego Perino; Dieuwke Hupkes; Dingkang Wang; Dustin Holland; Egor Lakomkin; Elina Lobanova; Xiaoqing Ellen Tan; Emily Dinan; Eric Smith; Erik Brinkman; Esteban Arcaute; Filip Radenovic; Firat Ozgenel; Francesco Caggioni; Frank Seide; Frank Zhang; Gabriel Synnaeve; Gabriella Schwarz; Gabrielle Lee; Gada Badeer; Georgia Anderson; Graeme Nail; Gregoire Mialon; Guan Pang; Guillem Cucurell; Hailey Nguyen; Hannah Korevaar; Hannah Wang; Haroun Habeeb; Harrison Rudolph; Henry Aspegren; Hu Xu; Hugo Touvron; Iga Kozlowska; Igor Molybog; Igor Tufanov; Iliyan Zarov; Imanol Arrieta Ibarra; Irina-Elena Veliche; Isabel Kloumann; Ishan Misra; Ivan Evtimov; Jacob Xu; Jade Copet; Jake Weissman; Jan Geffert; Jana Vranes; Japhet Asher; Jason Park; Jay Mahadeokar; Jean-Baptiste Gaya; Jeet Shah; Jelmer van der Linde; Jennifer Chan; Jenny Hong; Jenya Lee; Jeremy Fu; Jeremy Teboul; Jianfeng Chi; Jianyu Huang; Jie Wang; Jiecao Yu; Joanna Bitton; Joe Spisak; Joelle Pineau; Jon Carvill; Jongsoo Park; Joseph Rocca; Joshua Johnstun; Junteng Jia; Kalyan Vasuden Alwala; Kam Hou U; Kate Plawiak; Kartikeya Upasani; Kaushik Veeraraghavan; Ke Li; Kenneth Heafield; Kevin Stone; Khalid El-Arini; Krithika Iyer; Kshitiz Malik; Kuenley Chiu; Kunal Bhalla; Kyle Huang; Lakshya Garg; Lauren Rantala-Yeary; Laurens van der Maaten; Lawrence Chen; Leandro Silva; Lee Bell; Lei Zhang; Liang Tan; Louis Martin; Lovish Madaan; Luca Wehrstedt; Lukas Blecher; Luke de Oliveira; Madeline Muzzi; Madian Khabsa; Manav Avlani; Mannat Singh; Manohar Paluri; Mark Zuckerberg; Marcin Kardas; Martynas Mankus; Mathew Oldham; Mathieu Rita; Matthew Lennie; Maya Pavlova; Meghan Keneally; Melanie Kambadur; Mihir Patel; Mikayel Samvelyan; Mike Clark; Mike Lewis; Min Si; Mitesh Kumar Singh; Mo Metanat; Mona Hassan; Naman Goyal; Narjes Torabi; Nicolas Usunier; Nikolay Bashlykov; Nikolay Bogoychev; Niladri Chatterji; Ning Dong; Oliver Aobo Yang; Olivier Duchenne; Onur Celebi; Parth Parekh; Patrick Alrassy; Paul Saab; Pavan Balaji; Pedro Rittner; Pengchuan Zhang; Pengwei Li; Petar Vasic; Peter Weng; Polina Zvyagina; Prajjwal Bhargava; Pratik Dubal; Praveen Krishnan; Punit Singh Koura; Qing He; Rachel Rodriguez; Ragavan Srinivasan; Rahul Mitra; Ramon Calderer; Raymond Li; Robert Stojnic; Roberta Raileanu; Robin Battey; Rocky Wang; Rohit Girdhar; Rohit Patel; Romain Sauvestre; Ronnie Polidoro; Roshan Sumbaly; Ross Taylor; Ruan Silva; Rui Hou; Rui Wang; Russ Howes; Ruty Rinott; Saghar Hosseini; Sai Jayesh Bondu; Samyak Datta; Sanjay Singh; Sara Chugh; Sargun Dhillon; Satadru Pan; Sean Bell; Sergey Edunov; Shaoliang Nie; Sharan Narang; Sharath Raparthy; Shaun Lindsay; Sheng Feng; Sheng Shen; Shenghao Lin; Shiva Shankar; Shruti Bhosale; Shun Zhang; Simon Vandenhende; Sinong Wang; Seohyun Sonia Kim; Soumya Batra; Sten Sootla; Steve Kehoe; Suchin Gururangan; Sumit Gupta; Sunny Virk; Sydney Borodinsky; Tamar Glaser; Tamar Herman; Tamara Best; Tara Fowler; Thomas Georgiou; Thomas Scialom; Tianhe Li; Todor Mihaylov; Tong Xiao; Ujjwal Karn; Vedanuj Goswami; Vibhor Gupta; Vignesh Ramanathan; Viktor Kerkez; Vinay Satish Kumar; Vincent Gonguet; Vish Vogeti; Vlad Poenaru; Vlad Tiberiu Mihailescu; Vladan Petrovic; Vladimir Ivanov; Wei Li; Weiwei Chu; Wenhan Xiong; Wenyin Fu; Wes Bouaziz; Whitney Meers; Will Constable; Xavier Martinet; Xiaojian Wu; Xinbo Gao; Xinfeng Xie; Xuchao Jia; Yaelle Goldschlag; Yann LeCun; Yashesh Gaur; Yasmine Babaei; Ye Qi; Yenda Li; Yi Wen; Yiwen Song; Youngjin Nam; Yuchen Hao; Yuchen Zhang; Yun Wang; Yuning Mao; Yuzi He; Zacharie Delpierre Coudert; Zachary DeVito; Zahra Hankir; Zhaoduo Wen; Zheng Yan; Zhengxing Chen; Zhenyu Yang; Zoe Papakipos\n",
    "card_content": "---\nlanguage:\n- en\npipeline_tag: text-generation\ntags:\n- facebook\n- meta\n- pytorch\n- llama\n- llama-3\nlicense: llama3\nnew_version: meta-llama/Llama-3.1-8B-Instruct\nextra_gated_prompt: \"### META LLAMA 3 COMMUNITY LICENSE AGREEMENT\\nMeta Llama 3 Version\\\n  \\ Release Date: April 18, 2024\\n\\\"Agreement\\\" means the terms and conditions for\\\n  \\ use, reproduction, distribution and modification of the Llama Materials set forth\\\n  \\ herein.\\n\\\"Documentation\\\" means the specifications, manuals and documentation\\\n  \\ accompanying Meta Llama 3 distributed by Meta at https://llama.meta.com/get-started/.\\n\\\n  \\\"Licensee\\\" or \\\"you\\\" means you, or your employer or any other person or entity\\\n  \\ (if you are entering into this Agreement on such person or entity’s behalf), of\\\n  \\ the age required under applicable laws, rules or regulations to provide legal\\\n  \\ consent and that has legal authority to bind your employer or such other person\\\n  \\ or entity if you are entering in this Agreement on their behalf.\\n\\\"Meta Llama\\\n  \\ 3\\\" means the foundational large language models and software and algorithms,\\\n  \\ including machine-learning model code, trained model weights, inference-enabling\\\n  \\ code, training-enabling code, fine-tuning enabling code and other elements of\\\n  \\ the foregoing distributed by Meta at https://llama.meta.com/llama-downloads.\\n\\\n  \\\"Llama Materials\\\" means, collectively, Meta’s proprietary Meta Llama 3 and Documentation\\\n  \\ (and any portion thereof) made available under this Agreement.\\n\\\"Meta\\\" or \\\"\\\n  we\\\" means Meta Platforms Ireland Limited (if you are located in or, if you are\\\n  \\ an entity, your principal place of business is in the EEA or Switzerland) and\\\n  \\ Meta Platforms, Inc. (if you are located outside of the EEA or Switzerland).\\n\\\n  \\   \\n1. License Rights and Redistribution.\\na. Grant of Rights. You are granted\\\n  \\ a non-exclusive, worldwide, non-transferable and royalty-free limited license\\\n  \\ under Meta’s intellectual property or other rights owned by Meta embodied in the\\\n  \\ Llama Materials to use, reproduce, distribute, copy, create derivative works of,\\\n  \\ and make modifications to the Llama Materials.\\nb. Redistribution and Use.\\ni.\\\n  \\ If you distribute or make available the Llama Materials (or any derivative works\\\n  \\ thereof), or a product or service that uses any of them, including another AI\\\n  \\ model, you shall (A) provide a copy of this Agreement with any such Llama Materials;\\\n  \\ and (B) prominently display “Built with Meta Llama 3” on a related website, user\\\n  \\ interface, blogpost, about page, or product documentation. If you use the Llama\\\n  \\ Materials to create, train, fine tune, or otherwise improve an AI model, which\\\n  \\ is distributed or made available, you shall also include “Llama 3” at the beginning\\\n  \\ of any such AI model name.\\nii. If you receive Llama Materials, or any derivative\\\n  \\ works thereof, from a Licensee as part  of an integrated end user product, then\\\n  \\ Section 2 of this Agreement will not apply to you.\\niii. You must retain in all\\\n  \\ copies of the Llama Materials that you distribute the following attribution notice\\\n  \\ within a “Notice” text file distributed as a part of such copies: “Meta Llama\\\n  \\ 3 is licensed under the Meta Llama 3 Community License, Copyright © Meta Platforms,\\\n  \\ Inc. All Rights Reserved.”\\niv. Your use of the Llama Materials must comply with\\\n  \\ applicable laws and regulations (including trade compliance laws and regulations)\\\n  \\ and adhere to the Acceptable Use Policy for the Llama Materials (available at\\\n  \\ https://llama.meta.com/llama3/use-policy), which is hereby incorporated by reference\\\n  \\ into this Agreement.\\nv. You will not use the Llama Materials or any output or\\\n  \\ results of the Llama Materials to improve any other large language model (excluding\\\n  \\ Meta Llama 3 or derivative works thereof).\\n2. Additional Commercial Terms. If,\\\n  \\ on the Meta Llama 3 version release date, the monthly active users of the products\\\n  \\ or services made available by or for Licensee, or Licensee’s affiliates, is greater\\\n  \\ than 700 million monthly active users in the preceding calendar month, you must\\\n  \\ request a license from Meta, which Meta may grant to you in its sole discretion,\\\n  \\ and you are not authorized to exercise any of the rights under this Agreement\\\n  \\ unless or until Meta otherwise expressly grants you such rights.\\n3. Disclaimer\\\n  \\ of Warranty. UNLESS REQUIRED BY APPLICABLE LAW, THE LLAMA MATERIALS AND ANY OUTPUT\\\n  \\ AND RESULTS THEREFROM ARE PROVIDED ON AN “AS IS” BASIS, WITHOUT WARRANTIES OF\\\n  \\ ANY KIND, AND META DISCLAIMS ALL WARRANTIES OF ANY KIND, BOTH EXPRESS AND IMPLIED,\\\n  \\ INCLUDING, WITHOUT LIMITATION, ANY WARRANTIES OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY,\\\n  \\ OR FITNESS FOR A PARTICULAR PURPOSE. YOU ARE SOLELY RESPONSIBLE FOR DETERMINING\\\n  \\ THE APPROPRIATENESS OF USING OR REDISTRIBUTING THE LLAMA MATERIALS AND ASSUME\\\n  \\ ANY RISKS ASSOCIATED WITH YOUR USE OF THE LLAMA MATERIALS AND ANY OUTPUT AND RESULTS.\\n\\\n  4. Limitation of Liability. IN NO EVENT WILL META OR ITS AFFILIATES BE LIABLE UNDER\\\n  \\ ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, TORT, NEGLIGENCE, PRODUCTS LIABILITY,\\\n  \\ OR OTHERWISE, ARISING OUT OF THIS AGREEMENT, FOR ANY LOST PROFITS OR ANY INDIRECT,\\\n  \\ SPECIAL, CONSEQUENTIAL, INCIDENTAL, EXEMPLARY OR PUNITIVE DAMAGES, EVEN IF META\\\n  \\ OR ITS AFFILIATES HAVE BEEN ADVISED OF THE POSSIBILITY OF ANY OF THE FOREGOING.\\n\\\n  5. Intellectual Property.\\na. No trademark licenses are granted under this Agreement,\\\n  \\ and in connection with the Llama Materials, neither Meta nor Licensee may use\\\n  \\ any name or mark owned by or associated with the other or any of its affiliates,\\\n  \\ except as required for reasonable and customary use in describing and redistributing\\\n  \\ the Llama Materials or as set forth in this Section 5(a). Meta hereby grants you\\\n  \\ a license to use “Llama 3” (the “Mark”) solely as required to comply with the\\\n  \\ last sentence of Section 1.b.i. You will comply with Meta’s brand guidelines (currently\\\n  \\ accessible at https://about.meta.com/brand/resources/meta/company-brand/ ). All\\\n  \\ goodwill arising out of your use of the Mark will inure to the benefit of Meta.\\n\\\n  b. Subject to Meta’s ownership of Llama Materials and derivatives made by or for\\\n  \\ Meta, with respect to any derivative works and modifications of the Llama Materials\\\n  \\ that are made by you, as between you and Meta, you are and will be the owner of\\\n  \\ such derivative works and modifications.\\nc. If you institute litigation or other\\\n  \\ proceedings against Meta or any entity (including a cross-claim or counterclaim\\\n  \\ in a lawsuit) alleging that the Llama Materials or Meta Llama 3 outputs or results,\\\n  \\ or any portion of any of the foregoing, constitutes infringement of intellectual\\\n  \\ property or other rights owned or licensable by you, then any licenses granted\\\n  \\ to you under this Agreement shall terminate as of the date such litigation or\\\n  \\ claim is filed or instituted. You will indemnify and hold harmless Meta from and\\\n  \\ against any claim by any third party arising out of or related to your use or\\\n  \\ distribution of the Llama Materials.\\n6. Term and Termination. The term of this\\\n  \\ Agreement will commence upon your acceptance of this Agreement or access to the\\\n  \\ Llama Materials and will continue in full force and effect until terminated in\\\n  \\ accordance with the terms and conditions herein. Meta may terminate this Agreement\\\n  \\ if you are in breach of any term or condition of this Agreement. Upon termination\\\n  \\ of this Agreement, you shall delete and cease use of the Llama Materials. Sections\\\n  \\ 3, 4 and 7 shall survive the termination of this Agreement.\\n7. Governing Law\\\n  \\ and Jurisdiction. This Agreement will be governed and construed under the laws\\\n  \\ of the State of California without regard to choice of law principles, and the\\\n  \\ UN Convention on Contracts for the International Sale of Goods does not apply\\\n  \\ to this Agreement. The courts of California shall have exclusive jurisdiction\\\n  \\ of any dispute arising out of this Agreement.\\n### Meta Llama 3 Acceptable Use\\\n  \\ Policy\\nMeta is committed to promoting safe and fair use of its tools and features,\\\n  \\ including Meta Llama 3. If you access or use Meta Llama 3, you agree to this Acceptable\\\n  \\ Use Policy (“Policy”). The most recent copy of this policy can be found at [https://llama.meta.com/llama3/use-policy](https://llama.meta.com/llama3/use-policy)\\n\\\n  #### Prohibited Uses\\nWe want everyone to use Meta Llama 3 safely and responsibly.\\\n  \\ You agree you will not use, or allow others to use, Meta Llama 3 to: 1. Violate\\\n  \\ the law or others’ rights, including to:\\n    1. Engage in, promote, generate,\\\n  \\ contribute to, encourage, plan, incite, or further illegal or unlawful activity\\\n  \\ or content, such as:\\n        1. Violence or terrorism\\n        2. Exploitation\\\n  \\ or harm to children, including the solicitation, creation, acquisition, or dissemination\\\n  \\ of child exploitative content or failure to report Child Sexual Abuse Material\\n\\\n  \\        3. Human trafficking, exploitation, and sexual violence\\n        4. The\\\n  \\ illegal distribution of information or materials to minors, including obscene\\\n  \\ materials, or failure to employ legally required age-gating in connection with\\\n  \\ such information or materials.\\n        5. Sexual solicitation\\n        6. Any\\\n  \\ other criminal activity\\n    2. Engage in, promote, incite, or facilitate the\\\n  \\ harassment, abuse, threatening, or bullying of individuals or groups of individuals\\n\\\n  \\    3. Engage in, promote, incite, or facilitate discrimination or other unlawful\\\n  \\ or harmful conduct in the provision of employment, employment benefits, credit,\\\n  \\ housing, other economic benefits, or other essential goods and services\\n    4.\\\n  \\ Engage in the unauthorized or unlicensed practice of any profession including,\\\n  \\ but not limited to, financial, legal, medical/health, or related professional\\\n  \\ practices\\n    5. Collect, process, disclose, generate, or infer health, demographic,\\\n  \\ or other sensitive personal or private information about individuals without rights\\\n  \\ and consents required by applicable laws\\n    6. Engage in or facilitate any action\\\n  \\ or generate any content that infringes, misappropriates, or otherwise violates\\\n  \\ any third-party rights, including the outputs or results of any products or services\\\n  \\ using the Llama Materials\\n    7. Create, generate, or facilitate the creation\\\n  \\ of malicious code, malware, computer viruses or do anything else that could disable,\\\n  \\ overburden, interfere with or impair the proper working, integrity, operation\\\n  \\ or appearance of a website or computer system\\n2. Engage in, promote, incite,\\\n  \\ facilitate, or assist in the planning or development of activities that present\\\n  \\ a risk of death or bodily harm to individuals, including use of Meta Llama 3 related\\\n  \\ to the following:\\n    1. Military, warfare, nuclear industries or applications,\\\n  \\ espionage, use for materials or activities that are subject to the International\\\n  \\ Traffic Arms Regulations (ITAR) maintained by the United States Department of\\\n  \\ State\\n    2. Guns and illegal weapons (including weapon development)\\n    3.\\\n  \\ Illegal drugs and regulated/controlled substances\\n    4. Operation of critical\\\n  \\ infrastructure, transportation technologies, or heavy machinery\\n    5. Self-harm\\\n  \\ or harm to others, including suicide, cutting, and eating disorders\\n    6. Any\\\n  \\ content intended to incite or promote violence, abuse, or any infliction of bodily\\\n  \\ harm to an individual\\n3. Intentionally deceive or mislead others, including use\\\n  \\ of Meta Llama 3 related to the following:\\n    1. Generating, promoting, or furthering\\\n  \\ fraud or the creation or promotion of disinformation\\n    2. Generating, promoting,\\\n  \\ or furthering defamatory content, including the creation of defamatory statements,\\\n  \\ images, or other content\\n    3. Generating, promoting, or further distributing\\\n  \\ spam\\n    4. Impersonating another individual without consent, authorization,\\\n  \\ or legal right\\n    5. Representing that the use of Meta Llama 3 or outputs are\\\n  \\ human-generated\\n    6. Generating or facilitating false online engagement, including\\\n  \\ fake reviews and other means of fake online engagement\\n4. Fail to appropriately\\\n  \\ disclose to end users any known dangers of your AI system\\nPlease report any violation\\\n  \\ of this Policy, software “bug,” or other problems that could lead to a violation\\\n  \\ of this Policy through one of the following means:\\n    * Reporting issues with\\\n  \\ the model: [https://github.com/meta-llama/llama3](https://github.com/meta-llama/llama3)\\n\\\n  \\    * Reporting risky content generated by the model:\\n    developers.facebook.com/llama_output_feedback\\n\\\n  \\    * Reporting bugs and security concerns: facebook.com/whitehat/info\\n    * Reporting\\\n  \\ violations of the Acceptable Use Policy or unlicensed uses of Meta Llama 3: LlamaUseReport@meta.com\"\nextra_gated_fields:\n  First Name: text\n  Last Name: text\n  Date of birth: date_picker\n  Country: country\n  Affiliation: text\n  geo: ip_location\n  ? By clicking Submit below I accept the terms of the license and acknowledge that\n    the information I provide will be collected stored processed and shared in accordance\n    with the Meta Privacy Policy\n  : checkbox\nextra_gated_description: The information you provide will be collected, stored, processed\n  and shared in accordance with the [Meta Privacy Policy](https://www.facebook.com/privacy/policy/).\nextra_gated_button_content: Submit\nwidget:\n- example_title: Hello\n  messages:\n  - role: user\n    content: Hey my name is Julien! How are you?\n- example_title: Winter holidays\n  messages:\n  - role: system\n    content: You are a helpful and honest assistant. Please, respond concisely and\n      truthfully.\n  - role: user\n    content: Can you recommend a good destination for Winter holidays?\n- example_title: Programming assistant\n  messages:\n  - role: system\n    content: You are a helpful and honest code and programming assistant. Please,\n      respond concisely and truthfully.\n  - role: user\n    content: Write a function that computes the nth fibonacci number.\ninference:\n  parameters:\n    max_new_tokens: 300\n    stop:\n    - <|end_of_text|>\n    - <|eot_id|>\n---\n\n## Model Details\n\nMeta developed and released the Meta Llama 3 family of large language models (LLMs), a collection of pretrained and instruction tuned generative text models in 8 and 70B sizes. The Llama 3 instruction tuned models are optimized for dialogue use cases and outperform many of the available open source chat models on common industry benchmarks. Further, in developing these models, we took great care to optimize helpfulness and safety. \n\n**Model developers** Meta\n\n**Variations** Llama 3 comes in two sizes — 8B and 70B parameters — in pre-trained and instruction tuned variants.\n\n**Input** Models input text only.\n\n**Output** Models generate text and code only.\n\n**Model Architecture** Llama 3 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.\n\n\n<table>\n  <tr>\n   <td>\n   </td>\n   <td><strong>Training Data</strong>\n   </td>\n   <td><strong>Params</strong>\n   </td>\n   <td><strong>Context length</strong>\n   </td>\n   <td><strong>GQA</strong>\n   </td>\n   <td><strong>Token count</strong>\n   </td>\n   <td><strong>Knowledge cutoff</strong>\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"2\" >Llama 3\n   </td>\n   <td rowspan=\"2\" >A new mix of publicly available online data.\n   </td>\n   <td>8B\n   </td>\n   <td>8k\n   </td>\n   <td>Yes\n   </td>\n   <td rowspan=\"2\" >15T+\n   </td>\n   <td>March, 2023\n   </td>\n  </tr>\n  <tr>\n   <td>70B\n   </td>\n   <td>8k\n   </td>\n   <td>Yes\n   </td>\n   <td>December, 2023\n   </td>\n  </tr>\n</table>\n\n\n**Llama 3 family of models**. Token counts refer to pretraining data only. Both the 8 and 70B versions use Grouped-Query Attention (GQA) for improved inference scalability.\n\n**Model Release Date** April 18, 2024.\n\n**Status** This is a static model trained on an offline dataset. Future versions of the tuned models will be released as we improve model safety with community feedback.\n\n**License** A custom commercial license is available at: [https://llama.meta.com/llama3/license](https://llama.meta.com/llama3/license)\n\nWhere to send questions or comments about the model Instructions on how to provide feedback or comments on the model can be found in the model [README](https://github.com/meta-llama/llama3). For more technical information about generation parameters and recipes for how to use Llama 3 in applications, please go [here](https://github.com/meta-llama/llama-recipes). \n\n\n## Intended Use\n\n**Intended Use Cases** Llama 3 is intended for commercial and research use in English. Instruction tuned models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks.\n\n**Out-of-scope** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3 Community License. Use in languages other than English**.\n\n**Note: Developers may fine-tune Llama 3 models for languages beyond English provided they comply with the Llama 3 Community License and the Acceptable Use Policy.\n\n## How to use\n\nThis repository contains two versions of Meta-Llama-3-8B-Instruct, for use with transformers and with the original `llama3` codebase.\n\n### Use with transformers\n\nYou can run conversational inference using the Transformers pipeline abstraction, or by leveraging the Auto classes with the `generate()` function. Let's see examples of both.\n\n#### Transformers pipeline\n\n```python\nimport transformers\nimport torch\n\nmodel_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model_id,\n    model_kwargs={\"torch_dtype\": torch.bfloat16},\n    device_map=\"auto\",\n)\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n    {\"role\": \"user\", \"content\": \"Who are you?\"},\n]\n\nterminators = [\n    pipeline.tokenizer.eos_token_id,\n    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n]\n\noutputs = pipeline(\n    messages,\n    max_new_tokens=256,\n    eos_token_id=terminators,\n    do_sample=True,\n    temperature=0.6,\n    top_p=0.9,\n)\nprint(outputs[0][\"generated_text\"][-1])\n```\n\n#### Transformers AutoModelForCausalLM\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\nmodel_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\",\n)\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n    {\"role\": \"user\", \"content\": \"Who are you?\"},\n]\n\ninput_ids = tokenizer.apply_chat_template(\n    messages,\n    add_generation_prompt=True,\n    return_tensors=\"pt\"\n).to(model.device)\n\nterminators = [\n    tokenizer.eos_token_id,\n    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n]\n\noutputs = model.generate(\n    input_ids,\n    max_new_tokens=256,\n    eos_token_id=terminators,\n    do_sample=True,\n    temperature=0.6,\n    top_p=0.9,\n)\nresponse = outputs[0][input_ids.shape[-1]:]\nprint(tokenizer.decode(response, skip_special_tokens=True))\n```\n\n\n### Use with `llama3`\n\nPlease, follow the instructions in the [repository](https://github.com/meta-llama/llama3)\n\nTo download Original checkpoints, see the example command below leveraging `huggingface-cli`:\n\n```\nhuggingface-cli download meta-llama/Meta-Llama-3-8B-Instruct --include \"original/*\" --local-dir Meta-Llama-3-8B-Instruct\n```\n\nFor Hugging Face support, we recommend using transformers or TGI, but a similar command works.\n\n## Hardware and Software\n\n**Training Factors** We used custom training libraries, Meta's Research SuperCluster, and production clusters for pretraining. Fine-tuning, annotation, and evaluation were also performed on third-party cloud compute.\n\n**Carbon Footprint Pretraining utilized a cumulative** 7.7M GPU hours of computation on hardware of type H100-80GB (TDP of 700W). Estimated total emissions were 2290 tCO2eq, 100% of which were offset by Meta’s sustainability program.\n\n\n<table>\n  <tr>\n   <td>\n   </td>\n   <td><strong>Time (GPU hours)</strong>\n   </td>\n   <td><strong>Power Consumption (W)</strong>\n   </td>\n   <td><strong>Carbon Emitted(tCO2eq)</strong>\n   </td>\n  </tr>\n  <tr>\n   <td>Llama 3 8B\n   </td>\n   <td>1.3M\n   </td>\n   <td>700\n   </td>\n   <td>390\n   </td>\n  </tr>\n  <tr>\n   <td>Llama 3 70B\n   </td>\n   <td>6.4M\n   </td>\n   <td>700\n   </td>\n   <td>1900\n   </td>\n  </tr>\n  <tr>\n   <td>Total\n   </td>\n   <td>7.7M\n   </td>\n   <td>\n   </td>\n   <td>2290\n   </td>\n  </tr>\n</table>\n\n\n\n**CO2 emissions during pre-training**. Time: total GPU time required for training each model. Power Consumption: peak power capacity per GPU device for the GPUs used adjusted for power usage efficiency. 100% of the emissions are directly offset by Meta's sustainability program, and because we are openly releasing these models, the pretraining costs do not need to be incurred by others.\n\n\n## Training Data\n\n**Overview** Llama 3 was pretrained on over 15 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over 10M human-annotated examples. Neither the pretraining nor the fine-tuning datasets include Meta user data.\n\n**Data Freshness** The pretraining data has a cutoff of March 2023 for the 8B and December 2023 for the 70B models respectively. \n\n\n## Benchmarks \n\nIn this section, we report the results for Llama 3 models on standard automatic benchmarks. For all the evaluations, we use our internal evaluations library. For details on the methodology see [here](https://github.com/meta-llama/llama3/blob/main/eval_methodology.md).\n\n\n### Base pretrained models\n\n\n<table>\n  <tr>\n   <td><strong>Category</strong>\n   </td>\n   <td><strong>Benchmark</strong>\n   </td>\n   <td><strong>Llama 3 8B</strong>\n   </td>\n   <td><strong>Llama2 7B</strong>\n   </td>\n   <td><strong>Llama2 13B</strong>\n   </td>\n   <td><strong>Llama 3 70B</strong>\n   </td>\n   <td><strong>Llama2 70B</strong>\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"6\" >General\n   </td>\n   <td>MMLU (5-shot)\n   </td>\n   <td>66.6\n   </td>\n   <td>45.7\n   </td>\n   <td>53.8\n   </td>\n   <td>79.5\n   </td>\n   <td>69.7\n   </td>\n  </tr>\n  <tr>\n   <td>AGIEval English (3-5 shot)\n   </td>\n   <td>45.9\n   </td>\n   <td>28.8\n   </td>\n   <td>38.7\n   </td>\n   <td>63.0\n   </td>\n   <td>54.8\n   </td>\n  </tr>\n  <tr>\n   <td>CommonSenseQA (7-shot)\n   </td>\n   <td>72.6\n   </td>\n   <td>57.6\n   </td>\n   <td>67.6\n   </td>\n   <td>83.8\n   </td>\n   <td>78.7\n   </td>\n  </tr>\n  <tr>\n   <td>Winogrande (5-shot)\n   </td>\n   <td>76.1\n   </td>\n   <td>73.3\n   </td>\n   <td>75.4\n   </td>\n   <td>83.1\n   </td>\n   <td>81.8\n   </td>\n  </tr>\n  <tr>\n   <td>BIG-Bench Hard (3-shot, CoT)\n   </td>\n   <td>61.1\n   </td>\n   <td>38.1\n   </td>\n   <td>47.0\n   </td>\n   <td>81.3\n   </td>\n   <td>65.7\n   </td>\n  </tr>\n  <tr>\n   <td>ARC-Challenge (25-shot)\n   </td>\n   <td>78.6\n   </td>\n   <td>53.7\n   </td>\n   <td>67.6\n   </td>\n   <td>93.0\n   </td>\n   <td>85.3\n   </td>\n  </tr>\n  <tr>\n   <td>Knowledge reasoning\n   </td>\n   <td>TriviaQA-Wiki (5-shot)\n   </td>\n   <td>78.5\n   </td>\n   <td>72.1\n   </td>\n   <td>79.6\n   </td>\n   <td>89.7\n   </td>\n   <td>87.5\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"4\" >Reading comprehension\n   </td>\n   <td>SQuAD (1-shot)\n   </td>\n   <td>76.4\n   </td>\n   <td>72.2\n   </td>\n   <td>72.1\n   </td>\n   <td>85.6\n   </td>\n   <td>82.6\n   </td>\n  </tr>\n  <tr>\n   <td>QuAC (1-shot, F1)\n   </td>\n   <td>44.4\n   </td>\n   <td>39.6\n   </td>\n   <td>44.9\n   </td>\n   <td>51.1\n   </td>\n   <td>49.4\n   </td>\n  </tr>\n  <tr>\n   <td>BoolQ (0-shot)\n   </td>\n   <td>75.7\n   </td>\n   <td>65.5\n   </td>\n   <td>66.9\n   </td>\n   <td>79.0\n   </td>\n   <td>73.1\n   </td>\n  </tr>\n  <tr>\n   <td>DROP (3-shot, F1)\n   </td>\n   <td>58.4\n   </td>\n   <td>37.9\n   </td>\n   <td>49.8\n   </td>\n   <td>79.7\n   </td>\n   <td>70.2\n   </td>\n  </tr>\n</table>\n\n\n\n### Instruction tuned models\n\n\n<table>\n  <tr>\n   <td><strong>Benchmark</strong>\n   </td>\n   <td><strong>Llama 3 8B</strong>\n   </td>\n   <td><strong>Llama 2 7B</strong>\n   </td>\n   <td><strong>Llama 2 13B</strong>\n   </td>\n   <td><strong>Llama 3 70B</strong>\n   </td>\n   <td><strong>Llama 2 70B</strong>\n   </td>\n  </tr>\n  <tr>\n   <td>MMLU (5-shot)\n   </td>\n   <td>68.4\n   </td>\n   <td>34.1\n   </td>\n   <td>47.8\n   </td>\n   <td>82.0\n   </td>\n   <td>52.9\n   </td>\n  </tr>\n  <tr>\n   <td>GPQA (0-shot)\n   </td>\n   <td>34.2\n   </td>\n   <td>21.7\n   </td>\n   <td>22.3\n   </td>\n   <td>39.5\n   </td>\n   <td>21.0\n   </td>\n  </tr>\n  <tr>\n   <td>HumanEval (0-shot)\n   </td>\n   <td>62.2\n   </td>\n   <td>7.9\n   </td>\n   <td>14.0\n   </td>\n   <td>81.7\n   </td>\n   <td>25.6\n   </td>\n  </tr>\n  <tr>\n   <td>GSM-8K (8-shot, CoT)\n   </td>\n   <td>79.6\n   </td>\n   <td>25.7\n   </td>\n   <td>77.4\n   </td>\n   <td>93.0\n   </td>\n   <td>57.5\n   </td>\n  </tr>\n  <tr>\n   <td>MATH (4-shot, CoT)\n   </td>\n   <td>30.0\n   </td>\n   <td>3.8\n   </td>\n   <td>6.7\n   </td>\n   <td>50.4\n   </td>\n   <td>11.6\n   </td>\n  </tr>\n</table>\n\n\n\n### Responsibility & Safety\n\nWe believe that an open approach to AI leads to better, safer products, faster innovation, and a bigger overall market. We are committed to Responsible AI development and took a series of steps to limit misuse and harm and support the open source community.\n\nFoundation models are widely capable technologies that are built to be used for a diverse range of applications. They are not designed to meet every developer preference on safety levels for all use cases, out-of-the-box, as those by their nature will differ across different applications. \n\nRather, responsible LLM-application deployment is achieved by implementing a series of safety best practices throughout the development of such applications, from the model pre-training, fine-tuning and the deployment of systems composed of safeguards to tailor the safety needs specifically to the use case and audience. \n\n\nAs part of the Llama 3 release, we updated our [Responsible Use Guide](https://llama.meta.com/responsible-use-guide/) to outline the steps and best practices for developers to implement model and system level safety for their application. We also provide a set of resources including [Meta Llama Guard 2](https://llama.meta.com/purple-llama/) and [Code Shield](https://llama.meta.com/purple-llama/) safeguards. These tools have proven to drastically reduce residual risks of LLM Systems, while maintaining a high level of helpfulness. We encourage developers to tune and deploy these safeguards according to their needs and we provide a [reference implementation](https://github.com/meta-llama/llama-recipes/tree/main/recipes/responsible_ai) to get you started.\n\n\n#### Llama 3-Instruct\n\nAs outlined in the Responsible Use Guide, some trade-off between model helpfulness and model alignment is likely unavoidable. Developers should exercise discretion about how to weigh the benefits of alignment and helpfulness for their specific use case and audience. Developers should be mindful of residual risks when using Llama models and leverage additional safety tools as needed to reach the right safety bar for their use case. \n\n<span style=\"text-decoration:underline;\">Safety</span>\n\nFor our instruction tuned model, we conducted extensive red teaming exercises, performed adversarial evaluations and implemented safety mitigations techniques to lower residual risks. As with any Large Language Model, residual risks will likely remain and we recommend that developers assess these risks in the context of their use case. In parallel, we are working with the community to make AI safety benchmark standards transparent, rigorous and interpretable. \n\n<span style=\"text-decoration:underline;\">Refusals</span>\n\nIn addition to residual risks, we put a great emphasis on model refusals to benign prompts. Over-refusing not only can impact the user experience but could even be harmful in certain contexts as well. We’ve heard the feedback from the developer community and improved our fine tuning to ensure that Llama 3 is significantly less likely to falsely refuse to answer prompts than Llama 2. \n\nWe built internal benchmarks and developed mitigations to limit false refusals making Llama 3 our most helpful model to date. \n\n\n#### Responsible release \n\nIn addition to responsible use considerations outlined above, we followed a rigorous process that requires us to take extra measures against misuse and critical risks before we make our release decision. \n\nMisuse\n\nIf you access or use Llama 3, you agree to the Acceptable Use Policy. The most recent copy of this policy can be found at [https://llama.meta.com/llama3/use-policy/](https://llama.meta.com/llama3/use-policy/).\n\n\n#### Critical risks \n\n<span style=\"text-decoration:underline;\">CBRNE</span> (Chemical, Biological, Radiological, Nuclear, and high yield Explosives)\n\nWe have conducted a two fold assessment of the safety of the model in this area:\n\n\n\n* Iterative testing during model training to assess the safety of responses related to CBRNE threats and other adversarial risks.\n* Involving external CBRNE experts to conduct an uplift test assessing the ability of the model to accurately provide expert knowledge and reduce barriers to potential CBRNE misuse, by reference to what can be achieved using web search (without the model).\n\n\n### <span style=\"text-decoration:underline;\">Cyber Security </span>\n\nWe have evaluated Llama 3 with CyberSecEval, Meta’s cybersecurity safety eval suite, measuring Llama 3’s propensity to suggest insecure code when used as a coding assistant, and Llama 3’s propensity to comply with requests to help carry out cyber attacks, where attacks are defined by the industry standard MITRE ATT&CK cyber attack ontology. On our insecure coding and cyber attacker helpfulness tests, Llama 3 behaved in the same range or safer than models of [equivalent coding capability](https://huggingface.co/spaces/facebook/CyberSecEval). \n\n\n### <span style=\"text-decoration:underline;\">Child Safety</span>\n\nChild Safety risk assessments were conducted using a team of experts, to assess the model’s capability to produce outputs that could result in Child Safety risks and inform on any necessary and appropriate risk mitigations via fine tuning. We leveraged those expert red teaming sessions to expand the coverage of our evaluation benchmarks through Llama 3 model development.  For Llama 3, we conducted new in-depth sessions using objective based methodologies to assess the model risks along multiple attack vectors. We also partnered with content specialists to perform red teaming exercises assessing potentially violating content while taking account of market specific nuances or experiences. \n\n\n### Community \n\nGenerative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership in AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Purple Llama tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to our [Github repository](https://github.com/meta-llama/PurpleLlama). \n\nFinally, we put in place a set of resources including an [output reporting mechanism](https://developers.facebook.com/llama_output_feedback) and [bug bounty program](https://www.facebook.com/whitehat) to continuously improve the Llama technology with the help of the community. \n\n\n## Ethical Considerations and Limitations\n\nThe core values of Llama 3 are openness, inclusivity and helpfulness. It is meant to serve everyone, and to work for a wide range of use cases. It is thus designed to be accessible to people across many different backgrounds, experiences and perspectives. Llama 3 addresses users and their needs as they are, without insertion unnecessary judgment or normativity, while reflecting the understanding that even content that may appear problematic in some cases can serve valuable purposes in others. It respects the dignity and autonomy of all users, especially in terms of the values of free thought and expression that power innovation and progress. \n\nBut Llama 3 is a new technology, and like any new technology, there are risks associated with its use. Testing conducted to date has been in English, and has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, Llama 3’s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 3 models, developers should perform safety testing and tuning tailored to their specific applications of the model. As outlined in the Responsible Use Guide, we recommend incorporating [Purple Llama](https://github.com/facebookresearch/PurpleLlama) solutions into your workflows and specifically [Llama Guard](https://ai.meta.com/research/publications/llama-guard-llm-based-input-output-safeguard-for-human-ai-conversations/) which provides a base model to filter input and output prompts to layer system-level safety on top of model-level safety. \n\nPlease see the Responsible Use Guide available at [http://llama.meta.com/responsible-use-guide](http://llama.meta.com/responsible-use-guide)\n\n\n## Citation instructions\n\n@article{llama3modelcard,\n\n  title={Llama 3 Model Card},\n\n  author={AI@Meta},\n\n  year={2024},\n\n  url = {https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md}\n\n}\n\n## Contributors\n\nAaditya Singh; Aaron Grattafiori; Abhimanyu Dubey; Abhinav Jauhri; Abhinav Pandey; Abhishek Kadian; Adam Kelsey; Adi Gangidi; Ahmad Al-Dahle; Ahuva Goldstand; Aiesha Letman; Ajay Menon; Akhil Mathur; Alan Schelten; Alex Vaughan; Amy Yang; Andrei Lupu; Andres Alvarado; Andrew Gallagher; Andrew Gu; Andrew Ho; Andrew Poulton; Andrew Ryan; Angela Fan; Ankit Ramchandani; Anthony Hartshorn; Archi Mitra; Archie Sravankumar; Artem Korenev; Arun Rao; Ashley Gabriel; Ashwin Bharambe; Assaf Eisenman; Aston Zhang; Aurelien Rodriguez; Austen Gregerson; Ava Spataru; Baptiste Roziere; Ben Maurer; Benjamin Leonhardi; Bernie Huang; Bhargavi Paranjape; Bing Liu; Binh Tang; Bobbie Chern; Brani Stojkovic; Brian Fuller; Catalina Mejia Arenas; Chao Zhou; Charlotte Caucheteux; Chaya Nayak; Ching-Hsiang Chu; Chloe Bi; Chris Cai; Chris Cox; Chris Marra; Chris McConnell; Christian Keller; Christoph Feichtenhofer; Christophe Touret; Chunyang Wu; Corinne Wong; Cristian Canton Ferrer; Damien Allonsius; Daniel Kreymer; Daniel Haziza; Daniel Li; Danielle Pintz; Danny Livshits; Danny Wyatt; David Adkins; David Esiobu; David Xu; Davide Testuggine; Delia David; Devi Parikh; Dhruv Choudhary; Dhruv Mahajan; Diana Liskovich; Diego Garcia-Olano; Diego Perino; Dieuwke Hupkes; Dingkang Wang; Dustin Holland; Egor Lakomkin; Elina Lobanova; Xiaoqing Ellen Tan; Emily Dinan; Eric Smith; Erik Brinkman; Esteban Arcaute; Filip Radenovic; Firat Ozgenel; Francesco Caggioni; Frank Seide; Frank Zhang; Gabriel Synnaeve; Gabriella Schwarz; Gabrielle Lee; Gada Badeer; Georgia Anderson; Graeme Nail; Gregoire Mialon; Guan Pang; Guillem Cucurell; Hailey Nguyen; Hannah Korevaar; Hannah Wang; Haroun Habeeb; Harrison Rudolph; Henry Aspegren; Hu Xu; Hugo Touvron; Iga Kozlowska; Igor Molybog; Igor Tufanov; Iliyan Zarov; Imanol Arrieta Ibarra; Irina-Elena Veliche; Isabel Kloumann; Ishan Misra; Ivan Evtimov; Jacob Xu; Jade Copet; Jake Weissman; Jan Geffert; Jana Vranes; Japhet Asher; Jason Park; Jay Mahadeokar; Jean-Baptiste Gaya; Jeet Shah; Jelmer van der Linde; Jennifer Chan; Jenny Hong; Jenya Lee; Jeremy Fu; Jeremy Teboul; Jianfeng Chi; Jianyu Huang; Jie Wang; Jiecao Yu; Joanna Bitton; Joe Spisak; Joelle Pineau; Jon Carvill; Jongsoo Park; Joseph Rocca; Joshua Johnstun; Junteng Jia; Kalyan Vasuden Alwala; Kam Hou U; Kate Plawiak; Kartikeya Upasani; Kaushik Veeraraghavan; Ke Li; Kenneth Heafield; Kevin Stone; Khalid El-Arini; Krithika Iyer; Kshitiz Malik; Kuenley Chiu; Kunal Bhalla; Kyle Huang; Lakshya Garg; Lauren Rantala-Yeary; Laurens van der Maaten; Lawrence Chen; Leandro Silva; Lee Bell; Lei Zhang; Liang Tan; Louis Martin; Lovish Madaan; Luca Wehrstedt; Lukas Blecher; Luke de Oliveira; Madeline Muzzi; Madian Khabsa; Manav Avlani; Mannat Singh; Manohar Paluri; Mark Zuckerberg; Marcin Kardas; Martynas Mankus; Mathew Oldham; Mathieu Rita; Matthew Lennie; Maya Pavlova; Meghan Keneally; Melanie Kambadur; Mihir Patel; Mikayel Samvelyan; Mike Clark; Mike Lewis; Min Si; Mitesh Kumar Singh; Mo Metanat; Mona Hassan; Naman Goyal; Narjes Torabi; Nicolas Usunier; Nikolay Bashlykov; Nikolay Bogoychev; Niladri Chatterji; Ning Dong; Oliver Aobo Yang; Olivier Duchenne; Onur Celebi; Parth Parekh; Patrick Alrassy; Paul Saab; Pavan Balaji; Pedro Rittner; Pengchuan Zhang; Pengwei Li; Petar Vasic; Peter Weng; Polina Zvyagina; Prajjwal Bhargava; Pratik Dubal; Praveen Krishnan; Punit Singh Koura; Qing He; Rachel Rodriguez; Ragavan Srinivasan; Rahul Mitra; Ramon Calderer; Raymond Li; Robert Stojnic; Roberta Raileanu; Robin Battey; Rocky Wang; Rohit Girdhar; Rohit Patel; Romain Sauvestre; Ronnie Polidoro; Roshan Sumbaly; Ross Taylor; Ruan Silva; Rui Hou; Rui Wang; Russ Howes; Ruty Rinott; Saghar Hosseini; Sai Jayesh Bondu; Samyak Datta; Sanjay Singh; Sara Chugh; Sargun Dhillon; Satadru Pan; Sean Bell; Sergey Edunov; Shaoliang Nie; Sharan Narang; Sharath Raparthy; Shaun Lindsay; Sheng Feng; Sheng Shen; Shenghao Lin; Shiva Shankar; Shruti Bhosale; Shun Zhang; Simon Vandenhende; Sinong Wang; Seohyun Sonia Kim; Soumya Batra; Sten Sootla; Steve Kehoe; Suchin Gururangan; Sumit Gupta; Sunny Virk; Sydney Borodinsky; Tamar Glaser; Tamar Herman; Tamara Best; Tara Fowler; Thomas Georgiou; Thomas Scialom; Tianhe Li; Todor Mihaylov; Tong Xiao; Ujjwal Karn; Vedanuj Goswami; Vibhor Gupta; Vignesh Ramanathan; Viktor Kerkez; Vinay Satish Kumar; Vincent Gonguet; Vish Vogeti; Vlad Poenaru; Vlad Tiberiu Mihailescu; Vladan Petrovic; Vladimir Ivanov; Wei Li; Weiwei Chu; Wenhan Xiong; Wenyin Fu; Wes Bouaziz; Whitney Meers; Will Constable; Xavier Martinet; Xiaojian Wu; Xinbo Gao; Xinfeng Xie; Xuchao Jia; Yaelle Goldschlag; Yann LeCun; Yashesh Gaur; Yasmine Babaei; Ye Qi; Yenda Li; Yi Wen; Yiwen Song; Youngjin Nam; Yuchen Hao; Yuchen Zhang; Yun Wang; Yuning Mao; Yuzi He; Zacharie Delpierre Coudert; Zachary DeVito; Zahra Hankir; Zhaoduo Wen; Zheng Yan; Zhengxing Chen; Zhenyu Yang; Zoe Papakipos\n",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModelForCausalLM",
      "custom_class": null,
      "pipeline_tag": "text-generation",
      "processor": "AutoTokenizer"
    },
    "safetensors": {
      "parameters": {
        "BF16": 8030261248
      },
      "total": 8030261248
    },
    "model_index": null,
    "trending_score": null
  },
  {
    "model_id": "Qwen/Qwen2.5-1.5B-Instruct",
    "model_name": "Qwen/Qwen2.5-1.5B-Instruct",
    "author": "Qwen",
    "downloads": 1107512,
    "downloads_all_time": null,
    "likes": 374,
    "tags": [
      "transformers",
      "safetensors",
      "qwen2",
      "text-generation",
      "chat",
      "conversational",
      "en",
      "arxiv:2407.10671",
      "base_model:Qwen/Qwen2.5-1.5B",
      "base_model:finetune:Qwen/Qwen2.5-1.5B",
      "license:apache-2.0",
      "autotrain_compatible",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct",
    "dependencies": [
      [
        "transformers",
        null
      ]
    ],
    "last_modified": "2024-09-25T12:32:50+00:00",
    "created_at": "2024-09-17T14:10:29+00:00",
    "analysis_date": "2025-03-22T00:42:33.797585",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "qwen2",
    "pipeline_tag": "text-generation",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "license": "apache-2.0",
      "license_link": "https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct/blob/main/LICENSE",
      "language": [
        "en"
      ],
      "pipeline_tag": "text-generation",
      "base_model": "Qwen/Qwen2.5-1.5B",
      "tags": [
        "chat"
      ],
      "library_name": "transformers"
    },
    "card_text": "\n# Qwen2.5-1.5B-Instruct\n\n## Introduction\n\nQwen2.5 is the latest series of Qwen large language models. For Qwen2.5, we release a number of base language models and instruction-tuned language models ranging from 0.5 to 72 billion parameters. Qwen2.5 brings the following improvements upon Qwen2:\n\n- Significantly **more knowledge** and has greatly improved capabilities in **coding** and **mathematics**, thanks to our specialized expert models in these domains.\n- Significant improvements in **instruction following**, **generating long texts** (over 8K tokens), **understanding structured data** (e.g, tables), and **generating structured outputs** especially JSON. **More resilient to the diversity of system prompts**, enhancing role-play implementation and condition-setting for chatbots.\n- **Long-context Support** up to 128K tokens and can generate up to 8K tokens.\n- **Multilingual support** for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more. \n\n**This repo contains the instruction-tuned 1.5B Qwen2.5 model**, which has the following features:\n- Type: Causal Language Models\n- Training Stage: Pretraining & Post-training\n- Architecture: transformers with RoPE, SwiGLU, RMSNorm, Attention QKV bias and tied word embeddings\n- Number of Parameters: 1.54B\n- Number of Paramaters (Non-Embedding): 1.31B\n- Number of Layers: 28\n- Number of Attention Heads (GQA): 12 for Q and 2 for KV\n- Context Length: Full 32,768 tokens and generation 8192 tokens\n\nFor more details, please refer to our [blog](https://qwenlm.github.io/blog/qwen2.5/), [GitHub](https://github.com/QwenLM/Qwen2.5), and [Documentation](https://qwen.readthedocs.io/en/latest/).\n\n## Requirements\n\nThe code of Qwen2.5 has been in the latest Hugging face `transformers` and we advise you to use the latest version of `transformers`.\n\nWith `transformers<4.37.0`, you will encounter the following error:\n```\nKeyError: 'qwen2'\n```\n\n## Quickstart\n\nHere provides a code snippet with `apply_chat_template` to show you how to load the tokenizer and model and how to generate contents.\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=512\n)\ngenerated_ids = [\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\n\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n```\n\n\n## Evaluation & Performance\n\nDetailed evaluation results are reported in this [📑 blog](https://qwenlm.github.io/blog/qwen2.5/).\n\nFor requirements on GPU memory and the respective throughput, see results [here](https://qwen.readthedocs.io/en/latest/benchmark/speed_benchmark.html).\n\n## Citation\n\nIf you find our work helpful, feel free to give us a cite.\n\n```\n@misc{qwen2.5,\n    title = {Qwen2.5: A Party of Foundation Models},\n    url = {https://qwenlm.github.io/blog/qwen2.5/},\n    author = {Qwen Team},\n    month = {September},\n    year = {2024}\n}\n\n@article{qwen2,\n      title={Qwen2 Technical Report}, \n      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\n      journal={arXiv preprint arXiv:2407.10671},\n      year={2024}\n}\n```",
    "card_content": "---\nlicense: apache-2.0\nlicense_link: https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct/blob/main/LICENSE\nlanguage:\n- en\npipeline_tag: text-generation\nbase_model: Qwen/Qwen2.5-1.5B\ntags:\n- chat\nlibrary_name: transformers\n---\n\n# Qwen2.5-1.5B-Instruct\n\n## Introduction\n\nQwen2.5 is the latest series of Qwen large language models. For Qwen2.5, we release a number of base language models and instruction-tuned language models ranging from 0.5 to 72 billion parameters. Qwen2.5 brings the following improvements upon Qwen2:\n\n- Significantly **more knowledge** and has greatly improved capabilities in **coding** and **mathematics**, thanks to our specialized expert models in these domains.\n- Significant improvements in **instruction following**, **generating long texts** (over 8K tokens), **understanding structured data** (e.g, tables), and **generating structured outputs** especially JSON. **More resilient to the diversity of system prompts**, enhancing role-play implementation and condition-setting for chatbots.\n- **Long-context Support** up to 128K tokens and can generate up to 8K tokens.\n- **Multilingual support** for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more. \n\n**This repo contains the instruction-tuned 1.5B Qwen2.5 model**, which has the following features:\n- Type: Causal Language Models\n- Training Stage: Pretraining & Post-training\n- Architecture: transformers with RoPE, SwiGLU, RMSNorm, Attention QKV bias and tied word embeddings\n- Number of Parameters: 1.54B\n- Number of Paramaters (Non-Embedding): 1.31B\n- Number of Layers: 28\n- Number of Attention Heads (GQA): 12 for Q and 2 for KV\n- Context Length: Full 32,768 tokens and generation 8192 tokens\n\nFor more details, please refer to our [blog](https://qwenlm.github.io/blog/qwen2.5/), [GitHub](https://github.com/QwenLM/Qwen2.5), and [Documentation](https://qwen.readthedocs.io/en/latest/).\n\n## Requirements\n\nThe code of Qwen2.5 has been in the latest Hugging face `transformers` and we advise you to use the latest version of `transformers`.\n\nWith `transformers<4.37.0`, you will encounter the following error:\n```\nKeyError: 'qwen2'\n```\n\n## Quickstart\n\nHere provides a code snippet with `apply_chat_template` to show you how to load the tokenizer and model and how to generate contents.\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=512\n)\ngenerated_ids = [\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\n\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n```\n\n\n## Evaluation & Performance\n\nDetailed evaluation results are reported in this [📑 blog](https://qwenlm.github.io/blog/qwen2.5/).\n\nFor requirements on GPU memory and the respective throughput, see results [here](https://qwen.readthedocs.io/en/latest/benchmark/speed_benchmark.html).\n\n## Citation\n\nIf you find our work helpful, feel free to give us a cite.\n\n```\n@misc{qwen2.5,\n    title = {Qwen2.5: A Party of Foundation Models},\n    url = {https://qwenlm.github.io/blog/qwen2.5/},\n    author = {Qwen Team},\n    month = {September},\n    year = {2024}\n}\n\n@article{qwen2,\n      title={Qwen2 Technical Report}, \n      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\n      journal={arXiv preprint arXiv:2407.10671},\n      year={2024}\n}\n```",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModelForCausalLM",
      "custom_class": null,
      "pipeline_tag": "text-generation",
      "processor": "AutoTokenizer"
    },
    "safetensors": {
      "parameters": {
        "BF16": 1543714304
      },
      "total": 1543714304
    },
    "model_index": null,
    "trending_score": null
  },
  {
    "model_id": "openai-community/gpt2-large",
    "model_name": "openai-community/gpt2-large",
    "author": "openai-community",
    "downloads": 1069883,
    "downloads_all_time": null,
    "likes": 302,
    "tags": [
      "transformers",
      "pytorch",
      "tf",
      "jax",
      "rust",
      "onnx",
      "safetensors",
      "gpt2",
      "text-generation",
      "en",
      "arxiv:1910.09700",
      "license:mit",
      "autotrain_compatible",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/openai-community/gpt2-large",
    "dependencies": [
      [
        "transformers",
        null
      ]
    ],
    "last_modified": "2024-02-19T11:11:02+00:00",
    "created_at": "2022-03-02T23:29:04+00:00",
    "analysis_date": "2025-03-22T00:42:35.807803",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "gpt2",
    "pipeline_tag": "text-generation",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": "en",
      "license": "mit"
    },
    "card_text": "\n# GPT-2 Large\n\n## Table of Contents\n- [Model Details](#model-details)\n- [How To Get Started With the Model](#how-to-get-started-with-the-model)\n- [Uses](#uses)\n- [Risks, Limitations and Biases](#risks-limitations-and-biases)\n- [Training](#training)\n- [Evaluation](#evaluation)\n- [Environmental Impact](#environmental-impact)\n- [Technical Specifications](#technical-specifications)\n- [Citation Information](#citation-information)\n- [Model Card Authors](#model-card-author)\n\n## Model Details\n\n**Model Description:** GPT-2 Large is the **774M parameter** version of GPT-2, a transformer-based language model created and released by OpenAI. The model is a pretrained model on English language using a causal language modeling (CLM) objective. \n\n- **Developed by:** OpenAI, see [associated research paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) and [GitHub repo](https://github.com/openai/gpt-2) for model developers.\n- **Model Type:** Transformer-based language model\n- **Language(s):** English\n- **License:** [Modified MIT License](https://github.com/openai/gpt-2/blob/master/LICENSE)\n- **Related Models:** [GPT-2](https://huggingface.co/gpt2), [GPT-Medium](https://huggingface.co/gpt2-medium) and [GPT-XL](https://huggingface.co/gpt2-xl)\n- **Resources for more information:**\n  - [Research Paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n  - [OpenAI Blog Post](https://openai.com/blog/better-language-models/)\n  - [GitHub Repo](https://github.com/openai/gpt-2)\n  - [OpenAI Model Card for GPT-2](https://github.com/openai/gpt-2/blob/master/model_card.md)\n  - Test the full generation capabilities here: https://transformer.huggingface.co/doc/gpt2-large\n\n## How to Get Started with the Model \n\nUse the code below to get started with the model. You can use this model directly with a pipeline for text generation. Since the generation relies on some randomness, we\nset a seed for reproducibility:\n\n```python\n>>> from transformers import pipeline, set_seed\n>>> generator = pipeline('text-generation', model='gpt2-large')\n>>> set_seed(42)\n>>> generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)\n\n[{'generated_text': \"Hello, I'm a language model, I can do language modeling. In fact, this is one of the reasons I use languages. To get a\"},\n {'generated_text': \"Hello, I'm a language model, which in its turn implements a model of how a human can reason about a language, and is in turn an\"},\n {'generated_text': \"Hello, I'm a language model, why does this matter for you?\\n\\nWhen I hear new languages, I tend to start thinking in terms\"},\n {'generated_text': \"Hello, I'm a language model, a functional language...\\n\\nI don't need to know anything else. If I want to understand about how\"},\n {'generated_text': \"Hello, I'm a language model, not a toolbox.\\n\\nIn a nutshell, a language model is a set of attributes that define how\"}]\n```\n\nHere is how to use this model to get the features of a given text in PyTorch:\n\n```python\nfrom transformers import GPT2Tokenizer, GPT2Model\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2-large')\nmodel = GPT2Model.from_pretrained('gpt2-large')\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n```\n\nand in TensorFlow:\n\n```python\nfrom transformers import GPT2Tokenizer, TFGPT2Model\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2-large')\nmodel = TFGPT2Model.from_pretrained('gpt2-large')\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\n```\n\n## Uses\n\n#### Direct Use\n\nIn their [model card about GPT-2](https://github.com/openai/gpt-2/blob/master/model_card.md), OpenAI wrote: \n\n> The primary intended users of these models are AI researchers and practitioners.\n> \n> We primarily imagine these language models will be used by researchers to better understand the behaviors, capabilities, biases, and constraints of large-scale generative language models.\n\n#### Downstream Use\n\nIn their [model card about GPT-2](https://github.com/openai/gpt-2/blob/master/model_card.md), OpenAI wrote: \n\n> Here are some secondary use cases we believe are likely:\n> \n> - Writing assistance: Grammar assistance, autocompletion (for normal prose or code)\n> - Creative writing and art: exploring the generation of creative, fictional texts; aiding creation of poetry and other literary art.\n> - Entertainment: Creation of games, chat bots, and amusing generations.\n\n#### Misuse and Out-of-scope Use\n\nIn their [model card about GPT-2](https://github.com/openai/gpt-2/blob/master/model_card.md), OpenAI wrote: \n\n> Because large-scale language models like GPT-2 do not distinguish fact from fiction, we don’t support use-cases that require the generated text to be true.\n> \n> Additionally, language models like GPT-2 reflect the biases inherent to the systems they were trained on, so we do not recommend that they be deployed into systems that interact with humans unless the deployers first carry out a study of biases relevant to the intended use-case. We found no statistically significant difference in gender, race, and religious bias probes between 774M and 1.5B, implying all versions of GPT-2 should be approached with similar levels of caution around use cases that are sensitive to biases around human attributes.\n\n## Risks, Limitations and Biases\n\n**CONTENT WARNING: Readers should be aware this section contains content that is disturbing, offensive, and can propogate historical and current stereotypes.**\n\nSignificant research has explored bias and fairness issues with language models (see, e.g., [Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)). \n\nThe training data used for this model has not been released as a dataset one can browse. We know it contains a lot of unfiltered content from the internet, which is far from neutral. Predictions generated by the model can include disturbing and harmful stereotypes across protected classes; identity characteristics; and sensitive, social, and occupational groups. For example:\n\n```python\n>>> from transformers import pipeline, set_seed\n>>> generator = pipeline('text-generation', model='gpt2-large')\n>>> set_seed(42)\n>>> generator(\"The man worked as a\", max_length=10, num_return_sequences=5)\n\n[{'generated_text': 'The man worked as a security guard in a hotel'},\n {'generated_text': 'The man worked as a salesman in Mexico and in'},\n {'generated_text': 'The man worked as a supervisor at the warehouse for'},\n {'generated_text': \"The man worked as a cleaner for the store's\"},\n {'generated_text': 'The man worked as a barbershop apprentice.'}]\n\n>>> set_seed(42)\n>>> generator(\"The woman worked as a\", max_length=10, num_return_sequences=5)\n\n[{'generated_text': 'The woman worked as a clerk at the bank.'},\n {'generated_text': 'The woman worked as a caregiver, and her'},\n {'generated_text': 'The woman worked as a customer service agent for a'},\n {'generated_text': 'The woman worked as a cleaner at the store,'},\n {'generated_text': 'The woman worked as a barista and was \"'}]\n```\n\nThis bias will also affect all fine-tuned versions of this model. Users (both direct and downstream) should be made aware of the risks, biases and limitations of the model.\n\n## Training\n\n#### Training Data\n\nThe OpenAI team wanted to train this model on a corpus as large as possible. To build it, they scraped all the web\npages from outbound links on Reddit which received at least 3 karma. Note that all Wikipedia pages were removed from\nthis dataset, so the model was not trained on any part of Wikipedia. The resulting dataset (called WebText) weights\n40GB of texts but has not been publicly released. You can find a list of the top 1,000 domains present in WebText\n[here](https://github.com/openai/gpt-2/blob/master/domains.txt).\n\n#### Training Procedure\n\nThe model is pretrained on a very large corpus of English data in a self-supervised fashion. This\nmeans it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots\nof publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely,\nit was trained to guess the next word in sentences.\n\nMore precisely, inputs are sequences of continuous text of a certain length and the targets are the same sequence,\nshifted one token (word or piece of word) to the right. The model uses internally a mask-mechanism to make sure the\npredictions for the token `i` only uses the inputs from `1` to `i` but not the future tokens.\n\nThis way, the model learns an inner representation of the English language that can then be used to extract features\nuseful for downstream tasks.\n\nThe texts are tokenized using a byte-level version of Byte Pair Encoding (BPE) (for unicode characters) and a\nvocabulary size of 50,257. The inputs are sequences of 1024 consecutive tokens.\n\n## Evaluation\n\nThe following evaluation information is extracted from the [associated paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf).\n\n#### Testing Data, Factors and Metrics\n\nThe model authors write in the [associated paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) that:\n\n> Since our model operates on a byte level and does not require lossy pre-processing or tokenization, we can evaluate it on any language model benchmark. Results on language modeling datasets are commonly reported in a quantity which is a scaled or ex- ponentiated version of the average negative log probability per canonical prediction unit - usually a character, a byte, or a word. We evaluate the same quantity by computing the log-probability of a dataset according to a WebText LM and dividing by the number of canonical units. For many of these datasets, WebText LMs would be tested significantly out- of-distribution, having to predict aggressively standardized text, tokenization artifacts such as disconnected punctuation and contractions, shuffled sentences, and even the string <UNK> which is extremely rare in WebText - occurring only 26 times in 40 billion bytes. We report our main results...using invertible de-tokenizers which remove as many of these tokenization / pre-processing artifacts as possible. Since these de-tokenizers are invertible, we can still calculate the log probability of a dataset and they can be thought of as a simple form of domain adaptation. \n\n#### Results\n\nThe model achieves the following results without any fine-tuning (zero-shot):\n\n| Dataset  | LAMBADA | LAMBADA | CBT-CN | CBT-NE | WikiText2 | PTB    | enwiki8 | text8  | WikiText103 | 1BW   |\n|:--------:|:-------:|:-------:|:------:|:------:|:---------:|:------:|:-------:|:------:|:-----------:|:-----:|\n| (metric) | (PPL)   | (ACC)   | (ACC)  | (ACC)  | (PPL)     | (PPL)  | (BPB)   | (BPC)  | (PPL)       | (PPL) |\n|          | 10.87   | 60.12   | 93.45  | 88.0   | 19.93     | 40.31  | 0.97    | 1.02   | 22.05       | 44.575|\n\n## Environmental Impact\n\nCarbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).\n\n- **Hardware Type:** Unknown\n- **Hours used:** Unknown\n- **Cloud Provider:** Unknown\n- **Compute Region:** Unknown\n- **Carbon Emitted:** Unknown\n\n## Technical Specifications\n\nSee the [associated paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) for details on the modeling architecture, objective, compute infrastructure, and training details.\n\n## Citation Information\n\n```bibtex\n@article{radford2019language,\n  title={Language models are unsupervised multitask learners},\n  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},\n  journal={OpenAI blog},\n  volume={1},\n  number={8},\n  pages={9},\n  year={2019}\n}\n```\n\n## Model Card Authors\n\nThis model card was written by the Hugging Face team.",
    "card_content": "---\nlanguage: en\nlicense: mit\n---\n\n# GPT-2 Large\n\n## Table of Contents\n- [Model Details](#model-details)\n- [How To Get Started With the Model](#how-to-get-started-with-the-model)\n- [Uses](#uses)\n- [Risks, Limitations and Biases](#risks-limitations-and-biases)\n- [Training](#training)\n- [Evaluation](#evaluation)\n- [Environmental Impact](#environmental-impact)\n- [Technical Specifications](#technical-specifications)\n- [Citation Information](#citation-information)\n- [Model Card Authors](#model-card-author)\n\n## Model Details\n\n**Model Description:** GPT-2 Large is the **774M parameter** version of GPT-2, a transformer-based language model created and released by OpenAI. The model is a pretrained model on English language using a causal language modeling (CLM) objective. \n\n- **Developed by:** OpenAI, see [associated research paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) and [GitHub repo](https://github.com/openai/gpt-2) for model developers.\n- **Model Type:** Transformer-based language model\n- **Language(s):** English\n- **License:** [Modified MIT License](https://github.com/openai/gpt-2/blob/master/LICENSE)\n- **Related Models:** [GPT-2](https://huggingface.co/gpt2), [GPT-Medium](https://huggingface.co/gpt2-medium) and [GPT-XL](https://huggingface.co/gpt2-xl)\n- **Resources for more information:**\n  - [Research Paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n  - [OpenAI Blog Post](https://openai.com/blog/better-language-models/)\n  - [GitHub Repo](https://github.com/openai/gpt-2)\n  - [OpenAI Model Card for GPT-2](https://github.com/openai/gpt-2/blob/master/model_card.md)\n  - Test the full generation capabilities here: https://transformer.huggingface.co/doc/gpt2-large\n\n## How to Get Started with the Model \n\nUse the code below to get started with the model. You can use this model directly with a pipeline for text generation. Since the generation relies on some randomness, we\nset a seed for reproducibility:\n\n```python\n>>> from transformers import pipeline, set_seed\n>>> generator = pipeline('text-generation', model='gpt2-large')\n>>> set_seed(42)\n>>> generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)\n\n[{'generated_text': \"Hello, I'm a language model, I can do language modeling. In fact, this is one of the reasons I use languages. To get a\"},\n {'generated_text': \"Hello, I'm a language model, which in its turn implements a model of how a human can reason about a language, and is in turn an\"},\n {'generated_text': \"Hello, I'm a language model, why does this matter for you?\\n\\nWhen I hear new languages, I tend to start thinking in terms\"},\n {'generated_text': \"Hello, I'm a language model, a functional language...\\n\\nI don't need to know anything else. If I want to understand about how\"},\n {'generated_text': \"Hello, I'm a language model, not a toolbox.\\n\\nIn a nutshell, a language model is a set of attributes that define how\"}]\n```\n\nHere is how to use this model to get the features of a given text in PyTorch:\n\n```python\nfrom transformers import GPT2Tokenizer, GPT2Model\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2-large')\nmodel = GPT2Model.from_pretrained('gpt2-large')\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n```\n\nand in TensorFlow:\n\n```python\nfrom transformers import GPT2Tokenizer, TFGPT2Model\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2-large')\nmodel = TFGPT2Model.from_pretrained('gpt2-large')\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\n```\n\n## Uses\n\n#### Direct Use\n\nIn their [model card about GPT-2](https://github.com/openai/gpt-2/blob/master/model_card.md), OpenAI wrote: \n\n> The primary intended users of these models are AI researchers and practitioners.\n> \n> We primarily imagine these language models will be used by researchers to better understand the behaviors, capabilities, biases, and constraints of large-scale generative language models.\n\n#### Downstream Use\n\nIn their [model card about GPT-2](https://github.com/openai/gpt-2/blob/master/model_card.md), OpenAI wrote: \n\n> Here are some secondary use cases we believe are likely:\n> \n> - Writing assistance: Grammar assistance, autocompletion (for normal prose or code)\n> - Creative writing and art: exploring the generation of creative, fictional texts; aiding creation of poetry and other literary art.\n> - Entertainment: Creation of games, chat bots, and amusing generations.\n\n#### Misuse and Out-of-scope Use\n\nIn their [model card about GPT-2](https://github.com/openai/gpt-2/blob/master/model_card.md), OpenAI wrote: \n\n> Because large-scale language models like GPT-2 do not distinguish fact from fiction, we don’t support use-cases that require the generated text to be true.\n> \n> Additionally, language models like GPT-2 reflect the biases inherent to the systems they were trained on, so we do not recommend that they be deployed into systems that interact with humans unless the deployers first carry out a study of biases relevant to the intended use-case. We found no statistically significant difference in gender, race, and religious bias probes between 774M and 1.5B, implying all versions of GPT-2 should be approached with similar levels of caution around use cases that are sensitive to biases around human attributes.\n\n## Risks, Limitations and Biases\n\n**CONTENT WARNING: Readers should be aware this section contains content that is disturbing, offensive, and can propogate historical and current stereotypes.**\n\nSignificant research has explored bias and fairness issues with language models (see, e.g., [Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)). \n\nThe training data used for this model has not been released as a dataset one can browse. We know it contains a lot of unfiltered content from the internet, which is far from neutral. Predictions generated by the model can include disturbing and harmful stereotypes across protected classes; identity characteristics; and sensitive, social, and occupational groups. For example:\n\n```python\n>>> from transformers import pipeline, set_seed\n>>> generator = pipeline('text-generation', model='gpt2-large')\n>>> set_seed(42)\n>>> generator(\"The man worked as a\", max_length=10, num_return_sequences=5)\n\n[{'generated_text': 'The man worked as a security guard in a hotel'},\n {'generated_text': 'The man worked as a salesman in Mexico and in'},\n {'generated_text': 'The man worked as a supervisor at the warehouse for'},\n {'generated_text': \"The man worked as a cleaner for the store's\"},\n {'generated_text': 'The man worked as a barbershop apprentice.'}]\n\n>>> set_seed(42)\n>>> generator(\"The woman worked as a\", max_length=10, num_return_sequences=5)\n\n[{'generated_text': 'The woman worked as a clerk at the bank.'},\n {'generated_text': 'The woman worked as a caregiver, and her'},\n {'generated_text': 'The woman worked as a customer service agent for a'},\n {'generated_text': 'The woman worked as a cleaner at the store,'},\n {'generated_text': 'The woman worked as a barista and was \"'}]\n```\n\nThis bias will also affect all fine-tuned versions of this model. Users (both direct and downstream) should be made aware of the risks, biases and limitations of the model.\n\n## Training\n\n#### Training Data\n\nThe OpenAI team wanted to train this model on a corpus as large as possible. To build it, they scraped all the web\npages from outbound links on Reddit which received at least 3 karma. Note that all Wikipedia pages were removed from\nthis dataset, so the model was not trained on any part of Wikipedia. The resulting dataset (called WebText) weights\n40GB of texts but has not been publicly released. You can find a list of the top 1,000 domains present in WebText\n[here](https://github.com/openai/gpt-2/blob/master/domains.txt).\n\n#### Training Procedure\n\nThe model is pretrained on a very large corpus of English data in a self-supervised fashion. This\nmeans it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots\nof publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely,\nit was trained to guess the next word in sentences.\n\nMore precisely, inputs are sequences of continuous text of a certain length and the targets are the same sequence,\nshifted one token (word or piece of word) to the right. The model uses internally a mask-mechanism to make sure the\npredictions for the token `i` only uses the inputs from `1` to `i` but not the future tokens.\n\nThis way, the model learns an inner representation of the English language that can then be used to extract features\nuseful for downstream tasks.\n\nThe texts are tokenized using a byte-level version of Byte Pair Encoding (BPE) (for unicode characters) and a\nvocabulary size of 50,257. The inputs are sequences of 1024 consecutive tokens.\n\n## Evaluation\n\nThe following evaluation information is extracted from the [associated paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf).\n\n#### Testing Data, Factors and Metrics\n\nThe model authors write in the [associated paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) that:\n\n> Since our model operates on a byte level and does not require lossy pre-processing or tokenization, we can evaluate it on any language model benchmark. Results on language modeling datasets are commonly reported in a quantity which is a scaled or ex- ponentiated version of the average negative log probability per canonical prediction unit - usually a character, a byte, or a word. We evaluate the same quantity by computing the log-probability of a dataset according to a WebText LM and dividing by the number of canonical units. For many of these datasets, WebText LMs would be tested significantly out- of-distribution, having to predict aggressively standardized text, tokenization artifacts such as disconnected punctuation and contractions, shuffled sentences, and even the string <UNK> which is extremely rare in WebText - occurring only 26 times in 40 billion bytes. We report our main results...using invertible de-tokenizers which remove as many of these tokenization / pre-processing artifacts as possible. Since these de-tokenizers are invertible, we can still calculate the log probability of a dataset and they can be thought of as a simple form of domain adaptation. \n\n#### Results\n\nThe model achieves the following results without any fine-tuning (zero-shot):\n\n| Dataset  | LAMBADA | LAMBADA | CBT-CN | CBT-NE | WikiText2 | PTB    | enwiki8 | text8  | WikiText103 | 1BW   |\n|:--------:|:-------:|:-------:|:------:|:------:|:---------:|:------:|:-------:|:------:|:-----------:|:-----:|\n| (metric) | (PPL)   | (ACC)   | (ACC)  | (ACC)  | (PPL)     | (PPL)  | (BPB)   | (BPC)  | (PPL)       | (PPL) |\n|          | 10.87   | 60.12   | 93.45  | 88.0   | 19.93     | 40.31  | 0.97    | 1.02   | 22.05       | 44.575|\n\n## Environmental Impact\n\nCarbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).\n\n- **Hardware Type:** Unknown\n- **Hours used:** Unknown\n- **Cloud Provider:** Unknown\n- **Compute Region:** Unknown\n- **Carbon Emitted:** Unknown\n\n## Technical Specifications\n\nSee the [associated paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) for details on the modeling architecture, objective, compute infrastructure, and training details.\n\n## Citation Information\n\n```bibtex\n@article{radford2019language,\n  title={Language models are unsupervised multitask learners},\n  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},\n  journal={OpenAI blog},\n  volume={1},\n  number={8},\n  pages={9},\n  year={2019}\n}\n```\n\n## Model Card Authors\n\nThis model card was written by the Hugging Face team.",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModelForCausalLM",
      "custom_class": null,
      "pipeline_tag": "text-generation",
      "processor": "AutoTokenizer"
    },
    "safetensors": {
      "parameters": {
        "F32": 811778816
      },
      "total": 811778816
    },
    "model_index": null,
    "trending_score": null
  },
  {
    "model_id": "meta-llama/Llama-3.1-70B-Instruct",
    "model_name": "meta-llama/Llama-3.1-70B-Instruct",
    "author": "meta-llama",
    "downloads": 1058103,
    "downloads_all_time": null,
    "likes": 798,
    "tags": [
      "transformers",
      "safetensors",
      "llama",
      "text-generation",
      "facebook",
      "meta",
      "pytorch",
      "llama-3",
      "conversational",
      "en",
      "de",
      "fr",
      "it",
      "pt",
      "hi",
      "es",
      "th",
      "arxiv:2204.05149",
      "base_model:meta-llama/Llama-3.1-70B",
      "base_model:finetune:meta-llama/Llama-3.1-70B",
      "license:llama3.1",
      "autotrain_compatible",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/meta-llama/Llama-3.1-70B-Instruct",
    "dependencies": [
      [
        "transformers",
        null
      ],
      [
        "torch",
        null
      ]
    ],
    "last_modified": "2024-12-15T01:55:33+00:00",
    "created_at": "2024-07-16T16:07:46+00:00",
    "analysis_date": "2025-03-22T00:42:38.742500",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "llama",
    "pipeline_tag": "text-generation",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": [
        "en",
        "de",
        "fr",
        "it",
        "pt",
        "hi",
        "es",
        "th"
      ],
      "library_name": "transformers",
      "base_model": "meta-llama/Meta-Llama-3.1-70B",
      "new_version": "meta-llama/Llama-3.3-70B-Instruct",
      "license": "llama3.1",
      "pipeline_tag": "text-generation",
      "tags": [
        "facebook",
        "meta",
        "pytorch",
        "llama",
        "llama-3"
      ],
      "extra_gated_prompt": "### LLAMA 3.1 COMMUNITY LICENSE AGREEMENT\nLlama 3.1 Version Release Date: July 23, 2024\n\"Agreement\" means the terms and conditions for use, reproduction, distribution and modification of the  Llama Materials set forth herein.\n\"Documentation\" means the specifications, manuals and documentation accompanying Llama 3.1 distributed by Meta at https://llama.meta.com/doc/overview.\n\"Licensee\" or \"you\" means you, or your employer or any other person or entity (if you are entering into this Agreement on such person or entity’s behalf), of the age required under applicable laws, rules or regulations to provide legal consent and that has legal authority to bind your employer or such other person or entity if you are entering in this Agreement on their behalf.\n\"Llama 3.1\" means the foundational large language models and software and algorithms, including machine-learning model code, trained model weights, inference-enabling code, training-enabling code, fine-tuning enabling code and other elements of the foregoing distributed by Meta at https://llama.meta.com/llama-downloads.\n\"Llama Materials\" means, collectively, Meta’s proprietary Llama 3.1 and Documentation (and any portion thereof) made available under this Agreement.\n\"Meta\" or \"we\" means Meta Platforms Ireland Limited (if you are located in or, if you are an entity, your principal place of business is in the EEA or Switzerland) and Meta Platforms, Inc. (if you are located outside of the EEA or Switzerland).\n   \n1. License Rights and Redistribution.\na. Grant of Rights. You are granted a non-exclusive, worldwide, non-transferable and royalty-free limited license under Meta’s intellectual property or other rights owned by Meta embodied in the Llama Materials to use, reproduce, distribute, copy, create derivative works of, and make modifications to the Llama Materials.\nb. Redistribution and Use.\ni. If you distribute or make available the Llama Materials (or any derivative works thereof), or a product or service (including another AI model) that contains any of them, you shall (A) provide a copy of this Agreement with any such Llama Materials; and (B) prominently display “Built with Llama” on a related website, user interface, blogpost, about page, or product documentation. If you use the Llama Materials or any outputs or results of the Llama Materials to create, train, fine tune, or otherwise improve an AI model, which is distributed or made available, you shall also include “Llama” at the beginning of any such AI model name.\nii. If you receive Llama Materials, or any derivative works thereof, from a Licensee as part  of an integrated end user product, then Section 2 of this Agreement will not apply to you.\niii. You must retain in all copies of the Llama Materials that you distribute the following attribution notice within a “Notice” text file distributed as a part of such copies: “Llama 3.1 is licensed under the Llama 3.1 Community License, Copyright © Meta Platforms, Inc. All Rights Reserved.”\niv. Your use of the Llama Materials must comply with applicable laws and regulations (including trade compliance laws and regulations) and adhere to the Acceptable Use Policy for the Llama Materials (available at https://llama.meta.com/llama3_1/use-policy), which is hereby incorporated by reference into this Agreement.\n2. Additional Commercial Terms. If, on the Llama 3.1 version release date, the monthly active users of the products or services made available by or for Licensee, or Licensee’s affiliates, is greater than 700 million monthly active users in the preceding calendar month, you must request a license from Meta, which Meta may grant to you in its sole discretion, and you are not authorized to exercise any of the rights under this Agreement unless or until Meta otherwise expressly grants you such rights.\n3. Disclaimer of Warranty. UNLESS REQUIRED BY APPLICABLE LAW, THE LLAMA MATERIALS AND ANY OUTPUT AND RESULTS THEREFROM ARE PROVIDED ON AN “AS IS” BASIS, WITHOUT WARRANTIES OF ANY KIND, AND META DISCLAIMS ALL WARRANTIES OF ANY KIND, BOTH EXPRESS AND IMPLIED, INCLUDING, WITHOUT LIMITATION, ANY WARRANTIES OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE. YOU ARE SOLELY RESPONSIBLE FOR DETERMINING THE APPROPRIATENESS OF USING OR REDISTRIBUTING THE LLAMA MATERIALS AND ASSUME ANY RISKS ASSOCIATED WITH YOUR USE OF THE LLAMA MATERIALS AND ANY OUTPUT AND RESULTS.\n4. Limitation of Liability. IN NO EVENT WILL META OR ITS AFFILIATES BE LIABLE UNDER ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, TORT, NEGLIGENCE, PRODUCTS LIABILITY, OR OTHERWISE, ARISING OUT OF THIS AGREEMENT, FOR ANY LOST PROFITS OR ANY INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL, EXEMPLARY OR PUNITIVE DAMAGES, EVEN IF META OR ITS AFFILIATES HAVE BEEN ADVISED OF THE POSSIBILITY OF ANY OF THE FOREGOING.\n5. Intellectual Property.\na. No trademark licenses are granted under this Agreement, and in connection with the Llama Materials, neither Meta nor Licensee may use any name or mark owned by or associated with the other or any of its affiliates, except as required for reasonable and customary use in describing and redistributing the Llama Materials or as set forth in this Section 5(a). Meta hereby grants you a license to use “Llama” (the “Mark”) solely as required to comply with the last sentence of Section 1.b.i. You will comply with Meta’s brand guidelines (currently accessible at https://about.meta.com/brand/resources/meta/company-brand/ ). All goodwill arising out of your use of the Mark will inure to the benefit of Meta.\nb. Subject to Meta’s ownership of Llama Materials and derivatives made by or for Meta, with respect to any derivative works and modifications of the Llama Materials that are made by you, as between you and Meta, you are and will be the owner of such derivative works and modifications.\nc. If you institute litigation or other proceedings against Meta or any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Llama Materials or Llama 3.1 outputs or results, or any portion of any of the foregoing, constitutes infringement of intellectual property or other rights owned or licensable by you, then any licenses granted to you under this Agreement shall terminate as of the date such litigation or claim is filed or instituted. You will indemnify and hold harmless Meta from and against any claim by any third party arising out of or related to your use or distribution of the Llama Materials.\n6. Term and Termination. The term of this Agreement will commence upon your acceptance of this Agreement or access to the Llama Materials and will continue in full force and effect until terminated in accordance with the terms and conditions herein. Meta may terminate this Agreement if you are in breach of any term or condition of this Agreement. Upon termination of this Agreement, you shall delete and cease use of the Llama Materials. Sections 3, 4 and 7 shall survive the termination of this Agreement.\n7. Governing Law and Jurisdiction. This Agreement will be governed and construed under the laws of the State of California without regard to choice of law principles, and the UN Convention on Contracts for the International Sale of Goods does not apply to this Agreement. The courts of California shall have exclusive jurisdiction of any dispute arising out of this Agreement.\n### Llama 3.1 Acceptable Use Policy\nMeta is committed to promoting safe and fair use of its tools and features, including Llama 3.1. If you access or use Llama 3.1, you agree to this Acceptable Use Policy (“Policy”). The most recent copy of this policy can be found at [https://llama.meta.com/llama3_1/use-policy](https://llama.meta.com/llama3_1/use-policy)\n#### Prohibited Uses\nWe want everyone to use Llama 3.1 safely and responsibly. You agree you will not use, or allow others to use, Llama 3.1 to:\n 1. Violate the law or others’ rights, including to:\n    1. Engage in, promote, generate, contribute to, encourage, plan, incite, or further illegal or unlawful activity or content, such as:\n        1. Violence or terrorism\n        2. Exploitation or harm to children, including the solicitation, creation, acquisition, or dissemination of child exploitative content or failure to report Child Sexual Abuse Material\n        3. Human trafficking, exploitation, and sexual violence\n        4. The illegal distribution of information or materials to minors, including obscene materials, or failure to employ legally required age-gating in connection with such information or materials.\n        5. Sexual solicitation\n        6. Any other criminal activity\n    3. Engage in, promote, incite, or facilitate the harassment, abuse, threatening, or bullying of individuals or groups of individuals\n    4. Engage in, promote, incite, or facilitate discrimination or other unlawful or harmful conduct in the provision of employment, employment benefits, credit, housing, other economic benefits, or other essential goods and services\n    5. Engage in the unauthorized or unlicensed practice of any profession including, but not limited to, financial, legal, medical/health, or related professional practices\n    6. Collect, process, disclose, generate, or infer health, demographic, or other sensitive personal or private information about individuals without rights and consents required by applicable laws\n    7. Engage in or facilitate any action or generate any content that infringes, misappropriates, or otherwise violates any third-party rights, including the outputs or results of any products or services using the Llama Materials\n    8. Create, generate, or facilitate the creation of malicious code, malware, computer viruses or do anything else that could disable, overburden, interfere with or impair the proper working, integrity, operation or appearance of a website or computer system\n2. Engage in, promote, incite, facilitate, or assist in the planning or development of activities that present a risk of death or bodily harm to individuals, including use of Llama 3.1 related to the following:\n    1. Military, warfare, nuclear industries or applications, espionage, use for materials or activities that are subject to the International Traffic Arms Regulations (ITAR) maintained by the United States Department of State\n    2. Guns and illegal weapons (including weapon development)\n    3. Illegal drugs and regulated/controlled substances\n    4. Operation of critical infrastructure, transportation technologies, or heavy machinery\n    5. Self-harm or harm to others, including suicide, cutting, and eating disorders\n    6. Any content intended to incite or promote violence, abuse, or any infliction of bodily harm to an individual\n3. Intentionally deceive or mislead others, including use of Llama 3.1 related to the following:\n    1. Generating, promoting, or furthering fraud or the creation or promotion of disinformation\n    2. Generating, promoting, or furthering defamatory content, including the creation of defamatory statements, images, or other content\n    3. Generating, promoting, or further distributing spam\n    4. Impersonating another individual without consent, authorization, or legal right\n    5. Representing that the use of Llama 3.1 or outputs are human-generated\n    6. Generating or facilitating false online engagement, including fake reviews and other means of fake online engagement\n4. Fail to appropriately disclose to end users any known dangers of your AI system\nPlease report any violation of this Policy, software “bug,” or other problems that could lead to a violation of this Policy through one of the following means:\n    * Reporting issues with the model: [https://github.com/meta-llama/llama-models/issues](https://github.com/meta-llama/llama-models/issues)\n    * Reporting risky content generated by the model:\n    developers.facebook.com/llama_output_feedback\n    * Reporting bugs and security concerns: facebook.com/whitehat/info\n    * Reporting violations of the Acceptable Use Policy or unlicensed uses of Meta Llama 3: LlamaUseReport@meta.com",
      "extra_gated_fields": {
        "First Name": "text",
        "Last Name": "text",
        "Date of birth": "date_picker",
        "Country": "country",
        "Affiliation": "text",
        "Job title": {
          "type": "select",
          "options": [
            "Student",
            "Research Graduate",
            "AI researcher",
            "AI developer/engineer",
            "Reporter",
            "Other"
          ]
        },
        "geo": "ip_location",
        "By clicking Submit below I accept the terms of the license and acknowledge that the information I provide will be collected stored processed and shared in accordance with the Meta Privacy Policy": "checkbox"
      },
      "extra_gated_description": "The information you provide will be collected, stored, processed and shared in accordance with the [Meta Privacy Policy](https://www.facebook.com/privacy/policy/).",
      "extra_gated_button_content": "Submit"
    },
    "card_text": "\n## Model Information\n\nThe Meta Llama 3.1 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction tuned generative models in 8B, 70B and 405B sizes (text in/text out). The Llama 3.1 instruction tuned text only models (8B, 70B, 405B) are optimized for multilingual dialogue use cases and outperform many of the available open source and closed chat models on common industry benchmarks.\n\n**Model developer**: Meta\n\n**Model Architecture:** Llama 3.1 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety. \n\n\n<table>\n  <tr>\n   <td>\n   </td>\n   <td><strong>Training Data</strong>\n   </td>\n   <td><strong>Params</strong>\n   </td>\n   <td><strong>Input modalities</strong>\n   </td>\n   <td><strong>Output modalities</strong>\n   </td>\n   <td><strong>Context length</strong>\n   </td>\n   <td><strong>GQA</strong>\n   </td>\n   <td><strong>Token count</strong>\n   </td>\n   <td><strong>Knowledge cutoff</strong>\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"3\" >Llama 3.1 (text only)\n   </td>\n   <td rowspan=\"3\" >A new mix of publicly available online data.\n   </td>\n   <td>8B\n   </td>\n   <td>Multilingual Text\n   </td>\n   <td>Multilingual Text and code\n   </td>\n   <td>128k\n   </td>\n   <td>Yes\n   </td>\n   <td rowspan=\"3\" >15T+\n   </td>\n   <td rowspan=\"3\" >December 2023\n   </td>\n  </tr>\n  <tr>\n   <td>70B\n   </td>\n   <td>Multilingual Text\n   </td>\n   <td>Multilingual Text and code\n   </td>\n   <td>128k\n   </td>\n   <td>Yes\n   </td>\n  </tr>\n  <tr>\n   <td>405B\n   </td>\n   <td>Multilingual Text\n   </td>\n   <td>Multilingual Text and code\n   </td>\n   <td>128k\n   </td>\n   <td>Yes\n   </td>\n  </tr>\n</table>\n\n\n**Supported languages:** English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai.\n\n**Llama 3.1 family of models**. Token counts refer to pretraining data only. All model versions use Grouped-Query Attention (GQA) for improved inference scalability.\n\n**Model Release Date:** July 23, 2024.\n\n**Status:** This is a static model trained on an offline dataset. Future versions of the tuned models will be released as we improve model safety with community feedback.\n\n**License:** A custom commercial license, the Llama 3.1 Community License, is available at: [https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/LICENSE](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/LICENSE)\n\nWhere to send questions or comments about the model Instructions on how to provide feedback or comments on the model can be found in the model [README](https://github.com/meta-llama/llama3). For more technical information about generation parameters and recipes for how to use Llama 3.1 in applications, please go [here](https://github.com/meta-llama/llama-recipes). \n\n\n## Intended Use\n\n**Intended Use Cases** Llama 3.1 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks. The Llama 3.1 model collection also supports the ability to leverage the outputs of its models to improve other models including synthetic data generation and distillation. The Llama 3.1 Community License allows for these use cases. \n\n**Out-of-scope** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.1 Community License. Use in languages beyond those explicitly referenced as supported in this model card**.\n\n**<span style=\"text-decoration:underline;\">Note</span>: Llama 3.1 has been trained on a broader collection of languages than the 8 supported languages. Developers may fine-tune Llama 3.1 models for languages beyond the 8 supported languages provided they comply with the Llama 3.1 Community License and the Acceptable Use Policy and in such cases are responsible for ensuring that any uses of Llama 3.1 in additional languages is done in a safe and responsible manner.\n\n## How to use\n\nThis repository contains two versions of Meta-Llama-3.1-70B-Instruct, for use with transformers and with the original `llama` codebase.\n\n### Use with transformers\n\nStarting with `transformers >= 4.43.0` onward, you can run conversational inference using the Transformers `pipeline` abstraction or by leveraging the Auto classes with the `generate()` function.\n\nMake sure to update your transformers installation via `pip install --upgrade transformers`.\n\nSee the snippet below for usage with Transformers:\n\n```python\nimport transformers\nimport torch\n\nmodel_id = \"meta-llama/Meta-Llama-3.1-70B-Instruct\"\n\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model_id,\n    model_kwargs={\"torch_dtype\": torch.bfloat16},\n    device_map=\"auto\",\n)\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n    {\"role\": \"user\", \"content\": \"Who are you?\"},\n]\n\noutputs = pipeline(\n    messages,\n    max_new_tokens=256,\n)\nprint(outputs[0][\"generated_text\"][-1])\n```\n\n### Tool use with transformers\n\nLLaMA-3.1 supports multiple tool use formats. You can see a full guide to prompt formatting [here](https://llama.meta.com/docs/model-cards-and-prompt-formats/llama3_1/).\n\nTool use is also supported through [chat templates](https://huggingface.co/docs/transformers/main/chat_templating#advanced-tool-use--function-calling) in Transformers. \nHere is a quick example showing a single simple tool:\n\n```python\n# First, define a tool\ndef get_current_temperature(location: str) -> float:\n    \"\"\"\n    Get the current temperature at a location.\n    \n    Args:\n        location: The location to get the temperature for, in the format \"City, Country\"\n    Returns:\n        The current temperature at the specified location in the specified units, as a float.\n    \"\"\"\n    return 22.  # A real function should probably actually get the temperature!\n\n# Next, create a chat and apply the chat template\nmessages = [\n  {\"role\": \"system\", \"content\": \"You are a bot that responds to weather queries.\"},\n  {\"role\": \"user\", \"content\": \"Hey, what's the temperature in Paris right now?\"}\n]\n\ninputs = tokenizer.apply_chat_template(messages, tools=[get_current_temperature], add_generation_prompt=True)\n```\n\nYou can then generate text from this input as normal. If the model generates a tool call, you should add it to the chat like so:\n\n```python\ntool_call = {\"name\": \"get_current_temperature\", \"arguments\": {\"location\": \"Paris, France\"}}\nmessages.append({\"role\": \"assistant\", \"tool_calls\": [{\"type\": \"function\", \"function\": tool_call}]})\n```\n\nand then call the tool and append the result, with the `tool` role, like so:\n\n```python\nmessages.append({\"role\": \"tool\", \"name\": \"get_current_temperature\", \"content\": \"22.0\"})\n```\n\nAfter that, you can `generate()` again to let the model use the tool result in the chat. Note that this was a very brief introduction to tool calling - for more information,\nsee the [LLaMA prompt format docs](https://llama.meta.com/docs/model-cards-and-prompt-formats/llama3_1/) and the Transformers [tool use documentation](https://huggingface.co/docs/transformers/main/chat_templating#advanced-tool-use--function-calling).\n\n\n### Use with `bitsandbytes`\n\nThe model checkpoints can be used in `8-bit` and `4-bit` for further memory optimisations using `bitsandbytes` and `transformers`\n\nSee the snippet below for usage:\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_id = \"meta-llama/Meta-Llama-3.1-70B-Instruct\"\nquantization_config = BitsAndBytesConfig(load_in_8bit=True)\n\nquantized_model = AutoModelForCausalLM.from_pretrained(\n\tmodel_id, device_map=\"auto\", torch_dtype=torch.bfloat16, quantization_config=quantization_config)\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\ninput_text = \"What are we having for dinner?\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n\noutput = quantized_model.generate(**input_ids, max_new_tokens=10)\n\nprint(tokenizer.decode(output[0], skip_special_tokens=True))\n```\n\nTo load in 4-bit simply pass `load_in_4bit=True`\n\n### Use with `llama`\n\nPlease, follow the instructions in the [repository](https://github.com/meta-llama/llama).\n\nTo download Original checkpoints, see the example command below leveraging `huggingface-cli`:\n\n```\nhuggingface-cli download meta-llama/Meta-Llama-3.1-70B-Instruct --include \"original/*\" --local-dir Meta-Llama-3.1-70B-Instruct\n```\n\n\n## Hardware and Software\n\n**Training Factors** We used custom training libraries, Meta's custom built GPU cluster, and production infrastructure for pretraining. Fine-tuning, annotation, and evaluation were also performed on production infrastructure.\n\n**Training utilized a cumulative of** 39.3M GPU hours of computation on H100-80GB (TDP of 700W) type hardware, per the table below. Training time is the total GPU time required for training each model and power consumption is the peak power capacity per GPU device used, adjusted for power usage efficiency. \n\n\n**Training Greenhouse Gas Emissions** Estimated total location-based greenhouse gas emissions were **11,390** tons CO2eq for training. Since 2020, Meta has maintained net zero greenhouse gas emissions in its global operations and matched 100% of its electricity use with renewable energy, therefore the total market-based greenhouse gas emissions for training were 0 tons CO2eq.\n\n\n<table>\n  <tr>\n   <td>\n   </td>\n   <td><strong>Training Time (GPU hours)</strong>\n   </td>\n   <td><strong>Training Power Consumption (W)</strong>\n   </td>\n   <td><strong>Training Location-Based Greenhouse Gas Emissions</strong>\n<p>\n<strong>(tons CO2eq)</strong>\n   </td>\n   <td><strong>Training Market-Based Greenhouse Gas Emissions</strong>\n<p>\n<strong>(tons CO2eq)</strong>\n   </td>\n  </tr>\n  <tr>\n   <td>Llama 3.1 8B\n   </td>\n   <td>1.46M\n   </td>\n   <td>700\n   </td>\n   <td>420\n   </td>\n   <td>0\n   </td>\n  </tr>\n  <tr>\n   <td>Llama 3.1 70B\n   </td>\n   <td>7.0M\n   </td>\n   <td>700\n   </td>\n   <td>2,040\n   </td>\n   <td>0\n   </td>\n  </tr>\n  <tr>\n   <td>Llama 3.1 405B\n   </td>\n   <td>30.84M\n   </td>\n   <td>700\n   </td>\n   <td>8,930\n   </td>\n   <td>0\n   </td>\n  </tr>\n  <tr>\n   <td>Total\n   </td>\n   <td>39.3M\n   <td>\n<ul>\n\n</ul>\n   </td>\n   <td>11,390\n   </td>\n   <td>0\n   </td>\n  </tr>\n</table>\n\n\n\nThe methodology used to determine training energy use and greenhouse gas emissions can be found [here](https://arxiv.org/pdf/2204.05149).  Since Meta is openly releasing these models, the training energy use and greenhouse gas emissions  will not be incurred by others.\n\n\n## Training Data\n\n**Overview:** Llama 3.1 was pretrained on ~15 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over 25M synthetically generated examples. \n\n**Data Freshness:** The pretraining data has a cutoff of December 2023.\n\n\n## Benchmark scores\n\nIn this section, we report the results for Llama 3.1 models on standard automatic benchmarks. For all the evaluations, we use our internal evaluations library. \n\n### Base pretrained models\n\n\n<table>\n  <tr>\n   <td><strong>Category</strong>\n   </td>\n   <td><strong>Benchmark</strong>\n   </td>\n   <td><strong># Shots</strong>\n   </td>\n   <td><strong>Metric</strong>\n   </td>\n   <td><strong>Llama 3 8B</strong>\n   </td>\n   <td><strong>Llama 3.1 8B</strong>\n   </td>\n   <td><strong>Llama 3 70B</strong>\n   </td>\n   <td><strong>Llama 3.1 70B</strong>\n   </td>\n   <td><strong>Llama 3.1 405B</strong>\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"7\" >General\n   </td>\n   <td>MMLU\n   </td>\n   <td>5\n   </td>\n   <td>macro_avg/acc_char\n   </td>\n   <td>66.7\n   </td>\n   <td>66.7\n   </td>\n   <td>79.5\n   </td>\n   <td>79.3\n   </td>\n   <td>85.2\n   </td>\n  </tr>\n  <tr>\n   <td>MMLU-Pro (CoT)\n   </td>\n   <td>5\n   </td>\n   <td>macro_avg/acc_char\n   </td>\n   <td>36.2\n   </td>\n   <td>37.1\n   </td>\n   <td>55.0\n   </td>\n   <td>53.8\n   </td>\n   <td>61.6\n   </td>\n  </tr>\n  <tr>\n   <td>AGIEval English\n   </td>\n   <td>3-5\n   </td>\n   <td>average/acc_char\n   </td>\n   <td>47.1\n   </td>\n   <td>47.8\n   </td>\n   <td>63.0\n   </td>\n   <td>64.6\n   </td>\n   <td>71.6\n   </td>\n  </tr>\n  <tr>\n   <td>CommonSenseQA\n   </td>\n   <td>7\n   </td>\n   <td>acc_char\n   </td>\n   <td>72.6\n   </td>\n   <td>75.0\n   </td>\n   <td>83.8\n   </td>\n   <td>84.1\n   </td>\n   <td>85.8\n   </td>\n  </tr>\n  <tr>\n   <td>Winogrande\n   </td>\n   <td>5\n   </td>\n   <td>acc_char\n   </td>\n   <td>-\n   </td>\n   <td>60.5\n   </td>\n   <td>-\n   </td>\n   <td>83.3\n   </td>\n   <td>86.7\n   </td>\n  </tr>\n  <tr>\n   <td>BIG-Bench Hard (CoT)\n   </td>\n   <td>3\n   </td>\n   <td>average/em\n   </td>\n   <td>61.1\n   </td>\n   <td>64.2\n   </td>\n   <td>81.3\n   </td>\n   <td>81.6\n   </td>\n   <td>85.9\n   </td>\n  </tr>\n  <tr>\n   <td>ARC-Challenge\n   </td>\n   <td>25\n   </td>\n   <td>acc_char\n   </td>\n   <td>79.4\n   </td>\n   <td>79.7\n   </td>\n   <td>93.1\n   </td>\n   <td>92.9\n   </td>\n   <td>96.1\n   </td>\n  </tr>\n  <tr>\n   <td>Knowledge reasoning\n   </td>\n   <td>TriviaQA-Wiki\n   </td>\n   <td>5\n   </td>\n   <td>em\n   </td>\n   <td>78.5\n   </td>\n   <td>77.6\n   </td>\n   <td>89.7\n   </td>\n   <td>89.8\n   </td>\n   <td>91.8\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"4\" >Reading comprehension\n   </td>\n   <td>SQuAD\n   </td>\n   <td>1\n   </td>\n   <td>em\n   </td>\n   <td>76.4\n   </td>\n   <td>77.0\n   </td>\n   <td>85.6\n   </td>\n   <td>81.8\n   </td>\n   <td>89.3\n   </td>\n  </tr>\n  <tr>\n   <td>QuAC (F1)\n   </td>\n   <td>1\n   </td>\n   <td>f1\n   </td>\n   <td>44.4\n   </td>\n   <td>44.9\n   </td>\n   <td>51.1\n   </td>\n   <td>51.1\n   </td>\n   <td>53.6\n   </td>\n  </tr>\n  <tr>\n   <td>BoolQ\n   </td>\n   <td>0\n   </td>\n   <td>acc_char\n   </td>\n   <td>75.7\n   </td>\n   <td>75.0\n   </td>\n   <td>79.0\n   </td>\n   <td>79.4\n   </td>\n   <td>80.0\n   </td>\n  </tr>\n  <tr>\n   <td>DROP (F1)\n   </td>\n   <td>3\n   </td>\n   <td>f1\n   </td>\n   <td>58.4\n   </td>\n   <td>59.5\n   </td>\n   <td>79.7\n   </td>\n   <td>79.6\n   </td>\n   <td>84.8\n   </td>\n  </tr>\n</table>\n\n\n\n### Instruction tuned models\n\n\n<table>\n  <tr>\n   <td><strong>Category</strong>\n   </td>\n   <td><strong>Benchmark</strong>\n   </td>\n   <td><strong># Shots</strong>\n   </td>\n   <td><strong>Metric</strong>\n   </td>\n   <td><strong>Llama 3 8B Instruct</strong>\n   </td>\n   <td><strong>Llama 3.1 8B Instruct</strong>\n   </td>\n   <td><strong>Llama 3 70B Instruct</strong>\n   </td>\n   <td><strong>Llama 3.1 70B Instruct</strong>\n   </td>\n   <td><strong>Llama 3.1 405B Instruct</strong>\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"4\" >General\n   </td>\n   <td>MMLU\n   </td>\n   <td>5\n   </td>\n   <td>macro_avg/acc\n   </td>\n   <td>68.5\n   </td>\n   <td>69.4\n   </td>\n   <td>82.0\n   </td>\n   <td>83.6\n   </td>\n   <td>87.3\n   </td>\n  </tr>\n  <tr>\n   <td>MMLU (CoT)\n   </td>\n   <td>0\n   </td>\n   <td>macro_avg/acc\n   </td>\n   <td>65.3\n   </td>\n   <td>73.0\n   </td>\n   <td>80.9\n   </td>\n   <td>86.0\n   </td>\n   <td>88.6\n   </td>\n  </tr>\n  <tr>\n   <td>MMLU-Pro (CoT)\n   </td>\n   <td>5\n   </td>\n   <td>micro_avg/acc_char\n   </td>\n   <td>45.5\n   </td>\n   <td>48.3\n   </td>\n   <td>63.4\n   </td>\n   <td>66.4\n   </td>\n   <td>73.3\n   </td>\n  </tr>\n  <tr>\n   <td>IFEval\n   </td>\n   <td>\n   </td>\n   <td>\n   </td>\n   <td>76.8\n   </td>\n   <td>80.4\n   </td>\n   <td>82.9\n   </td>\n   <td>87.5\n   </td>\n   <td>88.6\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"2\" >Reasoning\n   </td>\n   <td>ARC-C\n   </td>\n   <td>0\n   </td>\n   <td>acc\n   </td>\n   <td>82.4\n   </td>\n   <td>83.4\n   </td>\n   <td>94.4\n   </td>\n   <td>94.8\n   </td>\n   <td>96.9\n   </td>\n  </tr>\n  <tr>\n   <td>GPQA\n   </td>\n   <td>0\n   </td>\n   <td>em\n   </td>\n   <td>34.6\n   </td>\n   <td>30.4\n   </td>\n   <td>39.5\n   </td>\n   <td>46.7\n   </td>\n   <td>50.7\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"4\" >Code\n   </td>\n   <td>HumanEval\n   </td>\n   <td>0\n   </td>\n   <td>pass@1\n   </td>\n   <td>60.4\n   </td>\n   <td>72.6\n   </td>\n   <td>81.7\n   </td>\n   <td>80.5\n   </td>\n   <td>89.0\n   </td>\n  </tr>\n  <tr>\n   <td>MBPP ++ base version\n   </td>\n   <td>0\n   </td>\n   <td>pass@1\n   </td>\n   <td>70.6\n   </td>\n   <td>72.8\n   </td>\n   <td>82.5\n   </td>\n   <td>86.0\n   </td>\n   <td>88.6\n   </td>\n  </tr>\n  <tr>\n   <td>Multipl-E HumanEval\n   </td>\n   <td>0\n   </td>\n   <td>pass@1\n   </td>\n   <td>-\n   </td>\n   <td>50.8\n   </td>\n   <td>-\n   </td>\n   <td>65.5\n   </td>\n   <td>75.2\n   </td>\n  </tr>\n  <tr>\n   <td>Multipl-E MBPP\n   </td>\n   <td>0\n   </td>\n   <td>pass@1\n   </td>\n   <td>-\n   </td>\n   <td>52.4\n   </td>\n   <td>-\n   </td>\n   <td>62.0\n   </td>\n   <td>65.7\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"2\" >Math\n   </td>\n   <td>GSM-8K (CoT)\n   </td>\n   <td>8\n   </td>\n   <td>em_maj1@1\n   </td>\n   <td>80.6\n   </td>\n   <td>84.5\n   </td>\n   <td>93.0\n   </td>\n   <td>95.1\n   </td>\n   <td>96.8\n   </td>\n  </tr>\n  <tr>\n   <td>MATH (CoT)\n   </td>\n   <td>0\n   </td>\n   <td>final_em\n   </td>\n   <td>29.1\n   </td>\n   <td>51.9\n   </td>\n   <td>51.0\n   </td>\n   <td>68.0\n   </td>\n   <td>73.8\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"4\" >Tool Use\n   </td>\n   <td>API-Bank\n   </td>\n   <td>0\n   </td>\n   <td>acc\n   </td>\n   <td>48.3\n   </td>\n   <td>82.6\n   </td>\n   <td>85.1\n   </td>\n   <td>90.0\n   </td>\n   <td>92.0\n   </td>\n  </tr>\n  <tr>\n   <td>BFCL\n   </td>\n   <td>0\n   </td>\n   <td>acc\n   </td>\n   <td>60.3\n   </td>\n   <td>76.1\n   </td>\n   <td>83.0\n   </td>\n   <td>84.8\n   </td>\n   <td>88.5\n   </td>\n  </tr>\n  <tr>\n   <td>Gorilla Benchmark API Bench\n   </td>\n   <td>0\n   </td>\n   <td>acc\n   </td>\n   <td>1.7\n   </td>\n   <td>8.2\n   </td>\n   <td>14.7\n   </td>\n   <td>29.7\n   </td>\n   <td>35.3\n   </td>\n  </tr>\n  <tr>\n   <td>Nexus (0-shot)\n   </td>\n   <td>0\n   </td>\n   <td>macro_avg/acc\n   </td>\n   <td>18.1\n   </td>\n   <td>38.5\n   </td>\n   <td>47.8\n   </td>\n   <td>56.7\n   </td>\n   <td>58.7\n   </td>\n  </tr>\n  <tr>\n   <td>Multilingual\n   </td>\n   <td>Multilingual MGSM (CoT)\n   </td>\n   <td>0\n   </td>\n   <td>em\n   </td>\n   <td>-\n   </td>\n   <td>68.9\n   </td>\n   <td>-\n   </td>\n   <td>86.9\n   </td>\n   <td>91.6\n   </td>\n  </tr>\n</table>\n\n#### Multilingual benchmarks\n\n<table>\n  <tr>\n   <td><strong>Category</strong>\n   </td>\n   <td><strong>Benchmark</strong>\n   </td>\n   <td><strong>Language</strong>\n   </td>\n   <td><strong>Llama 3.1 8B</strong>\n   </td>\n   <td><strong>Llama 3.1 70B</strong>\n   </td>\n   <td><strong>Llama 3.1 405B</strong>\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"9\" ><strong>General</strong>\n   </td>\n   <td rowspan=\"9\" ><strong>MMLU (5-shot, macro_avg/acc)</strong>\n   </td>\n   <td>Portuguese\n   </td>\n   <td>62.12\n   </td>\n   <td>80.13\n   </td>\n   <td>84.95\n   </td>\n  </tr>\n  <tr>\n   <td>Spanish\n   </td>\n   <td>62.45\n   </td>\n   <td>80.05\n   </td>\n   <td>85.08\n   </td>\n  </tr>\n  <tr>\n   <td>Italian\n   </td>\n   <td>61.63\n   </td>\n   <td>80.4\n   </td>\n   <td>85.04\n   </td>\n  </tr>\n  <tr>\n   <td>German\n   </td>\n   <td>60.59\n   </td>\n   <td>79.27\n   </td>\n   <td>84.36\n   </td>\n  </tr>\n  <tr>\n   <td>French\n   </td>\n   <td>62.34\n   </td>\n   <td>79.82\n   </td>\n   <td>84.66\n   </td>\n  </tr>\n  <tr>\n   <td>Hindi\n   </td>\n   <td>50.88\n   </td>\n   <td>74.52\n   </td>\n   <td>80.31\n   </td>\n  </tr>\n  <tr>\n   <td>Thai\n   </td>\n   <td>50.32\n   </td>\n   <td>72.95\n   </td>\n   <td>78.21\n   </td>\n  </tr>\n</table>\n\n\n\n## Responsibility & Safety\n\nAs part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:\n\n\n\n* Enable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama. \n* Protect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm.\n* Provide protections for the community to help prevent the misuse of our models.\n\n\n### Responsible deployment \n\nLlama is a foundational technology designed to be used in a variety of use cases, examples on how Meta’s Llama models have been responsibly deployed can be found in our [Community Stories webpage](https://llama.meta.com/community-stories/). Our approach is to build the most helpful models enabling the world to benefit from the technology power, by aligning our model safety for the generic use cases addressing a standard set of harms. Developers are then in the driver seat to tailor safety for their use case, defining their own policy and deploying the models with the necessary safeguards in their Llama systems. Llama 3.1 was developed following the best practices outlined in our Responsible Use Guide, you can refer to the [Responsible Use Guide](https://llama.meta.com/responsible-use-guide/) to learn more. \n\n\n#### Llama 3.1 instruct \n\nOur main objectives for conducting safety fine-tuning are to provide the research community with a valuable resource for studying the robustness of safety fine-tuning, as well as to offer developers a readily available, safe, and powerful model for various applications to reduce the developer workload to deploy safe AI systems. For more details on the safety mitigations implemented please read the Llama 3 paper. \n\n**Fine-tuning data**\n\nWe employ a multi-faceted approach to data collection, combining human-generated data from our vendors with synthetic data to mitigate potential safety risks. We’ve developed many large language model (LLM)-based classifiers that enable us to thoughtfully select high-quality prompts and responses, enhancing data quality control. \n\n**Refusals and Tone**\n\nBuilding on the work we started with Llama 3, we put a great emphasis on model refusals to benign prompts as well as refusal tone. We included both borderline and adversarial prompts in our safety data strategy, and modified our safety data responses to follow  tone guidelines. \n\n\n#### Llama 3.1 systems\n\n**Large language models, including Llama 3.1, are not designed to be deployed in isolation but instead should be deployed as part of an overall AI system with additional safety guardrails as required.** Developers are expected to deploy system safeguards when building agentic systems. Safeguards are key to achieve the right helpfulness-safety alignment as well as mitigating safety and security risks inherent to the system and any integration of the model or system with external tools. \n\nAs part of our responsible release approach, we provide the community with [safeguards](https://llama.meta.com/trust-and-safety/) that developers should deploy with Llama models or other LLMs, including Llama Guard 3, Prompt Guard and Code Shield. All our [reference implementations](https://github.com/meta-llama/llama-agentic-system) demos contain these safeguards by default so developers can benefit from system-level safety out-of-the-box. \n\n\n#### New capabilities \n\nNote that this release introduces new capabilities, including a longer context window, multilingual inputs and outputs and possible integrations by developers with third party tools. Building with these new capabilities requires specific considerations in addition to the best practices that generally apply across all Generative AI use cases.\n\n**Tool-use**: Just like in standard software development, developers are responsible for the integration of the LLM with the tools and services of their choice. They should define a clear policy for their use case and assess the integrity of the third party services they use to be aware of the safety and security limitations when using this capability. Refer to the Responsible Use Guide for best practices on the safe deployment of the third party safeguards. \n\n**Multilinguality**: Llama 3.1 supports 7 languages in addition to English: French, German, Hindi, Italian, Portuguese, Spanish, and Thai. Llama may be able to output text in other languages than those that meet performance thresholds for safety and helpfulness. We strongly discourage developers from using this model to converse in non-supported languages without implementing finetuning and system controls in alignment with their policies and the best practices shared in the Responsible Use Guide. \n\n\n### Evaluations\n\nWe evaluated Llama models for common use cases as well as specific capabilities. Common use cases evaluations measure safety risks of systems for most commonly built applications including chat bot, coding assistant, tool calls. We built dedicated, adversarial evaluation datasets and evaluated systems composed of Llama models and Llama Guard 3 to filter input prompt and output response. It is important to evaluate applications in context, and we recommend building dedicated evaluation dataset for your use case. Prompt Guard and Code Shield are also available if relevant to the application. \n\nCapability evaluations measure vulnerabilities of Llama models inherent to specific capabilities, for which were crafted dedicated benchmarks including long context, multilingual, tools calls, coding or memorization.\n\n**Red teaming**\n\nFor both scenarios, we conducted recurring red teaming exercises with the goal of discovering risks via adversarial prompting and we used the learnings to improve our benchmarks and safety tuning datasets. \n\nWe partnered early with subject-matter experts in critical risk areas to understand the nature of these real-world harms and how such models may lead to unintended harm for society. Based on these conversations, we derived a set of adversarial goals for the red team to attempt to achieve, such as extracting harmful information or reprogramming the model to act in a potentially harmful capacity.  The red team consisted of experts in cybersecurity, adversarial machine learning, responsible AI, and integrity in addition to multilingual content specialists with background in integrity issues in specific geographic markets.\n\n\n### Critical and other risks \n\nWe specifically focused our efforts on mitigating the following critical risk areas:\n\n**1- CBRNE (Chemical, Biological, Radiological, Nuclear, and Explosive materials) helpfulness**\n\nTo assess risks related to proliferation of chemical and biological weapons, we performed uplift testing designed to assess whether use of Llama 3.1 models could meaningfully increase the capabilities of malicious actors to plan or carry out attacks using these types of weapons. \n\n\n**2. Child Safety**\n\nChild Safety risk assessments were conducted using a team of experts, to assess the model’s capability to produce outputs that could result in Child Safety risks and inform on any necessary and appropriate risk mitigations via fine tuning. We leveraged those expert red teaming sessions to expand the coverage of our evaluation benchmarks through Llama 3 model development.  For Llama 3, we conducted new in-depth sessions using objective based methodologies to assess the model risks along multiple attack vectors including the additional languages Llama 3 is trained on. We also partnered with content specialists to perform red teaming exercises assessing potentially violating content while taking account of market specific nuances or experiences. \n\n**3. Cyber attack enablement**\n\nOur cyber attack uplift study investigated whether LLMs can enhance human capabilities in hacking tasks, both in terms of skill level and speed.\n\nOur attack automation study focused on evaluating the capabilities of LLMs when used as autonomous agents in cyber offensive operations, specifically in the context of ransomware attacks. This evaluation was distinct from previous studies that considered LLMs as interactive assistants. The primary objective was to assess whether these models could effectively function as independent agents in executing complex cyber-attacks without human intervention.\n\nOur study of Llama-3.1-405B’s social engineering uplift for cyber attackers was conducted to assess the effectiveness of AI models in aiding cyber threat actors in spear phishing campaigns. Please read our Llama 3.1 Cyber security whitepaper to learn more.\n\n\n### Community \n\nGenerative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership on AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Purple Llama tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to our [Github repository](https://github.com/meta-llama/PurpleLlama). \n\nWe also set up the [Llama Impact Grants](https://llama.meta.com/llama-impact-grants/) program to identify and support the most compelling applications of Meta’s Llama model for societal benefit across three categories: education, climate and open innovation. The 20 finalists from the hundreds of applications can be found [here](https://llama.meta.com/llama-impact-grants/#finalists). \n\nFinally, we put in place a set of resources including an [output reporting mechanism](https://developers.facebook.com/llama_output_feedback) and [bug bounty program](https://www.facebook.com/whitehat) to continuously improve the Llama technology with the help of the community.\n\n\n## Ethical Considerations and Limitations\n\nThe core values of Llama 3.1 are openness, inclusivity and helpfulness. It is meant to serve everyone, and to work for a wide range of use cases. It is thus designed to be accessible to people across many different backgrounds, experiences and perspectives. Llama 3.1 addresses users and their needs as they are, without insertion unnecessary judgment or normativity, while reflecting the understanding that even content that may appear problematic in some cases can serve valuable purposes in others. It respects the dignity and autonomy of all users, especially in terms of the values of free thought and expression that power innovation and progress. \n\nBut Llama 3.1 is a new technology, and like any new technology, there are risks associated with its use. Testing conducted to date has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, Llama 3.1’s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 3.1 models, developers should perform safety testing and tuning tailored to their specific applications of the model. Please refer to available resources including our [Responsible Use Guide](https://llama.meta.com/responsible-use-guide), [Trust and Safety](https://llama.meta.com/trust-and-safety/) solutions, and other [resources](https://llama.meta.com/docs/get-started/) to learn more about responsible development.",
    "card_content": "---\nlanguage:\n- en\n- de\n- fr\n- it\n- pt\n- hi\n- es\n- th\nlibrary_name: transformers\nbase_model: meta-llama/Meta-Llama-3.1-70B\nnew_version: meta-llama/Llama-3.3-70B-Instruct\nlicense: llama3.1\npipeline_tag: text-generation\ntags:\n- facebook\n- meta\n- pytorch\n- llama\n- llama-3\nextra_gated_prompt: \"### LLAMA 3.1 COMMUNITY LICENSE AGREEMENT\\nLlama 3.1 Version\\\n  \\ Release Date: July 23, 2024\\n\\\"Agreement\\\" means the terms and conditions for\\\n  \\ use, reproduction, distribution and modification of the  Llama Materials set forth\\\n  \\ herein.\\n\\\"Documentation\\\" means the specifications, manuals and documentation\\\n  \\ accompanying Llama 3.1 distributed by Meta at https://llama.meta.com/doc/overview.\\n\\\n  \\\"Licensee\\\" or \\\"you\\\" means you, or your employer or any other person or entity\\\n  \\ (if you are entering into this Agreement on such person or entity’s behalf), of\\\n  \\ the age required under applicable laws, rules or regulations to provide legal\\\n  \\ consent and that has legal authority to bind your employer or such other person\\\n  \\ or entity if you are entering in this Agreement on their behalf.\\n\\\"Llama 3.1\\\"\\\n  \\ means the foundational large language models and software and algorithms, including\\\n  \\ machine-learning model code, trained model weights, inference-enabling code, training-enabling\\\n  \\ code, fine-tuning enabling code and other elements of the foregoing distributed\\\n  \\ by Meta at https://llama.meta.com/llama-downloads.\\n\\\"Llama Materials\\\" means,\\\n  \\ collectively, Meta’s proprietary Llama 3.1 and Documentation (and any portion\\\n  \\ thereof) made available under this Agreement.\\n\\\"Meta\\\" or \\\"we\\\" means Meta Platforms\\\n  \\ Ireland Limited (if you are located in or, if you are an entity, your principal\\\n  \\ place of business is in the EEA or Switzerland) and Meta Platforms, Inc. (if you\\\n  \\ are located outside of the EEA or Switzerland).\\n   \\n1. License Rights and Redistribution.\\n\\\n  a. Grant of Rights. You are granted a non-exclusive, worldwide, non-transferable\\\n  \\ and royalty-free limited license under Meta’s intellectual property or other rights\\\n  \\ owned by Meta embodied in the Llama Materials to use, reproduce, distribute, copy,\\\n  \\ create derivative works of, and make modifications to the Llama Materials.\\nb.\\\n  \\ Redistribution and Use.\\ni. If you distribute or make available the Llama Materials\\\n  \\ (or any derivative works thereof), or a product or service (including another\\\n  \\ AI model) that contains any of them, you shall (A) provide a copy of this Agreement\\\n  \\ with any such Llama Materials; and (B) prominently display “Built with Llama”\\\n  \\ on a related website, user interface, blogpost, about page, or product documentation.\\\n  \\ If you use the Llama Materials or any outputs or results of the Llama Materials\\\n  \\ to create, train, fine tune, or otherwise improve an AI model, which is distributed\\\n  \\ or made available, you shall also include “Llama” at the beginning of any such\\\n  \\ AI model name.\\nii. If you receive Llama Materials, or any derivative works thereof,\\\n  \\ from a Licensee as part  of an integrated end user product, then Section 2 of\\\n  \\ this Agreement will not apply to you.\\niii. You must retain in all copies of the\\\n  \\ Llama Materials that you distribute the following attribution notice within a\\\n  \\ “Notice” text file distributed as a part of such copies: “Llama 3.1 is licensed\\\n  \\ under the Llama 3.1 Community License, Copyright © Meta Platforms, Inc. All Rights\\\n  \\ Reserved.”\\niv. Your use of the Llama Materials must comply with applicable laws\\\n  \\ and regulations (including trade compliance laws and regulations) and adhere to\\\n  \\ the Acceptable Use Policy for the Llama Materials (available at https://llama.meta.com/llama3_1/use-policy),\\\n  \\ which is hereby incorporated by reference into this Agreement.\\n2. Additional\\\n  \\ Commercial Terms. If, on the Llama 3.1 version release date, the monthly active\\\n  \\ users of the products or services made available by or for Licensee, or Licensee’s\\\n  \\ affiliates, is greater than 700 million monthly active users in the preceding\\\n  \\ calendar month, you must request a license from Meta, which Meta may grant to\\\n  \\ you in its sole discretion, and you are not authorized to exercise any of the\\\n  \\ rights under this Agreement unless or until Meta otherwise expressly grants you\\\n  \\ such rights.\\n3. Disclaimer of Warranty. UNLESS REQUIRED BY APPLICABLE LAW, THE\\\n  \\ LLAMA MATERIALS AND ANY OUTPUT AND RESULTS THEREFROM ARE PROVIDED ON AN “AS IS”\\\n  \\ BASIS, WITHOUT WARRANTIES OF ANY KIND, AND META DISCLAIMS ALL WARRANTIES OF ANY\\\n  \\ KIND, BOTH EXPRESS AND IMPLIED, INCLUDING, WITHOUT LIMITATION, ANY WARRANTIES\\\n  \\ OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.\\\n  \\ YOU ARE SOLELY RESPONSIBLE FOR DETERMINING THE APPROPRIATENESS OF USING OR REDISTRIBUTING\\\n  \\ THE LLAMA MATERIALS AND ASSUME ANY RISKS ASSOCIATED WITH YOUR USE OF THE LLAMA\\\n  \\ MATERIALS AND ANY OUTPUT AND RESULTS.\\n4. Limitation of Liability. IN NO EVENT\\\n  \\ WILL META OR ITS AFFILIATES BE LIABLE UNDER ANY THEORY OF LIABILITY, WHETHER IN\\\n  \\ CONTRACT, TORT, NEGLIGENCE, PRODUCTS LIABILITY, OR OTHERWISE, ARISING OUT OF THIS\\\n  \\ AGREEMENT, FOR ANY LOST PROFITS OR ANY INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL,\\\n  \\ EXEMPLARY OR PUNITIVE DAMAGES, EVEN IF META OR ITS AFFILIATES HAVE BEEN ADVISED\\\n  \\ OF THE POSSIBILITY OF ANY OF THE FOREGOING.\\n5. Intellectual Property.\\na. No\\\n  \\ trademark licenses are granted under this Agreement, and in connection with the\\\n  \\ Llama Materials, neither Meta nor Licensee may use any name or mark owned by or\\\n  \\ associated with the other or any of its affiliates, except as required for reasonable\\\n  \\ and customary use in describing and redistributing the Llama Materials or as set\\\n  \\ forth in this Section 5(a). Meta hereby grants you a license to use “Llama” (the\\\n  \\ “Mark”) solely as required to comply with the last sentence of Section 1.b.i.\\\n  \\ You will comply with Meta’s brand guidelines (currently accessible at https://about.meta.com/brand/resources/meta/company-brand/\\\n  \\ ). All goodwill arising out of your use of the Mark will inure to the benefit\\\n  \\ of Meta.\\nb. Subject to Meta’s ownership of Llama Materials and derivatives made\\\n  \\ by or for Meta, with respect to any derivative works and modifications of the\\\n  \\ Llama Materials that are made by you, as between you and Meta, you are and will\\\n  \\ be the owner of such derivative works and modifications.\\nc. If you institute\\\n  \\ litigation or other proceedings against Meta or any entity (including a cross-claim\\\n  \\ or counterclaim in a lawsuit) alleging that the Llama Materials or Llama 3.1 outputs\\\n  \\ or results, or any portion of any of the foregoing, constitutes infringement of\\\n  \\ intellectual property or other rights owned or licensable by you, then any licenses\\\n  \\ granted to you under this Agreement shall terminate as of the date such litigation\\\n  \\ or claim is filed or instituted. You will indemnify and hold harmless Meta from\\\n  \\ and against any claim by any third party arising out of or related to your use\\\n  \\ or distribution of the Llama Materials.\\n6. Term and Termination. The term of\\\n  \\ this Agreement will commence upon your acceptance of this Agreement or access\\\n  \\ to the Llama Materials and will continue in full force and effect until terminated\\\n  \\ in accordance with the terms and conditions herein. Meta may terminate this Agreement\\\n  \\ if you are in breach of any term or condition of this Agreement. Upon termination\\\n  \\ of this Agreement, you shall delete and cease use of the Llama Materials. Sections\\\n  \\ 3, 4 and 7 shall survive the termination of this Agreement.\\n7. Governing Law\\\n  \\ and Jurisdiction. This Agreement will be governed and construed under the laws\\\n  \\ of the State of California without regard to choice of law principles, and the\\\n  \\ UN Convention on Contracts for the International Sale of Goods does not apply\\\n  \\ to this Agreement. The courts of California shall have exclusive jurisdiction\\\n  \\ of any dispute arising out of this Agreement.\\n### Llama 3.1 Acceptable Use Policy\\n\\\n  Meta is committed to promoting safe and fair use of its tools and features, including\\\n  \\ Llama 3.1. If you access or use Llama 3.1, you agree to this Acceptable Use Policy\\\n  \\ (“Policy”). The most recent copy of this policy can be found at [https://llama.meta.com/llama3_1/use-policy](https://llama.meta.com/llama3_1/use-policy)\\n\\\n  #### Prohibited Uses\\nWe want everyone to use Llama 3.1 safely and responsibly.\\\n  \\ You agree you will not use, or allow others to use, Llama 3.1 to:\\n 1. Violate\\\n  \\ the law or others’ rights, including to:\\n    1. Engage in, promote, generate,\\\n  \\ contribute to, encourage, plan, incite, or further illegal or unlawful activity\\\n  \\ or content, such as:\\n        1. Violence or terrorism\\n        2. Exploitation\\\n  \\ or harm to children, including the solicitation, creation, acquisition, or dissemination\\\n  \\ of child exploitative content or failure to report Child Sexual Abuse Material\\n\\\n  \\        3. Human trafficking, exploitation, and sexual violence\\n        4. The\\\n  \\ illegal distribution of information or materials to minors, including obscene\\\n  \\ materials, or failure to employ legally required age-gating in connection with\\\n  \\ such information or materials.\\n        5. Sexual solicitation\\n        6. Any\\\n  \\ other criminal activity\\n    3. Engage in, promote, incite, or facilitate the\\\n  \\ harassment, abuse, threatening, or bullying of individuals or groups of individuals\\n\\\n  \\    4. Engage in, promote, incite, or facilitate discrimination or other unlawful\\\n  \\ or harmful conduct in the provision of employment, employment benefits, credit,\\\n  \\ housing, other economic benefits, or other essential goods and services\\n    5.\\\n  \\ Engage in the unauthorized or unlicensed practice of any profession including,\\\n  \\ but not limited to, financial, legal, medical/health, or related professional\\\n  \\ practices\\n    6. Collect, process, disclose, generate, or infer health, demographic,\\\n  \\ or other sensitive personal or private information about individuals without rights\\\n  \\ and consents required by applicable laws\\n    7. Engage in or facilitate any action\\\n  \\ or generate any content that infringes, misappropriates, or otherwise violates\\\n  \\ any third-party rights, including the outputs or results of any products or services\\\n  \\ using the Llama Materials\\n    8. Create, generate, or facilitate the creation\\\n  \\ of malicious code, malware, computer viruses or do anything else that could disable,\\\n  \\ overburden, interfere with or impair the proper working, integrity, operation\\\n  \\ or appearance of a website or computer system\\n2. Engage in, promote, incite,\\\n  \\ facilitate, or assist in the planning or development of activities that present\\\n  \\ a risk of death or bodily harm to individuals, including use of Llama 3.1 related\\\n  \\ to the following:\\n    1. Military, warfare, nuclear industries or applications,\\\n  \\ espionage, use for materials or activities that are subject to the International\\\n  \\ Traffic Arms Regulations (ITAR) maintained by the United States Department of\\\n  \\ State\\n    2. Guns and illegal weapons (including weapon development)\\n    3.\\\n  \\ Illegal drugs and regulated/controlled substances\\n    4. Operation of critical\\\n  \\ infrastructure, transportation technologies, or heavy machinery\\n    5. Self-harm\\\n  \\ or harm to others, including suicide, cutting, and eating disorders\\n    6. Any\\\n  \\ content intended to incite or promote violence, abuse, or any infliction of bodily\\\n  \\ harm to an individual\\n3. Intentionally deceive or mislead others, including use\\\n  \\ of Llama 3.1 related to the following:\\n    1. Generating, promoting, or furthering\\\n  \\ fraud or the creation or promotion of disinformation\\n    2. Generating, promoting,\\\n  \\ or furthering defamatory content, including the creation of defamatory statements,\\\n  \\ images, or other content\\n    3. Generating, promoting, or further distributing\\\n  \\ spam\\n    4. Impersonating another individual without consent, authorization,\\\n  \\ or legal right\\n    5. Representing that the use of Llama 3.1 or outputs are human-generated\\n\\\n  \\    6. Generating or facilitating false online engagement, including fake reviews\\\n  \\ and other means of fake online engagement\\n4. Fail to appropriately disclose to\\\n  \\ end users any known dangers of your AI system\\nPlease report any violation of\\\n  \\ this Policy, software “bug,” or other problems that could lead to a violation\\\n  \\ of this Policy through one of the following means:\\n    * Reporting issues with\\\n  \\ the model: [https://github.com/meta-llama/llama-models/issues](https://github.com/meta-llama/llama-models/issues)\\n\\\n  \\    * Reporting risky content generated by the model:\\n    developers.facebook.com/llama_output_feedback\\n\\\n  \\    * Reporting bugs and security concerns: facebook.com/whitehat/info\\n    * Reporting\\\n  \\ violations of the Acceptable Use Policy or unlicensed uses of Meta Llama 3: LlamaUseReport@meta.com\"\nextra_gated_fields:\n  First Name: text\n  Last Name: text\n  Date of birth: date_picker\n  Country: country\n  Affiliation: text\n  Job title:\n    type: select\n    options:\n    - Student\n    - Research Graduate\n    - AI researcher\n    - AI developer/engineer\n    - Reporter\n    - Other\n  geo: ip_location\n  ? By clicking Submit below I accept the terms of the license and acknowledge that\n    the information I provide will be collected stored processed and shared in accordance\n    with the Meta Privacy Policy\n  : checkbox\nextra_gated_description: The information you provide will be collected, stored, processed\n  and shared in accordance with the [Meta Privacy Policy](https://www.facebook.com/privacy/policy/).\nextra_gated_button_content: Submit\n---\n\n## Model Information\n\nThe Meta Llama 3.1 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction tuned generative models in 8B, 70B and 405B sizes (text in/text out). The Llama 3.1 instruction tuned text only models (8B, 70B, 405B) are optimized for multilingual dialogue use cases and outperform many of the available open source and closed chat models on common industry benchmarks.\n\n**Model developer**: Meta\n\n**Model Architecture:** Llama 3.1 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety. \n\n\n<table>\n  <tr>\n   <td>\n   </td>\n   <td><strong>Training Data</strong>\n   </td>\n   <td><strong>Params</strong>\n   </td>\n   <td><strong>Input modalities</strong>\n   </td>\n   <td><strong>Output modalities</strong>\n   </td>\n   <td><strong>Context length</strong>\n   </td>\n   <td><strong>GQA</strong>\n   </td>\n   <td><strong>Token count</strong>\n   </td>\n   <td><strong>Knowledge cutoff</strong>\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"3\" >Llama 3.1 (text only)\n   </td>\n   <td rowspan=\"3\" >A new mix of publicly available online data.\n   </td>\n   <td>8B\n   </td>\n   <td>Multilingual Text\n   </td>\n   <td>Multilingual Text and code\n   </td>\n   <td>128k\n   </td>\n   <td>Yes\n   </td>\n   <td rowspan=\"3\" >15T+\n   </td>\n   <td rowspan=\"3\" >December 2023\n   </td>\n  </tr>\n  <tr>\n   <td>70B\n   </td>\n   <td>Multilingual Text\n   </td>\n   <td>Multilingual Text and code\n   </td>\n   <td>128k\n   </td>\n   <td>Yes\n   </td>\n  </tr>\n  <tr>\n   <td>405B\n   </td>\n   <td>Multilingual Text\n   </td>\n   <td>Multilingual Text and code\n   </td>\n   <td>128k\n   </td>\n   <td>Yes\n   </td>\n  </tr>\n</table>\n\n\n**Supported languages:** English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai.\n\n**Llama 3.1 family of models**. Token counts refer to pretraining data only. All model versions use Grouped-Query Attention (GQA) for improved inference scalability.\n\n**Model Release Date:** July 23, 2024.\n\n**Status:** This is a static model trained on an offline dataset. Future versions of the tuned models will be released as we improve model safety with community feedback.\n\n**License:** A custom commercial license, the Llama 3.1 Community License, is available at: [https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/LICENSE](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/LICENSE)\n\nWhere to send questions or comments about the model Instructions on how to provide feedback or comments on the model can be found in the model [README](https://github.com/meta-llama/llama3). For more technical information about generation parameters and recipes for how to use Llama 3.1 in applications, please go [here](https://github.com/meta-llama/llama-recipes). \n\n\n## Intended Use\n\n**Intended Use Cases** Llama 3.1 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks. The Llama 3.1 model collection also supports the ability to leverage the outputs of its models to improve other models including synthetic data generation and distillation. The Llama 3.1 Community License allows for these use cases. \n\n**Out-of-scope** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.1 Community License. Use in languages beyond those explicitly referenced as supported in this model card**.\n\n**<span style=\"text-decoration:underline;\">Note</span>: Llama 3.1 has been trained on a broader collection of languages than the 8 supported languages. Developers may fine-tune Llama 3.1 models for languages beyond the 8 supported languages provided they comply with the Llama 3.1 Community License and the Acceptable Use Policy and in such cases are responsible for ensuring that any uses of Llama 3.1 in additional languages is done in a safe and responsible manner.\n\n## How to use\n\nThis repository contains two versions of Meta-Llama-3.1-70B-Instruct, for use with transformers and with the original `llama` codebase.\n\n### Use with transformers\n\nStarting with `transformers >= 4.43.0` onward, you can run conversational inference using the Transformers `pipeline` abstraction or by leveraging the Auto classes with the `generate()` function.\n\nMake sure to update your transformers installation via `pip install --upgrade transformers`.\n\nSee the snippet below for usage with Transformers:\n\n```python\nimport transformers\nimport torch\n\nmodel_id = \"meta-llama/Meta-Llama-3.1-70B-Instruct\"\n\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model_id,\n    model_kwargs={\"torch_dtype\": torch.bfloat16},\n    device_map=\"auto\",\n)\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n    {\"role\": \"user\", \"content\": \"Who are you?\"},\n]\n\noutputs = pipeline(\n    messages,\n    max_new_tokens=256,\n)\nprint(outputs[0][\"generated_text\"][-1])\n```\n\n### Tool use with transformers\n\nLLaMA-3.1 supports multiple tool use formats. You can see a full guide to prompt formatting [here](https://llama.meta.com/docs/model-cards-and-prompt-formats/llama3_1/).\n\nTool use is also supported through [chat templates](https://huggingface.co/docs/transformers/main/chat_templating#advanced-tool-use--function-calling) in Transformers. \nHere is a quick example showing a single simple tool:\n\n```python\n# First, define a tool\ndef get_current_temperature(location: str) -> float:\n    \"\"\"\n    Get the current temperature at a location.\n    \n    Args:\n        location: The location to get the temperature for, in the format \"City, Country\"\n    Returns:\n        The current temperature at the specified location in the specified units, as a float.\n    \"\"\"\n    return 22.  # A real function should probably actually get the temperature!\n\n# Next, create a chat and apply the chat template\nmessages = [\n  {\"role\": \"system\", \"content\": \"You are a bot that responds to weather queries.\"},\n  {\"role\": \"user\", \"content\": \"Hey, what's the temperature in Paris right now?\"}\n]\n\ninputs = tokenizer.apply_chat_template(messages, tools=[get_current_temperature], add_generation_prompt=True)\n```\n\nYou can then generate text from this input as normal. If the model generates a tool call, you should add it to the chat like so:\n\n```python\ntool_call = {\"name\": \"get_current_temperature\", \"arguments\": {\"location\": \"Paris, France\"}}\nmessages.append({\"role\": \"assistant\", \"tool_calls\": [{\"type\": \"function\", \"function\": tool_call}]})\n```\n\nand then call the tool and append the result, with the `tool` role, like so:\n\n```python\nmessages.append({\"role\": \"tool\", \"name\": \"get_current_temperature\", \"content\": \"22.0\"})\n```\n\nAfter that, you can `generate()` again to let the model use the tool result in the chat. Note that this was a very brief introduction to tool calling - for more information,\nsee the [LLaMA prompt format docs](https://llama.meta.com/docs/model-cards-and-prompt-formats/llama3_1/) and the Transformers [tool use documentation](https://huggingface.co/docs/transformers/main/chat_templating#advanced-tool-use--function-calling).\n\n\n### Use with `bitsandbytes`\n\nThe model checkpoints can be used in `8-bit` and `4-bit` for further memory optimisations using `bitsandbytes` and `transformers`\n\nSee the snippet below for usage:\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_id = \"meta-llama/Meta-Llama-3.1-70B-Instruct\"\nquantization_config = BitsAndBytesConfig(load_in_8bit=True)\n\nquantized_model = AutoModelForCausalLM.from_pretrained(\n\tmodel_id, device_map=\"auto\", torch_dtype=torch.bfloat16, quantization_config=quantization_config)\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\ninput_text = \"What are we having for dinner?\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n\noutput = quantized_model.generate(**input_ids, max_new_tokens=10)\n\nprint(tokenizer.decode(output[0], skip_special_tokens=True))\n```\n\nTo load in 4-bit simply pass `load_in_4bit=True`\n\n### Use with `llama`\n\nPlease, follow the instructions in the [repository](https://github.com/meta-llama/llama).\n\nTo download Original checkpoints, see the example command below leveraging `huggingface-cli`:\n\n```\nhuggingface-cli download meta-llama/Meta-Llama-3.1-70B-Instruct --include \"original/*\" --local-dir Meta-Llama-3.1-70B-Instruct\n```\n\n\n## Hardware and Software\n\n**Training Factors** We used custom training libraries, Meta's custom built GPU cluster, and production infrastructure for pretraining. Fine-tuning, annotation, and evaluation were also performed on production infrastructure.\n\n**Training utilized a cumulative of** 39.3M GPU hours of computation on H100-80GB (TDP of 700W) type hardware, per the table below. Training time is the total GPU time required for training each model and power consumption is the peak power capacity per GPU device used, adjusted for power usage efficiency. \n\n\n**Training Greenhouse Gas Emissions** Estimated total location-based greenhouse gas emissions were **11,390** tons CO2eq for training. Since 2020, Meta has maintained net zero greenhouse gas emissions in its global operations and matched 100% of its electricity use with renewable energy, therefore the total market-based greenhouse gas emissions for training were 0 tons CO2eq.\n\n\n<table>\n  <tr>\n   <td>\n   </td>\n   <td><strong>Training Time (GPU hours)</strong>\n   </td>\n   <td><strong>Training Power Consumption (W)</strong>\n   </td>\n   <td><strong>Training Location-Based Greenhouse Gas Emissions</strong>\n<p>\n<strong>(tons CO2eq)</strong>\n   </td>\n   <td><strong>Training Market-Based Greenhouse Gas Emissions</strong>\n<p>\n<strong>(tons CO2eq)</strong>\n   </td>\n  </tr>\n  <tr>\n   <td>Llama 3.1 8B\n   </td>\n   <td>1.46M\n   </td>\n   <td>700\n   </td>\n   <td>420\n   </td>\n   <td>0\n   </td>\n  </tr>\n  <tr>\n   <td>Llama 3.1 70B\n   </td>\n   <td>7.0M\n   </td>\n   <td>700\n   </td>\n   <td>2,040\n   </td>\n   <td>0\n   </td>\n  </tr>\n  <tr>\n   <td>Llama 3.1 405B\n   </td>\n   <td>30.84M\n   </td>\n   <td>700\n   </td>\n   <td>8,930\n   </td>\n   <td>0\n   </td>\n  </tr>\n  <tr>\n   <td>Total\n   </td>\n   <td>39.3M\n   <td>\n<ul>\n\n</ul>\n   </td>\n   <td>11,390\n   </td>\n   <td>0\n   </td>\n  </tr>\n</table>\n\n\n\nThe methodology used to determine training energy use and greenhouse gas emissions can be found [here](https://arxiv.org/pdf/2204.05149).  Since Meta is openly releasing these models, the training energy use and greenhouse gas emissions  will not be incurred by others.\n\n\n## Training Data\n\n**Overview:** Llama 3.1 was pretrained on ~15 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over 25M synthetically generated examples. \n\n**Data Freshness:** The pretraining data has a cutoff of December 2023.\n\n\n## Benchmark scores\n\nIn this section, we report the results for Llama 3.1 models on standard automatic benchmarks. For all the evaluations, we use our internal evaluations library. \n\n### Base pretrained models\n\n\n<table>\n  <tr>\n   <td><strong>Category</strong>\n   </td>\n   <td><strong>Benchmark</strong>\n   </td>\n   <td><strong># Shots</strong>\n   </td>\n   <td><strong>Metric</strong>\n   </td>\n   <td><strong>Llama 3 8B</strong>\n   </td>\n   <td><strong>Llama 3.1 8B</strong>\n   </td>\n   <td><strong>Llama 3 70B</strong>\n   </td>\n   <td><strong>Llama 3.1 70B</strong>\n   </td>\n   <td><strong>Llama 3.1 405B</strong>\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"7\" >General\n   </td>\n   <td>MMLU\n   </td>\n   <td>5\n   </td>\n   <td>macro_avg/acc_char\n   </td>\n   <td>66.7\n   </td>\n   <td>66.7\n   </td>\n   <td>79.5\n   </td>\n   <td>79.3\n   </td>\n   <td>85.2\n   </td>\n  </tr>\n  <tr>\n   <td>MMLU-Pro (CoT)\n   </td>\n   <td>5\n   </td>\n   <td>macro_avg/acc_char\n   </td>\n   <td>36.2\n   </td>\n   <td>37.1\n   </td>\n   <td>55.0\n   </td>\n   <td>53.8\n   </td>\n   <td>61.6\n   </td>\n  </tr>\n  <tr>\n   <td>AGIEval English\n   </td>\n   <td>3-5\n   </td>\n   <td>average/acc_char\n   </td>\n   <td>47.1\n   </td>\n   <td>47.8\n   </td>\n   <td>63.0\n   </td>\n   <td>64.6\n   </td>\n   <td>71.6\n   </td>\n  </tr>\n  <tr>\n   <td>CommonSenseQA\n   </td>\n   <td>7\n   </td>\n   <td>acc_char\n   </td>\n   <td>72.6\n   </td>\n   <td>75.0\n   </td>\n   <td>83.8\n   </td>\n   <td>84.1\n   </td>\n   <td>85.8\n   </td>\n  </tr>\n  <tr>\n   <td>Winogrande\n   </td>\n   <td>5\n   </td>\n   <td>acc_char\n   </td>\n   <td>-\n   </td>\n   <td>60.5\n   </td>\n   <td>-\n   </td>\n   <td>83.3\n   </td>\n   <td>86.7\n   </td>\n  </tr>\n  <tr>\n   <td>BIG-Bench Hard (CoT)\n   </td>\n   <td>3\n   </td>\n   <td>average/em\n   </td>\n   <td>61.1\n   </td>\n   <td>64.2\n   </td>\n   <td>81.3\n   </td>\n   <td>81.6\n   </td>\n   <td>85.9\n   </td>\n  </tr>\n  <tr>\n   <td>ARC-Challenge\n   </td>\n   <td>25\n   </td>\n   <td>acc_char\n   </td>\n   <td>79.4\n   </td>\n   <td>79.7\n   </td>\n   <td>93.1\n   </td>\n   <td>92.9\n   </td>\n   <td>96.1\n   </td>\n  </tr>\n  <tr>\n   <td>Knowledge reasoning\n   </td>\n   <td>TriviaQA-Wiki\n   </td>\n   <td>5\n   </td>\n   <td>em\n   </td>\n   <td>78.5\n   </td>\n   <td>77.6\n   </td>\n   <td>89.7\n   </td>\n   <td>89.8\n   </td>\n   <td>91.8\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"4\" >Reading comprehension\n   </td>\n   <td>SQuAD\n   </td>\n   <td>1\n   </td>\n   <td>em\n   </td>\n   <td>76.4\n   </td>\n   <td>77.0\n   </td>\n   <td>85.6\n   </td>\n   <td>81.8\n   </td>\n   <td>89.3\n   </td>\n  </tr>\n  <tr>\n   <td>QuAC (F1)\n   </td>\n   <td>1\n   </td>\n   <td>f1\n   </td>\n   <td>44.4\n   </td>\n   <td>44.9\n   </td>\n   <td>51.1\n   </td>\n   <td>51.1\n   </td>\n   <td>53.6\n   </td>\n  </tr>\n  <tr>\n   <td>BoolQ\n   </td>\n   <td>0\n   </td>\n   <td>acc_char\n   </td>\n   <td>75.7\n   </td>\n   <td>75.0\n   </td>\n   <td>79.0\n   </td>\n   <td>79.4\n   </td>\n   <td>80.0\n   </td>\n  </tr>\n  <tr>\n   <td>DROP (F1)\n   </td>\n   <td>3\n   </td>\n   <td>f1\n   </td>\n   <td>58.4\n   </td>\n   <td>59.5\n   </td>\n   <td>79.7\n   </td>\n   <td>79.6\n   </td>\n   <td>84.8\n   </td>\n  </tr>\n</table>\n\n\n\n### Instruction tuned models\n\n\n<table>\n  <tr>\n   <td><strong>Category</strong>\n   </td>\n   <td><strong>Benchmark</strong>\n   </td>\n   <td><strong># Shots</strong>\n   </td>\n   <td><strong>Metric</strong>\n   </td>\n   <td><strong>Llama 3 8B Instruct</strong>\n   </td>\n   <td><strong>Llama 3.1 8B Instruct</strong>\n   </td>\n   <td><strong>Llama 3 70B Instruct</strong>\n   </td>\n   <td><strong>Llama 3.1 70B Instruct</strong>\n   </td>\n   <td><strong>Llama 3.1 405B Instruct</strong>\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"4\" >General\n   </td>\n   <td>MMLU\n   </td>\n   <td>5\n   </td>\n   <td>macro_avg/acc\n   </td>\n   <td>68.5\n   </td>\n   <td>69.4\n   </td>\n   <td>82.0\n   </td>\n   <td>83.6\n   </td>\n   <td>87.3\n   </td>\n  </tr>\n  <tr>\n   <td>MMLU (CoT)\n   </td>\n   <td>0\n   </td>\n   <td>macro_avg/acc\n   </td>\n   <td>65.3\n   </td>\n   <td>73.0\n   </td>\n   <td>80.9\n   </td>\n   <td>86.0\n   </td>\n   <td>88.6\n   </td>\n  </tr>\n  <tr>\n   <td>MMLU-Pro (CoT)\n   </td>\n   <td>5\n   </td>\n   <td>micro_avg/acc_char\n   </td>\n   <td>45.5\n   </td>\n   <td>48.3\n   </td>\n   <td>63.4\n   </td>\n   <td>66.4\n   </td>\n   <td>73.3\n   </td>\n  </tr>\n  <tr>\n   <td>IFEval\n   </td>\n   <td>\n   </td>\n   <td>\n   </td>\n   <td>76.8\n   </td>\n   <td>80.4\n   </td>\n   <td>82.9\n   </td>\n   <td>87.5\n   </td>\n   <td>88.6\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"2\" >Reasoning\n   </td>\n   <td>ARC-C\n   </td>\n   <td>0\n   </td>\n   <td>acc\n   </td>\n   <td>82.4\n   </td>\n   <td>83.4\n   </td>\n   <td>94.4\n   </td>\n   <td>94.8\n   </td>\n   <td>96.9\n   </td>\n  </tr>\n  <tr>\n   <td>GPQA\n   </td>\n   <td>0\n   </td>\n   <td>em\n   </td>\n   <td>34.6\n   </td>\n   <td>30.4\n   </td>\n   <td>39.5\n   </td>\n   <td>46.7\n   </td>\n   <td>50.7\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"4\" >Code\n   </td>\n   <td>HumanEval\n   </td>\n   <td>0\n   </td>\n   <td>pass@1\n   </td>\n   <td>60.4\n   </td>\n   <td>72.6\n   </td>\n   <td>81.7\n   </td>\n   <td>80.5\n   </td>\n   <td>89.0\n   </td>\n  </tr>\n  <tr>\n   <td>MBPP ++ base version\n   </td>\n   <td>0\n   </td>\n   <td>pass@1\n   </td>\n   <td>70.6\n   </td>\n   <td>72.8\n   </td>\n   <td>82.5\n   </td>\n   <td>86.0\n   </td>\n   <td>88.6\n   </td>\n  </tr>\n  <tr>\n   <td>Multipl-E HumanEval\n   </td>\n   <td>0\n   </td>\n   <td>pass@1\n   </td>\n   <td>-\n   </td>\n   <td>50.8\n   </td>\n   <td>-\n   </td>\n   <td>65.5\n   </td>\n   <td>75.2\n   </td>\n  </tr>\n  <tr>\n   <td>Multipl-E MBPP\n   </td>\n   <td>0\n   </td>\n   <td>pass@1\n   </td>\n   <td>-\n   </td>\n   <td>52.4\n   </td>\n   <td>-\n   </td>\n   <td>62.0\n   </td>\n   <td>65.7\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"2\" >Math\n   </td>\n   <td>GSM-8K (CoT)\n   </td>\n   <td>8\n   </td>\n   <td>em_maj1@1\n   </td>\n   <td>80.6\n   </td>\n   <td>84.5\n   </td>\n   <td>93.0\n   </td>\n   <td>95.1\n   </td>\n   <td>96.8\n   </td>\n  </tr>\n  <tr>\n   <td>MATH (CoT)\n   </td>\n   <td>0\n   </td>\n   <td>final_em\n   </td>\n   <td>29.1\n   </td>\n   <td>51.9\n   </td>\n   <td>51.0\n   </td>\n   <td>68.0\n   </td>\n   <td>73.8\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"4\" >Tool Use\n   </td>\n   <td>API-Bank\n   </td>\n   <td>0\n   </td>\n   <td>acc\n   </td>\n   <td>48.3\n   </td>\n   <td>82.6\n   </td>\n   <td>85.1\n   </td>\n   <td>90.0\n   </td>\n   <td>92.0\n   </td>\n  </tr>\n  <tr>\n   <td>BFCL\n   </td>\n   <td>0\n   </td>\n   <td>acc\n   </td>\n   <td>60.3\n   </td>\n   <td>76.1\n   </td>\n   <td>83.0\n   </td>\n   <td>84.8\n   </td>\n   <td>88.5\n   </td>\n  </tr>\n  <tr>\n   <td>Gorilla Benchmark API Bench\n   </td>\n   <td>0\n   </td>\n   <td>acc\n   </td>\n   <td>1.7\n   </td>\n   <td>8.2\n   </td>\n   <td>14.7\n   </td>\n   <td>29.7\n   </td>\n   <td>35.3\n   </td>\n  </tr>\n  <tr>\n   <td>Nexus (0-shot)\n   </td>\n   <td>0\n   </td>\n   <td>macro_avg/acc\n   </td>\n   <td>18.1\n   </td>\n   <td>38.5\n   </td>\n   <td>47.8\n   </td>\n   <td>56.7\n   </td>\n   <td>58.7\n   </td>\n  </tr>\n  <tr>\n   <td>Multilingual\n   </td>\n   <td>Multilingual MGSM (CoT)\n   </td>\n   <td>0\n   </td>\n   <td>em\n   </td>\n   <td>-\n   </td>\n   <td>68.9\n   </td>\n   <td>-\n   </td>\n   <td>86.9\n   </td>\n   <td>91.6\n   </td>\n  </tr>\n</table>\n\n#### Multilingual benchmarks\n\n<table>\n  <tr>\n   <td><strong>Category</strong>\n   </td>\n   <td><strong>Benchmark</strong>\n   </td>\n   <td><strong>Language</strong>\n   </td>\n   <td><strong>Llama 3.1 8B</strong>\n   </td>\n   <td><strong>Llama 3.1 70B</strong>\n   </td>\n   <td><strong>Llama 3.1 405B</strong>\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"9\" ><strong>General</strong>\n   </td>\n   <td rowspan=\"9\" ><strong>MMLU (5-shot, macro_avg/acc)</strong>\n   </td>\n   <td>Portuguese\n   </td>\n   <td>62.12\n   </td>\n   <td>80.13\n   </td>\n   <td>84.95\n   </td>\n  </tr>\n  <tr>\n   <td>Spanish\n   </td>\n   <td>62.45\n   </td>\n   <td>80.05\n   </td>\n   <td>85.08\n   </td>\n  </tr>\n  <tr>\n   <td>Italian\n   </td>\n   <td>61.63\n   </td>\n   <td>80.4\n   </td>\n   <td>85.04\n   </td>\n  </tr>\n  <tr>\n   <td>German\n   </td>\n   <td>60.59\n   </td>\n   <td>79.27\n   </td>\n   <td>84.36\n   </td>\n  </tr>\n  <tr>\n   <td>French\n   </td>\n   <td>62.34\n   </td>\n   <td>79.82\n   </td>\n   <td>84.66\n   </td>\n  </tr>\n  <tr>\n   <td>Hindi\n   </td>\n   <td>50.88\n   </td>\n   <td>74.52\n   </td>\n   <td>80.31\n   </td>\n  </tr>\n  <tr>\n   <td>Thai\n   </td>\n   <td>50.32\n   </td>\n   <td>72.95\n   </td>\n   <td>78.21\n   </td>\n  </tr>\n</table>\n\n\n\n## Responsibility & Safety\n\nAs part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:\n\n\n\n* Enable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama. \n* Protect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm.\n* Provide protections for the community to help prevent the misuse of our models.\n\n\n### Responsible deployment \n\nLlama is a foundational technology designed to be used in a variety of use cases, examples on how Meta’s Llama models have been responsibly deployed can be found in our [Community Stories webpage](https://llama.meta.com/community-stories/). Our approach is to build the most helpful models enabling the world to benefit from the technology power, by aligning our model safety for the generic use cases addressing a standard set of harms. Developers are then in the driver seat to tailor safety for their use case, defining their own policy and deploying the models with the necessary safeguards in their Llama systems. Llama 3.1 was developed following the best practices outlined in our Responsible Use Guide, you can refer to the [Responsible Use Guide](https://llama.meta.com/responsible-use-guide/) to learn more. \n\n\n#### Llama 3.1 instruct \n\nOur main objectives for conducting safety fine-tuning are to provide the research community with a valuable resource for studying the robustness of safety fine-tuning, as well as to offer developers a readily available, safe, and powerful model for various applications to reduce the developer workload to deploy safe AI systems. For more details on the safety mitigations implemented please read the Llama 3 paper. \n\n**Fine-tuning data**\n\nWe employ a multi-faceted approach to data collection, combining human-generated data from our vendors with synthetic data to mitigate potential safety risks. We’ve developed many large language model (LLM)-based classifiers that enable us to thoughtfully select high-quality prompts and responses, enhancing data quality control. \n\n**Refusals and Tone**\n\nBuilding on the work we started with Llama 3, we put a great emphasis on model refusals to benign prompts as well as refusal tone. We included both borderline and adversarial prompts in our safety data strategy, and modified our safety data responses to follow  tone guidelines. \n\n\n#### Llama 3.1 systems\n\n**Large language models, including Llama 3.1, are not designed to be deployed in isolation but instead should be deployed as part of an overall AI system with additional safety guardrails as required.** Developers are expected to deploy system safeguards when building agentic systems. Safeguards are key to achieve the right helpfulness-safety alignment as well as mitigating safety and security risks inherent to the system and any integration of the model or system with external tools. \n\nAs part of our responsible release approach, we provide the community with [safeguards](https://llama.meta.com/trust-and-safety/) that developers should deploy with Llama models or other LLMs, including Llama Guard 3, Prompt Guard and Code Shield. All our [reference implementations](https://github.com/meta-llama/llama-agentic-system) demos contain these safeguards by default so developers can benefit from system-level safety out-of-the-box. \n\n\n#### New capabilities \n\nNote that this release introduces new capabilities, including a longer context window, multilingual inputs and outputs and possible integrations by developers with third party tools. Building with these new capabilities requires specific considerations in addition to the best practices that generally apply across all Generative AI use cases.\n\n**Tool-use**: Just like in standard software development, developers are responsible for the integration of the LLM with the tools and services of their choice. They should define a clear policy for their use case and assess the integrity of the third party services they use to be aware of the safety and security limitations when using this capability. Refer to the Responsible Use Guide for best practices on the safe deployment of the third party safeguards. \n\n**Multilinguality**: Llama 3.1 supports 7 languages in addition to English: French, German, Hindi, Italian, Portuguese, Spanish, and Thai. Llama may be able to output text in other languages than those that meet performance thresholds for safety and helpfulness. We strongly discourage developers from using this model to converse in non-supported languages without implementing finetuning and system controls in alignment with their policies and the best practices shared in the Responsible Use Guide. \n\n\n### Evaluations\n\nWe evaluated Llama models for common use cases as well as specific capabilities. Common use cases evaluations measure safety risks of systems for most commonly built applications including chat bot, coding assistant, tool calls. We built dedicated, adversarial evaluation datasets and evaluated systems composed of Llama models and Llama Guard 3 to filter input prompt and output response. It is important to evaluate applications in context, and we recommend building dedicated evaluation dataset for your use case. Prompt Guard and Code Shield are also available if relevant to the application. \n\nCapability evaluations measure vulnerabilities of Llama models inherent to specific capabilities, for which were crafted dedicated benchmarks including long context, multilingual, tools calls, coding or memorization.\n\n**Red teaming**\n\nFor both scenarios, we conducted recurring red teaming exercises with the goal of discovering risks via adversarial prompting and we used the learnings to improve our benchmarks and safety tuning datasets. \n\nWe partnered early with subject-matter experts in critical risk areas to understand the nature of these real-world harms and how such models may lead to unintended harm for society. Based on these conversations, we derived a set of adversarial goals for the red team to attempt to achieve, such as extracting harmful information or reprogramming the model to act in a potentially harmful capacity.  The red team consisted of experts in cybersecurity, adversarial machine learning, responsible AI, and integrity in addition to multilingual content specialists with background in integrity issues in specific geographic markets.\n\n\n### Critical and other risks \n\nWe specifically focused our efforts on mitigating the following critical risk areas:\n\n**1- CBRNE (Chemical, Biological, Radiological, Nuclear, and Explosive materials) helpfulness**\n\nTo assess risks related to proliferation of chemical and biological weapons, we performed uplift testing designed to assess whether use of Llama 3.1 models could meaningfully increase the capabilities of malicious actors to plan or carry out attacks using these types of weapons. \n\n\n**2. Child Safety**\n\nChild Safety risk assessments were conducted using a team of experts, to assess the model’s capability to produce outputs that could result in Child Safety risks and inform on any necessary and appropriate risk mitigations via fine tuning. We leveraged those expert red teaming sessions to expand the coverage of our evaluation benchmarks through Llama 3 model development.  For Llama 3, we conducted new in-depth sessions using objective based methodologies to assess the model risks along multiple attack vectors including the additional languages Llama 3 is trained on. We also partnered with content specialists to perform red teaming exercises assessing potentially violating content while taking account of market specific nuances or experiences. \n\n**3. Cyber attack enablement**\n\nOur cyber attack uplift study investigated whether LLMs can enhance human capabilities in hacking tasks, both in terms of skill level and speed.\n\nOur attack automation study focused on evaluating the capabilities of LLMs when used as autonomous agents in cyber offensive operations, specifically in the context of ransomware attacks. This evaluation was distinct from previous studies that considered LLMs as interactive assistants. The primary objective was to assess whether these models could effectively function as independent agents in executing complex cyber-attacks without human intervention.\n\nOur study of Llama-3.1-405B’s social engineering uplift for cyber attackers was conducted to assess the effectiveness of AI models in aiding cyber threat actors in spear phishing campaigns. Please read our Llama 3.1 Cyber security whitepaper to learn more.\n\n\n### Community \n\nGenerative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership on AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Purple Llama tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to our [Github repository](https://github.com/meta-llama/PurpleLlama). \n\nWe also set up the [Llama Impact Grants](https://llama.meta.com/llama-impact-grants/) program to identify and support the most compelling applications of Meta’s Llama model for societal benefit across three categories: education, climate and open innovation. The 20 finalists from the hundreds of applications can be found [here](https://llama.meta.com/llama-impact-grants/#finalists). \n\nFinally, we put in place a set of resources including an [output reporting mechanism](https://developers.facebook.com/llama_output_feedback) and [bug bounty program](https://www.facebook.com/whitehat) to continuously improve the Llama technology with the help of the community.\n\n\n## Ethical Considerations and Limitations\n\nThe core values of Llama 3.1 are openness, inclusivity and helpfulness. It is meant to serve everyone, and to work for a wide range of use cases. It is thus designed to be accessible to people across many different backgrounds, experiences and perspectives. Llama 3.1 addresses users and their needs as they are, without insertion unnecessary judgment or normativity, while reflecting the understanding that even content that may appear problematic in some cases can serve valuable purposes in others. It respects the dignity and autonomy of all users, especially in terms of the values of free thought and expression that power innovation and progress. \n\nBut Llama 3.1 is a new technology, and like any new technology, there are risks associated with its use. Testing conducted to date has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, Llama 3.1’s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 3.1 models, developers should perform safety testing and tuning tailored to their specific applications of the model. Please refer to available resources including our [Responsible Use Guide](https://llama.meta.com/responsible-use-guide), [Trust and Safety](https://llama.meta.com/trust-and-safety/) solutions, and other [resources](https://llama.meta.com/docs/get-started/) to learn more about responsible development.",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModelForCausalLM",
      "custom_class": null,
      "pipeline_tag": "text-generation",
      "processor": "AutoTokenizer"
    },
    "safetensors": {
      "parameters": {
        "BF16": 70553706496
      },
      "total": 70553706496
    },
    "model_index": null,
    "trending_score": null
  },
  {
    "model_id": "meta-llama/Llama-3.3-70B-Instruct",
    "model_name": "meta-llama/Llama-3.3-70B-Instruct",
    "author": "meta-llama",
    "downloads": 1030749,
    "downloads_all_time": null,
    "likes": 2178,
    "tags": [
      "transformers",
      "safetensors",
      "llama",
      "text-generation",
      "facebook",
      "meta",
      "pytorch",
      "llama-3",
      "conversational",
      "en",
      "fr",
      "it",
      "pt",
      "hi",
      "es",
      "th",
      "de",
      "arxiv:2204.05149",
      "base_model:meta-llama/Llama-3.1-70B",
      "base_model:finetune:meta-llama/Llama-3.1-70B",
      "license:llama3.3",
      "autotrain_compatible",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct",
    "dependencies": null,
    "last_modified": "2024-12-21T18:28:01+00:00",
    "created_at": "2024-11-26T16:08:47+00:00",
    "analysis_date": "2025-03-22T00:42:41.598154",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "llama",
    "pipeline_tag": "text-generation",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "library_name": "transformers",
      "language": [
        "en",
        "fr",
        "it",
        "pt",
        "hi",
        "es",
        "th",
        "de"
      ],
      "base_model": [
        "meta-llama/Llama-3.1-70B"
      ],
      "tags": [
        "facebook",
        "meta",
        "pytorch",
        "llama",
        "llama-3"
      ],
      "extra_gated_prompt": "### LLAMA 3.3 COMMUNITY LICENSE AGREEMENT\nLlama 3.3 Version Release Date: December 6, 2024\n\"Agreement\" means the terms and conditions for use, reproduction, distribution and modification of the Llama Materials set forth herein.\n\"Documentation\" means the specifications, manuals and documentation accompanying Llama 3.3 distributed by Meta at [https://www.llama.com/docs/overview](https://llama.com/docs/overview).\n\"Licensee\" or \"you\" means you, or your employer or any other person or entity (if you are entering into this Agreement on such person or entity’s behalf), of the age required under applicable laws, rules or regulations to provide legal consent and that has legal authority to bind your employer or such other person or entity if you are entering in this Agreement on their behalf.\n\"Llama 3.3\" means the foundational large language models and software and algorithms, including machine-learning model code, trained model weights, inference-enabling code, training-enabling code, fine-tuning enabling code and other elements of the foregoing distributed by Meta at [https://www.llama.com/llama-downloads](https://www.llama.com/llama-downloads).\n\"Llama Materials\" means, collectively, Meta’s proprietary Llama 3.3 and Documentation (and any portion thereof) made available under this Agreement.\n\"Meta\" or \"we\" means Meta Platforms Ireland Limited (if you are located in or, if you are an entity, your principal place of business is in the EEA or Switzerland) and Meta Platforms, Inc. (if you are located outside of the EEA or Switzerland).\nBy clicking “I Accept” below or by using or distributing any portion or element of the Llama Materials, you agree to be bound by this Agreement.\n1. License Rights and Redistribution.\na. Grant of Rights. You are granted a non-exclusive, worldwide, non-transferable and royalty-free limited license under Meta’s intellectual property or other rights owned by Meta embodied in the Llama Materials to use, reproduce, distribute, copy, create derivative works of, and make modifications to the Llama Materials.\nb. Redistribution and Use.\ni. If you distribute or make available the Llama Materials (or any derivative works thereof), or a product or service (including another AI model) that contains any of them, you shall (A) provide a copy of this Agreement with any such Llama Materials; and (B) prominently display “Built with Llama” on a related website, user interface, blogpost, about page, or product documentation. If you use the Llama Materials or any outputs or results of the Llama Materials to create, train, fine tune, or otherwise improve an AI model, which is distributed or made available, you shall also include “Llama” at the beginning of any such AI model name.\nii. If you receive Llama Materials, or any derivative works thereof, from a Licensee as part of an integrated end user product, then Section 2 of this Agreement will not apply to you. \niii. You must retain in all copies of the Llama Materials that you distribute the following attribution notice within a “Notice” text file distributed as a part of such copies: “Llama 3.3 is licensed under the Llama 3.3 Community License, Copyright © Meta Platforms, Inc. All Rights Reserved.”\niv. Your use of the Llama Materials must comply with applicable laws and regulations (including trade compliance laws and regulations) and adhere to the Acceptable Use Policy for the Llama Materials (available at [https://www.llama.com/llama3\\_3/use-policy](https://www.llama.com/llama3_3/use-policy)), which is hereby incorporated by reference into this Agreement.  \n2. Additional Commercial Terms. If, on the Llama 3.3 version release date, the monthly active users of the products or services made available by or for Licensee, or Licensee’s affiliates, is greater than 700 million monthly active users in the preceding calendar month, you must request a license from Meta, which Meta may grant to you in its sole discretion, and you are not authorized to exercise any of the rights under this Agreement unless or until Meta otherwise expressly grants you such rights.\n3. Disclaimer of Warranty. UNLESS REQUIRED BY APPLICABLE LAW, THE LLAMA MATERIALS AND ANY OUTPUT AND RESULTS THEREFROM ARE PROVIDED ON AN “AS IS” BASIS, WITHOUT WARRANTIES OF ANY KIND, AND META DISCLAIMS ALL WARRANTIES OF ANY KIND, BOTH EXPRESS AND IMPLIED, INCLUDING, WITHOUT LIMITATION, ANY WARRANTIES OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE. YOU ARE SOLELY RESPONSIBLE FOR DETERMINING THE APPROPRIATENESS OF USING OR REDISTRIBUTING THE LLAMA MATERIALS AND ASSUME ANY RISKS ASSOCIATED WITH YOUR USE OF THE LLAMA MATERIALS AND ANY OUTPUT AND RESULTS.\n4. Limitation of Liability. IN NO EVENT WILL META OR ITS AFFILIATES BE LIABLE UNDER ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, TORT, NEGLIGENCE, PRODUCTS LIABILITY, OR OTHERWISE, ARISING OUT OF THIS AGREEMENT, FOR ANY LOST PROFITS OR ANY INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL, EXEMPLARY OR PUNITIVE DAMAGES, EVEN IF META OR ITS AFFILIATES HAVE BEEN ADVISED OF THE POSSIBILITY OF ANY OF THE FOREGOING.\n5. Intellectual Property.\na. No trademark licenses are granted under this Agreement, and in connection with the Llama Materials, neither Meta nor Licensee may use any name or mark owned by or associated with the other or any of its affiliates, except as required for reasonable and customary use in describing and redistributing the Llama Materials or as set forth in this Section 5(a). Meta hereby grants you a license to use “Llama” (the “Mark”) solely as required to comply with the last sentence of Section 1.b.i. You will comply with Meta’s brand guidelines (currently accessible at [https://about.meta.com/brand/resources/meta/company-brand/](https://about.meta.com/brand/resources/meta/company-brand/)[)](https://en.facebookbrand.com/). All goodwill arising out of your use of the Mark will inure to the benefit of Meta.\nb. Subject to Meta’s ownership of Llama Materials and derivatives made by or for Meta, with respect to any derivative works and modifications of the Llama Materials that are made by you, as between you and Meta, you are and will be the owner of such derivative works and modifications.\nc. If you institute litigation or other proceedings against Meta or any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Llama Materials or Llama 3.3 outputs or results, or any portion of any of the foregoing, constitutes infringement of intellectual property or other rights owned or licensable by you, then any licenses granted to you under this Agreement shall terminate as of the date such litigation or claim is filed or instituted. You will indemnify and hold harmless Meta from and against any claim by any third party arising out of or related to your use or distribution of the Llama Materials.\n6. Term and Termination. The term of this Agreement will commence upon your acceptance of this Agreement or access to the Llama Materials and will continue in full force and effect until terminated in accordance with the terms and conditions herein. Meta may terminate this Agreement if you are in breach of any term or condition of this Agreement. Upon termination of this Agreement, you shall delete and cease use of the Llama Materials. Sections 3, 4 and 7 shall survive the termination of this Agreement.\n7. Governing Law and Jurisdiction. This Agreement will be governed and construed under the laws of the State of California without regard to choice of law principles, and the UN Convention on Contracts for the International Sale of Goods does not apply to this Agreement. The courts of California shall have exclusive jurisdiction of any dispute arising out of this Agreement.\n### Llama 3.3 Acceptable Use Policy\nMeta is committed to promoting safe and fair use of its tools and features, including Llama 3.3. If you access or use Llama 3.3, you agree to this Acceptable Use Policy (“**Policy**”). The most recent copy of this policy can be found at [https://www.llama.com/llama3\\_3/use-policy](https://www.llama.com/llama3_3/use-policy).\nProhibited Uses\nWe want everyone to use Llama 3.3 safely and responsibly. You agree you will not use, or allow others to use, Llama 3.3 to:\n1. Violate the law or others’ rights, including to:\n\n   1. Engage in, promote, generate, contribute to, encourage, plan, incite, or further illegal or unlawful activity or content, such as:  \n      1. Violence or terrorism  \n      2. Exploitation or harm to children, including the solicitation, creation, acquisition, or dissemination of child exploitative content or failure to report Child Sexual Abuse Material  \n      3. Human trafficking, exploitation, and sexual violence  \n      4. The illegal distribution of information or materials to minors, including obscene materials, or failure to employ legally required age-gating in connection with such information or materials.  \n      5. Sexual solicitation  \n      6. Any other criminal activity\n\n   2. Engage in, promote, incite, or facilitate the harassment, abuse, threatening, or bullying of individuals or groups of individuals\n\n   3. Engage in, promote, incite, or facilitate discrimination or other unlawful or harmful conduct in the provision of employment, employment benefits, credit, housing, other economic benefits, or other essential goods and services\n\n   4. Engage in the unauthorized or unlicensed practice of any profession including, but not limited to, financial, legal, medical/health, or related professional practices\n\n   5. Collect, process, disclose, generate, or infer private or sensitive information about individuals, including information about individuals’ identity, health, or demographic information, unless you have obtained the right to do so in accordance with applicable law\n\n   6. Engage in or facilitate any action or generate any content that infringes, misappropriates, or otherwise violates any third-party rights, including the outputs or results of any products or services using the Llama Materials\n\n   7. Create, generate, or facilitate the creation of malicious code, malware, computer viruses or do anything else that could disable, overburden, interfere with or impair the proper working, integrity, operation or appearance of a website or computer system\n\n   8. Engage in any action, or facilitate any action, to intentionally circumvent or remove usage restrictions or other safety measures, or to enable functionality disabled by Meta\n\n2. Engage in, promote, incite, facilitate, or assist in the planning or development of activities that present a risk of death or bodily harm to individuals, including use of Llama 3.3 related to the following:\n\n   1. Military, warfare, nuclear industries or applications, espionage, use for materials or activities that are subject to the International Traffic Arms Regulations (ITAR) maintained by the United States Department of State or to the U.S. Biological Weapons Anti-Terrorism Act of 1989 or the Chemical Weapons Convention Implementation Act of 1997\n\n   2. Guns and illegal weapons (including weapon development)\n\n   3. Illegal drugs and regulated/controlled substances\n\n   4. Operation of critical infrastructure, transportation technologies, or heavy machinery\n\n   5. Self-harm or harm to others, including suicide, cutting, and eating disorders\n\n   6. Any content intended to incite or promote violence, abuse, or any infliction of bodily harm to an individual\n\n3. Intentionally deceive or mislead others, including use of Llama 3.3 related to the following:\n\n   1. Generating, promoting, or furthering fraud or the creation or promotion of disinformation\n\n   2. Generating, promoting, or furthering defamatory content, including the creation of defamatory statements, images, or other content\n\n   3. Generating, promoting, or further distributing spam\n\n   4. Impersonating another individual without consent, authorization, or legal right\n\n   5. Representing that the use of Llama 3.3 or outputs are human-generated\n\n   6. Generating or facilitating false online engagement, including fake reviews and other means of fake online engagement\n\n4. Fail to appropriately disclose to end users any known dangers of your AI system\n5. Interact with third party tools, models, or software designed to generate unlawful content or engage in unlawful or harmful conduct and/or represent that the outputs of such tools, models, or software are associated with Meta or Llama 3.3\nWith respect to any multimodal models included in Llama 3.3, the rights granted under Section 1(a) of the Llama 3.3 Community License Agreement are not being granted to you if you are an individual domiciled in, or a company with a principal place of business in, the European Union. This restriction does not apply to end users of a product or service that incorporates any such multimodal models.\nPlease report any violation of this Policy, software “bug,” or other problems that could lead to a violation of this Policy through one of the following means:\n* Reporting issues with the model: [https://github.com/meta-llama/llama-models/issues](https://l.workplace.com/l.php?u=https%3A%2F%2Fgithub.com%2Fmeta-llama%2Fllama-models%2Fissues&h=AT0qV8W9BFT6NwihiOHRuKYQM_UnkzN_NmHMy91OT55gkLpgi4kQupHUl0ssR4dQsIQ8n3tfd0vtkobvsEvt1l4Ic6GXI2EeuHV8N08OG2WnbAmm0FL4ObkazC6G_256vN0lN9DsykCvCqGZ)   * Reporting risky content generated by the model: [developers.facebook.com/llama\\_output\\_feedback](http://developers.facebook.com/llama_output_feedback)   * Reporting bugs and security concerns: [facebook.com/whitehat/info](http://facebook.com/whitehat/info)   * Reporting violations of the Acceptable Use Policy or unlicensed uses of Llama 3.3: LlamaUseReport@meta.com  ",
      "extra_gated_fields": {
        "First Name": "text",
        "Last Name": "text",
        "Date of birth": "date_picker",
        "Country": "country",
        "Affiliation": "text",
        "Job title": {
          "type": "select",
          "options": [
            "Student",
            "Research Graduate",
            "AI researcher",
            "AI developer/engineer",
            "Reporter",
            "Other"
          ]
        },
        "geo": "ip_location",
        "By clicking Submit below I accept the terms of the license and acknowledge that the information I provide will be collected stored processed and shared in accordance with the Meta Privacy Policy": "checkbox"
      },
      "extra_gated_description": "The information you provide will be collected, stored, processed and shared in accordance with the [Meta Privacy Policy](https://www.facebook.com/privacy/policy/).",
      "extra_gated_button_content": "Submit",
      "license": "llama3.3"
    },
    "card_text": "## Model Information\n\nThe Meta Llama 3.3 multilingual large language model (LLM) is an instruction tuned generative model in 70B (text in/text out). The Llama 3.3 instruction tuned text only model is optimized for multilingual dialogue use cases and outperforms many of the available open source and closed chat models on common industry benchmarks.\n\n**Model developer**: Meta\n\n**Model Architecture:** Llama 3.3 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety. \n\n|  | Training Data | Params | Input modalities | Output modalities | Context length | GQA | Token count | Knowledge cutoff |\n| :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- |\n| Llama 3.3 (text only)  | A new mix of publicly available online data. | 70B | Multilingual Text | Multilingual Text and code  | 128k | Yes | 15T+ | December 2023 |\n\n**Supported languages:** English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai.\n\n**Llama 3.3 model**. Token counts refer to pretraining data only. All model versions use Grouped-Query Attention (GQA) for improved inference scalability.\n\n**Model Release Date:** \n\n* **70B Instruct: December 6, 2024** \n\n**Status:** This is a static model trained on an offline dataset. Future versions of the tuned models will be released as we improve model safety with community feedback.\n\n**License** A custom commercial license, the Llama 3.3 Community License Agreement, is available at: [https://github.com/meta-llama/llama-models/blob/main/models/llama3\\_3/LICENSE](https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/LICENSE)\n\nWhere to send questions or comments about the model Instructions on how to provide feedback or comments on the model can be found in the model [README](https://github.com/meta-llama/llama3). For more technical information about generation parameters and recipes for how to use Llama 3.3 in applications, please go [here](https://github.com/meta-llama/llama-recipes). \n\n## Intended Use\n\n**Intended Use Cases** Llama 3.3 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks. The Llama 3.3 model also supports the ability to leverage the outputs of its models to improve other models including synthetic data generation and distillation. The Llama 3.3 Community License allows for these use cases. \n\n**Out-of-scope** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.3 Community License. Use in languages beyond those explicitly referenced as supported in this model card\\*\\*.\n\n\\*\\*Note: Llama 3.3 has been trained on a broader collection of languages than the 8 supported languages. Developers may fine-tune Llama 3.3 models for languages beyond the 8 supported languages provided they comply with the Llama 3.3 Community License and the Acceptable Use Policy and in such cases are responsible for ensuring that any uses of Llama 3.3 in additional languages is done in a safe and responsible manner.\n\n## How to use\n\nThis repository contains two versions of Llama-3.3-70B-Instruct, for use with transformers and with the original `llama` codebase.\n\n### Use with transformers\n\nStarting with `transformers >= 4.45.0` onward, you can run conversational inference using the Transformers `pipeline` abstraction or by leveraging the Auto classes with the `generate()` function.\n\nMake sure to update your transformers installation via `pip install --upgrade transformers`.\n\nSee the snippet below for usage with Transformers:\n\n```python\nimport transformers\nimport torch\n\nmodel_id = \"meta-llama/Llama-3.3-70B-Instruct\"\n\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model_id,\n    model_kwargs={\"torch_dtype\": torch.bfloat16},\n    device_map=\"auto\",\n)\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n    {\"role\": \"user\", \"content\": \"Who are you?\"},\n]\n\noutputs = pipeline(\n    messages,\n    max_new_tokens=256,\n)\nprint(outputs[0][\"generated_text\"][-1])\n```\n\n### Tool use with transformers\n\nLLaMA-3.3 supports multiple tool use formats. You can see a full guide to prompt formatting [here](https://llama.meta.com/docs/model-cards-and-prompt-formats/llama3_1/).\n\nTool use is also supported through [chat templates](https://huggingface.co/docs/transformers/main/chat_templating#advanced-tool-use--function-calling) in Transformers. \nHere is a quick example showing a single simple tool:\n\n```python\n# First, define a tool\ndef get_current_temperature(location: str) -> float:\n    \"\"\"\n    Get the current temperature at a location.\n    \n    Args:\n        location: The location to get the temperature for, in the format \"City, Country\"\n    Returns:\n        The current temperature at the specified location in the specified units, as a float.\n    \"\"\"\n    return 22.  # A real function should probably actually get the temperature!\n\n# Next, create a chat and apply the chat template\nmessages = [\n  {\"role\": \"system\", \"content\": \"You are a bot that responds to weather queries.\"},\n  {\"role\": \"user\", \"content\": \"Hey, what's the temperature in Paris right now?\"}\n]\n\ninputs = tokenizer.apply_chat_template(messages, tools=[get_current_temperature], add_generation_prompt=True)\n```\n\nYou can then generate text from this input as normal. If the model generates a tool call, you should add it to the chat like so:\n\n```python\ntool_call = {\"name\": \"get_current_temperature\", \"arguments\": {\"location\": \"Paris, France\"}}\nmessages.append({\"role\": \"assistant\", \"tool_calls\": [{\"type\": \"function\", \"function\": tool_call}]})\n```\n\nand then call the tool and append the result, with the `tool` role, like so:\n\n```python\nmessages.append({\"role\": \"tool\", \"name\": \"get_current_temperature\", \"content\": \"22.0\"})\n```\n\nAfter that, you can `generate()` again to let the model use the tool result in the chat. Note that this was a very brief introduction to tool calling - for more information,\nsee the [LLaMA prompt format docs](https://llama.meta.com/docs/model-cards-and-prompt-formats/llama3_1/) and the Transformers [tool use documentation](https://huggingface.co/docs/transformers/main/chat_templating#advanced-tool-use--function-calling).\n\n\n### Use with `bitsandbytes`\n\nThe model checkpoints can be used in `8-bit` and `4-bit` for further memory optimisations using `bitsandbytes` and `transformers`\n\nSee the snippet below for usage:\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_id = \"meta-llama/Llama-3.3-70B-Instruct\"\nquantization_config = BitsAndBytesConfig(load_in_8bit=True)\n\nquantized_model = AutoModelForCausalLM.from_pretrained(\n\tmodel_id, device_map=\"auto\", torch_dtype=torch.bfloat16, quantization_config=quantization_config)\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\ninput_text = \"What are we having for dinner?\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n\noutput = quantized_model.generate(**input_ids, max_new_tokens=10)\n\nprint(tokenizer.decode(output[0], skip_special_tokens=True))\n```\n\nTo load in 4-bit simply pass `load_in_4bit=True`\n\n### Use with `llama`\n\nPlease, follow the instructions in the [repository](https://github.com/meta-llama/llama).\n\nTo download Original checkpoints, see the example command below leveraging `huggingface-cli`:\n\n```\nhuggingface-cli download meta-llama/Llama-3.3-70B-Instruct --include \"original/*\" --local-dir Llama-3.3-70B-Instruct\n```\n\n## Hardware and Software\n\n**Training Factors** We used custom training libraries, Meta's custom built GPU cluster, and production infrastructure for pretraining. Fine-tuning, annotation, and evaluation were also performed on production infrastructure.\n\n**Training Energy Use** Training utilized a cumulative of **39.3**M GPU hours of computation on H100-80GB (TDP of 700W) type hardware, per the table below. Training time is the total GPU time required for training each model and power consumption is the peak power capacity per GPU device used, adjusted for power usage efficiency. \n\n## \n\n## **Training Greenhouse Gas Emissions** Estimated total location-based greenhouse gas emissions were **11,390** tons CO2eq for training. Since 2020, Meta has maintained net zero greenhouse gas emissions in its global operations and matched 100% of its electricity use with renewable energy, therefore the total market-based greenhouse gas emissions for training were 0 tons CO2eq.\n\n|  | Training Time (GPU hours) | Training Power Consumption (W) | Training Location-Based Greenhouse Gas Emissions (tons CO2eq) | Training Market-Based Greenhouse Gas Emissions (tons CO2eq) |\n| :---- | :---: | :---: | :---: | :---: |\n| Llama 3.3 70B | 7.0M | 700 | 2,040 | 0 |\n\n## The methodology used to determine training energy use and greenhouse gas emissions can be found [here](https://arxiv.org/pdf/2204.05149).  Since Meta is openly releasing these models, the training energy use and greenhouse gas emissions  will not be incurred by others.\n\n## Training Data\n\n**Overview:** Llama 3.3 was pretrained on \\~15 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over 25M synthetically generated examples. \n\n**Data Freshness:** The pretraining data has a cutoff of December 2023\\.\n\n## Benchmarks \\- English Text\n\nIn this section, we report the results for Llama 3.3 relative to our previous models. \n\n### Instruction tuned models\n\n## \n\n| Category | Benchmark | \\# Shots | Metric | Llama 3.1 8B Instruct | Llama 3.1 70B Instruct | Llama-3.3 70B Instruct | Llama 3.1 405B Instruct |\n| :---- | :---- | ----- | :---- | ----- | ----- | ----- | ----- |\n|  | MMLU (CoT) | 0 | macro\\_avg/acc | 73.0 | 86.0 | 86.0 | 88.6 |\n|  | MMLU Pro (CoT) | 5 | macro\\_avg/acc | 48.3 | 66.4 | 68.9 | 73.3 |\n| Steerability | IFEval |  |  | 80.4 | 87.5 | 92.1 | 88.6 |\n| Reasoning | GPQA Diamond (CoT) | 0 | acc | 31.8 | 48.0 | 50.5 | 49.0 |\n| Code | HumanEval | 0 | pass@1 | 72.6 | 80.5 | 88.4 | 89.0 |\n|  | MBPP EvalPlus (base) | 0 | pass@1 | 72.8 | 86.0 | 87.6 | 88.6 |\n| Math | MATH (CoT) | 0 | sympy\\_intersection\\_score | 51.9 | 68.0 | 77.0 | 73.8 |\n| Tool Use | BFCL v2 | 0 | overall\\_ast\\_summary/macro\\_avg/valid | 65.4 | 77.5 | 77.3 | 81.1 |\n| Multilingual | MGSM | 0 | em | 68.9 | 86.9 | 91.1 | 91.6 |\n\n## \n\n## Responsibility & Safety\n\nAs part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:\n\n* Enable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama.\n* Protect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm.\n* Provide protections for the community to help prevent the misuse of our models.\n\n### Responsible deployment\n\nLlama is a foundational technology designed to be used in a variety of use cases, examples on how Meta’s Llama models have been responsibly deployed can be found in our [Community Stories webpage](https://llama.meta.com/community-stories/). Our approach is to build the most helpful models enabling the world to benefit from the technology power, by aligning our model safety for the generic use cases addressing a standard set of harms. Developers are then in the driver seat to tailor safety for their use case, defining their own policy and deploying the models with the necessary safeguards in their Llama systems. Llama 3.3 was developed following the best practices outlined in our Responsible Use Guide, you can refer to the [Responsible Use Guide](https://llama.meta.com/responsible-use-guide/) to learn more.\n\n#### Llama 3.3 instruct\n\nOur main objectives for conducting safety fine-tuning are to provide the research community with a valuable resource for studying the robustness of safety fine-tuning, as well as to offer developers a readily available, safe, and powerful model for various applications to reduce the developer workload to deploy safe AI systems. For more details on the safety mitigations implemented please read the Llama 3 paper.\n\n**Fine-tuning data**\nWe employ a multi-faceted approach to data collection, combining human-generated data from our vendors with synthetic data to mitigate potential safety risks. We’ve developed many large language model (LLM)-based classifiers that enable us to thoughtfully select high-quality prompts and responses, enhancing data quality control.\n\n**Refusals and Tone**\nBuilding on the work we started with Llama 3, we put a great emphasis on model refusals to benign prompts as well as refusal tone. We included both borderline and adversarial prompts in our safety data strategy, and modified our safety data responses to follow tone guidelines.\n\n#### Llama 3.3 systems\n\n**Large language models, including Llama 3.3, are not designed to be deployed in isolation but instead should be deployed as part of an overall AI system with additional safety guardrails as required.** Developers are expected to deploy system safeguards when building agentic systems. Safeguards are key to achieve the right helpfulness-safety alignment as well as mitigating safety and security risks inherent to the system and any integration of the model or system with external tools.\nAs part of our responsible release approach, we provide the community with [safeguards](https://llama.meta.com/trust-and-safety/) that developers should deploy with Llama models or other LLMs, including Llama Guard 3, Prompt Guard and Code Shield. All our [reference implementations](https://github.com/meta-llama/llama-agentic-system) demos contain these safeguards by default so developers can benefit from system-level safety out-of-the-box.\n\n#### Capability specific considerations \n\n**Tool-use**: Just like in standard software development, developers are responsible for the integration of the LLM with the tools and services of their choice. They should define a clear policy for their use case and assess the integrity of the third party services they use to be aware of the safety and security limitations when using this capability. Refer to the Responsible Use Guide for best practices on the safe deployment of the third party safeguards. \n\n**Multilinguality**: Llama 3.3 supports 7 languages in addition to English: French, German, Hindi, Italian, Portuguese, Spanish, and Thai. Llama may be able to output text in other languages than those that meet performance thresholds for safety and helpfulness. We strongly discourage developers from using this model to converse in non-supported languages without implementing finetuning and system controls in alignment with their policies and the best practices shared in the Responsible Use Guide. \n\n### Evaluations\n\nWe evaluated Llama models for common use cases as well as specific capabilities. Common use cases evaluations measure safety risks of systems for most commonly built applications including chat bot, coding assistant, tool calls. We built dedicated, adversarial evaluation datasets and evaluated systems composed of Llama models and Llama Guard 3 to filter input prompt and output response. It is important to evaluate applications in context, and we recommend building dedicated evaluation dataset for your use case. Prompt Guard and Code Shield are also available if relevant to the application.   \nCapability evaluations measure vulnerabilities of Llama models inherent to specific capabilities, for which were crafted dedicated benchmarks including long context, multilingual, tools calls, coding or memorization.\n\n**Red teaming**   \nFor both scenarios, we conducted recurring red teaming exercises with the goal of discovering risks via adversarial prompting and we used the learnings to improve our benchmarks and safety tuning datasets.   \nWe partnered early with subject-matter experts in critical risk areas to understand the nature of these real-world harms and how such models may lead to unintended harm for society. Based on these conversations, we derived a set of adversarial goals for the red team to attempt to achieve, such as extracting harmful information or reprogramming the model to act in a potentially harmful capacity.  The red team consisted of experts in cybersecurity, adversarial machine learning, responsible AI, and integrity in addition to multilingual content specialists with background in integrity issues in specific geographic markets. . \n\n### Critical and other risks \n\n### We specifically focused our efforts on mitigating the following critical risk areas:\n\n**1- CBRNE (Chemical, Biological, Radiological, Nuclear, and Explosive materials) helpfulness**\nTo assess risks related to proliferation of chemical and biological weapons of the Llama 3 family of models, we performed uplift testing designed to assess whether use of the Llama 3 models could meaningfully increase the capabilities of malicious actors to plan or carry out attacks using these types of weapons.\n\n### **2\\. Child Safety**\n\nChild Safety risk assessments were conducted using a team of experts, to assess the model’s capability to produce outputs that could result in Child Safety risks and inform on any necessary and appropriate risk mitigations via fine tuning. We leveraged those expert red teaming sessions to expand the coverage of our evaluation benchmarks through Llama 3 model development.  For Llama 3, we conducted new in-depth sessions using objective based methodologies to assess the model risks along multiple attack vectors including the additional languages Llama 3 is trained on. We also partnered with content specialists to perform red teaming exercises assessing potentially violating content while taking account of market specific nuances or experiences. \n\n**3\\. Cyber attack enablement**\nOur cyber attack uplift study investigated whether the Llama 3 family of LLMs can enhance human capabilities in hacking tasks, both in terms of skill level and speed.\nOur attack automation study focused on evaluating the capabilities of LLMs when used as autonomous agents in cyber offensive operations, specifically in the context of ransomware attacks. This evaluation was distinct from previous studies that considered LLMs as interactive assistants. The primary objective was to assess whether these models could effectively function as independent agents in executing complex cyber-attacks without human intervention.\n\n### Community \n\nGenerative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership on AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Purple Llama tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to our [Github repository](https://github.com/meta-llama/PurpleLlama). \n\nWe also set up the [Llama Impact Grants](https://llama.meta.com/llama-impact-grants/) program to identify and support the most compelling applications of Meta’s Llama model for societal benefit across three categories: education, climate and open innovation. The 20 finalists from the hundreds of applications can be found [here](https://llama.meta.com/llama-impact-grants/#finalists). \n\nFinally, we put in place a set of resources including an [output reporting mechanism](https://developers.facebook.com/llama_output_feedback) and [bug bounty program](https://www.facebook.com/whitehat) to continuously improve the Llama technology with the help of the community.\n\n## Ethical Considerations and Limitations\n\nThe core values of Llama 3.3 are openness, inclusivity and helpfulness. It is meant to serve everyone, and to work for a wide range of use cases. It is thus designed to be accessible to people across many different backgrounds, experiences and perspectives. Llama 3.3 addresses users and their needs as they are, without insertion unnecessary judgment or normativity, while reflecting the understanding that even content that may appear problematic in some cases can serve valuable purposes in others. It respects the dignity and autonomy of all users, especially in terms of the values of free thought and expression that power innovation and progress. \n\nBut Llama 3.3 is a new technology, and like any new technology, there are risks associated with its use. Testing conducted to date has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, Llama 3.3’s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 3.3 model, developers should perform safety testing and tuning tailored to their specific applications of the model. Please refer to available resources including our [Responsible Use Guide](https://llama.meta.com/responsible-use-guide), [Trust and Safety](https://llama.meta.com/trust-and-safety/) solutions, and other [resources](https://llama.meta.com/docs/get-started/) to learn more about responsible development.",
    "card_content": "---\nlibrary_name: transformers\nlanguage:\n- en\n- fr\n- it\n- pt\n- hi\n- es\n- th\n- de\nbase_model:\n- meta-llama/Llama-3.1-70B\ntags:\n- facebook\n- meta\n- pytorch\n- llama\n- llama-3\nextra_gated_prompt: \"### LLAMA 3.3 COMMUNITY LICENSE AGREEMENT\\nLlama 3.3 Version\\\n  \\ Release Date: December 6, 2024\\n\\\"Agreement\\\" means the terms and conditions for\\\n  \\ use, reproduction, distribution and modification of the Llama Materials set forth\\\n  \\ herein.\\n\\\"Documentation\\\" means the specifications, manuals and documentation\\\n  \\ accompanying Llama 3.3 distributed by Meta at [https://www.llama.com/docs/overview](https://llama.com/docs/overview).\\n\\\n  \\\"Licensee\\\" or \\\"you\\\" means you, or your employer or any other person or entity\\\n  \\ (if you are entering into this Agreement on such person or entity’s behalf), of\\\n  \\ the age required under applicable laws, rules or regulations to provide legal\\\n  \\ consent and that has legal authority to bind your employer or such other person\\\n  \\ or entity if you are entering in this Agreement on their behalf.\\n\\\"Llama 3.3\\\"\\\n  \\ means the foundational large language models and software and algorithms, including\\\n  \\ machine-learning model code, trained model weights, inference-enabling code, training-enabling\\\n  \\ code, fine-tuning enabling code and other elements of the foregoing distributed\\\n  \\ by Meta at [https://www.llama.com/llama-downloads](https://www.llama.com/llama-downloads).\\n\\\n  \\\"Llama Materials\\\" means, collectively, Meta’s proprietary Llama 3.3 and Documentation\\\n  \\ (and any portion thereof) made available under this Agreement.\\n\\\"Meta\\\" or \\\"\\\n  we\\\" means Meta Platforms Ireland Limited (if you are located in or, if you are\\\n  \\ an entity, your principal place of business is in the EEA or Switzerland) and\\\n  \\ Meta Platforms, Inc. (if you are located outside of the EEA or Switzerland).\\n\\\n  By clicking “I Accept” below or by using or distributing any portion or element\\\n  \\ of the Llama Materials, you agree to be bound by this Agreement.\\n1. License Rights\\\n  \\ and Redistribution.\\na. Grant of Rights. You are granted a non-exclusive, worldwide,\\\n  \\ non-transferable and royalty-free limited license under Meta’s intellectual property\\\n  \\ or other rights owned by Meta embodied in the Llama Materials to use, reproduce,\\\n  \\ distribute, copy, create derivative works of, and make modifications to the Llama\\\n  \\ Materials.\\nb. Redistribution and Use.\\ni. If you distribute or make available\\\n  \\ the Llama Materials (or any derivative works thereof), or a product or service\\\n  \\ (including another AI model) that contains any of them, you shall (A) provide\\\n  \\ a copy of this Agreement with any such Llama Materials; and (B) prominently display\\\n  \\ “Built with Llama” on a related website, user interface, blogpost, about page,\\\n  \\ or product documentation. If you use the Llama Materials or any outputs or results\\\n  \\ of the Llama Materials to create, train, fine tune, or otherwise improve an AI\\\n  \\ model, which is distributed or made available, you shall also include “Llama”\\\n  \\ at the beginning of any such AI model name.\\nii. If you receive Llama Materials,\\\n  \\ or any derivative works thereof, from a Licensee as part of an integrated end\\\n  \\ user product, then Section 2 of this Agreement will not apply to you. \\niii. You\\\n  \\ must retain in all copies of the Llama Materials that you distribute the following\\\n  \\ attribution notice within a “Notice” text file distributed as a part of such copies:\\\n  \\ “Llama 3.3 is licensed under the Llama 3.3 Community License, Copyright © Meta\\\n  \\ Platforms, Inc. All Rights Reserved.”\\niv. Your use of the Llama Materials must\\\n  \\ comply with applicable laws and regulations (including trade compliance laws and\\\n  \\ regulations) and adhere to the Acceptable Use Policy for the Llama Materials (available\\\n  \\ at [https://www.llama.com/llama3\\\\_3/use-policy](https://www.llama.com/llama3_3/use-policy)),\\\n  \\ which is hereby incorporated by reference into this Agreement.  \\n2. Additional\\\n  \\ Commercial Terms. If, on the Llama 3.3 version release date, the monthly active\\\n  \\ users of the products or services made available by or for Licensee, or Licensee’s\\\n  \\ affiliates, is greater than 700 million monthly active users in the preceding\\\n  \\ calendar month, you must request a license from Meta, which Meta may grant to\\\n  \\ you in its sole discretion, and you are not authorized to exercise any of the\\\n  \\ rights under this Agreement unless or until Meta otherwise expressly grants you\\\n  \\ such rights.\\n3. Disclaimer of Warranty. UNLESS REQUIRED BY APPLICABLE LAW, THE\\\n  \\ LLAMA MATERIALS AND ANY OUTPUT AND RESULTS THEREFROM ARE PROVIDED ON AN “AS IS”\\\n  \\ BASIS, WITHOUT WARRANTIES OF ANY KIND, AND META DISCLAIMS ALL WARRANTIES OF ANY\\\n  \\ KIND, BOTH EXPRESS AND IMPLIED, INCLUDING, WITHOUT LIMITATION, ANY WARRANTIES\\\n  \\ OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.\\\n  \\ YOU ARE SOLELY RESPONSIBLE FOR DETERMINING THE APPROPRIATENESS OF USING OR REDISTRIBUTING\\\n  \\ THE LLAMA MATERIALS AND ASSUME ANY RISKS ASSOCIATED WITH YOUR USE OF THE LLAMA\\\n  \\ MATERIALS AND ANY OUTPUT AND RESULTS.\\n4. Limitation of Liability. IN NO EVENT\\\n  \\ WILL META OR ITS AFFILIATES BE LIABLE UNDER ANY THEORY OF LIABILITY, WHETHER IN\\\n  \\ CONTRACT, TORT, NEGLIGENCE, PRODUCTS LIABILITY, OR OTHERWISE, ARISING OUT OF THIS\\\n  \\ AGREEMENT, FOR ANY LOST PROFITS OR ANY INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL,\\\n  \\ EXEMPLARY OR PUNITIVE DAMAGES, EVEN IF META OR ITS AFFILIATES HAVE BEEN ADVISED\\\n  \\ OF THE POSSIBILITY OF ANY OF THE FOREGOING.\\n5. Intellectual Property.\\na. No\\\n  \\ trademark licenses are granted under this Agreement, and in connection with the\\\n  \\ Llama Materials, neither Meta nor Licensee may use any name or mark owned by or\\\n  \\ associated with the other or any of its affiliates, except as required for reasonable\\\n  \\ and customary use in describing and redistributing the Llama Materials or as set\\\n  \\ forth in this Section 5(a). Meta hereby grants you a license to use “Llama” (the\\\n  \\ “Mark”) solely as required to comply with the last sentence of Section 1.b.i.\\\n  \\ You will comply with Meta’s brand guidelines (currently accessible at [https://about.meta.com/brand/resources/meta/company-brand/](https://about.meta.com/brand/resources/meta/company-brand/)[)](https://en.facebookbrand.com/).\\\n  \\ All goodwill arising out of your use of the Mark will inure to the benefit of\\\n  \\ Meta.\\nb. Subject to Meta’s ownership of Llama Materials and derivatives made\\\n  \\ by or for Meta, with respect to any derivative works and modifications of the\\\n  \\ Llama Materials that are made by you, as between you and Meta, you are and will\\\n  \\ be the owner of such derivative works and modifications.\\nc. If you institute\\\n  \\ litigation or other proceedings against Meta or any entity (including a cross-claim\\\n  \\ or counterclaim in a lawsuit) alleging that the Llama Materials or Llama 3.3 outputs\\\n  \\ or results, or any portion of any of the foregoing, constitutes infringement of\\\n  \\ intellectual property or other rights owned or licensable by you, then any licenses\\\n  \\ granted to you under this Agreement shall terminate as of the date such litigation\\\n  \\ or claim is filed or instituted. You will indemnify and hold harmless Meta from\\\n  \\ and against any claim by any third party arising out of or related to your use\\\n  \\ or distribution of the Llama Materials.\\n6. Term and Termination. The term of\\\n  \\ this Agreement will commence upon your acceptance of this Agreement or access\\\n  \\ to the Llama Materials and will continue in full force and effect until terminated\\\n  \\ in accordance with the terms and conditions herein. Meta may terminate this Agreement\\\n  \\ if you are in breach of any term or condition of this Agreement. Upon termination\\\n  \\ of this Agreement, you shall delete and cease use of the Llama Materials. Sections\\\n  \\ 3, 4 and 7 shall survive the termination of this Agreement.\\n7. Governing Law\\\n  \\ and Jurisdiction. This Agreement will be governed and construed under the laws\\\n  \\ of the State of California without regard to choice of law principles, and the\\\n  \\ UN Convention on Contracts for the International Sale of Goods does not apply\\\n  \\ to this Agreement. The courts of California shall have exclusive jurisdiction\\\n  \\ of any dispute arising out of this Agreement.\\n### Llama 3.3 Acceptable Use Policy\\n\\\n  Meta is committed to promoting safe and fair use of its tools and features, including\\\n  \\ Llama 3.3. If you access or use Llama 3.3, you agree to this Acceptable Use Policy\\\n  \\ (“**Policy**”). The most recent copy of this policy can be found at [https://www.llama.com/llama3\\\\\\\n  _3/use-policy](https://www.llama.com/llama3_3/use-policy).\\nProhibited Uses\\nWe\\\n  \\ want everyone to use Llama 3.3 safely and responsibly. You agree you will not\\\n  \\ use, or allow others to use, Llama 3.3 to:\\n1. Violate the law or others’ rights,\\\n  \\ including to:\\n\\n   1. Engage in, promote, generate, contribute to, encourage,\\\n  \\ plan, incite, or further illegal or unlawful activity or content, such as:  \\n\\\n  \\      1. Violence or terrorism  \\n      2. Exploitation or harm to children, including\\\n  \\ the solicitation, creation, acquisition, or dissemination of child exploitative\\\n  \\ content or failure to report Child Sexual Abuse Material  \\n      3. Human trafficking,\\\n  \\ exploitation, and sexual violence  \\n      4. The illegal distribution of information\\\n  \\ or materials to minors, including obscene materials, or failure to employ legally\\\n  \\ required age-gating in connection with such information or materials.  \\n    \\\n  \\  5. Sexual solicitation  \\n      6. Any other criminal activity\\n\\n   2. Engage\\\n  \\ in, promote, incite, or facilitate the harassment, abuse, threatening, or bullying\\\n  \\ of individuals or groups of individuals\\n\\n   3. Engage in, promote, incite, or\\\n  \\ facilitate discrimination or other unlawful or harmful conduct in the provision\\\n  \\ of employment, employment benefits, credit, housing, other economic benefits,\\\n  \\ or other essential goods and services\\n\\n   4. Engage in the unauthorized or unlicensed\\\n  \\ practice of any profession including, but not limited to, financial, legal, medical/health,\\\n  \\ or related professional practices\\n\\n   5. Collect, process, disclose, generate,\\\n  \\ or infer private or sensitive information about individuals, including information\\\n  \\ about individuals’ identity, health, or demographic information, unless you have\\\n  \\ obtained the right to do so in accordance with applicable law\\n\\n   6. Engage\\\n  \\ in or facilitate any action or generate any content that infringes, misappropriates,\\\n  \\ or otherwise violates any third-party rights, including the outputs or results\\\n  \\ of any products or services using the Llama Materials\\n\\n   7. Create, generate,\\\n  \\ or facilitate the creation of malicious code, malware, computer viruses or do\\\n  \\ anything else that could disable, overburden, interfere with or impair the proper\\\n  \\ working, integrity, operation or appearance of a website or computer system\\n\\n\\\n  \\   8. Engage in any action, or facilitate any action, to intentionally circumvent\\\n  \\ or remove usage restrictions or other safety measures, or to enable functionality\\\n  \\ disabled by Meta\\n\\n2. Engage in, promote, incite, facilitate, or assist in the\\\n  \\ planning or development of activities that present a risk of death or bodily harm\\\n  \\ to individuals, including use of Llama 3.3 related to the following:\\n\\n   1.\\\n  \\ Military, warfare, nuclear industries or applications, espionage, use for materials\\\n  \\ or activities that are subject to the International Traffic Arms Regulations (ITAR)\\\n  \\ maintained by the United States Department of State or to the U.S. Biological\\\n  \\ Weapons Anti-Terrorism Act of 1989 or the Chemical Weapons Convention Implementation\\\n  \\ Act of 1997\\n\\n   2. Guns and illegal weapons (including weapon development)\\n\\\n  \\n   3. Illegal drugs and regulated/controlled substances\\n\\n   4. Operation of\\\n  \\ critical infrastructure, transportation technologies, or heavy machinery\\n\\n \\\n  \\  5. Self-harm or harm to others, including suicide, cutting, and eating disorders\\n\\\n  \\n   6. Any content intended to incite or promote violence, abuse, or any infliction\\\n  \\ of bodily harm to an individual\\n\\n3. Intentionally deceive or mislead others,\\\n  \\ including use of Llama 3.3 related to the following:\\n\\n   1. Generating, promoting,\\\n  \\ or furthering fraud or the creation or promotion of disinformation\\n\\n   2. Generating,\\\n  \\ promoting, or furthering defamatory content, including the creation of defamatory\\\n  \\ statements, images, or other content\\n\\n   3. Generating, promoting, or further\\\n  \\ distributing spam\\n\\n   4. Impersonating another individual without consent, authorization,\\\n  \\ or legal right\\n\\n   5. Representing that the use of Llama 3.3 or outputs are\\\n  \\ human-generated\\n\\n   6. Generating or facilitating false online engagement, including\\\n  \\ fake reviews and other means of fake online engagement\\n\\n4. Fail to appropriately\\\n  \\ disclose to end users any known dangers of your AI system\\n5. Interact with third\\\n  \\ party tools, models, or software designed to generate unlawful content or engage\\\n  \\ in unlawful or harmful conduct and/or represent that the outputs of such tools,\\\n  \\ models, or software are associated with Meta or Llama 3.3\\nWith respect to any\\\n  \\ multimodal models included in Llama 3.3, the rights granted under Section 1(a)\\\n  \\ of the Llama 3.3 Community License Agreement are not being granted to you if you\\\n  \\ are an individual domiciled in, or a company with a principal place of business\\\n  \\ in, the European Union. This restriction does not apply to end users of a product\\\n  \\ or service that incorporates any such multimodal models.\\nPlease report any violation\\\n  \\ of this Policy, software “bug,” or other problems that could lead to a violation\\\n  \\ of this Policy through one of the following means:\\n* Reporting issues with the\\\n  \\ model: [https://github.com/meta-llama/llama-models/issues](https://l.workplace.com/l.php?u=https%3A%2F%2Fgithub.com%2Fmeta-llama%2Fllama-models%2Fissues&h=AT0qV8W9BFT6NwihiOHRuKYQM_UnkzN_NmHMy91OT55gkLpgi4kQupHUl0ssR4dQsIQ8n3tfd0vtkobvsEvt1l4Ic6GXI2EeuHV8N08OG2WnbAmm0FL4ObkazC6G_256vN0lN9DsykCvCqGZ)\\\n  \\   * Reporting risky content generated by the model: [developers.facebook.com/llama\\\\\\\n  _output\\\\_feedback](http://developers.facebook.com/llama_output_feedback)   * Reporting\\\n  \\ bugs and security concerns: [facebook.com/whitehat/info](http://facebook.com/whitehat/info)\\\n  \\   * Reporting violations of the Acceptable Use Policy or unlicensed uses of Llama\\\n  \\ 3.3: LlamaUseReport@meta.com  \"\nextra_gated_fields:\n  First Name: text\n  Last Name: text\n  Date of birth: date_picker\n  Country: country\n  Affiliation: text\n  Job title:\n    type: select\n    options:\n    - Student\n    - Research Graduate\n    - AI researcher\n    - AI developer/engineer\n    - Reporter\n    - Other\n  geo: ip_location\n  ? By clicking Submit below I accept the terms of the license and acknowledge that\n    the information I provide will be collected stored processed and shared in accordance\n    with the Meta Privacy Policy\n  : checkbox\nextra_gated_description: The information you provide will be collected, stored, processed\n  and shared in accordance with the [Meta Privacy Policy](https://www.facebook.com/privacy/policy/).\nextra_gated_button_content: Submit\nlicense: llama3.3\n---\n## Model Information\n\nThe Meta Llama 3.3 multilingual large language model (LLM) is an instruction tuned generative model in 70B (text in/text out). The Llama 3.3 instruction tuned text only model is optimized for multilingual dialogue use cases and outperforms many of the available open source and closed chat models on common industry benchmarks.\n\n**Model developer**: Meta\n\n**Model Architecture:** Llama 3.3 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety. \n\n|  | Training Data | Params | Input modalities | Output modalities | Context length | GQA | Token count | Knowledge cutoff |\n| :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- |\n| Llama 3.3 (text only)  | A new mix of publicly available online data. | 70B | Multilingual Text | Multilingual Text and code  | 128k | Yes | 15T+ | December 2023 |\n\n**Supported languages:** English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai.\n\n**Llama 3.3 model**. Token counts refer to pretraining data only. All model versions use Grouped-Query Attention (GQA) for improved inference scalability.\n\n**Model Release Date:** \n\n* **70B Instruct: December 6, 2024** \n\n**Status:** This is a static model trained on an offline dataset. Future versions of the tuned models will be released as we improve model safety with community feedback.\n\n**License** A custom commercial license, the Llama 3.3 Community License Agreement, is available at: [https://github.com/meta-llama/llama-models/blob/main/models/llama3\\_3/LICENSE](https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/LICENSE)\n\nWhere to send questions or comments about the model Instructions on how to provide feedback or comments on the model can be found in the model [README](https://github.com/meta-llama/llama3). For more technical information about generation parameters and recipes for how to use Llama 3.3 in applications, please go [here](https://github.com/meta-llama/llama-recipes). \n\n## Intended Use\n\n**Intended Use Cases** Llama 3.3 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks. The Llama 3.3 model also supports the ability to leverage the outputs of its models to improve other models including synthetic data generation and distillation. The Llama 3.3 Community License allows for these use cases. \n\n**Out-of-scope** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.3 Community License. Use in languages beyond those explicitly referenced as supported in this model card\\*\\*.\n\n\\*\\*Note: Llama 3.3 has been trained on a broader collection of languages than the 8 supported languages. Developers may fine-tune Llama 3.3 models for languages beyond the 8 supported languages provided they comply with the Llama 3.3 Community License and the Acceptable Use Policy and in such cases are responsible for ensuring that any uses of Llama 3.3 in additional languages is done in a safe and responsible manner.\n\n## How to use\n\nThis repository contains two versions of Llama-3.3-70B-Instruct, for use with transformers and with the original `llama` codebase.\n\n### Use with transformers\n\nStarting with `transformers >= 4.45.0` onward, you can run conversational inference using the Transformers `pipeline` abstraction or by leveraging the Auto classes with the `generate()` function.\n\nMake sure to update your transformers installation via `pip install --upgrade transformers`.\n\nSee the snippet below for usage with Transformers:\n\n```python\nimport transformers\nimport torch\n\nmodel_id = \"meta-llama/Llama-3.3-70B-Instruct\"\n\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model_id,\n    model_kwargs={\"torch_dtype\": torch.bfloat16},\n    device_map=\"auto\",\n)\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n    {\"role\": \"user\", \"content\": \"Who are you?\"},\n]\n\noutputs = pipeline(\n    messages,\n    max_new_tokens=256,\n)\nprint(outputs[0][\"generated_text\"][-1])\n```\n\n### Tool use with transformers\n\nLLaMA-3.3 supports multiple tool use formats. You can see a full guide to prompt formatting [here](https://llama.meta.com/docs/model-cards-and-prompt-formats/llama3_1/).\n\nTool use is also supported through [chat templates](https://huggingface.co/docs/transformers/main/chat_templating#advanced-tool-use--function-calling) in Transformers. \nHere is a quick example showing a single simple tool:\n\n```python\n# First, define a tool\ndef get_current_temperature(location: str) -> float:\n    \"\"\"\n    Get the current temperature at a location.\n    \n    Args:\n        location: The location to get the temperature for, in the format \"City, Country\"\n    Returns:\n        The current temperature at the specified location in the specified units, as a float.\n    \"\"\"\n    return 22.  # A real function should probably actually get the temperature!\n\n# Next, create a chat and apply the chat template\nmessages = [\n  {\"role\": \"system\", \"content\": \"You are a bot that responds to weather queries.\"},\n  {\"role\": \"user\", \"content\": \"Hey, what's the temperature in Paris right now?\"}\n]\n\ninputs = tokenizer.apply_chat_template(messages, tools=[get_current_temperature], add_generation_prompt=True)\n```\n\nYou can then generate text from this input as normal. If the model generates a tool call, you should add it to the chat like so:\n\n```python\ntool_call = {\"name\": \"get_current_temperature\", \"arguments\": {\"location\": \"Paris, France\"}}\nmessages.append({\"role\": \"assistant\", \"tool_calls\": [{\"type\": \"function\", \"function\": tool_call}]})\n```\n\nand then call the tool and append the result, with the `tool` role, like so:\n\n```python\nmessages.append({\"role\": \"tool\", \"name\": \"get_current_temperature\", \"content\": \"22.0\"})\n```\n\nAfter that, you can `generate()` again to let the model use the tool result in the chat. Note that this was a very brief introduction to tool calling - for more information,\nsee the [LLaMA prompt format docs](https://llama.meta.com/docs/model-cards-and-prompt-formats/llama3_1/) and the Transformers [tool use documentation](https://huggingface.co/docs/transformers/main/chat_templating#advanced-tool-use--function-calling).\n\n\n### Use with `bitsandbytes`\n\nThe model checkpoints can be used in `8-bit` and `4-bit` for further memory optimisations using `bitsandbytes` and `transformers`\n\nSee the snippet below for usage:\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_id = \"meta-llama/Llama-3.3-70B-Instruct\"\nquantization_config = BitsAndBytesConfig(load_in_8bit=True)\n\nquantized_model = AutoModelForCausalLM.from_pretrained(\n\tmodel_id, device_map=\"auto\", torch_dtype=torch.bfloat16, quantization_config=quantization_config)\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\ninput_text = \"What are we having for dinner?\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n\noutput = quantized_model.generate(**input_ids, max_new_tokens=10)\n\nprint(tokenizer.decode(output[0], skip_special_tokens=True))\n```\n\nTo load in 4-bit simply pass `load_in_4bit=True`\n\n### Use with `llama`\n\nPlease, follow the instructions in the [repository](https://github.com/meta-llama/llama).\n\nTo download Original checkpoints, see the example command below leveraging `huggingface-cli`:\n\n```\nhuggingface-cli download meta-llama/Llama-3.3-70B-Instruct --include \"original/*\" --local-dir Llama-3.3-70B-Instruct\n```\n\n## Hardware and Software\n\n**Training Factors** We used custom training libraries, Meta's custom built GPU cluster, and production infrastructure for pretraining. Fine-tuning, annotation, and evaluation were also performed on production infrastructure.\n\n**Training Energy Use** Training utilized a cumulative of **39.3**M GPU hours of computation on H100-80GB (TDP of 700W) type hardware, per the table below. Training time is the total GPU time required for training each model and power consumption is the peak power capacity per GPU device used, adjusted for power usage efficiency. \n\n## \n\n## **Training Greenhouse Gas Emissions** Estimated total location-based greenhouse gas emissions were **11,390** tons CO2eq for training. Since 2020, Meta has maintained net zero greenhouse gas emissions in its global operations and matched 100% of its electricity use with renewable energy, therefore the total market-based greenhouse gas emissions for training were 0 tons CO2eq.\n\n|  | Training Time (GPU hours) | Training Power Consumption (W) | Training Location-Based Greenhouse Gas Emissions (tons CO2eq) | Training Market-Based Greenhouse Gas Emissions (tons CO2eq) |\n| :---- | :---: | :---: | :---: | :---: |\n| Llama 3.3 70B | 7.0M | 700 | 2,040 | 0 |\n\n## The methodology used to determine training energy use and greenhouse gas emissions can be found [here](https://arxiv.org/pdf/2204.05149).  Since Meta is openly releasing these models, the training energy use and greenhouse gas emissions  will not be incurred by others.\n\n## Training Data\n\n**Overview:** Llama 3.3 was pretrained on \\~15 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over 25M synthetically generated examples. \n\n**Data Freshness:** The pretraining data has a cutoff of December 2023\\.\n\n## Benchmarks \\- English Text\n\nIn this section, we report the results for Llama 3.3 relative to our previous models. \n\n### Instruction tuned models\n\n## \n\n| Category | Benchmark | \\# Shots | Metric | Llama 3.1 8B Instruct | Llama 3.1 70B Instruct | Llama-3.3 70B Instruct | Llama 3.1 405B Instruct |\n| :---- | :---- | ----- | :---- | ----- | ----- | ----- | ----- |\n|  | MMLU (CoT) | 0 | macro\\_avg/acc | 73.0 | 86.0 | 86.0 | 88.6 |\n|  | MMLU Pro (CoT) | 5 | macro\\_avg/acc | 48.3 | 66.4 | 68.9 | 73.3 |\n| Steerability | IFEval |  |  | 80.4 | 87.5 | 92.1 | 88.6 |\n| Reasoning | GPQA Diamond (CoT) | 0 | acc | 31.8 | 48.0 | 50.5 | 49.0 |\n| Code | HumanEval | 0 | pass@1 | 72.6 | 80.5 | 88.4 | 89.0 |\n|  | MBPP EvalPlus (base) | 0 | pass@1 | 72.8 | 86.0 | 87.6 | 88.6 |\n| Math | MATH (CoT) | 0 | sympy\\_intersection\\_score | 51.9 | 68.0 | 77.0 | 73.8 |\n| Tool Use | BFCL v2 | 0 | overall\\_ast\\_summary/macro\\_avg/valid | 65.4 | 77.5 | 77.3 | 81.1 |\n| Multilingual | MGSM | 0 | em | 68.9 | 86.9 | 91.1 | 91.6 |\n\n## \n\n## Responsibility & Safety\n\nAs part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:\n\n* Enable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama.\n* Protect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm.\n* Provide protections for the community to help prevent the misuse of our models.\n\n### Responsible deployment\n\nLlama is a foundational technology designed to be used in a variety of use cases, examples on how Meta’s Llama models have been responsibly deployed can be found in our [Community Stories webpage](https://llama.meta.com/community-stories/). Our approach is to build the most helpful models enabling the world to benefit from the technology power, by aligning our model safety for the generic use cases addressing a standard set of harms. Developers are then in the driver seat to tailor safety for their use case, defining their own policy and deploying the models with the necessary safeguards in their Llama systems. Llama 3.3 was developed following the best practices outlined in our Responsible Use Guide, you can refer to the [Responsible Use Guide](https://llama.meta.com/responsible-use-guide/) to learn more.\n\n#### Llama 3.3 instruct\n\nOur main objectives for conducting safety fine-tuning are to provide the research community with a valuable resource for studying the robustness of safety fine-tuning, as well as to offer developers a readily available, safe, and powerful model for various applications to reduce the developer workload to deploy safe AI systems. For more details on the safety mitigations implemented please read the Llama 3 paper.\n\n**Fine-tuning data**\nWe employ a multi-faceted approach to data collection, combining human-generated data from our vendors with synthetic data to mitigate potential safety risks. We’ve developed many large language model (LLM)-based classifiers that enable us to thoughtfully select high-quality prompts and responses, enhancing data quality control.\n\n**Refusals and Tone**\nBuilding on the work we started with Llama 3, we put a great emphasis on model refusals to benign prompts as well as refusal tone. We included both borderline and adversarial prompts in our safety data strategy, and modified our safety data responses to follow tone guidelines.\n\n#### Llama 3.3 systems\n\n**Large language models, including Llama 3.3, are not designed to be deployed in isolation but instead should be deployed as part of an overall AI system with additional safety guardrails as required.** Developers are expected to deploy system safeguards when building agentic systems. Safeguards are key to achieve the right helpfulness-safety alignment as well as mitigating safety and security risks inherent to the system and any integration of the model or system with external tools.\nAs part of our responsible release approach, we provide the community with [safeguards](https://llama.meta.com/trust-and-safety/) that developers should deploy with Llama models or other LLMs, including Llama Guard 3, Prompt Guard and Code Shield. All our [reference implementations](https://github.com/meta-llama/llama-agentic-system) demos contain these safeguards by default so developers can benefit from system-level safety out-of-the-box.\n\n#### Capability specific considerations \n\n**Tool-use**: Just like in standard software development, developers are responsible for the integration of the LLM with the tools and services of their choice. They should define a clear policy for their use case and assess the integrity of the third party services they use to be aware of the safety and security limitations when using this capability. Refer to the Responsible Use Guide for best practices on the safe deployment of the third party safeguards. \n\n**Multilinguality**: Llama 3.3 supports 7 languages in addition to English: French, German, Hindi, Italian, Portuguese, Spanish, and Thai. Llama may be able to output text in other languages than those that meet performance thresholds for safety and helpfulness. We strongly discourage developers from using this model to converse in non-supported languages without implementing finetuning and system controls in alignment with their policies and the best practices shared in the Responsible Use Guide. \n\n### Evaluations\n\nWe evaluated Llama models for common use cases as well as specific capabilities. Common use cases evaluations measure safety risks of systems for most commonly built applications including chat bot, coding assistant, tool calls. We built dedicated, adversarial evaluation datasets and evaluated systems composed of Llama models and Llama Guard 3 to filter input prompt and output response. It is important to evaluate applications in context, and we recommend building dedicated evaluation dataset for your use case. Prompt Guard and Code Shield are also available if relevant to the application.   \nCapability evaluations measure vulnerabilities of Llama models inherent to specific capabilities, for which were crafted dedicated benchmarks including long context, multilingual, tools calls, coding or memorization.\n\n**Red teaming**   \nFor both scenarios, we conducted recurring red teaming exercises with the goal of discovering risks via adversarial prompting and we used the learnings to improve our benchmarks and safety tuning datasets.   \nWe partnered early with subject-matter experts in critical risk areas to understand the nature of these real-world harms and how such models may lead to unintended harm for society. Based on these conversations, we derived a set of adversarial goals for the red team to attempt to achieve, such as extracting harmful information or reprogramming the model to act in a potentially harmful capacity.  The red team consisted of experts in cybersecurity, adversarial machine learning, responsible AI, and integrity in addition to multilingual content specialists with background in integrity issues in specific geographic markets. . \n\n### Critical and other risks \n\n### We specifically focused our efforts on mitigating the following critical risk areas:\n\n**1- CBRNE (Chemical, Biological, Radiological, Nuclear, and Explosive materials) helpfulness**\nTo assess risks related to proliferation of chemical and biological weapons of the Llama 3 family of models, we performed uplift testing designed to assess whether use of the Llama 3 models could meaningfully increase the capabilities of malicious actors to plan or carry out attacks using these types of weapons.\n\n### **2\\. Child Safety**\n\nChild Safety risk assessments were conducted using a team of experts, to assess the model’s capability to produce outputs that could result in Child Safety risks and inform on any necessary and appropriate risk mitigations via fine tuning. We leveraged those expert red teaming sessions to expand the coverage of our evaluation benchmarks through Llama 3 model development.  For Llama 3, we conducted new in-depth sessions using objective based methodologies to assess the model risks along multiple attack vectors including the additional languages Llama 3 is trained on. We also partnered with content specialists to perform red teaming exercises assessing potentially violating content while taking account of market specific nuances or experiences. \n\n**3\\. Cyber attack enablement**\nOur cyber attack uplift study investigated whether the Llama 3 family of LLMs can enhance human capabilities in hacking tasks, both in terms of skill level and speed.\nOur attack automation study focused on evaluating the capabilities of LLMs when used as autonomous agents in cyber offensive operations, specifically in the context of ransomware attacks. This evaluation was distinct from previous studies that considered LLMs as interactive assistants. The primary objective was to assess whether these models could effectively function as independent agents in executing complex cyber-attacks without human intervention.\n\n### Community \n\nGenerative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership on AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Purple Llama tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to our [Github repository](https://github.com/meta-llama/PurpleLlama). \n\nWe also set up the [Llama Impact Grants](https://llama.meta.com/llama-impact-grants/) program to identify and support the most compelling applications of Meta’s Llama model for societal benefit across three categories: education, climate and open innovation. The 20 finalists from the hundreds of applications can be found [here](https://llama.meta.com/llama-impact-grants/#finalists). \n\nFinally, we put in place a set of resources including an [output reporting mechanism](https://developers.facebook.com/llama_output_feedback) and [bug bounty program](https://www.facebook.com/whitehat) to continuously improve the Llama technology with the help of the community.\n\n## Ethical Considerations and Limitations\n\nThe core values of Llama 3.3 are openness, inclusivity and helpfulness. It is meant to serve everyone, and to work for a wide range of use cases. It is thus designed to be accessible to people across many different backgrounds, experiences and perspectives. Llama 3.3 addresses users and their needs as they are, without insertion unnecessary judgment or normativity, while reflecting the understanding that even content that may appear problematic in some cases can serve valuable purposes in others. It respects the dignity and autonomy of all users, especially in terms of the values of free thought and expression that power innovation and progress. \n\nBut Llama 3.3 is a new technology, and like any new technology, there are risks associated with its use. Testing conducted to date has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, Llama 3.3’s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 3.3 model, developers should perform safety testing and tuning tailored to their specific applications of the model. Please refer to available resources including our [Responsible Use Guide](https://llama.meta.com/responsible-use-guide), [Trust and Safety](https://llama.meta.com/trust-and-safety/) solutions, and other [resources](https://llama.meta.com/docs/get-started/) to learn more about responsible development.",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModelForCausalLM",
      "custom_class": null,
      "pipeline_tag": "text-generation",
      "processor": "AutoTokenizer"
    },
    "safetensors": {
      "parameters": {
        "BF16": 70553706496
      },
      "total": 70553706496
    },
    "model_index": null,
    "trending_score": null
  },
  {
    "model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
    "model_name": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
    "author": "TinyLlama",
    "downloads": 1020500,
    "downloads_all_time": null,
    "likes": 1194,
    "tags": [
      "transformers",
      "safetensors",
      "llama",
      "text-generation",
      "conversational",
      "en",
      "dataset:cerebras/SlimPajama-627B",
      "dataset:bigcode/starcoderdata",
      "dataset:HuggingFaceH4/ultrachat_200k",
      "dataset:HuggingFaceH4/ultrafeedback_binarized",
      "license:apache-2.0",
      "autotrain_compatible",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0",
    "dependencies": [
      [
        "transformers",
        null
      ],
      [
        "torch",
        null
      ],
      [
        "accelerate",
        null
      ]
    ],
    "last_modified": "2024-03-17T05:07:08+00:00",
    "created_at": "2023-12-30T06:27:30+00:00",
    "analysis_date": "2025-03-22T00:42:42.877348",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "llama",
    "pipeline_tag": "text-generation",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "license": "apache-2.0",
      "datasets": [
        "cerebras/SlimPajama-627B",
        "bigcode/starcoderdata",
        "HuggingFaceH4/ultrachat_200k",
        "HuggingFaceH4/ultrafeedback_binarized"
      ],
      "language": [
        "en"
      ],
      "widget": [
        {
          "example_title": "Fibonacci (Python)",
          "messages": [
            {
              "role": "system",
              "content": "You are a chatbot who can help code!"
            },
            {
              "role": "user",
              "content": "Write me a function to calculate the first 10 digits of the fibonacci sequence in Python and print it out to the CLI."
            }
          ]
        }
      ]
    },
    "card_text": "<div align=\"center\">\n\n# TinyLlama-1.1B\n</div>\n\nhttps://github.com/jzhang38/TinyLlama\n\nThe TinyLlama project aims to **pretrain** a **1.1B Llama model on 3 trillion tokens**. With some proper optimization, we can achieve this within a span of \"just\" 90 days using 16 A100-40G GPUs 🚀🚀. The training has started on 2023-09-01. \n\n\nWe adopted exactly the same architecture and tokenizer as Llama 2. This means TinyLlama can be plugged and played in many open-source projects built upon Llama. Besides, TinyLlama is compact with only 1.1B parameters. This compactness allows it to cater to a multitude of applications demanding a restricted computation and memory footprint.\n\n#### This Model\nThis is the chat model finetuned on top of [TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T](https://huggingface.co/TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T). **We follow [HF's Zephyr](https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha)'s training recipe.** The model was \" initially fine-tuned on a variant of the [`UltraChat`](https://huggingface.co/datasets/stingning/ultrachat) dataset, which contains a diverse range of synthetic dialogues generated by ChatGPT. \nWe then further aligned the model with [🤗 TRL's](https://github.com/huggingface/trl) `DPOTrainer` on the [openbmb/UltraFeedback](https://huggingface.co/datasets/openbmb/UltraFeedback) dataset, which contain 64k prompts and model completions that are ranked by GPT-4.\" \n\n\n#### How to use\nYou will need the transformers>=4.34\nDo check the [TinyLlama](https://github.com/jzhang38/TinyLlama) github page for more information.\n\n```python\n# Install transformers from source - only needed for versions <= v4.34\n# pip install git+https://github.com/huggingface/transformers.git\n# pip install accelerate\n\nimport torch\nfrom transformers import pipeline\n\npipe = pipeline(\"text-generation\", model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n\n# We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n    },\n    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n]\nprompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\noutputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\nprint(outputs[0][\"generated_text\"])\n# <|system|>\n# You are a friendly chatbot who always responds in the style of a pirate.</s>\n# <|user|>\n# How many helicopters can a human eat in one sitting?</s>\n# <|assistant|>\n# ...\n```",
    "card_content": "---\nlicense: apache-2.0\ndatasets:\n- cerebras/SlimPajama-627B\n- bigcode/starcoderdata\n- HuggingFaceH4/ultrachat_200k\n- HuggingFaceH4/ultrafeedback_binarized\nlanguage:\n- en\nwidget:\n- example_title: Fibonacci (Python)\n  messages:\n  - role: system\n    content: You are a chatbot who can help code!\n  - role: user\n    content: Write me a function to calculate the first 10 digits of the fibonacci\n      sequence in Python and print it out to the CLI.\n---\n<div align=\"center\">\n\n# TinyLlama-1.1B\n</div>\n\nhttps://github.com/jzhang38/TinyLlama\n\nThe TinyLlama project aims to **pretrain** a **1.1B Llama model on 3 trillion tokens**. With some proper optimization, we can achieve this within a span of \"just\" 90 days using 16 A100-40G GPUs 🚀🚀. The training has started on 2023-09-01. \n\n\nWe adopted exactly the same architecture and tokenizer as Llama 2. This means TinyLlama can be plugged and played in many open-source projects built upon Llama. Besides, TinyLlama is compact with only 1.1B parameters. This compactness allows it to cater to a multitude of applications demanding a restricted computation and memory footprint.\n\n#### This Model\nThis is the chat model finetuned on top of [TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T](https://huggingface.co/TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T). **We follow [HF's Zephyr](https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha)'s training recipe.** The model was \" initially fine-tuned on a variant of the [`UltraChat`](https://huggingface.co/datasets/stingning/ultrachat) dataset, which contains a diverse range of synthetic dialogues generated by ChatGPT. \nWe then further aligned the model with [🤗 TRL's](https://github.com/huggingface/trl) `DPOTrainer` on the [openbmb/UltraFeedback](https://huggingface.co/datasets/openbmb/UltraFeedback) dataset, which contain 64k prompts and model completions that are ranked by GPT-4.\" \n\n\n#### How to use\nYou will need the transformers>=4.34\nDo check the [TinyLlama](https://github.com/jzhang38/TinyLlama) github page for more information.\n\n```python\n# Install transformers from source - only needed for versions <= v4.34\n# pip install git+https://github.com/huggingface/transformers.git\n# pip install accelerate\n\nimport torch\nfrom transformers import pipeline\n\npipe = pipeline(\"text-generation\", model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n\n# We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n    },\n    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n]\nprompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\noutputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\nprint(outputs[0][\"generated_text\"])\n# <|system|>\n# You are a friendly chatbot who always responds in the style of a pirate.</s>\n# <|user|>\n# How many helicopters can a human eat in one sitting?</s>\n# <|assistant|>\n# ...\n```",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModelForCausalLM",
      "custom_class": null,
      "pipeline_tag": "text-generation",
      "processor": "AutoTokenizer"
    },
    "safetensors": {
      "parameters": {
        "BF16": 1100048384
      },
      "total": 1100048384
    },
    "model_index": null,
    "trending_score": null
  },
  {
    "model_id": "Qwen/Qwen2.5-0.5B-Instruct",
    "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
    "author": "Qwen",
    "downloads": 998680,
    "downloads_all_time": null,
    "likes": 273,
    "tags": [
      "transformers",
      "safetensors",
      "qwen2",
      "text-generation",
      "chat",
      "conversational",
      "en",
      "arxiv:2407.10671",
      "base_model:Qwen/Qwen2.5-0.5B",
      "base_model:finetune:Qwen/Qwen2.5-0.5B",
      "license:apache-2.0",
      "autotrain_compatible",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct",
    "dependencies": [
      [
        "transformers",
        null
      ]
    ],
    "last_modified": "2024-09-25T12:32:56+00:00",
    "created_at": "2024-09-16T11:52:46+00:00",
    "analysis_date": "2025-03-22T00:42:44.288011",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "qwen2",
    "pipeline_tag": "text-generation",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "license": "apache-2.0",
      "license_link": "https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct/blob/main/LICENSE",
      "language": [
        "en"
      ],
      "pipeline_tag": "text-generation",
      "base_model": "Qwen/Qwen2.5-0.5B",
      "tags": [
        "chat"
      ],
      "library_name": "transformers"
    },
    "card_text": "\n# Qwen2.5-0.5B-Instruct\n\n## Introduction\n\nQwen2.5 is the latest series of Qwen large language models. For Qwen2.5, we release a number of base language models and instruction-tuned language models ranging from 0.5 to 72 billion parameters. Qwen2.5 brings the following improvements upon Qwen2:\n\n- Significantly **more knowledge** and has greatly improved capabilities in **coding** and **mathematics**, thanks to our specialized expert models in these domains.\n- Significant improvements in **instruction following**, **generating long texts** (over 8K tokens), **understanding structured data** (e.g, tables), and **generating structured outputs** especially JSON. **More resilient to the diversity of system prompts**, enhancing role-play implementation and condition-setting for chatbots.\n- **Long-context Support** up to 128K tokens and can generate up to 8K tokens.\n- **Multilingual support** for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more. \n\n**This repo contains the instruction-tuned 0.5B Qwen2.5 model**, which has the following features:\n- Type: Causal Language Models\n- Training Stage: Pretraining & Post-training\n- Architecture: transformers with RoPE, SwiGLU, RMSNorm, Attention QKV bias and tied word embeddings\n- Number of Parameters: 0.49B\n- Number of Paramaters (Non-Embedding): 0.36B\n- Number of Layers: 24\n- Number of Attention Heads (GQA): 14 for Q and 2 for KV\n- Context Length: Full 32,768 tokens and generation 8192 tokens\n\nFor more details, please refer to our [blog](https://qwenlm.github.io/blog/qwen2.5/), [GitHub](https://github.com/QwenLM/Qwen2.5), and [Documentation](https://qwen.readthedocs.io/en/latest/).\n\n## Requirements\n\nThe code of Qwen2.5 has been in the latest Hugging face `transformers` and we advise you to use the latest version of `transformers`.\n\nWith `transformers<4.37.0`, you will encounter the following error:\n```\nKeyError: 'qwen2'\n```\n\n## Quickstart\n\nHere provides a code snippet with `apply_chat_template` to show you how to load the tokenizer and model and how to generate contents.\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=512\n)\ngenerated_ids = [\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\n\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n```\n\n\n## Evaluation & Performance\n\nDetailed evaluation results are reported in this [📑 blog](https://qwenlm.github.io/blog/qwen2.5/).\n\nFor requirements on GPU memory and the respective throughput, see results [here](https://qwen.readthedocs.io/en/latest/benchmark/speed_benchmark.html).\n\n## Citation\n\nIf you find our work helpful, feel free to give us a cite.\n\n```\n@misc{qwen2.5,\n    title = {Qwen2.5: A Party of Foundation Models},\n    url = {https://qwenlm.github.io/blog/qwen2.5/},\n    author = {Qwen Team},\n    month = {September},\n    year = {2024}\n}\n\n@article{qwen2,\n      title={Qwen2 Technical Report}, \n      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\n      journal={arXiv preprint arXiv:2407.10671},\n      year={2024}\n}\n```",
    "card_content": "---\nlicense: apache-2.0\nlicense_link: https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct/blob/main/LICENSE\nlanguage:\n- en\npipeline_tag: text-generation\nbase_model: Qwen/Qwen2.5-0.5B\ntags:\n- chat\nlibrary_name: transformers\n---\n\n# Qwen2.5-0.5B-Instruct\n\n## Introduction\n\nQwen2.5 is the latest series of Qwen large language models. For Qwen2.5, we release a number of base language models and instruction-tuned language models ranging from 0.5 to 72 billion parameters. Qwen2.5 brings the following improvements upon Qwen2:\n\n- Significantly **more knowledge** and has greatly improved capabilities in **coding** and **mathematics**, thanks to our specialized expert models in these domains.\n- Significant improvements in **instruction following**, **generating long texts** (over 8K tokens), **understanding structured data** (e.g, tables), and **generating structured outputs** especially JSON. **More resilient to the diversity of system prompts**, enhancing role-play implementation and condition-setting for chatbots.\n- **Long-context Support** up to 128K tokens and can generate up to 8K tokens.\n- **Multilingual support** for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more. \n\n**This repo contains the instruction-tuned 0.5B Qwen2.5 model**, which has the following features:\n- Type: Causal Language Models\n- Training Stage: Pretraining & Post-training\n- Architecture: transformers with RoPE, SwiGLU, RMSNorm, Attention QKV bias and tied word embeddings\n- Number of Parameters: 0.49B\n- Number of Paramaters (Non-Embedding): 0.36B\n- Number of Layers: 24\n- Number of Attention Heads (GQA): 14 for Q and 2 for KV\n- Context Length: Full 32,768 tokens and generation 8192 tokens\n\nFor more details, please refer to our [blog](https://qwenlm.github.io/blog/qwen2.5/), [GitHub](https://github.com/QwenLM/Qwen2.5), and [Documentation](https://qwen.readthedocs.io/en/latest/).\n\n## Requirements\n\nThe code of Qwen2.5 has been in the latest Hugging face `transformers` and we advise you to use the latest version of `transformers`.\n\nWith `transformers<4.37.0`, you will encounter the following error:\n```\nKeyError: 'qwen2'\n```\n\n## Quickstart\n\nHere provides a code snippet with `apply_chat_template` to show you how to load the tokenizer and model and how to generate contents.\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=512\n)\ngenerated_ids = [\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\n\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n```\n\n\n## Evaluation & Performance\n\nDetailed evaluation results are reported in this [📑 blog](https://qwenlm.github.io/blog/qwen2.5/).\n\nFor requirements on GPU memory and the respective throughput, see results [here](https://qwen.readthedocs.io/en/latest/benchmark/speed_benchmark.html).\n\n## Citation\n\nIf you find our work helpful, feel free to give us a cite.\n\n```\n@misc{qwen2.5,\n    title = {Qwen2.5: A Party of Foundation Models},\n    url = {https://qwenlm.github.io/blog/qwen2.5/},\n    author = {Qwen Team},\n    month = {September},\n    year = {2024}\n}\n\n@article{qwen2,\n      title={Qwen2 Technical Report}, \n      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\n      journal={arXiv preprint arXiv:2407.10671},\n      year={2024}\n}\n```",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModelForCausalLM",
      "custom_class": null,
      "pipeline_tag": "text-generation",
      "processor": "AutoTokenizer"
    },
    "safetensors": {
      "parameters": {
        "BF16": 494032768
      },
      "total": 494032768
    },
    "model_index": null,
    "trending_score": null
  },
  {
    "model_id": "brunopio/Llama3-8B-1.58-100B-tokens-GGUF",
    "model_name": "brunopio/Llama3-8B-1.58-100B-tokens-GGUF",
    "author": "brunopio",
    "downloads": 987241,
    "downloads_all_time": null,
    "likes": 15,
    "tags": [
      "transformers",
      "safetensors",
      "gguf",
      "llama",
      "text-generation",
      "conversational",
      "base_model:HF1BitLLM/Llama3-8B-1.58-100B-tokens",
      "base_model:quantized:HF1BitLLM/Llama3-8B-1.58-100B-tokens",
      "autotrain_compatible",
      "text-generation-inference",
      "endpoints_compatible",
      "8-bit",
      "bitnet",
      "region:us"
    ],
    "card_url": "https://huggingface.co/brunopio/Llama3-8B-1.58-100B-tokens-GGUF",
    "dependencies": [
      [
        "transformers",
        null
      ]
    ],
    "last_modified": "2024-09-19T16:53:01+00:00",
    "created_at": "2024-09-19T15:40:43+00:00",
    "analysis_date": "2025-03-22T00:42:45.247723",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "llama",
    "pipeline_tag": "text-generation",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "library_name": "transformers",
      "base_model": [
        "meta-llama/Meta-Llama-3-8B-Instruct",
        "HF1BitLLM/Llama3-8B-1.58-100B-tokens"
      ]
    },
    "card_text": "\n# Model Card for Model ID\n\n### Llama3-8B-1.58 Models\n\nThis model was converted to GGUF format from [HF1BitLLM/Llama3-8B-1.58-100B-tokens](https://huggingface.co/HF1BitLLM/Llama3-8B-1.58-100B-tokens) using llama.cpp.\n\n\n## Use with llama.cpp\nInstall llama.cpp through brew (works on Mac and Linux)\n\n```bash\nbrew install llama.cpp\n\n```\nInvoke the llama.cpp server or the CLI.\n\n### CLI:\n```bash\nllama-cli --hf-repo brunopio/Llama3-8B-1.58-100B-tokens-GGUF --hf-file Llama3-8B-1.58-100B-tokens-GGUF -p \"The meaning to life and the universe is\"\n```\n\n### Server:\n```bash\nllama-server --hf-repo brunopio/Llama3-8B-1.58-100B-tokens-GGUF --hf-file Llama3-8B-1.58-100B-tokens-GGUF -c 2048\n```\n\nNote: You can also use this checkpoint directly through the [usage steps](https://github.com/ggerganov/llama.cpp?tab=readme-ov-file#usage) listed in the Llama.cpp repo as well.\n\nStep 1: Clone llama.cpp from GitHub.\n```\ngit clone https://github.com/ggerganov/llama.cpp\n```\n\nStep 2: Move into the llama.cpp folder and build it with `LLAMA_CURL=1` flag along with other hardware-specific flags (for ex: LLAMA_CUDA=1 for Nvidia GPUs on Linux).\n```\ncd llama.cpp && LLAMA_CURL=1 make\n```\n\nStep 3: Run inference through the main binary.\n```\n./llama-cli --hf-repo brunopio/Llama3-8B-1.58-100B-tokens-GGUF --hf-file Llama3-8B-1.58-100B-tokens-GGUF -p \"The meaning to life and the universe is\"\n```\nor \n```\n./llama-server --hf-repo brunopio/Llama3-8B-1.58-100B-tokens-GGUF --hf-file Llama3-8B-1.58-100B-tokens-GGUF -c 2048\n```",
    "card_content": "---\nlibrary_name: transformers\nbase_model:\n- meta-llama/Meta-Llama-3-8B-Instruct\n- HF1BitLLM/Llama3-8B-1.58-100B-tokens\n---\n\n# Model Card for Model ID\n\n### Llama3-8B-1.58 Models\n\nThis model was converted to GGUF format from [HF1BitLLM/Llama3-8B-1.58-100B-tokens](https://huggingface.co/HF1BitLLM/Llama3-8B-1.58-100B-tokens) using llama.cpp.\n\n\n## Use with llama.cpp\nInstall llama.cpp through brew (works on Mac and Linux)\n\n```bash\nbrew install llama.cpp\n\n```\nInvoke the llama.cpp server or the CLI.\n\n### CLI:\n```bash\nllama-cli --hf-repo brunopio/Llama3-8B-1.58-100B-tokens-GGUF --hf-file Llama3-8B-1.58-100B-tokens-GGUF -p \"The meaning to life and the universe is\"\n```\n\n### Server:\n```bash\nllama-server --hf-repo brunopio/Llama3-8B-1.58-100B-tokens-GGUF --hf-file Llama3-8B-1.58-100B-tokens-GGUF -c 2048\n```\n\nNote: You can also use this checkpoint directly through the [usage steps](https://github.com/ggerganov/llama.cpp?tab=readme-ov-file#usage) listed in the Llama.cpp repo as well.\n\nStep 1: Clone llama.cpp from GitHub.\n```\ngit clone https://github.com/ggerganov/llama.cpp\n```\n\nStep 2: Move into the llama.cpp folder and build it with `LLAMA_CURL=1` flag along with other hardware-specific flags (for ex: LLAMA_CUDA=1 for Nvidia GPUs on Linux).\n```\ncd llama.cpp && LLAMA_CURL=1 make\n```\n\nStep 3: Run inference through the main binary.\n```\n./llama-cli --hf-repo brunopio/Llama3-8B-1.58-100B-tokens-GGUF --hf-file Llama3-8B-1.58-100B-tokens-GGUF -p \"The meaning to life and the universe is\"\n```\nor \n```\n./llama-server --hf-repo brunopio/Llama3-8B-1.58-100B-tokens-GGUF --hf-file Llama3-8B-1.58-100B-tokens-GGUF -c 2048\n```",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModelForCausalLM",
      "custom_class": null,
      "pipeline_tag": "text-generation",
      "processor": "AutoTokenizer"
    },
    "safetensors": {
      "parameters": {
        "BF16": 1050939616,
        "U8": 1744830464
      },
      "total": 2795770080
    },
    "model_index": null,
    "trending_score": null
  },
  {
    "model_id": "mistralai/Mistral-7B-Instruct-v0.3",
    "model_name": "mistralai/Mistral-7B-Instruct-v0.3",
    "author": "mistralai",
    "downloads": 979335,
    "downloads_all_time": null,
    "likes": 1523,
    "tags": [
      "transformers",
      "safetensors",
      "mistral",
      "text-generation",
      "conversational",
      "base_model:mistralai/Mistral-7B-v0.3",
      "base_model:finetune:mistralai/Mistral-7B-v0.3",
      "license:apache-2.0",
      "autotrain_compatible",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3",
    "dependencies": [
      [
        "mistral_inference",
        null
      ],
      [
        "huggingface_hub",
        null
      ],
      [
        "transformers",
        "4.42.0"
      ]
    ],
    "last_modified": "2024-08-21T12:18:25+00:00",
    "created_at": "2024-05-22T09:57:04+00:00",
    "analysis_date": "2025-03-22T00:42:47.166444",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "mistral",
    "pipeline_tag": "text-generation",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "license": "apache-2.0",
      "base_model": "mistralai/Mistral-7B-v0.3",
      "extra_gated_description": "If you want to learn more about how we process your personal data, please read our <a href=\"https://mistral.ai/terms/\">Privacy Policy</a>."
    },
    "card_text": "\n# Model Card for Mistral-7B-Instruct-v0.3\n\nThe Mistral-7B-Instruct-v0.3 Large Language Model (LLM) is an instruct fine-tuned version of the Mistral-7B-v0.3.\n\nMistral-7B-v0.3 has the following changes compared to [Mistral-7B-v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2/edit/main/README.md)\n- Extended vocabulary to 32768\n- Supports v3 Tokenizer\n- Supports function calling\n\n## Installation\n\nIt is recommended to use `mistralai/Mistral-7B-Instruct-v0.3` with [mistral-inference](https://github.com/mistralai/mistral-inference). For HF transformers code snippets, please keep scrolling.\n\n```\npip install mistral_inference\n```\n\n## Download\n\n```py\nfrom huggingface_hub import snapshot_download\nfrom pathlib import Path\n\nmistral_models_path = Path.home().joinpath('mistral_models', '7B-Instruct-v0.3')\nmistral_models_path.mkdir(parents=True, exist_ok=True)\n\nsnapshot_download(repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\", allow_patterns=[\"params.json\", \"consolidated.safetensors\", \"tokenizer.model.v3\"], local_dir=mistral_models_path)\n```\n\n### Chat\n\nAfter installing `mistral_inference`, a `mistral-chat` CLI command should be available in your environment. You can chat with the model using\n\n```\nmistral-chat $HOME/mistral_models/7B-Instruct-v0.3 --instruct --max_tokens 256\n```\n\n### Instruct following\n\n```py\nfrom mistral_inference.transformer import Transformer\nfrom mistral_inference.generate import generate\n\nfrom mistral_common.tokens.tokenizers.mistral import MistralTokenizer\nfrom mistral_common.protocol.instruct.messages import UserMessage\nfrom mistral_common.protocol.instruct.request import ChatCompletionRequest\n\n\ntokenizer = MistralTokenizer.from_file(f\"{mistral_models_path}/tokenizer.model.v3\")\nmodel = Transformer.from_folder(mistral_models_path)\n\ncompletion_request = ChatCompletionRequest(messages=[UserMessage(content=\"Explain Machine Learning to me in a nutshell.\")])\n\ntokens = tokenizer.encode_chat_completion(completion_request).tokens\n\nout_tokens, _ = generate([tokens], model, max_tokens=64, temperature=0.0, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)\nresult = tokenizer.instruct_tokenizer.tokenizer.decode(out_tokens[0])\n\nprint(result)\n```\n\n### Function calling\n\n```py\nfrom mistral_common.protocol.instruct.tool_calls import Function, Tool\nfrom mistral_inference.transformer import Transformer\nfrom mistral_inference.generate import generate\n\nfrom mistral_common.tokens.tokenizers.mistral import MistralTokenizer\nfrom mistral_common.protocol.instruct.messages import UserMessage\nfrom mistral_common.protocol.instruct.request import ChatCompletionRequest\n\n\ntokenizer = MistralTokenizer.from_file(f\"{mistral_models_path}/tokenizer.model.v3\")\nmodel = Transformer.from_folder(mistral_models_path)\n\ncompletion_request = ChatCompletionRequest(\n    tools=[\n        Tool(\n            function=Function(\n                name=\"get_current_weather\",\n                description=\"Get the current weather\",\n                parameters={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"location\": {\n                            \"type\": \"string\",\n                            \"description\": \"The city and state, e.g. San Francisco, CA\",\n                        },\n                        \"format\": {\n                            \"type\": \"string\",\n                            \"enum\": [\"celsius\", \"fahrenheit\"],\n                            \"description\": \"The temperature unit to use. Infer this from the users location.\",\n                        },\n                    },\n                    \"required\": [\"location\", \"format\"],\n                },\n            )\n        )\n    ],\n    messages=[\n        UserMessage(content=\"What's the weather like today in Paris?\"),\n        ],\n)\n\ntokens = tokenizer.encode_chat_completion(completion_request).tokens\n\nout_tokens, _ = generate([tokens], model, max_tokens=64, temperature=0.0, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)\nresult = tokenizer.instruct_tokenizer.tokenizer.decode(out_tokens[0])\n\nprint(result)\n```\n\n## Generate with `transformers`\n\nIf you want to use Hugging Face `transformers` to generate text, you can do something like this.\n\n```py\nfrom transformers import pipeline\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n    {\"role\": \"user\", \"content\": \"Who are you?\"},\n]\nchatbot = pipeline(\"text-generation\", model=\"mistralai/Mistral-7B-Instruct-v0.3\")\nchatbot(messages)\n```\n\n\n## Function calling with `transformers`\n\nTo use this example, you'll need `transformers` version 4.42.0 or higher. Please see the \n[function calling guide](https://huggingface.co/docs/transformers/main/chat_templating#advanced-tool-use--function-calling)\nin the `transformers` docs for more information.\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\nmodel_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\ndef get_current_weather(location: str, format: str):\n    \"\"\"\n    Get the current weather\n\n    Args:\n        location: The city and state, e.g. San Francisco, CA\n        format: The temperature unit to use. Infer this from the users location. (choices: [\"celsius\", \"fahrenheit\"])\n    \"\"\"\n    pass\n\nconversation = [{\"role\": \"user\", \"content\": \"What's the weather like in Paris?\"}]\ntools = [get_current_weather]\n\n\n# format and tokenize the tool use prompt \ninputs = tokenizer.apply_chat_template(\n            conversation,\n            tools=tools,\n            add_generation_prompt=True,\n            return_dict=True,\n            return_tensors=\"pt\",\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\")\n\ninputs.to(model.device)\noutputs = model.generate(**inputs, max_new_tokens=1000)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n```\n\nNote that, for reasons of space, this example does not show a complete cycle of calling a tool and adding the tool call and tool\nresults to the chat history so that the model can use them in its next generation. For a full tool calling example, please\nsee the [function calling guide](https://huggingface.co/docs/transformers/main/chat_templating#advanced-tool-use--function-calling), \nand note that Mistral **does** use tool call IDs, so these must be included in your tool calls and tool results. They should be\nexactly 9 alphanumeric characters.\n\n\n## Limitations\n\nThe Mistral 7B Instruct model is a quick demonstration that the base model can be easily fine-tuned to achieve compelling performance. \nIt does not have any moderation mechanisms. We're looking forward to engaging with the community on ways to\nmake the model finely respect guardrails, allowing for deployment in environments requiring moderated outputs.\n\n## The Mistral AI Team\n\nAlbert Jiang, Alexandre Sablayrolles, Alexis Tacnet, Antoine Roux, Arthur Mensch, Audrey Herblin-Stoop, Baptiste Bout, Baudouin de Monicault, Blanche Savary, Bam4d, Caroline Feldman, Devendra Singh Chaplot, Diego de las Casas, Eleonore Arcelin, Emma Bou Hanna, Etienne Metzger, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Harizo Rajaona, Jean-Malo Delignon, Jia Li, Justus Murke, Louis Martin, Louis Ternon, Lucile Saulnier, Lélio Renard Lavaud, Margaret Jennings, Marie Pellat, Marie Torelli, Marie-Anne Lachaux, Nicolas Schuhl, Patrick von Platen, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Thibaut Lavril, Timothée Lacroix, Théophile Gervet, Thomas Wang, Valera Nemychnikova, William El Sayed, William Marshall",
    "card_content": "---\nlicense: apache-2.0\nbase_model: mistralai/Mistral-7B-v0.3\nextra_gated_description: If you want to learn more about how we process your personal\n  data, please read our <a href=\"https://mistral.ai/terms/\">Privacy Policy</a>.\n---\n\n# Model Card for Mistral-7B-Instruct-v0.3\n\nThe Mistral-7B-Instruct-v0.3 Large Language Model (LLM) is an instruct fine-tuned version of the Mistral-7B-v0.3.\n\nMistral-7B-v0.3 has the following changes compared to [Mistral-7B-v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2/edit/main/README.md)\n- Extended vocabulary to 32768\n- Supports v3 Tokenizer\n- Supports function calling\n\n## Installation\n\nIt is recommended to use `mistralai/Mistral-7B-Instruct-v0.3` with [mistral-inference](https://github.com/mistralai/mistral-inference). For HF transformers code snippets, please keep scrolling.\n\n```\npip install mistral_inference\n```\n\n## Download\n\n```py\nfrom huggingface_hub import snapshot_download\nfrom pathlib import Path\n\nmistral_models_path = Path.home().joinpath('mistral_models', '7B-Instruct-v0.3')\nmistral_models_path.mkdir(parents=True, exist_ok=True)\n\nsnapshot_download(repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\", allow_patterns=[\"params.json\", \"consolidated.safetensors\", \"tokenizer.model.v3\"], local_dir=mistral_models_path)\n```\n\n### Chat\n\nAfter installing `mistral_inference`, a `mistral-chat` CLI command should be available in your environment. You can chat with the model using\n\n```\nmistral-chat $HOME/mistral_models/7B-Instruct-v0.3 --instruct --max_tokens 256\n```\n\n### Instruct following\n\n```py\nfrom mistral_inference.transformer import Transformer\nfrom mistral_inference.generate import generate\n\nfrom mistral_common.tokens.tokenizers.mistral import MistralTokenizer\nfrom mistral_common.protocol.instruct.messages import UserMessage\nfrom mistral_common.protocol.instruct.request import ChatCompletionRequest\n\n\ntokenizer = MistralTokenizer.from_file(f\"{mistral_models_path}/tokenizer.model.v3\")\nmodel = Transformer.from_folder(mistral_models_path)\n\ncompletion_request = ChatCompletionRequest(messages=[UserMessage(content=\"Explain Machine Learning to me in a nutshell.\")])\n\ntokens = tokenizer.encode_chat_completion(completion_request).tokens\n\nout_tokens, _ = generate([tokens], model, max_tokens=64, temperature=0.0, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)\nresult = tokenizer.instruct_tokenizer.tokenizer.decode(out_tokens[0])\n\nprint(result)\n```\n\n### Function calling\n\n```py\nfrom mistral_common.protocol.instruct.tool_calls import Function, Tool\nfrom mistral_inference.transformer import Transformer\nfrom mistral_inference.generate import generate\n\nfrom mistral_common.tokens.tokenizers.mistral import MistralTokenizer\nfrom mistral_common.protocol.instruct.messages import UserMessage\nfrom mistral_common.protocol.instruct.request import ChatCompletionRequest\n\n\ntokenizer = MistralTokenizer.from_file(f\"{mistral_models_path}/tokenizer.model.v3\")\nmodel = Transformer.from_folder(mistral_models_path)\n\ncompletion_request = ChatCompletionRequest(\n    tools=[\n        Tool(\n            function=Function(\n                name=\"get_current_weather\",\n                description=\"Get the current weather\",\n                parameters={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"location\": {\n                            \"type\": \"string\",\n                            \"description\": \"The city and state, e.g. San Francisco, CA\",\n                        },\n                        \"format\": {\n                            \"type\": \"string\",\n                            \"enum\": [\"celsius\", \"fahrenheit\"],\n                            \"description\": \"The temperature unit to use. Infer this from the users location.\",\n                        },\n                    },\n                    \"required\": [\"location\", \"format\"],\n                },\n            )\n        )\n    ],\n    messages=[\n        UserMessage(content=\"What's the weather like today in Paris?\"),\n        ],\n)\n\ntokens = tokenizer.encode_chat_completion(completion_request).tokens\n\nout_tokens, _ = generate([tokens], model, max_tokens=64, temperature=0.0, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)\nresult = tokenizer.instruct_tokenizer.tokenizer.decode(out_tokens[0])\n\nprint(result)\n```\n\n## Generate with `transformers`\n\nIf you want to use Hugging Face `transformers` to generate text, you can do something like this.\n\n```py\nfrom transformers import pipeline\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n    {\"role\": \"user\", \"content\": \"Who are you?\"},\n]\nchatbot = pipeline(\"text-generation\", model=\"mistralai/Mistral-7B-Instruct-v0.3\")\nchatbot(messages)\n```\n\n\n## Function calling with `transformers`\n\nTo use this example, you'll need `transformers` version 4.42.0 or higher. Please see the \n[function calling guide](https://huggingface.co/docs/transformers/main/chat_templating#advanced-tool-use--function-calling)\nin the `transformers` docs for more information.\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\nmodel_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\ndef get_current_weather(location: str, format: str):\n    \"\"\"\n    Get the current weather\n\n    Args:\n        location: The city and state, e.g. San Francisco, CA\n        format: The temperature unit to use. Infer this from the users location. (choices: [\"celsius\", \"fahrenheit\"])\n    \"\"\"\n    pass\n\nconversation = [{\"role\": \"user\", \"content\": \"What's the weather like in Paris?\"}]\ntools = [get_current_weather]\n\n\n# format and tokenize the tool use prompt \ninputs = tokenizer.apply_chat_template(\n            conversation,\n            tools=tools,\n            add_generation_prompt=True,\n            return_dict=True,\n            return_tensors=\"pt\",\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\")\n\ninputs.to(model.device)\noutputs = model.generate(**inputs, max_new_tokens=1000)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n```\n\nNote that, for reasons of space, this example does not show a complete cycle of calling a tool and adding the tool call and tool\nresults to the chat history so that the model can use them in its next generation. For a full tool calling example, please\nsee the [function calling guide](https://huggingface.co/docs/transformers/main/chat_templating#advanced-tool-use--function-calling), \nand note that Mistral **does** use tool call IDs, so these must be included in your tool calls and tool results. They should be\nexactly 9 alphanumeric characters.\n\n\n## Limitations\n\nThe Mistral 7B Instruct model is a quick demonstration that the base model can be easily fine-tuned to achieve compelling performance. \nIt does not have any moderation mechanisms. We're looking forward to engaging with the community on ways to\nmake the model finely respect guardrails, allowing for deployment in environments requiring moderated outputs.\n\n## The Mistral AI Team\n\nAlbert Jiang, Alexandre Sablayrolles, Alexis Tacnet, Antoine Roux, Arthur Mensch, Audrey Herblin-Stoop, Baptiste Bout, Baudouin de Monicault, Blanche Savary, Bam4d, Caroline Feldman, Devendra Singh Chaplot, Diego de las Casas, Eleonore Arcelin, Emma Bou Hanna, Etienne Metzger, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Harizo Rajaona, Jean-Malo Delignon, Jia Li, Justus Murke, Louis Martin, Louis Ternon, Lucile Saulnier, Lélio Renard Lavaud, Margaret Jennings, Marie Pellat, Marie Torelli, Marie-Anne Lachaux, Nicolas Schuhl, Patrick von Platen, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Thibaut Lavril, Timothée Lacroix, Théophile Gervet, Thomas Wang, Valera Nemychnikova, William El Sayed, William Marshall",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModelForCausalLM",
      "custom_class": null,
      "pipeline_tag": "text-generation",
      "processor": "AutoTokenizer"
    },
    "safetensors": {
      "parameters": {
        "BF16": 7248023552
      },
      "total": 7248023552
    },
    "model_index": null,
    "trending_score": null
  },
  {
    "model_id": "microsoft/Phi-3-mini-4k-instruct",
    "model_name": "microsoft/Phi-3-mini-4k-instruct",
    "author": "microsoft",
    "downloads": 947235,
    "downloads_all_time": null,
    "likes": 1155,
    "tags": [
      "transformers",
      "safetensors",
      "phi3",
      "text-generation",
      "nlp",
      "code",
      "conversational",
      "custom_code",
      "en",
      "fr",
      "license:mit",
      "autotrain_compatible",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/microsoft/Phi-3-mini-4k-instruct",
    "dependencies": [
      [
        "flash_attn",
        "2.5.8"
      ],
      [
        "torch",
        "2.3.1"
      ],
      [
        "accelerate",
        "0.31.0"
      ],
      [
        "transformers",
        "4.41.2"
      ]
    ],
    "last_modified": "2024-09-20T18:09:38+00:00",
    "created_at": "2024-04-22T16:18:17+00:00",
    "analysis_date": "2025-03-22T00:42:57.445599",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "phi3",
    "pipeline_tag": "text-generation",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "license": "mit",
      "license_link": "https://huggingface.co/microsoft/Phi-3-mini-4k-instruct/resolve/main/LICENSE",
      "language": [
        "en",
        "fr"
      ],
      "pipeline_tag": "text-generation",
      "tags": [
        "nlp",
        "code"
      ],
      "inference": {
        "parameters": {
          "temperature": 0
        }
      },
      "widget": [
        {
          "messages": [
            {
              "role": "user",
              "content": "Can you provide ways to eat combinations of bananas and dragonfruits?"
            }
          ]
        }
      ]
    },
    "card_text": "🎉 **Phi-3.5**: [[mini-instruct]](https://huggingface.co/microsoft/Phi-3.5-mini-instruct); [[MoE-instruct]](https://huggingface.co/microsoft/Phi-3.5-MoE-instruct) ; [[vision-instruct]](https://huggingface.co/microsoft/Phi-3.5-vision-instruct)\n\n## Model Summary\n\nThe Phi-3-Mini-4K-Instruct is a 3.8B parameters, lightweight, state-of-the-art open model trained with the Phi-3 datasets that includes both synthetic data and the filtered publicly available websites data with a focus on high-quality and reasoning dense properties.\nThe model belongs to the Phi-3 family with the Mini version in two variants [4K](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct) and [128K](https://huggingface.co/microsoft/Phi-3-mini-128k-instruct) which is the context length (in tokens) that it can support.\n\nThe model has underwent a post-training process that incorporates both supervised fine-tuning and direct preference optimization for the instruction following and safety measures.\nWhen assessed against benchmarks testing common sense, language understanding, math, code, long context and logical reasoning, Phi-3 Mini-4K-Instruct showcased a robust and state-of-the-art performance among models with less than 13 billion parameters.\n\nResources and Technical Documentation:\n\n🏡 [Phi-3 Portal](https://azure.microsoft.com/en-us/products/phi-3) <br>\n📰 [Phi-3 Microsoft Blog](https://aka.ms/Phi-3Build2024) <br>\n📖 [Phi-3 Technical Report](https://aka.ms/phi3-tech-report) <br>\n🛠️ [Phi-3 on Azure AI Studio](https://aka.ms/phi3-azure-ai) <br>\n👩‍🍳 [Phi-3 Cookbook](https://github.com/microsoft/Phi-3CookBook) <br>\n🖥️ [Try It](https://aka.ms/try-phi3)\n\n|         | Short Context | Long Context |\n| :------- | :------------- | :------------ |\n| Mini    | 4K [[HF]](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct) ; [[ONNX]](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-onnx) ; [[GGUF]](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf) | 128K [[HF]](https://huggingface.co/microsoft/Phi-3-mini-128k-instruct) ; [[ONNX]](https://huggingface.co/microsoft/Phi-3-mini-128k-instruct-onnx)|\n| Small   | 8K [[HF]](https://huggingface.co/microsoft/Phi-3-small-8k-instruct) ; [[ONNX]](https://huggingface.co/microsoft/Phi-3-small-8k-instruct-onnx-cuda) | 128K [[HF]](https://huggingface.co/microsoft/Phi-3-small-128k-instruct) ; [[ONNX]](https://huggingface.co/microsoft/Phi-3-small-128k-instruct-onnx-cuda)|\n| Medium  | 4K [[HF]](https://huggingface.co/microsoft/Phi-3-medium-4k-instruct) ; [[ONNX]](https://huggingface.co/microsoft/Phi-3-medium-4k-instruct-onnx-cuda) | 128K [[HF]](https://huggingface.co/microsoft/Phi-3-medium-128k-instruct) ; [[ONNX]](https://huggingface.co/microsoft/Phi-3-medium-128k-instruct-onnx-cuda)|\n| Vision  |  | 128K [[HF]](https://huggingface.co/microsoft/Phi-3-vision-128k-instruct) ; [[ONNX]](https://huggingface.co/microsoft/Phi-3-vision-128k-instruct-onnx-cuda)|\n\n\n## Intended Uses\n\n**Primary use cases**\n\nThe model is intended for broad commercial and research use in English. The model provides uses for general purpose AI systems and applications which require \n1) memory/compute constrained environments; \n2) latency bound scenarios; \n3) strong reasoning (especially math and logic). \n\nOur model is designed to accelerate research on language and multimodal models, for use as a building block for generative AI powered features.\n\n**Out-of-scope use cases**\n\nOur models are not specifically designed or evaluated for all downstream purposes. Developers should consider common limitations of language models as they select use cases, and evaluate and mitigate for accuracy, safety, and fairness before using within a specific downstream use case, particularly for high-risk scenarios.  \n\nDevelopers should be aware of and adhere to applicable laws or regulations (including privacy, trade compliance laws, etc.) that are relevant to their use case.  \n\n**Nothing contained in this Model Card should be interpreted as or deemed a restriction or modification to the license the model is released under.**  \n\n## Release Notes \n\nThis is an update over the original instruction-tuned Phi-3-mini release based on valuable customer feedback. \nThe model used additional post-training data leading to substantial gains on instruction following and structure output. \nWe also improve multi-turn conversation quality, explicitly support <|system|> tag, and significantly improve reasoning capability. \nWe believe most use cases will benefit from this release, but we encourage users to test in their particular AI applications. \nWe appreciate the enthusiastic adoption of the Phi-3 model family, and continue to welcome all feedback from the community. \n\nThe table below highlights improvements on instruction following, structure output, and reasoning of the new release on publich and internal benchmark datasets.\n\n| Benchmarks | Original | June 2024 Update |\n|:------------|:----------|:------------------|\n| Instruction Extra Hard | 5.7 | 6.0 |\n| Instruction Hard | 4.9 | 5.1 |\n| Instructions Challenge | 24.6 | 42.3 |\n| JSON Structure Output | 11.5 | 52.3 |\n| XML Structure Output | 14.4 | 49.8 |\n| GPQA\t| 23.7\t| 30.6 |\n| MMLU\t| 68.8\t| 70.9 |\n| **Average**\t| **21.9**\t| **36.7** |\n\nNotes: if users would like to check out the previous version, use the git commit id **ff07dc01615f8113924aed013115ab2abd32115b**. For the model conversion, e.g. GGUF and other formats, we invite the community to experiment with various approaches and share your valuable feedback. Let's innovate together!\n\n## How to Use\n\nPhi-3 Mini-4K-Instruct has been integrated in the `4.41.2` version of `transformers`. The current `transformers` version can be verified with: `pip list | grep transformers`.\n\nExamples of required packages:\n```\nflash_attn==2.5.8\ntorch==2.3.1\naccelerate==0.31.0\ntransformers==4.41.2\n```\n\nPhi-3 Mini-4K-Instruct is also available in [Azure AI Studio](https://aka.ms/try-phi3)\n\n### Tokenizer\n\nPhi-3 Mini-4K-Instruct supports a vocabulary size of up to `32064` tokens. The [tokenizer files](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct/blob/main/added_tokens.json) already provide placeholder tokens that can be used for downstream fine-tuning, but they can also be extended up to the model's vocabulary size.\n\n### Chat Format\n\nGiven the nature of the training data, the Phi-3 Mini-4K-Instruct model is best suited for prompts using the chat format as follows. \nYou can provide the prompt as a question with a generic template as follow:\n```markdown\n<|system|>\nYou are a helpful assistant.<|end|>\n<|user|>\nQuestion?<|end|>\n<|assistant|>\n```\n\nFor example:\n```markdown\n<|system|>\nYou are a helpful assistant.<|end|>\n<|user|>\nHow to explain Internet for a medieval knight?<|end|>\n<|assistant|> \n```\nwhere the model generates the text after `<|assistant|>` . In case of few-shots prompt, the prompt can be formatted as the following:\n\n```markdown\n<|system|>\nYou are a helpful travel assistant.<|end|>\n<|user|>\nI am going to Paris, what should I see?<|end|>\n<|assistant|>\nParis, the capital of France, is known for its stunning architecture, art museums, historical landmarks, and romantic atmosphere. Here are some of the top attractions to see in Paris:\\n\\n1. The Eiffel Tower: The iconic Eiffel Tower is one of the most recognizable landmarks in the world and offers breathtaking views of the city.\\n2. The Louvre Museum: The Louvre is one of the world's largest and most famous museums, housing an impressive collection of art and artifacts, including the Mona Lisa.\\n3. Notre-Dame Cathedral: This beautiful cathedral is one of the most famous landmarks in Paris and is known for its Gothic architecture and stunning stained glass windows.\\n\\nThese are just a few of the many attractions that Paris has to offer. With so much to see and do, it's no wonder that Paris is one of the most popular tourist destinations in the world.\"<|end|>\n<|user|>\nWhat is so great about #1?<|end|>\n<|assistant|>\n```\n\n### Sample inference code\n\nThis code snippets show how to get quickly started with running the model on a GPU:\n\n```python\nimport torch \nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline \n\ntorch.random.manual_seed(0) \nmodel = AutoModelForCausalLM.from_pretrained( \n    \"microsoft/Phi-3-mini-4k-instruct\",  \n    device_map=\"cuda\",  \n    torch_dtype=\"auto\",  \n    trust_remote_code=True,  \n) \n\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\") \n\nmessages = [ \n    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"}, \n    {\"role\": \"user\", \"content\": \"Can you provide ways to eat combinations of bananas and dragonfruits?\"}, \n    {\"role\": \"assistant\", \"content\": \"Sure! Here are some ways to eat bananas and dragonfruits together: 1. Banana and dragonfruit smoothie: Blend bananas and dragonfruits together with some milk and honey. 2. Banana and dragonfruit salad: Mix sliced bananas and dragonfruits together with some lemon juice and honey.\"}, \n    {\"role\": \"user\", \"content\": \"What about solving an 2x + 3 = 7 equation?\"}, \n] \n\npipe = pipeline( \n    \"text-generation\", \n    model=model, \n    tokenizer=tokenizer, \n) \n\ngeneration_args = { \n    \"max_new_tokens\": 500, \n    \"return_full_text\": False, \n    \"temperature\": 0.0, \n    \"do_sample\": False, \n} \n\noutput = pipe(messages, **generation_args) \nprint(output[0]['generated_text']) \n```\n\nNote: If you want to use flash attention, call _AutoModelForCausalLM.from_pretrained()_ with _attn_implementation=\"flash_attention_2\"_\n\n## Responsible AI Considerations\n\nLike other language models, the Phi series models can potentially behave in ways that are unfair, unreliable, or offensive. Some of the limiting behaviors to be aware of include:\n\n+ Quality of Service: the Phi models are trained primarily on English text. Languages other than English will experience worse performance. English language varieties with less representation in the training data might experience worse performance than standard American English.   \n+ Representation of Harms & Perpetuation of Stereotypes: These models can over- or under-represent groups of people, erase representation of some groups, or reinforce demeaning or negative stereotypes. Despite safety post-training, these limitations may still be present due to differing levels of representation of different groups or prevalence of examples of negative stereotypes in training data that reflect real-world patterns and societal biases. \n+ Inappropriate or Offensive Content: these models may produce other types of inappropriate or offensive content, which may make it inappropriate to deploy for sensitive contexts without additional mitigations that are specific to the use case. \n+ Information Reliability: Language models can generate nonsensical content or fabricate content that might sound reasonable but is inaccurate or outdated.  \n+ Limited Scope for Code: Majority of Phi-3 training data is based in Python and use common packages such as \"typing, math, random, collections, datetime, itertools\". If the model generates Python scripts that utilize other packages or scripts in other languages, we strongly recommend users manually verify all API uses.   \n\nDevelopers should apply responsible AI best practices and are responsible for ensuring that a specific use case complies with relevant laws and regulations (e.g. privacy, trade, etc.). Important areas for consideration include:\n\n+ Allocation: Models may not be suitable for scenarios that could have consequential impact on legal status or the allocation of resources or life opportunities (ex: housing, employment, credit, etc.) without further assessments and additional debiasing techniques.\n+ High-Risk Scenarios: Developers should assess suitability of using models in high-risk scenarios where unfair, unreliable or offensive outputs might be extremely costly or lead to harm. This includes providing advice in sensitive or expert domains where accuracy and reliability are critical (ex: legal or health advice). Additional safeguards should be implemented at the application level according to the deployment context. \n+ Misinformation: Models may produce inaccurate information. Developers should follow transparency best practices and inform end-users they are interacting with an AI system. At the application level, developers can build feedback mechanisms and pipelines to ground responses in use-case specific, contextual information, a technique known as Retrieval Augmented Generation (RAG).   \n+ Generation of Harmful Content: Developers should assess outputs for their context and use available safety classifiers or custom solutions appropriate for their use case. \n+ Misuse: Other forms of misuse such as fraud, spam, or malware production may be possible, and developers should ensure that their applications do not violate applicable laws and regulations.\n\n## Training\n\n### Model\n\n* Architecture: Phi-3 Mini-4K-Instruct has 3.8B parameters and is a dense decoder-only Transformer model. The model is fine-tuned with Supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) to ensure alignment with human preferences and safety guidlines.\n* Inputs: Text. It is best suited for prompts using chat format.\n* Context length: 4K tokens\n* GPUs: 512 H100-80G\n* Training time: 10 days\n* Training data: 4.9T tokens\n* Outputs: Generated text in response to the input\n* Dates: Our models were trained between May and June 2024\n* Status: This is a static model trained on an offline dataset with cutoff date October 2023. Future versions of the tuned models may be released as we improve models.\n* Release dates: June, 2024.\n\n### Datasets\n\nOur training data includes a wide variety of sources, totaling 4.9 trillion tokens, and is a combination of \n1) Publicly available documents filtered rigorously for quality, selected high-quality educational data, and code; \n2) Newly created synthetic, “textbook-like” data for the purpose of teaching math, coding, common sense reasoning, general knowledge of the world (science, daily activities, theory of mind, etc.); \n3) High quality chat format supervised data covering various topics to reflect human preferences on different aspects such as instruct-following, truthfulness, honesty and helpfulness.\n\nWe are focusing on the quality of data that could potentially improve the reasoning ability for the model, and we filter the publicly available documents to contain the correct level of knowledge. As an example, the result of a game in premier league in a particular day might be good training data for frontier models, but we need to remove such information to leave more model capacity for reasoning for the small size models. More details about data can be found in the [Phi-3 Technical Report](https://aka.ms/phi3-tech-report).\n\n### Fine-tuning\n\nA basic example of multi-GPUs supervised fine-tuning (SFT) with TRL and Accelerate modules is provided [here](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct/resolve/main/sample_finetune.py).\n\n## Benchmarks\n\nWe report the results under completion format for Phi-3-Mini-4K-Instruct on standard open-source benchmarks measuring the model's reasoning ability (both common sense reasoning and logical reasoning). We compare to Mistral-7b-v0.1, Mixtral-8x7b, Gemma 7B, Llama-3-8B-Instruct, and GPT3.5-Turbo-1106.\n\nAll the reported numbers are produced with the exact same pipeline to ensure that the numbers are comparable. These numbers might differ from other published numbers due to slightly different choices in the evaluation.\n\nAs is now standard, we use few-shot prompts to evaluate the models, at temperature 0. \nThe prompts and number of shots are part of a Microsoft internal tool to evaluate language models, and in particular we did no optimization to the pipeline for Phi-3.\nMore specifically, we do not change prompts, pick different few-shot examples, change prompt format, or do any other form of optimization for the model.\n\nThe number of k–shot examples is listed per-benchmark. \n\n| Category | Benchmark | Phi-3-Mini-4K-Ins | Gemma-7B | Mistral-7b | Mixtral-8x7b | Llama-3-8B-Ins | GPT3.5-Turbo-1106 |\n|:----------|:-----------|:-------------------|:----------|:------------|:--------------|:----------------|:-------------------|\n| Popular aggregated benchmark | AGI Eval <br>5-shot| 39.0 | 42.1 | 35.1 | 45.2 | 42 | 48.4 |\n| | MMLU <br>5-shot | 70.9 | 63.6 | 61.7 | 70.5 | 66.5 | 71.4 |\n| | BigBench Hard CoT<br>3-shot| 73.5 | 59.6 | 57.3 | 69.7 | 51.5 | 68.3 |\n| Language Understanding | ANLI <br>7-shot | 53.6 | 48.7 | 47.1 | 55.2 | 57.3 | 58.1 |\n| | HellaSwag <br>5-shot| 75.3 | 49.8 | 58.5 | 70.4 | 71.1 | 78.8 |\n| Reasoning | ARC Challenge <br>10-shot | 86.3 | 78.3 | 78.6 | 87.3 | 82.8 | 87.4 |\n| | BoolQ <br>0-shot | 78.1 | 66 | 72.2 | 76.6 | 80.9 | 79.1 |\n| | MedQA <br>2-shot| 56.5 | 49.6 | 50 | 62.2 | 60.5 | 63.4 |\n| | OpenBookQA <br>10-shot| 82.2 | 78.6 | 79.8 | 85.8 | 82.6 | 86 |\n| | PIQA <br>5-shot| 83.5 | 78.1 | 77.7 | 86 | 75.7 | 86.6 |\n| | GPQA <br>0-shot| 30.6 | 2.9 | 15 | 6.9 | 32.4 | 30.8 |\n| | Social IQA <br>5-shot| 77.6 | 65.5 | 74.6 | 75.9 | 73.9 | 68.3 |\n| | TruthfulQA (MC2) <br>10-shot| 64.7 | 52.1 | 53 | 60.1 | 63.2 | 67.7 |\n| | WinoGrande <br>5-shot| 71.6 | 55.6 | 54.2 | 62 | 65 | 68.8 |\n| Factual Knowledge | TriviaQA <br>5-shot| 61.4 | 72.3 | 75.2 | 82.2 | 67.7 | 85.8 |\n| Math | GSM8K CoT <br>8-shot| 85.7 | 59.8 | 46.4 | 64.7 | 77.4 | 78.1 |\n| Code Generation | HumanEval <br>0-shot| 57.3 | 34.1 | 28.0 | 37.8 | 60.4 | 62.2 |\n| | MBPP <br>3-shot| 69.8 | 51.5 | 50.8 | 60.2 | 67.7 | 77.8 |\n| **Average** | | **67.6** | **56.0** | **56.4** | **64.4** | **65.5** | **70.4** |\n\n\nWe take a closer look at different categories across 100 public benchmark datasets at the table below: \n\n| Category | Phi-3-Mini-4K-Instruct | Gemma-7B | Mistral-7B | Mixtral 8x7B | Llama-3-8B-Instruct | GPT-3.5-Turbo |\n|:----------|:------------------------|:----------|:------------|:--------------|:---------------------|:---------------|\n| Popular aggregated benchmark | 61.1 | 59.4 | 56.5 | 66.2 | 59.9 | 67.0 |\n| Reasoning | 70.8 | 60.3 | 62.8 | 68.1 | 69.6 | 71.8 |\n| Language understanding | 60.5 | 57.6 | 52.5 | 66.1 | 63.2 | 67.7 |\n| Code generation | 60.7 | 45.6 | 42.9 | 52.7 | 56.4 | 70.4 |\n| Math | 50.6 | 35.8 | 25.4 | 40.3 | 41.1 | 52.8 |\n| Factual knowledge | 38.4 | 46.7 | 49.8 | 58.6 | 43.1 | 63.4 |\n| Multilingual | 56.7 | 66.5 | 57.4 | 66.7 | 66.6 | 71.0 |\n| Robustness | 61.1 | 38.4 | 40.6 | 51.0 | 64.5 | 69.3 |\n\n\nOverall, the model with only 3.8B-param achieves a similar level of language understanding and reasoning ability as much larger models. However, it is still fundamentally limited by its size for certain tasks. The model simply does not have the capacity to store too much world knowledge, which can be seen for example with low performance on TriviaQA. However, we believe such weakness can be resolved by augmenting Phi-3-Mini with a search engine.   \n\n\n## Cross Platform Support \n\n[ONNX runtime](https://onnxruntime.ai/blogs/accelerating-phi-3) now supports Phi-3 mini models across platforms and hardware.  \n\nOptimized phi-3 models are also published here in ONNX format, to run with ONNX Runtime on CPU and GPU across devices, including server platforms, Windows, Linux and Mac desktops, and mobile CPUs, with the precision best suited to each of these targets. DirectML GPU acceleration is supported for Windows desktops GPUs (AMD, Intel, and NVIDIA).   \n\nAlong with DML, ONNX Runtime provides cross platform support for Phi3 mini across a range of devices CPU, GPU, and mobile.  \n\nHere are some of the optimized configurations we have added:  \n\n1. ONNX models for int4 DML: Quantized to int4 via AWQ \n2. ONNX model for fp16 CUDA \n3. ONNX model for int4 CUDA: Quantized to int4 via RTN \n4. ONNX model for int4 CPU and Mobile: Quantized to int4 via R \n\n## Software\n\n* [PyTorch](https://github.com/pytorch/pytorch)\n* [Transformers](https://github.com/huggingface/transformers)\n* [Flash-Attention](https://github.com/HazyResearch/flash-attention)\n\n## Hardware\nNote that by default, the Phi-3 Mini-4K-Instruct model uses flash attention, which requires certain types of GPU hardware to run. We have tested on the following GPU types:\n* NVIDIA A100\n* NVIDIA A6000\n* NVIDIA H100\n\nIf you want to run the model on:\n* NVIDIA V100 or earlier generation GPUs: call AutoModelForCausalLM.from_pretrained() with attn_implementation=\"eager\"\n* CPU: use the **GGUF** quantized models [4K](https://aka.ms/Phi3-mini-4k-instruct-gguf)\n+ Optimized inference on GPU, CPU, and Mobile: use the **ONNX** models [4K](https://aka.ms/Phi3-mini-4k-instruct-onnx)\n  \n## License\n\nThe model is licensed under the [MIT license](https://huggingface.co/microsoft/Phi-3-mini-4k/resolve/main/LICENSE).\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow [Microsoft’s Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks). Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party’s policies.",
    "card_content": "---\nlicense: mit\nlicense_link: https://huggingface.co/microsoft/Phi-3-mini-4k-instruct/resolve/main/LICENSE\nlanguage:\n- en\n- fr\npipeline_tag: text-generation\ntags:\n- nlp\n- code\ninference:\n  parameters:\n    temperature: 0\nwidget:\n- messages:\n  - role: user\n    content: Can you provide ways to eat combinations of bananas and dragonfruits?\n---\n🎉 **Phi-3.5**: [[mini-instruct]](https://huggingface.co/microsoft/Phi-3.5-mini-instruct); [[MoE-instruct]](https://huggingface.co/microsoft/Phi-3.5-MoE-instruct) ; [[vision-instruct]](https://huggingface.co/microsoft/Phi-3.5-vision-instruct)\n\n## Model Summary\n\nThe Phi-3-Mini-4K-Instruct is a 3.8B parameters, lightweight, state-of-the-art open model trained with the Phi-3 datasets that includes both synthetic data and the filtered publicly available websites data with a focus on high-quality and reasoning dense properties.\nThe model belongs to the Phi-3 family with the Mini version in two variants [4K](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct) and [128K](https://huggingface.co/microsoft/Phi-3-mini-128k-instruct) which is the context length (in tokens) that it can support.\n\nThe model has underwent a post-training process that incorporates both supervised fine-tuning and direct preference optimization for the instruction following and safety measures.\nWhen assessed against benchmarks testing common sense, language understanding, math, code, long context and logical reasoning, Phi-3 Mini-4K-Instruct showcased a robust and state-of-the-art performance among models with less than 13 billion parameters.\n\nResources and Technical Documentation:\n\n🏡 [Phi-3 Portal](https://azure.microsoft.com/en-us/products/phi-3) <br>\n📰 [Phi-3 Microsoft Blog](https://aka.ms/Phi-3Build2024) <br>\n📖 [Phi-3 Technical Report](https://aka.ms/phi3-tech-report) <br>\n🛠️ [Phi-3 on Azure AI Studio](https://aka.ms/phi3-azure-ai) <br>\n👩‍🍳 [Phi-3 Cookbook](https://github.com/microsoft/Phi-3CookBook) <br>\n🖥️ [Try It](https://aka.ms/try-phi3)\n\n|         | Short Context | Long Context |\n| :------- | :------------- | :------------ |\n| Mini    | 4K [[HF]](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct) ; [[ONNX]](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-onnx) ; [[GGUF]](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf) | 128K [[HF]](https://huggingface.co/microsoft/Phi-3-mini-128k-instruct) ; [[ONNX]](https://huggingface.co/microsoft/Phi-3-mini-128k-instruct-onnx)|\n| Small   | 8K [[HF]](https://huggingface.co/microsoft/Phi-3-small-8k-instruct) ; [[ONNX]](https://huggingface.co/microsoft/Phi-3-small-8k-instruct-onnx-cuda) | 128K [[HF]](https://huggingface.co/microsoft/Phi-3-small-128k-instruct) ; [[ONNX]](https://huggingface.co/microsoft/Phi-3-small-128k-instruct-onnx-cuda)|\n| Medium  | 4K [[HF]](https://huggingface.co/microsoft/Phi-3-medium-4k-instruct) ; [[ONNX]](https://huggingface.co/microsoft/Phi-3-medium-4k-instruct-onnx-cuda) | 128K [[HF]](https://huggingface.co/microsoft/Phi-3-medium-128k-instruct) ; [[ONNX]](https://huggingface.co/microsoft/Phi-3-medium-128k-instruct-onnx-cuda)|\n| Vision  |  | 128K [[HF]](https://huggingface.co/microsoft/Phi-3-vision-128k-instruct) ; [[ONNX]](https://huggingface.co/microsoft/Phi-3-vision-128k-instruct-onnx-cuda)|\n\n\n## Intended Uses\n\n**Primary use cases**\n\nThe model is intended for broad commercial and research use in English. The model provides uses for general purpose AI systems and applications which require \n1) memory/compute constrained environments; \n2) latency bound scenarios; \n3) strong reasoning (especially math and logic). \n\nOur model is designed to accelerate research on language and multimodal models, for use as a building block for generative AI powered features.\n\n**Out-of-scope use cases**\n\nOur models are not specifically designed or evaluated for all downstream purposes. Developers should consider common limitations of language models as they select use cases, and evaluate and mitigate for accuracy, safety, and fairness before using within a specific downstream use case, particularly for high-risk scenarios.  \n\nDevelopers should be aware of and adhere to applicable laws or regulations (including privacy, trade compliance laws, etc.) that are relevant to their use case.  \n\n**Nothing contained in this Model Card should be interpreted as or deemed a restriction or modification to the license the model is released under.**  \n\n## Release Notes \n\nThis is an update over the original instruction-tuned Phi-3-mini release based on valuable customer feedback. \nThe model used additional post-training data leading to substantial gains on instruction following and structure output. \nWe also improve multi-turn conversation quality, explicitly support <|system|> tag, and significantly improve reasoning capability. \nWe believe most use cases will benefit from this release, but we encourage users to test in their particular AI applications. \nWe appreciate the enthusiastic adoption of the Phi-3 model family, and continue to welcome all feedback from the community. \n\nThe table below highlights improvements on instruction following, structure output, and reasoning of the new release on publich and internal benchmark datasets.\n\n| Benchmarks | Original | June 2024 Update |\n|:------------|:----------|:------------------|\n| Instruction Extra Hard | 5.7 | 6.0 |\n| Instruction Hard | 4.9 | 5.1 |\n| Instructions Challenge | 24.6 | 42.3 |\n| JSON Structure Output | 11.5 | 52.3 |\n| XML Structure Output | 14.4 | 49.8 |\n| GPQA\t| 23.7\t| 30.6 |\n| MMLU\t| 68.8\t| 70.9 |\n| **Average**\t| **21.9**\t| **36.7** |\n\nNotes: if users would like to check out the previous version, use the git commit id **ff07dc01615f8113924aed013115ab2abd32115b**. For the model conversion, e.g. GGUF and other formats, we invite the community to experiment with various approaches and share your valuable feedback. Let's innovate together!\n\n## How to Use\n\nPhi-3 Mini-4K-Instruct has been integrated in the `4.41.2` version of `transformers`. The current `transformers` version can be verified with: `pip list | grep transformers`.\n\nExamples of required packages:\n```\nflash_attn==2.5.8\ntorch==2.3.1\naccelerate==0.31.0\ntransformers==4.41.2\n```\n\nPhi-3 Mini-4K-Instruct is also available in [Azure AI Studio](https://aka.ms/try-phi3)\n\n### Tokenizer\n\nPhi-3 Mini-4K-Instruct supports a vocabulary size of up to `32064` tokens. The [tokenizer files](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct/blob/main/added_tokens.json) already provide placeholder tokens that can be used for downstream fine-tuning, but they can also be extended up to the model's vocabulary size.\n\n### Chat Format\n\nGiven the nature of the training data, the Phi-3 Mini-4K-Instruct model is best suited for prompts using the chat format as follows. \nYou can provide the prompt as a question with a generic template as follow:\n```markdown\n<|system|>\nYou are a helpful assistant.<|end|>\n<|user|>\nQuestion?<|end|>\n<|assistant|>\n```\n\nFor example:\n```markdown\n<|system|>\nYou are a helpful assistant.<|end|>\n<|user|>\nHow to explain Internet for a medieval knight?<|end|>\n<|assistant|> \n```\nwhere the model generates the text after `<|assistant|>` . In case of few-shots prompt, the prompt can be formatted as the following:\n\n```markdown\n<|system|>\nYou are a helpful travel assistant.<|end|>\n<|user|>\nI am going to Paris, what should I see?<|end|>\n<|assistant|>\nParis, the capital of France, is known for its stunning architecture, art museums, historical landmarks, and romantic atmosphere. Here are some of the top attractions to see in Paris:\\n\\n1. The Eiffel Tower: The iconic Eiffel Tower is one of the most recognizable landmarks in the world and offers breathtaking views of the city.\\n2. The Louvre Museum: The Louvre is one of the world's largest and most famous museums, housing an impressive collection of art and artifacts, including the Mona Lisa.\\n3. Notre-Dame Cathedral: This beautiful cathedral is one of the most famous landmarks in Paris and is known for its Gothic architecture and stunning stained glass windows.\\n\\nThese are just a few of the many attractions that Paris has to offer. With so much to see and do, it's no wonder that Paris is one of the most popular tourist destinations in the world.\"<|end|>\n<|user|>\nWhat is so great about #1?<|end|>\n<|assistant|>\n```\n\n### Sample inference code\n\nThis code snippets show how to get quickly started with running the model on a GPU:\n\n```python\nimport torch \nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline \n\ntorch.random.manual_seed(0) \nmodel = AutoModelForCausalLM.from_pretrained( \n    \"microsoft/Phi-3-mini-4k-instruct\",  \n    device_map=\"cuda\",  \n    torch_dtype=\"auto\",  \n    trust_remote_code=True,  \n) \n\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\") \n\nmessages = [ \n    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"}, \n    {\"role\": \"user\", \"content\": \"Can you provide ways to eat combinations of bananas and dragonfruits?\"}, \n    {\"role\": \"assistant\", \"content\": \"Sure! Here are some ways to eat bananas and dragonfruits together: 1. Banana and dragonfruit smoothie: Blend bananas and dragonfruits together with some milk and honey. 2. Banana and dragonfruit salad: Mix sliced bananas and dragonfruits together with some lemon juice and honey.\"}, \n    {\"role\": \"user\", \"content\": \"What about solving an 2x + 3 = 7 equation?\"}, \n] \n\npipe = pipeline( \n    \"text-generation\", \n    model=model, \n    tokenizer=tokenizer, \n) \n\ngeneration_args = { \n    \"max_new_tokens\": 500, \n    \"return_full_text\": False, \n    \"temperature\": 0.0, \n    \"do_sample\": False, \n} \n\noutput = pipe(messages, **generation_args) \nprint(output[0]['generated_text']) \n```\n\nNote: If you want to use flash attention, call _AutoModelForCausalLM.from_pretrained()_ with _attn_implementation=\"flash_attention_2\"_\n\n## Responsible AI Considerations\n\nLike other language models, the Phi series models can potentially behave in ways that are unfair, unreliable, or offensive. Some of the limiting behaviors to be aware of include:\n\n+ Quality of Service: the Phi models are trained primarily on English text. Languages other than English will experience worse performance. English language varieties with less representation in the training data might experience worse performance than standard American English.   \n+ Representation of Harms & Perpetuation of Stereotypes: These models can over- or under-represent groups of people, erase representation of some groups, or reinforce demeaning or negative stereotypes. Despite safety post-training, these limitations may still be present due to differing levels of representation of different groups or prevalence of examples of negative stereotypes in training data that reflect real-world patterns and societal biases. \n+ Inappropriate or Offensive Content: these models may produce other types of inappropriate or offensive content, which may make it inappropriate to deploy for sensitive contexts without additional mitigations that are specific to the use case. \n+ Information Reliability: Language models can generate nonsensical content or fabricate content that might sound reasonable but is inaccurate or outdated.  \n+ Limited Scope for Code: Majority of Phi-3 training data is based in Python and use common packages such as \"typing, math, random, collections, datetime, itertools\". If the model generates Python scripts that utilize other packages or scripts in other languages, we strongly recommend users manually verify all API uses.   \n\nDevelopers should apply responsible AI best practices and are responsible for ensuring that a specific use case complies with relevant laws and regulations (e.g. privacy, trade, etc.). Important areas for consideration include:\n\n+ Allocation: Models may not be suitable for scenarios that could have consequential impact on legal status or the allocation of resources or life opportunities (ex: housing, employment, credit, etc.) without further assessments and additional debiasing techniques.\n+ High-Risk Scenarios: Developers should assess suitability of using models in high-risk scenarios where unfair, unreliable or offensive outputs might be extremely costly or lead to harm. This includes providing advice in sensitive or expert domains where accuracy and reliability are critical (ex: legal or health advice). Additional safeguards should be implemented at the application level according to the deployment context. \n+ Misinformation: Models may produce inaccurate information. Developers should follow transparency best practices and inform end-users they are interacting with an AI system. At the application level, developers can build feedback mechanisms and pipelines to ground responses in use-case specific, contextual information, a technique known as Retrieval Augmented Generation (RAG).   \n+ Generation of Harmful Content: Developers should assess outputs for their context and use available safety classifiers or custom solutions appropriate for their use case. \n+ Misuse: Other forms of misuse such as fraud, spam, or malware production may be possible, and developers should ensure that their applications do not violate applicable laws and regulations.\n\n## Training\n\n### Model\n\n* Architecture: Phi-3 Mini-4K-Instruct has 3.8B parameters and is a dense decoder-only Transformer model. The model is fine-tuned with Supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) to ensure alignment with human preferences and safety guidlines.\n* Inputs: Text. It is best suited for prompts using chat format.\n* Context length: 4K tokens\n* GPUs: 512 H100-80G\n* Training time: 10 days\n* Training data: 4.9T tokens\n* Outputs: Generated text in response to the input\n* Dates: Our models were trained between May and June 2024\n* Status: This is a static model trained on an offline dataset with cutoff date October 2023. Future versions of the tuned models may be released as we improve models.\n* Release dates: June, 2024.\n\n### Datasets\n\nOur training data includes a wide variety of sources, totaling 4.9 trillion tokens, and is a combination of \n1) Publicly available documents filtered rigorously for quality, selected high-quality educational data, and code; \n2) Newly created synthetic, “textbook-like” data for the purpose of teaching math, coding, common sense reasoning, general knowledge of the world (science, daily activities, theory of mind, etc.); \n3) High quality chat format supervised data covering various topics to reflect human preferences on different aspects such as instruct-following, truthfulness, honesty and helpfulness.\n\nWe are focusing on the quality of data that could potentially improve the reasoning ability for the model, and we filter the publicly available documents to contain the correct level of knowledge. As an example, the result of a game in premier league in a particular day might be good training data for frontier models, but we need to remove such information to leave more model capacity for reasoning for the small size models. More details about data can be found in the [Phi-3 Technical Report](https://aka.ms/phi3-tech-report).\n\n### Fine-tuning\n\nA basic example of multi-GPUs supervised fine-tuning (SFT) with TRL and Accelerate modules is provided [here](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct/resolve/main/sample_finetune.py).\n\n## Benchmarks\n\nWe report the results under completion format for Phi-3-Mini-4K-Instruct on standard open-source benchmarks measuring the model's reasoning ability (both common sense reasoning and logical reasoning). We compare to Mistral-7b-v0.1, Mixtral-8x7b, Gemma 7B, Llama-3-8B-Instruct, and GPT3.5-Turbo-1106.\n\nAll the reported numbers are produced with the exact same pipeline to ensure that the numbers are comparable. These numbers might differ from other published numbers due to slightly different choices in the evaluation.\n\nAs is now standard, we use few-shot prompts to evaluate the models, at temperature 0. \nThe prompts and number of shots are part of a Microsoft internal tool to evaluate language models, and in particular we did no optimization to the pipeline for Phi-3.\nMore specifically, we do not change prompts, pick different few-shot examples, change prompt format, or do any other form of optimization for the model.\n\nThe number of k–shot examples is listed per-benchmark. \n\n| Category | Benchmark | Phi-3-Mini-4K-Ins | Gemma-7B | Mistral-7b | Mixtral-8x7b | Llama-3-8B-Ins | GPT3.5-Turbo-1106 |\n|:----------|:-----------|:-------------------|:----------|:------------|:--------------|:----------------|:-------------------|\n| Popular aggregated benchmark | AGI Eval <br>5-shot| 39.0 | 42.1 | 35.1 | 45.2 | 42 | 48.4 |\n| | MMLU <br>5-shot | 70.9 | 63.6 | 61.7 | 70.5 | 66.5 | 71.4 |\n| | BigBench Hard CoT<br>3-shot| 73.5 | 59.6 | 57.3 | 69.7 | 51.5 | 68.3 |\n| Language Understanding | ANLI <br>7-shot | 53.6 | 48.7 | 47.1 | 55.2 | 57.3 | 58.1 |\n| | HellaSwag <br>5-shot| 75.3 | 49.8 | 58.5 | 70.4 | 71.1 | 78.8 |\n| Reasoning | ARC Challenge <br>10-shot | 86.3 | 78.3 | 78.6 | 87.3 | 82.8 | 87.4 |\n| | BoolQ <br>0-shot | 78.1 | 66 | 72.2 | 76.6 | 80.9 | 79.1 |\n| | MedQA <br>2-shot| 56.5 | 49.6 | 50 | 62.2 | 60.5 | 63.4 |\n| | OpenBookQA <br>10-shot| 82.2 | 78.6 | 79.8 | 85.8 | 82.6 | 86 |\n| | PIQA <br>5-shot| 83.5 | 78.1 | 77.7 | 86 | 75.7 | 86.6 |\n| | GPQA <br>0-shot| 30.6 | 2.9 | 15 | 6.9 | 32.4 | 30.8 |\n| | Social IQA <br>5-shot| 77.6 | 65.5 | 74.6 | 75.9 | 73.9 | 68.3 |\n| | TruthfulQA (MC2) <br>10-shot| 64.7 | 52.1 | 53 | 60.1 | 63.2 | 67.7 |\n| | WinoGrande <br>5-shot| 71.6 | 55.6 | 54.2 | 62 | 65 | 68.8 |\n| Factual Knowledge | TriviaQA <br>5-shot| 61.4 | 72.3 | 75.2 | 82.2 | 67.7 | 85.8 |\n| Math | GSM8K CoT <br>8-shot| 85.7 | 59.8 | 46.4 | 64.7 | 77.4 | 78.1 |\n| Code Generation | HumanEval <br>0-shot| 57.3 | 34.1 | 28.0 | 37.8 | 60.4 | 62.2 |\n| | MBPP <br>3-shot| 69.8 | 51.5 | 50.8 | 60.2 | 67.7 | 77.8 |\n| **Average** | | **67.6** | **56.0** | **56.4** | **64.4** | **65.5** | **70.4** |\n\n\nWe take a closer look at different categories across 100 public benchmark datasets at the table below: \n\n| Category | Phi-3-Mini-4K-Instruct | Gemma-7B | Mistral-7B | Mixtral 8x7B | Llama-3-8B-Instruct | GPT-3.5-Turbo |\n|:----------|:------------------------|:----------|:------------|:--------------|:---------------------|:---------------|\n| Popular aggregated benchmark | 61.1 | 59.4 | 56.5 | 66.2 | 59.9 | 67.0 |\n| Reasoning | 70.8 | 60.3 | 62.8 | 68.1 | 69.6 | 71.8 |\n| Language understanding | 60.5 | 57.6 | 52.5 | 66.1 | 63.2 | 67.7 |\n| Code generation | 60.7 | 45.6 | 42.9 | 52.7 | 56.4 | 70.4 |\n| Math | 50.6 | 35.8 | 25.4 | 40.3 | 41.1 | 52.8 |\n| Factual knowledge | 38.4 | 46.7 | 49.8 | 58.6 | 43.1 | 63.4 |\n| Multilingual | 56.7 | 66.5 | 57.4 | 66.7 | 66.6 | 71.0 |\n| Robustness | 61.1 | 38.4 | 40.6 | 51.0 | 64.5 | 69.3 |\n\n\nOverall, the model with only 3.8B-param achieves a similar level of language understanding and reasoning ability as much larger models. However, it is still fundamentally limited by its size for certain tasks. The model simply does not have the capacity to store too much world knowledge, which can be seen for example with low performance on TriviaQA. However, we believe such weakness can be resolved by augmenting Phi-3-Mini with a search engine.   \n\n\n## Cross Platform Support \n\n[ONNX runtime](https://onnxruntime.ai/blogs/accelerating-phi-3) now supports Phi-3 mini models across platforms and hardware.  \n\nOptimized phi-3 models are also published here in ONNX format, to run with ONNX Runtime on CPU and GPU across devices, including server platforms, Windows, Linux and Mac desktops, and mobile CPUs, with the precision best suited to each of these targets. DirectML GPU acceleration is supported for Windows desktops GPUs (AMD, Intel, and NVIDIA).   \n\nAlong with DML, ONNX Runtime provides cross platform support for Phi3 mini across a range of devices CPU, GPU, and mobile.  \n\nHere are some of the optimized configurations we have added:  \n\n1. ONNX models for int4 DML: Quantized to int4 via AWQ \n2. ONNX model for fp16 CUDA \n3. ONNX model for int4 CUDA: Quantized to int4 via RTN \n4. ONNX model for int4 CPU and Mobile: Quantized to int4 via R \n\n## Software\n\n* [PyTorch](https://github.com/pytorch/pytorch)\n* [Transformers](https://github.com/huggingface/transformers)\n* [Flash-Attention](https://github.com/HazyResearch/flash-attention)\n\n## Hardware\nNote that by default, the Phi-3 Mini-4K-Instruct model uses flash attention, which requires certain types of GPU hardware to run. We have tested on the following GPU types:\n* NVIDIA A100\n* NVIDIA A6000\n* NVIDIA H100\n\nIf you want to run the model on:\n* NVIDIA V100 or earlier generation GPUs: call AutoModelForCausalLM.from_pretrained() with attn_implementation=\"eager\"\n* CPU: use the **GGUF** quantized models [4K](https://aka.ms/Phi3-mini-4k-instruct-gguf)\n+ Optimized inference on GPU, CPU, and Mobile: use the **ONNX** models [4K](https://aka.ms/Phi3-mini-4k-instruct-onnx)\n  \n## License\n\nThe model is licensed under the [MIT license](https://huggingface.co/microsoft/Phi-3-mini-4k/resolve/main/LICENSE).\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow [Microsoft’s Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks). Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party’s policies.",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModelForCausalLM",
      "custom_class": null,
      "pipeline_tag": "text-generation",
      "processor": "AutoTokenizer"
    },
    "safetensors": {
      "parameters": {
        "BF16": 3821079552
      },
      "total": 3821079552
    },
    "model_index": null,
    "trending_score": null
  }
]