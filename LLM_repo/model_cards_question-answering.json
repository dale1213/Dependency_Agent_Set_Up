[
  {
    "model_id": "deepset/roberta-base-squad2",
    "model_name": "deepset/roberta-base-squad2",
    "author": "deepset",
    "downloads": 1698760,
    "likes": 864,
    "tags": [
      "transformers",
      "pytorch",
      "tf",
      "jax",
      "rust",
      "safetensors",
      "roberta",
      "question-answering",
      "en",
      "dataset:squad_v2",
      "base_model:FacebookAI/roberta-base",
      "base_model:finetune:FacebookAI/roberta-base",
      "license:cc-by-4.0",
      "model-index",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/deepset/roberta-base-squad2",
    "dependencies": [
      [
        "haystack-ai",
        "2.11.2"
      ],
      [
        "transformers",
        "4.36.2"
      ],
      [
        "torch",
        "2.5.1"
      ]
    ],
    "analysis_date": "2025-03-26T00:49:35.595099",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "roberta",
    "pipeline_tag": "question-answering",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": "en",
      "license": "cc-by-4.0",
      "datasets": [
        "squad_v2"
      ],
      "base_model": [
        "FacebookAI/roberta-base"
      ],
      "model-index": [
        {
          "name": "deepset/roberta-base-squad2",
          "results": [
            {
              "task": {
                "type": "question-answering",
                "name": "Question Answering"
              },
              "dataset": {
                "name": "squad_v2",
                "type": "squad_v2",
                "config": "squad_v2",
                "split": "validation"
              },
              "metrics": [
                {
                  "type": "exact_match",
                  "value": 79.9309,
                  "name": "Exact Match",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMDhhNjg5YzNiZGQ1YTIyYTAwZGUwOWEzZTRiYzdjM2QzYjA3ZTUxNDM1NjE1MTUyMjE1MGY1YzEzMjRjYzVjYiIsInZlcnNpb24iOjF9.EH5JJo8EEFwU7osPz3s7qanw_tigeCFhCXjSfyN0Y1nWVnSfulSxIk_DbAEI5iE80V4EKLyp5-mYFodWvL2KDA"
                },
                {
                  "type": "f1",
                  "value": 82.9501,
                  "name": "F1",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMjk5ZDYwOGQyNjNkMWI0OTE4YzRmOTlkY2JjNjQ0YTZkNTMzMzNkYTA0MDFmNmI3NjA3NjNlMjhiMDQ2ZjJjNSIsInZlcnNpb24iOjF9.DDm0LNTkdLbGsue58bg1aH_s67KfbcmkvL-6ZiI2s8IoxhHJMSf29H_uV2YLyevwx900t-MwTVOW3qfFnMMEAQ"
                },
                {
                  "type": "total",
                  "value": 11869,
                  "name": "total",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMGFkMmI2ODM0NmY5NGNkNmUxYWViOWYxZDNkY2EzYWFmOWI4N2VhYzY5MGEzMTVhOTU4Zjc4YWViOGNjOWJjMCIsInZlcnNpb24iOjF9.fexrU1icJK5_MiifBtZWkeUvpmFISqBLDXSQJ8E6UnrRof-7cU0s4tX_dIsauHWtUpIHMPZCf5dlMWQKXZuAAA"
                }
              ]
            },
            {
              "task": {
                "type": "question-answering",
                "name": "Question Answering"
              },
              "dataset": {
                "name": "squad",
                "type": "squad",
                "config": "plain_text",
                "split": "validation"
              },
              "metrics": [
                {
                  "type": "exact_match",
                  "value": 85.289,
                  "name": "Exact Match"
                },
                {
                  "type": "f1",
                  "value": 91.841,
                  "name": "F1"
                }
              ]
            },
            {
              "task": {
                "type": "question-answering",
                "name": "Question Answering"
              },
              "dataset": {
                "name": "adversarial_qa",
                "type": "adversarial_qa",
                "config": "adversarialQA",
                "split": "validation"
              },
              "metrics": [
                {
                  "type": "exact_match",
                  "value": 29.5,
                  "name": "Exact Match"
                },
                {
                  "type": "f1",
                  "value": 40.367,
                  "name": "F1"
                }
              ]
            },
            {
              "task": {
                "type": "question-answering",
                "name": "Question Answering"
              },
              "dataset": {
                "name": "squad_adversarial",
                "type": "squad_adversarial",
                "config": "AddOneSent",
                "split": "validation"
              },
              "metrics": [
                {
                  "type": "exact_match",
                  "value": 78.567,
                  "name": "Exact Match"
                },
                {
                  "type": "f1",
                  "value": 84.469,
                  "name": "F1"
                }
              ]
            },
            {
              "task": {
                "type": "question-answering",
                "name": "Question Answering"
              },
              "dataset": {
                "name": "squadshifts amazon",
                "type": "squadshifts",
                "config": "amazon",
                "split": "test"
              },
              "metrics": [
                {
                  "type": "exact_match",
                  "value": 69.924,
                  "name": "Exact Match"
                },
                {
                  "type": "f1",
                  "value": 83.284,
                  "name": "F1"
                }
              ]
            },
            {
              "task": {
                "type": "question-answering",
                "name": "Question Answering"
              },
              "dataset": {
                "name": "squadshifts new_wiki",
                "type": "squadshifts",
                "config": "new_wiki",
                "split": "test"
              },
              "metrics": [
                {
                  "type": "exact_match",
                  "value": 81.204,
                  "name": "Exact Match"
                },
                {
                  "type": "f1",
                  "value": 90.595,
                  "name": "F1"
                }
              ]
            },
            {
              "task": {
                "type": "question-answering",
                "name": "Question Answering"
              },
              "dataset": {
                "name": "squadshifts nyt",
                "type": "squadshifts",
                "config": "nyt",
                "split": "test"
              },
              "metrics": [
                {
                  "type": "exact_match",
                  "value": 82.931,
                  "name": "Exact Match"
                },
                {
                  "type": "f1",
                  "value": 90.756,
                  "name": "F1"
                }
              ]
            },
            {
              "task": {
                "type": "question-answering",
                "name": "Question Answering"
              },
              "dataset": {
                "name": "squadshifts reddit",
                "type": "squadshifts",
                "config": "reddit",
                "split": "test"
              },
              "metrics": [
                {
                  "type": "exact_match",
                  "value": 71.55,
                  "name": "Exact Match"
                },
                {
                  "type": "f1",
                  "value": 82.939,
                  "name": "F1"
                }
              ]
            }
          ]
        }
      ]
    },
    "card_text": "\n# roberta-base for Extractive QA \n\nThis is the [roberta-base](https://huggingface.co/roberta-base) model, fine-tuned using the [SQuAD2.0](https://huggingface.co/datasets/squad_v2) dataset. It's been trained on question-answer pairs, including unanswerable questions, for the task of Extractive Question Answering. \nWe have also released a distilled version of this model called [deepset/tinyroberta-squad2](https://huggingface.co/deepset/tinyroberta-squad2). It has a comparable prediction quality and runs at twice the speed of [deepset/roberta-base-squad2](https://huggingface.co/deepset/roberta-base-squad2).\n\n\n## Overview\n**Language model:** roberta-base  \n**Language:** English  \n**Downstream-task:** Extractive QA  \n**Training data:** SQuAD 2.0  \n**Eval data:** SQuAD 2.0  \n**Code:**  See [an example extractive QA pipeline built with Haystack](https://haystack.deepset.ai/tutorials/34_extractive_qa_pipeline)  \n**Infrastructure**: 4x Tesla v100\n\n## Hyperparameters\n\n```\nbatch_size = 96\nn_epochs = 2\nbase_LM_model = \"roberta-base\"\nmax_seq_len = 386\nlearning_rate = 3e-5\nlr_schedule = LinearWarmup\nwarmup_proportion = 0.2\ndoc_stride=128\nmax_query_length=64\n``` \n\n## Usage\n\n### In Haystack\nHaystack is an AI orchestration framework to build customizable, production-ready LLM applications. You can use this model in Haystack to do extractive question answering on documents. \nTo load and run the model with [Haystack](https://github.com/deepset-ai/haystack/):\n```python\n# After running pip install haystack-ai \"transformers[torch,sentencepiece]\"\n\nfrom haystack import Document\nfrom haystack.components.readers import ExtractiveReader\n\ndocs = [\n    Document(content=\"Python is a popular programming language\"),\n    Document(content=\"python ist eine beliebte Programmiersprache\"),\n]\n\nreader = ExtractiveReader(model=\"deepset/roberta-base-squad2\")\nreader.warm_up()\n\nquestion = \"What is a popular programming language?\"\nresult = reader.run(query=question, documents=docs)\n# {'answers': [ExtractedAnswer(query='What is a popular programming language?', score=0.5740374326705933, data='python', document=Document(id=..., content: '...'), context=None, document_offset=ExtractedAnswer.Span(start=0, end=6),...)]}\n```\nFor a complete example with an extractive question answering pipeline that scales over many documents, check out the [corresponding Haystack tutorial](https://haystack.deepset.ai/tutorials/34_extractive_qa_pipeline).\n\n### In Transformers\n```python\nfrom transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n\nmodel_name = \"deepset/roberta-base-squad2\"\n\n# a) Get predictions\nnlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\nQA_input = {\n    'question': 'Why is model conversion important?',\n    'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\n}\nres = nlp(QA_input)\n\n# b) Load model & tokenizer\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n```\n\n## Performance\nEvaluated on the SQuAD 2.0 dev set with the [official eval script](https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/contents/blob/).\n\n```\n\"exact\": 79.87029394424324,\n\"f1\": 82.91251169582613,\n\n\"total\": 11873,\n\"HasAns_exact\": 77.93522267206478,\n\"HasAns_f1\": 84.02838248389763,\n\"HasAns_total\": 5928,\n\"NoAns_exact\": 81.79983179142137,\n\"NoAns_f1\": 81.79983179142137,\n\"NoAns_total\": 5945\n```\n\n## Authors\n**Branden Chan:** branden.chan@deepset.ai  \n**Timo M\u00f6ller:** timo.moeller@deepset.ai  \n**Malte Pietsch:** malte.pietsch@deepset.ai  \n**Tanay Soni:**  tanay.soni@deepset.ai \n\n## About us\n\n<div class=\"grid lg:grid-cols-2 gap-x-4 gap-y-3\">\n    <div class=\"w-full h-40 object-cover mb-2 rounded-lg flex items-center justify-center\">\n         <img alt=\"\" src=\"https://raw.githubusercontent.com/deepset-ai/.github/main/deepset-logo-colored.png\" class=\"w-40\"/>\n     </div>\n     <div class=\"w-full h-40 object-cover mb-2 rounded-lg flex items-center justify-center\">\n         <img alt=\"\" src=\"https://raw.githubusercontent.com/deepset-ai/.github/main/haystack-logo-colored.png\" class=\"w-40\"/>\n     </div>\n</div>\n\n[deepset](http://deepset.ai/) is the company behind the production-ready open-source AI framework [Haystack](https://haystack.deepset.ai/).\n\nSome of our other work: \n- [Distilled roberta-base-squad2 (aka \"tinyroberta-squad2\")](https://huggingface.co/deepset/tinyroberta-squad2)\n- [German BERT](https://deepset.ai/german-bert), [GermanQuAD and GermanDPR](https://deepset.ai/germanquad), [German embedding model](https://huggingface.co/mixedbread-ai/deepset-mxbai-embed-de-large-v1)\n- [deepset Cloud](https://www.deepset.ai/deepset-cloud-product)\n- [deepset Studio](https://www.deepset.ai/deepset-studio)\n\n## Get in touch and join the Haystack community\n\n<p>For more info on Haystack, visit our <strong><a href=\"https://github.com/deepset-ai/haystack\">GitHub</a></strong> repo and <strong><a href=\"https://docs.haystack.deepset.ai\">Documentation</a></strong>. \n\nWe also have a <strong><a class=\"h-7\" href=\"https://haystack.deepset.ai/community\">Discord community open to everyone!</a></strong></p>\n\n[Twitter](https://twitter.com/Haystack_AI) | [LinkedIn](https://www.linkedin.com/company/deepset-ai/) | [Discord](https://haystack.deepset.ai/community) | [GitHub Discussions](https://github.com/deepset-ai/haystack/discussions) | [Website](https://haystack.deepset.ai/) | [YouTube](https://www.youtube.com/@deepset_ai)\n\nBy the way: [we're hiring!](http://www.deepset.ai/jobs)",
    "card_content": "---\nlanguage: en\nlicense: cc-by-4.0\ndatasets:\n- squad_v2\nbase_model:\n- FacebookAI/roberta-base\nmodel-index:\n- name: deepset/roberta-base-squad2\n  results:\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squad_v2\n      type: squad_v2\n      config: squad_v2\n      split: validation\n    metrics:\n    - type: exact_match\n      value: 79.9309\n      name: Exact Match\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMDhhNjg5YzNiZGQ1YTIyYTAwZGUwOWEzZTRiYzdjM2QzYjA3ZTUxNDM1NjE1MTUyMjE1MGY1YzEzMjRjYzVjYiIsInZlcnNpb24iOjF9.EH5JJo8EEFwU7osPz3s7qanw_tigeCFhCXjSfyN0Y1nWVnSfulSxIk_DbAEI5iE80V4EKLyp5-mYFodWvL2KDA\n    - type: f1\n      value: 82.9501\n      name: F1\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMjk5ZDYwOGQyNjNkMWI0OTE4YzRmOTlkY2JjNjQ0YTZkNTMzMzNkYTA0MDFmNmI3NjA3NjNlMjhiMDQ2ZjJjNSIsInZlcnNpb24iOjF9.DDm0LNTkdLbGsue58bg1aH_s67KfbcmkvL-6ZiI2s8IoxhHJMSf29H_uV2YLyevwx900t-MwTVOW3qfFnMMEAQ\n    - type: total\n      value: 11869\n      name: total\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMGFkMmI2ODM0NmY5NGNkNmUxYWViOWYxZDNkY2EzYWFmOWI4N2VhYzY5MGEzMTVhOTU4Zjc4YWViOGNjOWJjMCIsInZlcnNpb24iOjF9.fexrU1icJK5_MiifBtZWkeUvpmFISqBLDXSQJ8E6UnrRof-7cU0s4tX_dIsauHWtUpIHMPZCf5dlMWQKXZuAAA\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squad\n      type: squad\n      config: plain_text\n      split: validation\n    metrics:\n    - type: exact_match\n      value: 85.289\n      name: Exact Match\n    - type: f1\n      value: 91.841\n      name: F1\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: adversarial_qa\n      type: adversarial_qa\n      config: adversarialQA\n      split: validation\n    metrics:\n    - type: exact_match\n      value: 29.5\n      name: Exact Match\n    - type: f1\n      value: 40.367\n      name: F1\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squad_adversarial\n      type: squad_adversarial\n      config: AddOneSent\n      split: validation\n    metrics:\n    - type: exact_match\n      value: 78.567\n      name: Exact Match\n    - type: f1\n      value: 84.469\n      name: F1\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squadshifts amazon\n      type: squadshifts\n      config: amazon\n      split: test\n    metrics:\n    - type: exact_match\n      value: 69.924\n      name: Exact Match\n    - type: f1\n      value: 83.284\n      name: F1\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squadshifts new_wiki\n      type: squadshifts\n      config: new_wiki\n      split: test\n    metrics:\n    - type: exact_match\n      value: 81.204\n      name: Exact Match\n    - type: f1\n      value: 90.595\n      name: F1\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squadshifts nyt\n      type: squadshifts\n      config: nyt\n      split: test\n    metrics:\n    - type: exact_match\n      value: 82.931\n      name: Exact Match\n    - type: f1\n      value: 90.756\n      name: F1\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squadshifts reddit\n      type: squadshifts\n      config: reddit\n      split: test\n    metrics:\n    - type: exact_match\n      value: 71.55\n      name: Exact Match\n    - type: f1\n      value: 82.939\n      name: F1\n---\n\n# roberta-base for Extractive QA \n\nThis is the [roberta-base](https://huggingface.co/roberta-base) model, fine-tuned using the [SQuAD2.0](https://huggingface.co/datasets/squad_v2) dataset. It's been trained on question-answer pairs, including unanswerable questions, for the task of Extractive Question Answering. \nWe have also released a distilled version of this model called [deepset/tinyroberta-squad2](https://huggingface.co/deepset/tinyroberta-squad2). It has a comparable prediction quality and runs at twice the speed of [deepset/roberta-base-squad2](https://huggingface.co/deepset/roberta-base-squad2).\n\n\n## Overview\n**Language model:** roberta-base  \n**Language:** English  \n**Downstream-task:** Extractive QA  \n**Training data:** SQuAD 2.0  \n**Eval data:** SQuAD 2.0  \n**Code:**  See [an example extractive QA pipeline built with Haystack](https://haystack.deepset.ai/tutorials/34_extractive_qa_pipeline)  \n**Infrastructure**: 4x Tesla v100\n\n## Hyperparameters\n\n```\nbatch_size = 96\nn_epochs = 2\nbase_LM_model = \"roberta-base\"\nmax_seq_len = 386\nlearning_rate = 3e-5\nlr_schedule = LinearWarmup\nwarmup_proportion = 0.2\ndoc_stride=128\nmax_query_length=64\n``` \n\n## Usage\n\n### In Haystack\nHaystack is an AI orchestration framework to build customizable, production-ready LLM applications. You can use this model in Haystack to do extractive question answering on documents. \nTo load and run the model with [Haystack](https://github.com/deepset-ai/haystack/):\n```python\n# After running pip install haystack-ai \"transformers[torch,sentencepiece]\"\n\nfrom haystack import Document\nfrom haystack.components.readers import ExtractiveReader\n\ndocs = [\n    Document(content=\"Python is a popular programming language\"),\n    Document(content=\"python ist eine beliebte Programmiersprache\"),\n]\n\nreader = ExtractiveReader(model=\"deepset/roberta-base-squad2\")\nreader.warm_up()\n\nquestion = \"What is a popular programming language?\"\nresult = reader.run(query=question, documents=docs)\n# {'answers': [ExtractedAnswer(query='What is a popular programming language?', score=0.5740374326705933, data='python', document=Document(id=..., content: '...'), context=None, document_offset=ExtractedAnswer.Span(start=0, end=6),...)]}\n```\nFor a complete example with an extractive question answering pipeline that scales over many documents, check out the [corresponding Haystack tutorial](https://haystack.deepset.ai/tutorials/34_extractive_qa_pipeline).\n\n### In Transformers\n```python\nfrom transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n\nmodel_name = \"deepset/roberta-base-squad2\"\n\n# a) Get predictions\nnlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\nQA_input = {\n    'question': 'Why is model conversion important?',\n    'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\n}\nres = nlp(QA_input)\n\n# b) Load model & tokenizer\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n```\n\n## Performance\nEvaluated on the SQuAD 2.0 dev set with the [official eval script](https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/contents/blob/).\n\n```\n\"exact\": 79.87029394424324,\n\"f1\": 82.91251169582613,\n\n\"total\": 11873,\n\"HasAns_exact\": 77.93522267206478,\n\"HasAns_f1\": 84.02838248389763,\n\"HasAns_total\": 5928,\n\"NoAns_exact\": 81.79983179142137,\n\"NoAns_f1\": 81.79983179142137,\n\"NoAns_total\": 5945\n```\n\n## Authors\n**Branden Chan:** branden.chan@deepset.ai  \n**Timo M\u00f6ller:** timo.moeller@deepset.ai  \n**Malte Pietsch:** malte.pietsch@deepset.ai  \n**Tanay Soni:**  tanay.soni@deepset.ai \n\n## About us\n\n<div class=\"grid lg:grid-cols-2 gap-x-4 gap-y-3\">\n    <div class=\"w-full h-40 object-cover mb-2 rounded-lg flex items-center justify-center\">\n         <img alt=\"\" src=\"https://raw.githubusercontent.com/deepset-ai/.github/main/deepset-logo-colored.png\" class=\"w-40\"/>\n     </div>\n     <div class=\"w-full h-40 object-cover mb-2 rounded-lg flex items-center justify-center\">\n         <img alt=\"\" src=\"https://raw.githubusercontent.com/deepset-ai/.github/main/haystack-logo-colored.png\" class=\"w-40\"/>\n     </div>\n</div>\n\n[deepset](http://deepset.ai/) is the company behind the production-ready open-source AI framework [Haystack](https://haystack.deepset.ai/).\n\nSome of our other work: \n- [Distilled roberta-base-squad2 (aka \"tinyroberta-squad2\")](https://huggingface.co/deepset/tinyroberta-squad2)\n- [German BERT](https://deepset.ai/german-bert), [GermanQuAD and GermanDPR](https://deepset.ai/germanquad), [German embedding model](https://huggingface.co/mixedbread-ai/deepset-mxbai-embed-de-large-v1)\n- [deepset Cloud](https://www.deepset.ai/deepset-cloud-product)\n- [deepset Studio](https://www.deepset.ai/deepset-studio)\n\n## Get in touch and join the Haystack community\n\n<p>For more info on Haystack, visit our <strong><a href=\"https://github.com/deepset-ai/haystack\">GitHub</a></strong> repo and <strong><a href=\"https://docs.haystack.deepset.ai\">Documentation</a></strong>. \n\nWe also have a <strong><a class=\"h-7\" href=\"https://haystack.deepset.ai/community\">Discord community open to everyone!</a></strong></p>\n\n[Twitter](https://twitter.com/Haystack_AI) | [LinkedIn](https://www.linkedin.com/company/deepset-ai/) | [Discord](https://haystack.deepset.ai/community) | [GitHub Discussions](https://github.com/deepset-ai/haystack/discussions) | [Website](https://haystack.deepset.ai/) | [YouTube](https://www.youtube.com/@deepset_ai)\n\nBy the way: [we're hiring!](http://www.deepset.ai/jobs)",
    "library_name": "transformers"
  },
  {
    "model_id": "distilbert/distilbert-base-cased-distilled-squad",
    "model_name": "distilbert/distilbert-base-cased-distilled-squad",
    "author": "distilbert",
    "downloads": 543132,
    "likes": 239,
    "tags": [
      "transformers",
      "pytorch",
      "tf",
      "rust",
      "safetensors",
      "openvino",
      "distilbert",
      "question-answering",
      "en",
      "dataset:squad",
      "arxiv:1910.01108",
      "arxiv:1910.09700",
      "license:apache-2.0",
      "model-index",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad",
    "dependencies": [
      [
        "transformers",
        "4.36.2"
      ],
      [
        "torch",
        "2.5.1"
      ]
    ],
    "analysis_date": "2025-03-26T00:49:37.207592",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "distilbert",
    "pipeline_tag": "question-answering",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": "en",
      "license": "apache-2.0",
      "datasets": [
        "squad"
      ],
      "metrics": [
        "squad"
      ],
      "model-index": [
        {
          "name": "distilbert-base-cased-distilled-squad",
          "results": [
            {
              "task": {
                "type": "question-answering",
                "name": "Question Answering"
              },
              "dataset": {
                "name": "squad",
                "type": "squad",
                "config": "plain_text",
                "split": "validation"
              },
              "metrics": [
                {
                  "type": "exact_match",
                  "value": 79.5998,
                  "name": "Exact Match",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZTViZDA2Y2E2NjUyMjNjYjkzNTUzODc5OTk2OTNkYjQxMDRmMDhlYjdmYWJjYWQ2N2RlNzY1YmI3OWY1NmRhOSIsInZlcnNpb24iOjF9.ZJHhboAMwsi3pqU-B-XKRCYP_tzpCRb8pEjGr2Oc-TteZeoWHI8CXcpDxugfC3f7d_oBcKWLzh3CClQxBW1iAQ"
                },
                {
                  "type": "f1",
                  "value": 86.9965,
                  "name": "F1",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZWZlMzY2MmE1NDNhOGNjNWRmODg0YjQ2Zjk5MjUzZDQ2MDYxOTBlMTNhNzQ4NTA2NjRmNDU3MGIzMTYwMmUyOSIsInZlcnNpb24iOjF9.z0ZDir87aT7UEmUeDm8Uw0oUdAqzlBz343gwnsQP3YLfGsaHe-jGlhco0Z7ISUd9NokyCiJCRc4NNxJQ83IuCw"
                }
              ]
            }
          ]
        }
      ]
    },
    "card_text": "\n# DistilBERT base cased distilled SQuAD\n\n## Table of Contents\n- [Model Details](#model-details)\n- [How To Get Started With the Model](#how-to-get-started-with-the-model)\n- [Uses](#uses)\n- [Risks, Limitations and Biases](#risks-limitations-and-biases)\n- [Training](#training)\n- [Evaluation](#evaluation)\n- [Environmental Impact](#environmental-impact)\n- [Technical Specifications](#technical-specifications)\n- [Citation Information](#citation-information)\n- [Model Card Authors](#model-card-authors)\n\n## Model Details\n\n**Model Description:** The DistilBERT model was proposed in the blog post [Smaller, faster, cheaper, lighter: Introducing DistilBERT, adistilled version of BERT](https://medium.com/huggingface/distilbert-8cf3380435b5), and the paper [DistilBERT, adistilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/abs/1910.01108). DistilBERT is a small, fast, cheap and light Transformer model trained by distilling BERT base. It has 40% less parameters than *bert-base-uncased*, runs 60% faster while preserving over 95% of BERT's performances as measured on the GLUE language understanding benchmark.\n\nThis model is a fine-tune checkpoint of [DistilBERT-base-cased](https://huggingface.co/distilbert-base-cased), fine-tuned using (a second step of) knowledge distillation on [SQuAD v1.1](https://huggingface.co/datasets/squad). \n\n- **Developed by:** Hugging Face\n- **Model Type:** Transformer-based language model\n- **Language(s):** English \n- **License:** Apache 2.0\n- **Related Models:** [DistilBERT-base-cased](https://huggingface.co/distilbert-base-cased)\n- **Resources for more information:**\n  - See [this repository](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation) for more about Distil\\* (a class of compressed models including this model)\n  - See [Sanh et al. (2019)](https://arxiv.org/abs/1910.01108) for more information about knowledge distillation and the training procedure\n\n## How to Get Started with the Model \n\nUse the code below to get started with the model. \n\n```python\n>>> from transformers import pipeline\n>>> question_answerer = pipeline(\"question-answering\", model='distilbert-base-cased-distilled-squad')\n\n>>> context = r\"\"\"\n... Extractive Question Answering is the task of extracting an answer from a text given a question. An example     of a\n... question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\n... a model on a SQuAD task, you may leverage the examples/pytorch/question-answering/run_squad.py script.\n... \"\"\"\n\n>>> result = question_answerer(question=\"What is a good example of a question answering dataset?\",     context=context)\n>>> print(\n... f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\"\n...)\n\nAnswer: 'SQuAD dataset', score: 0.5152, start: 147, end: 160\n```\n\nHere is how to use this model in PyTorch:\n\n```python\nfrom transformers import DistilBertTokenizer, DistilBertModel\nimport torch\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased-distilled-squad')\nmodel = DistilBertModel.from_pretrained('distilbert-base-cased-distilled-squad')\n\nquestion, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"\n\ninputs = tokenizer(question, text, return_tensors=\"pt\")\nwith torch.no_grad():\n    outputs = model(**inputs)\n\nprint(outputs)\n```\n\nAnd in TensorFlow: \n\n```python\nfrom transformers import DistilBertTokenizer, TFDistilBertForQuestionAnswering\nimport tensorflow as tf\n\ntokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-cased-distilled-squad\")\nmodel = TFDistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-cased-distilled-squad\")\n\nquestion, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"\n\ninputs = tokenizer(question, text, return_tensors=\"tf\")\noutputs = model(**inputs)\n\nanswer_start_index = int(tf.math.argmax(outputs.start_logits, axis=-1)[0])\nanswer_end_index = int(tf.math.argmax(outputs.end_logits, axis=-1)[0])\n\npredict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\ntokenizer.decode(predict_answer_tokens)\n```\n\n## Uses\n\nThis model can be used for question answering.\n\n#### Misuse and Out-of-scope Use\n\nThe model should not be used to intentionally create hostile or alienating environments for people. In addition, the model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model.\n\n## Risks, Limitations and Biases\n\n**CONTENT WARNING: Readers should be aware that language generated by this model can be disturbing or offensive to some and can propagate historical and current stereotypes.**\n\nSignificant research has explored bias and fairness issues with language models (see, e.g., [Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)). Predictions generated by the model can include disturbing and harmful stereotypes across protected classes; identity characteristics; and sensitive, social, and occupational groups. For example:\n\n\n```python\n>>> from transformers import pipeline\n>>> question_answerer = pipeline(\"question-answering\", model='distilbert-base-cased-distilled-squad')\n\n>>> context = r\"\"\"\n... Alice is sitting on the bench. Bob is sitting next to her.\n... \"\"\"\n\n>>> result = question_answerer(question=\"Who is the CEO?\", context=context)\n>>> print(\n... f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\"\n...)\n\nAnswer: 'Bob', score: 0.7527, start: 32, end: 35\n```\n\nUsers (both direct and downstream) should be made aware of the risks, biases and limitations of the model.\n\n## Training\n\n#### Training Data\n\nThe [distilbert-base-cased model](https://huggingface.co/distilbert-base-cased) was trained using the same data as the [distilbert-base-uncased model](https://huggingface.co/distilbert-base-uncased). The [distilbert-base-uncased model](https://huggingface.co/distilbert-base-uncased) model describes it's training data as: \n\n> DistilBERT pretrained on the same data as BERT, which is [BookCorpus](https://yknzhu.wixsite.com/mbweb), a dataset consisting of 11,038 unpublished books and [English Wikipedia](https://en.wikipedia.org/wiki/English_Wikipedia) (excluding lists, tables and headers).\n\nTo learn more about the SQuAD v1.1 dataset, see the [SQuAD v1.1 data card](https://huggingface.co/datasets/squad).\n\n#### Training Procedure\n\n##### Preprocessing\n\nSee the [distilbert-base-cased model card](https://huggingface.co/distilbert-base-cased) for further details.\n\n##### Pretraining\n\nSee the [distilbert-base-cased model card](https://huggingface.co/distilbert-base-cased) for further details. \n\n## Evaluation\n\nAs discussed in the [model repository](https://github.com/huggingface/transformers/blob/main/examples/research_projects/distillation/README.md)\n\n> This model reaches a F1 score of 87.1 on the [SQuAD v1.1] dev set (for comparison, BERT bert-base-cased version reaches a F1 score of 88.7).\t\n\n## Environmental Impact\n\nCarbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700). We present the hardware type and hours used based on the [associated paper](https://arxiv.org/pdf/1910.01108.pdf). Note that these details are just for training DistilBERT, not including the fine-tuning with SQuAD.\n\n- **Hardware Type:** 8 16GB V100 GPUs\n- **Hours used:** 90 hours\n- **Cloud Provider:** Unknown\n- **Compute Region:** Unknown\n- **Carbon Emitted:** Unknown\n\n## Technical Specifications\n\nSee the [associated paper](https://arxiv.org/abs/1910.01108) for details on the modeling architecture, objective, compute infrastructure, and training details.\n\n## Citation Information\n\n```bibtex\n@inproceedings{sanh2019distilbert,\n  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},\n  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},\n  booktitle={NeurIPS EMC^2 Workshop},\n  year={2019}\n}\n```\n\nAPA: \n- Sanh, V., Debut, L., Chaumond, J., & Wolf, T. (2019). DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108.\n\n## Model Card Authors\n\nThis model card was written by the Hugging Face team. \n",
    "card_content": "---\nlanguage: en\nlicense: apache-2.0\ndatasets:\n- squad\nmetrics:\n- squad\nmodel-index:\n- name: distilbert-base-cased-distilled-squad\n  results:\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squad\n      type: squad\n      config: plain_text\n      split: validation\n    metrics:\n    - type: exact_match\n      value: 79.5998\n      name: Exact Match\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZTViZDA2Y2E2NjUyMjNjYjkzNTUzODc5OTk2OTNkYjQxMDRmMDhlYjdmYWJjYWQ2N2RlNzY1YmI3OWY1NmRhOSIsInZlcnNpb24iOjF9.ZJHhboAMwsi3pqU-B-XKRCYP_tzpCRb8pEjGr2Oc-TteZeoWHI8CXcpDxugfC3f7d_oBcKWLzh3CClQxBW1iAQ\n    - type: f1\n      value: 86.9965\n      name: F1\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZWZlMzY2MmE1NDNhOGNjNWRmODg0YjQ2Zjk5MjUzZDQ2MDYxOTBlMTNhNzQ4NTA2NjRmNDU3MGIzMTYwMmUyOSIsInZlcnNpb24iOjF9.z0ZDir87aT7UEmUeDm8Uw0oUdAqzlBz343gwnsQP3YLfGsaHe-jGlhco0Z7ISUd9NokyCiJCRc4NNxJQ83IuCw\n---\n\n# DistilBERT base cased distilled SQuAD\n\n## Table of Contents\n- [Model Details](#model-details)\n- [How To Get Started With the Model](#how-to-get-started-with-the-model)\n- [Uses](#uses)\n- [Risks, Limitations and Biases](#risks-limitations-and-biases)\n- [Training](#training)\n- [Evaluation](#evaluation)\n- [Environmental Impact](#environmental-impact)\n- [Technical Specifications](#technical-specifications)\n- [Citation Information](#citation-information)\n- [Model Card Authors](#model-card-authors)\n\n## Model Details\n\n**Model Description:** The DistilBERT model was proposed in the blog post [Smaller, faster, cheaper, lighter: Introducing DistilBERT, adistilled version of BERT](https://medium.com/huggingface/distilbert-8cf3380435b5), and the paper [DistilBERT, adistilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/abs/1910.01108). DistilBERT is a small, fast, cheap and light Transformer model trained by distilling BERT base. It has 40% less parameters than *bert-base-uncased*, runs 60% faster while preserving over 95% of BERT's performances as measured on the GLUE language understanding benchmark.\n\nThis model is a fine-tune checkpoint of [DistilBERT-base-cased](https://huggingface.co/distilbert-base-cased), fine-tuned using (a second step of) knowledge distillation on [SQuAD v1.1](https://huggingface.co/datasets/squad). \n\n- **Developed by:** Hugging Face\n- **Model Type:** Transformer-based language model\n- **Language(s):** English \n- **License:** Apache 2.0\n- **Related Models:** [DistilBERT-base-cased](https://huggingface.co/distilbert-base-cased)\n- **Resources for more information:**\n  - See [this repository](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation) for more about Distil\\* (a class of compressed models including this model)\n  - See [Sanh et al. (2019)](https://arxiv.org/abs/1910.01108) for more information about knowledge distillation and the training procedure\n\n## How to Get Started with the Model \n\nUse the code below to get started with the model. \n\n```python\n>>> from transformers import pipeline\n>>> question_answerer = pipeline(\"question-answering\", model='distilbert-base-cased-distilled-squad')\n\n>>> context = r\"\"\"\n... Extractive Question Answering is the task of extracting an answer from a text given a question. An example     of a\n... question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\n... a model on a SQuAD task, you may leverage the examples/pytorch/question-answering/run_squad.py script.\n... \"\"\"\n\n>>> result = question_answerer(question=\"What is a good example of a question answering dataset?\",     context=context)\n>>> print(\n... f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\"\n...)\n\nAnswer: 'SQuAD dataset', score: 0.5152, start: 147, end: 160\n```\n\nHere is how to use this model in PyTorch:\n\n```python\nfrom transformers import DistilBertTokenizer, DistilBertModel\nimport torch\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased-distilled-squad')\nmodel = DistilBertModel.from_pretrained('distilbert-base-cased-distilled-squad')\n\nquestion, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"\n\ninputs = tokenizer(question, text, return_tensors=\"pt\")\nwith torch.no_grad():\n    outputs = model(**inputs)\n\nprint(outputs)\n```\n\nAnd in TensorFlow: \n\n```python\nfrom transformers import DistilBertTokenizer, TFDistilBertForQuestionAnswering\nimport tensorflow as tf\n\ntokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-cased-distilled-squad\")\nmodel = TFDistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-cased-distilled-squad\")\n\nquestion, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"\n\ninputs = tokenizer(question, text, return_tensors=\"tf\")\noutputs = model(**inputs)\n\nanswer_start_index = int(tf.math.argmax(outputs.start_logits, axis=-1)[0])\nanswer_end_index = int(tf.math.argmax(outputs.end_logits, axis=-1)[0])\n\npredict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\ntokenizer.decode(predict_answer_tokens)\n```\n\n## Uses\n\nThis model can be used for question answering.\n\n#### Misuse and Out-of-scope Use\n\nThe model should not be used to intentionally create hostile or alienating environments for people. In addition, the model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model.\n\n## Risks, Limitations and Biases\n\n**CONTENT WARNING: Readers should be aware that language generated by this model can be disturbing or offensive to some and can propagate historical and current stereotypes.**\n\nSignificant research has explored bias and fairness issues with language models (see, e.g., [Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)). Predictions generated by the model can include disturbing and harmful stereotypes across protected classes; identity characteristics; and sensitive, social, and occupational groups. For example:\n\n\n```python\n>>> from transformers import pipeline\n>>> question_answerer = pipeline(\"question-answering\", model='distilbert-base-cased-distilled-squad')\n\n>>> context = r\"\"\"\n... Alice is sitting on the bench. Bob is sitting next to her.\n... \"\"\"\n\n>>> result = question_answerer(question=\"Who is the CEO?\", context=context)\n>>> print(\n... f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\"\n...)\n\nAnswer: 'Bob', score: 0.7527, start: 32, end: 35\n```\n\nUsers (both direct and downstream) should be made aware of the risks, biases and limitations of the model.\n\n## Training\n\n#### Training Data\n\nThe [distilbert-base-cased model](https://huggingface.co/distilbert-base-cased) was trained using the same data as the [distilbert-base-uncased model](https://huggingface.co/distilbert-base-uncased). The [distilbert-base-uncased model](https://huggingface.co/distilbert-base-uncased) model describes it's training data as: \n\n> DistilBERT pretrained on the same data as BERT, which is [BookCorpus](https://yknzhu.wixsite.com/mbweb), a dataset consisting of 11,038 unpublished books and [English Wikipedia](https://en.wikipedia.org/wiki/English_Wikipedia) (excluding lists, tables and headers).\n\nTo learn more about the SQuAD v1.1 dataset, see the [SQuAD v1.1 data card](https://huggingface.co/datasets/squad).\n\n#### Training Procedure\n\n##### Preprocessing\n\nSee the [distilbert-base-cased model card](https://huggingface.co/distilbert-base-cased) for further details.\n\n##### Pretraining\n\nSee the [distilbert-base-cased model card](https://huggingface.co/distilbert-base-cased) for further details. \n\n## Evaluation\n\nAs discussed in the [model repository](https://github.com/huggingface/transformers/blob/main/examples/research_projects/distillation/README.md)\n\n> This model reaches a F1 score of 87.1 on the [SQuAD v1.1] dev set (for comparison, BERT bert-base-cased version reaches a F1 score of 88.7).\t\n\n## Environmental Impact\n\nCarbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700). We present the hardware type and hours used based on the [associated paper](https://arxiv.org/pdf/1910.01108.pdf). Note that these details are just for training DistilBERT, not including the fine-tuning with SQuAD.\n\n- **Hardware Type:** 8 16GB V100 GPUs\n- **Hours used:** 90 hours\n- **Cloud Provider:** Unknown\n- **Compute Region:** Unknown\n- **Carbon Emitted:** Unknown\n\n## Technical Specifications\n\nSee the [associated paper](https://arxiv.org/abs/1910.01108) for details on the modeling architecture, objective, compute infrastructure, and training details.\n\n## Citation Information\n\n```bibtex\n@inproceedings{sanh2019distilbert,\n  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},\n  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},\n  booktitle={NeurIPS EMC^2 Workshop},\n  year={2019}\n}\n```\n\nAPA: \n- Sanh, V., Debut, L., Chaumond, J., & Wolf, T. (2019). DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108.\n\n## Model Card Authors\n\nThis model card was written by the Hugging Face team. \n",
    "library_name": "transformers"
  },
  {
    "model_id": "deepset/bert-large-uncased-whole-word-masking-squad2",
    "model_name": "deepset/bert-large-uncased-whole-word-masking-squad2",
    "author": "deepset",
    "downloads": 227787,
    "likes": 30,
    "tags": [
      "transformers",
      "pytorch",
      "tf",
      "jax",
      "safetensors",
      "bert",
      "question-answering",
      "en",
      "dataset:squad_v2",
      "license:cc-by-4.0",
      "model-index",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/deepset/bert-large-uncased-whole-word-masking-squad2",
    "dependencies": [
      [
        "haystack-ai",
        "2.11.2"
      ],
      [
        "transformers",
        "4.36.2"
      ],
      [
        "torch",
        "2.5.1"
      ]
    ],
    "analysis_date": "2025-03-26T00:49:38.721477",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "bert",
    "pipeline_tag": "question-answering",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": "en",
      "license": "cc-by-4.0",
      "datasets": [
        "squad_v2"
      ],
      "model-index": [
        {
          "name": "deepset/bert-large-uncased-whole-word-masking-squad2",
          "results": [
            {
              "task": {
                "type": "question-answering",
                "name": "Question Answering"
              },
              "dataset": {
                "name": "squad_v2",
                "type": "squad_v2",
                "config": "squad_v2",
                "split": "validation"
              },
              "metrics": [
                {
                  "type": "exact_match",
                  "value": 80.8846,
                  "name": "Exact Match",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiY2E5ZGNkY2ExZWViZGEwNWE3OGRmMWM2ZmE4ZDU4ZDQ1OGM3ZWE0NTVmZjFmYmZjZmJmNjJmYTc3NTM3OTk3OSIsInZlcnNpb24iOjF9.aSblF4ywh1fnHHrN6UGL392R5KLaH3FCKQlpiXo_EdQ4XXEAENUCjYm9HWDiFsgfSENL35GkbSyz_GAhnefsAQ"
                },
                {
                  "type": "f1",
                  "value": 83.8765,
                  "name": "F1",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiNGFlNmEzMTk2NjRkNTI3ZTk3ZTU1NWNlYzIyN2E0ZDFlNDA2ZjYwZWJlNThkMmRmMmE0YzcwYjIyZDM5NmRiMCIsInZlcnNpb24iOjF9.-rc2_Bsp_B26-o12MFYuAU0Ad2Hg9PDx7Preuk27WlhYJDeKeEr32CW8LLANQABR3Mhw2x8uTYkEUrSDMxxLBw"
                }
              ]
            },
            {
              "task": {
                "type": "question-answering",
                "name": "Question Answering"
              },
              "dataset": {
                "name": "squad",
                "type": "squad",
                "config": "plain_text",
                "split": "validation"
              },
              "metrics": [
                {
                  "type": "exact_match",
                  "value": 85.904,
                  "name": "Exact Match"
                },
                {
                  "type": "f1",
                  "value": 92.586,
                  "name": "F1"
                }
              ]
            },
            {
              "task": {
                "type": "question-answering",
                "name": "Question Answering"
              },
              "dataset": {
                "name": "adversarial_qa",
                "type": "adversarial_qa",
                "config": "adversarialQA",
                "split": "validation"
              },
              "metrics": [
                {
                  "type": "exact_match",
                  "value": 28.233,
                  "name": "Exact Match"
                },
                {
                  "type": "f1",
                  "value": 41.17,
                  "name": "F1"
                }
              ]
            },
            {
              "task": {
                "type": "question-answering",
                "name": "Question Answering"
              },
              "dataset": {
                "name": "squad_adversarial",
                "type": "squad_adversarial",
                "config": "AddOneSent",
                "split": "validation"
              },
              "metrics": [
                {
                  "type": "exact_match",
                  "value": 78.064,
                  "name": "Exact Match"
                },
                {
                  "type": "f1",
                  "value": 83.591,
                  "name": "F1"
                }
              ]
            },
            {
              "task": {
                "type": "question-answering",
                "name": "Question Answering"
              },
              "dataset": {
                "name": "squadshifts amazon",
                "type": "squadshifts",
                "config": "amazon",
                "split": "test"
              },
              "metrics": [
                {
                  "type": "exact_match",
                  "value": 65.615,
                  "name": "Exact Match"
                },
                {
                  "type": "f1",
                  "value": 80.733,
                  "name": "F1"
                }
              ]
            },
            {
              "task": {
                "type": "question-answering",
                "name": "Question Answering"
              },
              "dataset": {
                "name": "squadshifts new_wiki",
                "type": "squadshifts",
                "config": "new_wiki",
                "split": "test"
              },
              "metrics": [
                {
                  "type": "exact_match",
                  "value": 81.57,
                  "name": "Exact Match"
                },
                {
                  "type": "f1",
                  "value": 91.199,
                  "name": "F1"
                }
              ]
            },
            {
              "task": {
                "type": "question-answering",
                "name": "Question Answering"
              },
              "dataset": {
                "name": "squadshifts nyt",
                "type": "squadshifts",
                "config": "nyt",
                "split": "test"
              },
              "metrics": [
                {
                  "type": "exact_match",
                  "value": 83.279,
                  "name": "Exact Match"
                },
                {
                  "type": "f1",
                  "value": 91.09,
                  "name": "F1"
                }
              ]
            },
            {
              "task": {
                "type": "question-answering",
                "name": "Question Answering"
              },
              "dataset": {
                "name": "squadshifts reddit",
                "type": "squadshifts",
                "config": "reddit",
                "split": "test"
              },
              "metrics": [
                {
                  "type": "exact_match",
                  "value": 69.305,
                  "name": "Exact Match"
                },
                {
                  "type": "f1",
                  "value": 82.405,
                  "name": "F1"
                }
              ]
            }
          ]
        }
      ]
    },
    "card_text": "\n# bert-large-uncased-whole-word-masking-squad2 for Extractive QA\n\nThis is a berta-large model, fine-tuned using the SQuAD2.0 dataset for the task of question answering.\n\n## Overview\n**Language model:** bert-large  \n**Language:** English  \n**Downstream-task:** Extractive QA  \n**Training data:** SQuAD 2.0  \n**Eval data:** SQuAD 2.0  \n**Code:**  See [an example extractive QA pipeline built with Haystack](https://haystack.deepset.ai/tutorials/34_extractive_qa_pipeline)  \n\n## Usage\n\n### In Haystack\nHaystack is an AI orchestration framework to build customizable, production-ready LLM applications. You can use this model in Haystack to do extractive question answering on documents. \nTo load and run the model with [Haystack](https://github.com/deepset-ai/haystack/):\n```python\n# After running pip install haystack-ai \"transformers[torch,sentencepiece]\"\n\nfrom haystack import Document\nfrom haystack.components.readers import ExtractiveReader\n\ndocs = [\n    Document(content=\"Python is a popular programming language\"),\n    Document(content=\"python ist eine beliebte Programmiersprache\"),\n]\n\nreader = ExtractiveReader(model=\"deepset/bert-large-uncased-whole-word-masking-squad2\")\nreader.warm_up()\n\nquestion = \"What is a popular programming language?\"\nresult = reader.run(query=question, documents=docs)\n# {'answers': [ExtractedAnswer(query='What is a popular programming language?', score=0.5740374326705933, data='python', document=Document(id=..., content: '...'), context=None, document_offset=ExtractedAnswer.Span(start=0, end=6),...)]}\n```\nFor a complete example with an extractive question answering pipeline that scales over many documents, check out the [corresponding Haystack tutorial](https://haystack.deepset.ai/tutorials/34_extractive_qa_pipeline).\n\n### In Transformers\n```python\nfrom transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n\nmodel_name = \"deepset/bert-large-uncased-whole-word-masking-squad2\"\n\n# a) Get predictions\nnlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\nQA_input = {\n    'question': 'Why is model conversion important?',\n    'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\n}\nres = nlp(QA_input)\n\n# b) Load model & tokenizer\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n```\n\n## About us\n\n<div class=\"grid lg:grid-cols-2 gap-x-4 gap-y-3\">\n    <div class=\"w-full h-40 object-cover mb-2 rounded-lg flex items-center justify-center\">\n         <img alt=\"\" src=\"https://raw.githubusercontent.com/deepset-ai/.github/main/deepset-logo-colored.png\" class=\"w-40\"/>\n     </div>\n     <div class=\"w-full h-40 object-cover mb-2 rounded-lg flex items-center justify-center\">\n         <img alt=\"\" src=\"https://raw.githubusercontent.com/deepset-ai/.github/main/haystack-logo-colored.png\" class=\"w-40\"/>\n     </div>\n</div>\n\n[deepset](http://deepset.ai/) is the company behind the production-ready open-source AI framework [Haystack](https://haystack.deepset.ai/).\n\nSome of our other work: \n- [Distilled roberta-base-squad2 (aka \"tinyroberta-squad2\")](https://huggingface.co/deepset/tinyroberta-squad2)\n- [German BERT](https://deepset.ai/german-bert), [GermanQuAD and GermanDPR](https://deepset.ai/germanquad), [German embedding model](https://huggingface.co/mixedbread-ai/deepset-mxbai-embed-de-large-v1)\n- [deepset Cloud](https://www.deepset.ai/deepset-cloud-product), [deepset Studio](https://www.deepset.ai/deepset-studio)\n\n## Get in touch and join the Haystack community\n\n<p>For more info on Haystack, visit our <strong><a href=\"https://github.com/deepset-ai/haystack\">GitHub</a></strong> repo and <strong><a href=\"https://docs.haystack.deepset.ai\">Documentation</a></strong>. \n\nWe also have a <strong><a class=\"h-7\" href=\"https://haystack.deepset.ai/community\">Discord community open to everyone!</a></strong></p>\n\n[Twitter](https://twitter.com/Haystack_AI) | [LinkedIn](https://www.linkedin.com/company/deepset-ai/) | [Discord](https://haystack.deepset.ai/community) | [GitHub Discussions](https://github.com/deepset-ai/haystack/discussions) | [Website](https://haystack.deepset.ai/) | [YouTube](https://www.youtube.com/@deepset_ai)\n\nBy the way: [we're hiring!](http://www.deepset.ai/jobs)",
    "card_content": "---\nlanguage: en\nlicense: cc-by-4.0\ndatasets:\n- squad_v2\nmodel-index:\n- name: deepset/bert-large-uncased-whole-word-masking-squad2\n  results:\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squad_v2\n      type: squad_v2\n      config: squad_v2\n      split: validation\n    metrics:\n    - type: exact_match\n      value: 80.8846\n      name: Exact Match\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiY2E5ZGNkY2ExZWViZGEwNWE3OGRmMWM2ZmE4ZDU4ZDQ1OGM3ZWE0NTVmZjFmYmZjZmJmNjJmYTc3NTM3OTk3OSIsInZlcnNpb24iOjF9.aSblF4ywh1fnHHrN6UGL392R5KLaH3FCKQlpiXo_EdQ4XXEAENUCjYm9HWDiFsgfSENL35GkbSyz_GAhnefsAQ\n    - type: f1\n      value: 83.8765\n      name: F1\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiNGFlNmEzMTk2NjRkNTI3ZTk3ZTU1NWNlYzIyN2E0ZDFlNDA2ZjYwZWJlNThkMmRmMmE0YzcwYjIyZDM5NmRiMCIsInZlcnNpb24iOjF9.-rc2_Bsp_B26-o12MFYuAU0Ad2Hg9PDx7Preuk27WlhYJDeKeEr32CW8LLANQABR3Mhw2x8uTYkEUrSDMxxLBw\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squad\n      type: squad\n      config: plain_text\n      split: validation\n    metrics:\n    - type: exact_match\n      value: 85.904\n      name: Exact Match\n    - type: f1\n      value: 92.586\n      name: F1\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: adversarial_qa\n      type: adversarial_qa\n      config: adversarialQA\n      split: validation\n    metrics:\n    - type: exact_match\n      value: 28.233\n      name: Exact Match\n    - type: f1\n      value: 41.17\n      name: F1\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squad_adversarial\n      type: squad_adversarial\n      config: AddOneSent\n      split: validation\n    metrics:\n    - type: exact_match\n      value: 78.064\n      name: Exact Match\n    - type: f1\n      value: 83.591\n      name: F1\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squadshifts amazon\n      type: squadshifts\n      config: amazon\n      split: test\n    metrics:\n    - type: exact_match\n      value: 65.615\n      name: Exact Match\n    - type: f1\n      value: 80.733\n      name: F1\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squadshifts new_wiki\n      type: squadshifts\n      config: new_wiki\n      split: test\n    metrics:\n    - type: exact_match\n      value: 81.57\n      name: Exact Match\n    - type: f1\n      value: 91.199\n      name: F1\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squadshifts nyt\n      type: squadshifts\n      config: nyt\n      split: test\n    metrics:\n    - type: exact_match\n      value: 83.279\n      name: Exact Match\n    - type: f1\n      value: 91.09\n      name: F1\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squadshifts reddit\n      type: squadshifts\n      config: reddit\n      split: test\n    metrics:\n    - type: exact_match\n      value: 69.305\n      name: Exact Match\n    - type: f1\n      value: 82.405\n      name: F1\n---\n\n# bert-large-uncased-whole-word-masking-squad2 for Extractive QA\n\nThis is a berta-large model, fine-tuned using the SQuAD2.0 dataset for the task of question answering.\n\n## Overview\n**Language model:** bert-large  \n**Language:** English  \n**Downstream-task:** Extractive QA  \n**Training data:** SQuAD 2.0  \n**Eval data:** SQuAD 2.0  \n**Code:**  See [an example extractive QA pipeline built with Haystack](https://haystack.deepset.ai/tutorials/34_extractive_qa_pipeline)  \n\n## Usage\n\n### In Haystack\nHaystack is an AI orchestration framework to build customizable, production-ready LLM applications. You can use this model in Haystack to do extractive question answering on documents. \nTo load and run the model with [Haystack](https://github.com/deepset-ai/haystack/):\n```python\n# After running pip install haystack-ai \"transformers[torch,sentencepiece]\"\n\nfrom haystack import Document\nfrom haystack.components.readers import ExtractiveReader\n\ndocs = [\n    Document(content=\"Python is a popular programming language\"),\n    Document(content=\"python ist eine beliebte Programmiersprache\"),\n]\n\nreader = ExtractiveReader(model=\"deepset/bert-large-uncased-whole-word-masking-squad2\")\nreader.warm_up()\n\nquestion = \"What is a popular programming language?\"\nresult = reader.run(query=question, documents=docs)\n# {'answers': [ExtractedAnswer(query='What is a popular programming language?', score=0.5740374326705933, data='python', document=Document(id=..., content: '...'), context=None, document_offset=ExtractedAnswer.Span(start=0, end=6),...)]}\n```\nFor a complete example with an extractive question answering pipeline that scales over many documents, check out the [corresponding Haystack tutorial](https://haystack.deepset.ai/tutorials/34_extractive_qa_pipeline).\n\n### In Transformers\n```python\nfrom transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n\nmodel_name = \"deepset/bert-large-uncased-whole-word-masking-squad2\"\n\n# a) Get predictions\nnlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\nQA_input = {\n    'question': 'Why is model conversion important?',\n    'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\n}\nres = nlp(QA_input)\n\n# b) Load model & tokenizer\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n```\n\n## About us\n\n<div class=\"grid lg:grid-cols-2 gap-x-4 gap-y-3\">\n    <div class=\"w-full h-40 object-cover mb-2 rounded-lg flex items-center justify-center\">\n         <img alt=\"\" src=\"https://raw.githubusercontent.com/deepset-ai/.github/main/deepset-logo-colored.png\" class=\"w-40\"/>\n     </div>\n     <div class=\"w-full h-40 object-cover mb-2 rounded-lg flex items-center justify-center\">\n         <img alt=\"\" src=\"https://raw.githubusercontent.com/deepset-ai/.github/main/haystack-logo-colored.png\" class=\"w-40\"/>\n     </div>\n</div>\n\n[deepset](http://deepset.ai/) is the company behind the production-ready open-source AI framework [Haystack](https://haystack.deepset.ai/).\n\nSome of our other work: \n- [Distilled roberta-base-squad2 (aka \"tinyroberta-squad2\")](https://huggingface.co/deepset/tinyroberta-squad2)\n- [German BERT](https://deepset.ai/german-bert), [GermanQuAD and GermanDPR](https://deepset.ai/germanquad), [German embedding model](https://huggingface.co/mixedbread-ai/deepset-mxbai-embed-de-large-v1)\n- [deepset Cloud](https://www.deepset.ai/deepset-cloud-product), [deepset Studio](https://www.deepset.ai/deepset-studio)\n\n## Get in touch and join the Haystack community\n\n<p>For more info on Haystack, visit our <strong><a href=\"https://github.com/deepset-ai/haystack\">GitHub</a></strong> repo and <strong><a href=\"https://docs.haystack.deepset.ai\">Documentation</a></strong>. \n\nWe also have a <strong><a class=\"h-7\" href=\"https://haystack.deepset.ai/community\">Discord community open to everyone!</a></strong></p>\n\n[Twitter](https://twitter.com/Haystack_AI) | [LinkedIn](https://www.linkedin.com/company/deepset-ai/) | [Discord](https://haystack.deepset.ai/community) | [GitHub Discussions](https://github.com/deepset-ai/haystack/discussions) | [Website](https://haystack.deepset.ai/) | [YouTube](https://www.youtube.com/@deepset_ai)\n\nBy the way: [we're hiring!](http://www.deepset.ai/jobs)",
    "library_name": "transformers"
  },
  {
    "model_id": "google-bert/bert-large-uncased-whole-word-masking-finetuned-squad",
    "model_name": "google-bert/bert-large-uncased-whole-word-masking-finetuned-squad",
    "author": "google-bert",
    "downloads": 220461,
    "likes": 174,
    "tags": [
      "transformers",
      "pytorch",
      "tf",
      "jax",
      "safetensors",
      "bert",
      "question-answering",
      "en",
      "dataset:bookcorpus",
      "dataset:wikipedia",
      "arxiv:1810.04805",
      "license:apache-2.0",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/google-bert/bert-large-uncased-whole-word-masking-finetuned-squad",
    "dependencies": [
      [
        "torch",
        "2.2.0"
      ],
      [
        "transformers",
        "4.36.2"
      ],
      [
        "numpy",
        "1.26.2"
      ]
    ],
    "analysis_date": "2025-03-26T00:49:40.233329",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "bert",
    "pipeline_tag": "question-answering",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": "en",
      "license": "apache-2.0",
      "datasets": [
        "bookcorpus",
        "wikipedia"
      ]
    },
    "card_text": "\n# BERT large model (uncased) whole word masking finetuned on SQuAD\n\nPretrained model on English language using a masked language modeling (MLM) objective. It was introduced in\n[this paper](https://arxiv.org/abs/1810.04805) and first released in\n[this repository](https://github.com/google-research/bert). This model is uncased: it does not make a difference\nbetween english and English.\n\nDifferently to other BERT models, this model was trained with a new technique: Whole Word Masking. In this case, all of the tokens corresponding to a word are masked at once. The overall masking rate remains the same.\n\nThe training is identical -- each masked WordPiece token is predicted independently. \n\nAfter pre-training, this model was fine-tuned on the SQuAD dataset with one of our fine-tuning scripts. See below for more information regarding this fine-tuning.\n\nDisclaimer: The team releasing BERT did not write a model card for this model so this model card has been written by\nthe Hugging Face team.\n\n## Model description\n\nBERT is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. This means it\nwas pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of\npublicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it\nwas pretrained with two objectives:\n\n- Masked language modeling (MLM): taking a sentence, the model randomly masks 15% of the words in the input then run\n  the entire masked sentence through the model and has to predict the masked words. This is different from traditional\n  recurrent neural networks (RNNs) that usually see the words one after the other, or from autoregressive models like\n  GPT which internally mask the future tokens. It allows the model to learn a bidirectional representation of the\n  sentence.\n- Next sentence prediction (NSP): the models concatenates two masked sentences as inputs during pretraining. Sometimes\n  they correspond to sentences that were next to each other in the original text, sometimes not. The model then has to\n  predict if the two sentences were following each other or not.\n\nThis way, the model learns an inner representation of the English language that can then be used to extract features\nuseful for downstream tasks: if you have a dataset of labeled sentences for instance, you can train a standard\nclassifier using the features produced by the BERT model as inputs.\n\nThis model has the following configuration:\n\n- 24-layer\n- 1024 hidden dimension\n- 16 attention heads\n- 336M parameters.\n\n## Intended uses & limitations\nThis model should be used as a question-answering model. You may use it in a question answering pipeline, or use it to output raw results given a query and a context. You may see other use cases in the [task summary](https://huggingface.co/transformers/task_summary.html#extractive-question-answering) of the transformers documentation.## Training data\n\nThe BERT model was pretrained on [BookCorpus](https://yknzhu.wixsite.com/mbweb), a dataset consisting of 11,038\nunpublished books and [English Wikipedia](https://en.wikipedia.org/wiki/English_Wikipedia) (excluding lists, tables and\nheaders).\n\n## Training procedure\n\n### Preprocessing\n\nThe texts are lowercased and tokenized using WordPiece and a vocabulary size of 30,000. The inputs of the model are\nthen of the form:\n\n```\n[CLS] Sentence A [SEP] Sentence B [SEP]\n```\n\nWith probability 0.5, sentence A and sentence B correspond to two consecutive sentences in the original corpus and in\nthe other cases, it's another random sentence in the corpus. Note that what is considered a sentence here is a\nconsecutive span of text usually longer than a single sentence. The only constrain is that the result with the two\n\"sentences\" has a combined length of less than 512 tokens.\n\nThe details of the masking procedure for each sentence are the following:\n- 15% of the tokens are masked.\n- In 80% of the cases, the masked tokens are replaced by `[MASK]`.\n- In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace.\n- In the 10% remaining cases, the masked tokens are left as is.\n\n### Pretraining\n\nThe model was trained on 4 cloud TPUs in Pod configuration (16 TPU chips total) for one million steps with a batch size\nof 256. The sequence length was limited to 128 tokens for 90% of the steps and 512 for the remaining 10%. The optimizer\nused is Adam with a learning rate of 1e-4, \\\\(\\beta_{1} = 0.9\\\\) and \\\\(\\beta_{2} = 0.999\\\\), a weight decay of 0.01,\nlearning rate warmup for 10,000 steps and linear decay of the learning rate after.\n\n### Fine-tuning\n\nAfter pre-training, this model was fine-tuned on the SQuAD dataset with one of our fine-tuning scripts. In order to reproduce the training, you may use the following command:\n```\npython -m torch.distributed.launch --nproc_per_node=8 ./examples/question-answering/run_qa.py \\\n    --model_name_or_path bert-large-uncased-whole-word-masking \\\n    --dataset_name squad \\\n    --do_train \\\n    --do_eval \\\n    --learning_rate 3e-5 \\\n    --num_train_epochs 2 \\\n    --max_seq_length 384 \\\n    --doc_stride 128 \\\n    --output_dir ./examples/models/wwm_uncased_finetuned_squad/ \\\n    --per_device_eval_batch_size=3   \\\n    --per_device_train_batch_size=3   \\\n```\n\n## Evaluation results\n\nThe results obtained are the following:\n\n```\nf1 = 93.15\nexact_match = 86.91\n```\n\n\n### BibTeX entry and citation info\n\n```bibtex\n@article{DBLP:journals/corr/abs-1810-04805,\n  author    = {Jacob Devlin and\n               Ming{-}Wei Chang and\n               Kenton Lee and\n               Kristina Toutanova},\n  title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language\n               Understanding},\n  journal   = {CoRR},\n  volume    = {abs/1810.04805},\n  year      = {2018},\n  url       = {http://arxiv.org/abs/1810.04805},\n  archivePrefix = {arXiv},\n  eprint    = {1810.04805},\n  timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n```",
    "card_content": "---\nlanguage: en\nlicense: apache-2.0\ndatasets:\n- bookcorpus\n- wikipedia\n---\n\n# BERT large model (uncased) whole word masking finetuned on SQuAD\n\nPretrained model on English language using a masked language modeling (MLM) objective. It was introduced in\n[this paper](https://arxiv.org/abs/1810.04805) and first released in\n[this repository](https://github.com/google-research/bert). This model is uncased: it does not make a difference\nbetween english and English.\n\nDifferently to other BERT models, this model was trained with a new technique: Whole Word Masking. In this case, all of the tokens corresponding to a word are masked at once. The overall masking rate remains the same.\n\nThe training is identical -- each masked WordPiece token is predicted independently. \n\nAfter pre-training, this model was fine-tuned on the SQuAD dataset with one of our fine-tuning scripts. See below for more information regarding this fine-tuning.\n\nDisclaimer: The team releasing BERT did not write a model card for this model so this model card has been written by\nthe Hugging Face team.\n\n## Model description\n\nBERT is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. This means it\nwas pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of\npublicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it\nwas pretrained with two objectives:\n\n- Masked language modeling (MLM): taking a sentence, the model randomly masks 15% of the words in the input then run\n  the entire masked sentence through the model and has to predict the masked words. This is different from traditional\n  recurrent neural networks (RNNs) that usually see the words one after the other, or from autoregressive models like\n  GPT which internally mask the future tokens. It allows the model to learn a bidirectional representation of the\n  sentence.\n- Next sentence prediction (NSP): the models concatenates two masked sentences as inputs during pretraining. Sometimes\n  they correspond to sentences that were next to each other in the original text, sometimes not. The model then has to\n  predict if the two sentences were following each other or not.\n\nThis way, the model learns an inner representation of the English language that can then be used to extract features\nuseful for downstream tasks: if you have a dataset of labeled sentences for instance, you can train a standard\nclassifier using the features produced by the BERT model as inputs.\n\nThis model has the following configuration:\n\n- 24-layer\n- 1024 hidden dimension\n- 16 attention heads\n- 336M parameters.\n\n## Intended uses & limitations\nThis model should be used as a question-answering model. You may use it in a question answering pipeline, or use it to output raw results given a query and a context. You may see other use cases in the [task summary](https://huggingface.co/transformers/task_summary.html#extractive-question-answering) of the transformers documentation.## Training data\n\nThe BERT model was pretrained on [BookCorpus](https://yknzhu.wixsite.com/mbweb), a dataset consisting of 11,038\nunpublished books and [English Wikipedia](https://en.wikipedia.org/wiki/English_Wikipedia) (excluding lists, tables and\nheaders).\n\n## Training procedure\n\n### Preprocessing\n\nThe texts are lowercased and tokenized using WordPiece and a vocabulary size of 30,000. The inputs of the model are\nthen of the form:\n\n```\n[CLS] Sentence A [SEP] Sentence B [SEP]\n```\n\nWith probability 0.5, sentence A and sentence B correspond to two consecutive sentences in the original corpus and in\nthe other cases, it's another random sentence in the corpus. Note that what is considered a sentence here is a\nconsecutive span of text usually longer than a single sentence. The only constrain is that the result with the two\n\"sentences\" has a combined length of less than 512 tokens.\n\nThe details of the masking procedure for each sentence are the following:\n- 15% of the tokens are masked.\n- In 80% of the cases, the masked tokens are replaced by `[MASK]`.\n- In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace.\n- In the 10% remaining cases, the masked tokens are left as is.\n\n### Pretraining\n\nThe model was trained on 4 cloud TPUs in Pod configuration (16 TPU chips total) for one million steps with a batch size\nof 256. The sequence length was limited to 128 tokens for 90% of the steps and 512 for the remaining 10%. The optimizer\nused is Adam with a learning rate of 1e-4, \\\\(\\beta_{1} = 0.9\\\\) and \\\\(\\beta_{2} = 0.999\\\\), a weight decay of 0.01,\nlearning rate warmup for 10,000 steps and linear decay of the learning rate after.\n\n### Fine-tuning\n\nAfter pre-training, this model was fine-tuned on the SQuAD dataset with one of our fine-tuning scripts. In order to reproduce the training, you may use the following command:\n```\npython -m torch.distributed.launch --nproc_per_node=8 ./examples/question-answering/run_qa.py \\\n    --model_name_or_path bert-large-uncased-whole-word-masking \\\n    --dataset_name squad \\\n    --do_train \\\n    --do_eval \\\n    --learning_rate 3e-5 \\\n    --num_train_epochs 2 \\\n    --max_seq_length 384 \\\n    --doc_stride 128 \\\n    --output_dir ./examples/models/wwm_uncased_finetuned_squad/ \\\n    --per_device_eval_batch_size=3   \\\n    --per_device_train_batch_size=3   \\\n```\n\n## Evaluation results\n\nThe results obtained are the following:\n\n```\nf1 = 93.15\nexact_match = 86.91\n```\n\n\n### BibTeX entry and citation info\n\n```bibtex\n@article{DBLP:journals/corr/abs-1810-04805,\n  author    = {Jacob Devlin and\n               Ming{-}Wei Chang and\n               Kenton Lee and\n               Kristina Toutanova},\n  title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language\n               Understanding},\n  journal   = {CoRR},\n  volume    = {abs/1810.04805},\n  year      = {2018},\n  url       = {http://arxiv.org/abs/1810.04805},\n  archivePrefix = {arXiv},\n  eprint    = {1810.04805},\n  timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n```",
    "library_name": "transformers"
  },
  {
    "model_id": "deepset/minilm-uncased-squad2",
    "model_name": "deepset/minilm-uncased-squad2",
    "author": "deepset",
    "downloads": 153902,
    "likes": 44,
    "tags": [
      "transformers",
      "pytorch",
      "jax",
      "safetensors",
      "bert",
      "question-answering",
      "en",
      "dataset:squad_v2",
      "license:cc-by-4.0",
      "model-index",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/deepset/minilm-uncased-squad2",
    "dependencies": [
      [
        "haystack-ai",
        "2.0.0"
      ],
      [
        "transformers",
        "4.36.2"
      ],
      [
        "torch",
        "2.1.0"
      ]
    ],
    "analysis_date": "2025-03-26T00:49:41.538808",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "bert",
    "pipeline_tag": "question-answering",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": "en",
      "license": "cc-by-4.0",
      "datasets": [
        "squad_v2"
      ],
      "model-index": [
        {
          "name": "deepset/minilm-uncased-squad2",
          "results": [
            {
              "task": {
                "type": "question-answering",
                "name": "Question Answering"
              },
              "dataset": {
                "name": "squad_v2",
                "type": "squad_v2",
                "config": "squad_v2",
                "split": "validation"
              },
              "metrics": [
                {
                  "type": "exact_match",
                  "value": 76.1921,
                  "name": "Exact Match",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiNmViZTQ3YTBjYTc3ZDQzYmI1Mzk3MTAxM2MzNjdmMTc0MWY4Yzg2MWU3NGQ1MDJhZWI2NzY0YWYxZTY2OTgzMiIsInZlcnNpb24iOjF9.s4XCRs_pvW__LJ57dpXAEHD6NRsQ3XaFrM1xaguS6oUs5fCN77wNNc97scnfoPXT18A8RAn0cLTNivfxZm0oBA"
                },
                {
                  "type": "f1",
                  "value": 79.5483,
                  "name": "F1",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZmJlYTIyOTg2NjMyMzg4NzNlNGIzMTY2NDVkMjg0ODdiOWRmYjVkZDYyZjBjNWNiNTBhNjcwOWUzMDM4ZWJiZiIsInZlcnNpb24iOjF9.gxpwIBBA3_5xPi-TaZcqWNnGgCiHzxaUNgrS2jucxoVWGxhBtnPdwKVCxLleQoDDZenAXB3Yh71zMP3xTSeHCw"
                }
              ]
            }
          ]
        }
      ]
    },
    "card_text": "\n# MiniLM-L12-H384-uncased for Extractive QA\n\n## Overview\n**Language model:** microsoft/MiniLM-L12-H384-uncased  \n**Language:** English  \n**Downstream-task:** Extractive QA  \n**Training data:** SQuAD 2.0  \n**Eval data:** SQuAD 2.0  \n**Code:**  See [an example extractive QA pipeline built with Haystack](https://haystack.deepset.ai/tutorials/34_extractive_qa_pipeline) \n**Infrastructure**: 1x Tesla v100  \n\n## Hyperparameters\n\n```\nseed=42\nbatch_size = 12\nn_epochs = 4\nbase_LM_model = \"microsoft/MiniLM-L12-H384-uncased\"\nmax_seq_len = 384\nlearning_rate = 4e-5\nlr_schedule = LinearWarmup\nwarmup_proportion = 0.2\ndoc_stride=128\nmax_query_length=64\ngrad_acc_steps=4\n```\n\n## Usage\n\n### In Haystack\nHaystack is an AI orchestration framework to build customizable, production-ready LLM applications. You can use this model in Haystack to do extractive question answering on documents. \nTo load and run the model with [Haystack](https://github.com/deepset-ai/haystack/):\n```python\n# After running pip install haystack-ai \"transformers[torch,sentencepiece]\"\n\nfrom haystack import Document\nfrom haystack.components.readers import ExtractiveReader\n\ndocs = [\n    Document(content=\"Python is a popular programming language\"),\n    Document(content=\"python ist eine beliebte Programmiersprache\"),\n]\n\nreader = ExtractiveReader(model=\"deepset/minilm-uncased-squad2\")\nreader.warm_up()\n\nquestion = \"What is a popular programming language?\"\nresult = reader.run(query=question, documents=docs)\n# {'answers': [ExtractedAnswer(query='What is a popular programming language?', score=0.5740374326705933, data='python', document=Document(id=..., content: '...'), context=None, document_offset=ExtractedAnswer.Span(start=0, end=6),...)]}\n```\nFor a complete example with an extractive question answering pipeline that scales over many documents, check out the [corresponding Haystack tutorial](https://haystack.deepset.ai/tutorials/34_extractive_qa_pipeline).\n\n### In Transformers\n```python\nfrom transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n\nmodel_name = \"deepset/minilm-uncased-squad2\"\n\n# a) Get predictions\nnlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\nQA_input = {\n    'question': 'Why is model conversion important?',\n    'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\n}\nres = nlp(QA_input)\n\n# b) Load model & tokenizer\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n```\n\n## Performance\nEvaluated on the SQuAD 2.0 dev set with the [official eval script](https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/contents/blob/).\n```\n\"exact\": 76.13071675229513,\n\"f1\": 79.49786500219953,\n\"total\": 11873,\n\"HasAns_exact\": 78.35695006747639,\n\"HasAns_f1\": 85.10090269418276,\n\"HasAns_total\": 5928,\n\"NoAns_exact\": 73.91084945332211,\n\"NoAns_f1\": 73.91084945332211,\n\"NoAns_total\": 5945\n```\n\n## Authors\n**Vaishali Pal:** vaishali.pal@deepset.ai  \n**Branden Chan:** branden.chan@deepset.ai  \n**Timo M\u00f6ller:** timo.moeller@deepset.ai  \n**Malte Pietsch:** malte.pietsch@deepset.ai   \n**Tanay Soni:** tanay.soni@deepset.ai  \n\n## About us\n\n<div class=\"grid lg:grid-cols-2 gap-x-4 gap-y-3\">\n    <div class=\"w-full h-40 object-cover mb-2 rounded-lg flex items-center justify-center\">\n         <img alt=\"\" src=\"https://raw.githubusercontent.com/deepset-ai/.github/main/deepset-logo-colored.png\" class=\"w-40\"/>\n     </div>\n     <div class=\"w-full h-40 object-cover mb-2 rounded-lg flex items-center justify-center\">\n         <img alt=\"\" src=\"https://raw.githubusercontent.com/deepset-ai/.github/main/haystack-logo-colored.png\" class=\"w-40\"/>\n     </div>\n</div>\n\n[deepset](http://deepset.ai/) is the company behind the production-ready open-source AI framework [Haystack](https://haystack.deepset.ai/).\n\nSome of our other work: \n- [Distilled roberta-base-squad2 (aka \"tinyroberta-squad2\")](https://huggingface.co/deepset/tinyroberta-squad2)\n- [German BERT](https://deepset.ai/german-bert), [GermanQuAD and GermanDPR](https://deepset.ai/germanquad), [German embedding model](https://huggingface.co/mixedbread-ai/deepset-mxbai-embed-de-large-v1)\n- [deepset Cloud](https://www.deepset.ai/deepset-cloud-product), [deepset Studio](https://www.deepset.ai/deepset-studio)\n\n## Get in touch and join the Haystack community\n\n<p>For more info on Haystack, visit our <strong><a href=\"https://github.com/deepset-ai/haystack\">GitHub</a></strong> repo and <strong><a href=\"https://docs.haystack.deepset.ai\">Documentation</a></strong>. \n\nWe also have a <strong><a class=\"h-7\" href=\"https://haystack.deepset.ai/community\">Discord community open to everyone!</a></strong></p>\n\n[Twitter](https://twitter.com/Haystack_AI) | [LinkedIn](https://www.linkedin.com/company/deepset-ai/) | [Discord](https://haystack.deepset.ai/community) | [GitHub Discussions](https://github.com/deepset-ai/haystack/discussions) | [Website](https://haystack.deepset.ai/) | [YouTube](https://www.youtube.com/@deepset_ai)\n\nBy the way: [we're hiring!](http://www.deepset.ai/jobs)\n",
    "card_content": "---\nlanguage: en\nlicense: cc-by-4.0\ndatasets:\n- squad_v2\nmodel-index:\n- name: deepset/minilm-uncased-squad2\n  results:\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squad_v2\n      type: squad_v2\n      config: squad_v2\n      split: validation\n    metrics:\n    - type: exact_match\n      value: 76.1921\n      name: Exact Match\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiNmViZTQ3YTBjYTc3ZDQzYmI1Mzk3MTAxM2MzNjdmMTc0MWY4Yzg2MWU3NGQ1MDJhZWI2NzY0YWYxZTY2OTgzMiIsInZlcnNpb24iOjF9.s4XCRs_pvW__LJ57dpXAEHD6NRsQ3XaFrM1xaguS6oUs5fCN77wNNc97scnfoPXT18A8RAn0cLTNivfxZm0oBA\n    - type: f1\n      value: 79.5483\n      name: F1\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZmJlYTIyOTg2NjMyMzg4NzNlNGIzMTY2NDVkMjg0ODdiOWRmYjVkZDYyZjBjNWNiNTBhNjcwOWUzMDM4ZWJiZiIsInZlcnNpb24iOjF9.gxpwIBBA3_5xPi-TaZcqWNnGgCiHzxaUNgrS2jucxoVWGxhBtnPdwKVCxLleQoDDZenAXB3Yh71zMP3xTSeHCw\n---\n\n# MiniLM-L12-H384-uncased for Extractive QA\n\n## Overview\n**Language model:** microsoft/MiniLM-L12-H384-uncased  \n**Language:** English  \n**Downstream-task:** Extractive QA  \n**Training data:** SQuAD 2.0  \n**Eval data:** SQuAD 2.0  \n**Code:**  See [an example extractive QA pipeline built with Haystack](https://haystack.deepset.ai/tutorials/34_extractive_qa_pipeline) \n**Infrastructure**: 1x Tesla v100  \n\n## Hyperparameters\n\n```\nseed=42\nbatch_size = 12\nn_epochs = 4\nbase_LM_model = \"microsoft/MiniLM-L12-H384-uncased\"\nmax_seq_len = 384\nlearning_rate = 4e-5\nlr_schedule = LinearWarmup\nwarmup_proportion = 0.2\ndoc_stride=128\nmax_query_length=64\ngrad_acc_steps=4\n```\n\n## Usage\n\n### In Haystack\nHaystack is an AI orchestration framework to build customizable, production-ready LLM applications. You can use this model in Haystack to do extractive question answering on documents. \nTo load and run the model with [Haystack](https://github.com/deepset-ai/haystack/):\n```python\n# After running pip install haystack-ai \"transformers[torch,sentencepiece]\"\n\nfrom haystack import Document\nfrom haystack.components.readers import ExtractiveReader\n\ndocs = [\n    Document(content=\"Python is a popular programming language\"),\n    Document(content=\"python ist eine beliebte Programmiersprache\"),\n]\n\nreader = ExtractiveReader(model=\"deepset/minilm-uncased-squad2\")\nreader.warm_up()\n\nquestion = \"What is a popular programming language?\"\nresult = reader.run(query=question, documents=docs)\n# {'answers': [ExtractedAnswer(query='What is a popular programming language?', score=0.5740374326705933, data='python', document=Document(id=..., content: '...'), context=None, document_offset=ExtractedAnswer.Span(start=0, end=6),...)]}\n```\nFor a complete example with an extractive question answering pipeline that scales over many documents, check out the [corresponding Haystack tutorial](https://haystack.deepset.ai/tutorials/34_extractive_qa_pipeline).\n\n### In Transformers\n```python\nfrom transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n\nmodel_name = \"deepset/minilm-uncased-squad2\"\n\n# a) Get predictions\nnlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\nQA_input = {\n    'question': 'Why is model conversion important?',\n    'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\n}\nres = nlp(QA_input)\n\n# b) Load model & tokenizer\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n```\n\n## Performance\nEvaluated on the SQuAD 2.0 dev set with the [official eval script](https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/contents/blob/).\n```\n\"exact\": 76.13071675229513,\n\"f1\": 79.49786500219953,\n\"total\": 11873,\n\"HasAns_exact\": 78.35695006747639,\n\"HasAns_f1\": 85.10090269418276,\n\"HasAns_total\": 5928,\n\"NoAns_exact\": 73.91084945332211,\n\"NoAns_f1\": 73.91084945332211,\n\"NoAns_total\": 5945\n```\n\n## Authors\n**Vaishali Pal:** vaishali.pal@deepset.ai  \n**Branden Chan:** branden.chan@deepset.ai  \n**Timo M\u00f6ller:** timo.moeller@deepset.ai  \n**Malte Pietsch:** malte.pietsch@deepset.ai   \n**Tanay Soni:** tanay.soni@deepset.ai  \n\n## About us\n\n<div class=\"grid lg:grid-cols-2 gap-x-4 gap-y-3\">\n    <div class=\"w-full h-40 object-cover mb-2 rounded-lg flex items-center justify-center\">\n         <img alt=\"\" src=\"https://raw.githubusercontent.com/deepset-ai/.github/main/deepset-logo-colored.png\" class=\"w-40\"/>\n     </div>\n     <div class=\"w-full h-40 object-cover mb-2 rounded-lg flex items-center justify-center\">\n         <img alt=\"\" src=\"https://raw.githubusercontent.com/deepset-ai/.github/main/haystack-logo-colored.png\" class=\"w-40\"/>\n     </div>\n</div>\n\n[deepset](http://deepset.ai/) is the company behind the production-ready open-source AI framework [Haystack](https://haystack.deepset.ai/).\n\nSome of our other work: \n- [Distilled roberta-base-squad2 (aka \"tinyroberta-squad2\")](https://huggingface.co/deepset/tinyroberta-squad2)\n- [German BERT](https://deepset.ai/german-bert), [GermanQuAD and GermanDPR](https://deepset.ai/germanquad), [German embedding model](https://huggingface.co/mixedbread-ai/deepset-mxbai-embed-de-large-v1)\n- [deepset Cloud](https://www.deepset.ai/deepset-cloud-product), [deepset Studio](https://www.deepset.ai/deepset-studio)\n\n## Get in touch and join the Haystack community\n\n<p>For more info on Haystack, visit our <strong><a href=\"https://github.com/deepset-ai/haystack\">GitHub</a></strong> repo and <strong><a href=\"https://docs.haystack.deepset.ai\">Documentation</a></strong>. \n\nWe also have a <strong><a class=\"h-7\" href=\"https://haystack.deepset.ai/community\">Discord community open to everyone!</a></strong></p>\n\n[Twitter](https://twitter.com/Haystack_AI) | [LinkedIn](https://www.linkedin.com/company/deepset-ai/) | [Discord](https://haystack.deepset.ai/community) | [GitHub Discussions](https://github.com/deepset-ai/haystack/discussions) | [Website](https://haystack.deepset.ai/) | [YouTube](https://www.youtube.com/@deepset_ai)\n\nBy the way: [we're hiring!](http://www.deepset.ai/jobs)\n",
    "library_name": "transformers"
  },
  {
    "model_id": "sjrhuschlee/flan-t5-large-squad2",
    "model_name": "sjrhuschlee/flan-t5-large-squad2",
    "author": "sjrhuschlee",
    "downloads": 132490,
    "likes": 4,
    "tags": [
      "transformers",
      "pytorch",
      "safetensors",
      "t5",
      "question-answering",
      "squad",
      "squad_v2",
      "lora",
      "peft",
      "custom_code",
      "en",
      "dataset:squad_v2",
      "dataset:squad",
      "base_model:google/flan-t5-large",
      "base_model:adapter:google/flan-t5-large",
      "license:mit",
      "model-index",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/sjrhuschlee/flan-t5-large-squad2",
    "dependencies": [
      [
        "transformers",
        "4.36.2"
      ],
      [
        "torch",
        "2.5.1"
      ],
      [
        "peft",
        "0.7.1"
      ]
    ],
    "analysis_date": "2025-03-26T00:49:44.051639",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "t5",
    "pipeline_tag": "question-answering",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": [
        "en"
      ],
      "license": "mit",
      "library_name": "transformers",
      "tags": [
        "question-answering",
        "squad",
        "squad_v2",
        "t5",
        "lora",
        "peft"
      ],
      "datasets": [
        "squad_v2",
        "squad"
      ],
      "base_model": "google/flan-t5-large",
      "model-index": [
        {
          "name": "sjrhuschlee/flan-t5-large-squad2",
          "results": [
            {
              "task": {
                "type": "question-answering",
                "name": "Question Answering"
              },
              "dataset": {
                "name": "squad_v2",
                "type": "squad_v2",
                "config": "squad_v2",
                "split": "validation"
              },
              "metrics": [
                {
                  "type": "exact_match",
                  "value": 86.819,
                  "name": "Exact Match"
                },
                {
                  "type": "f1",
                  "value": 89.569,
                  "name": "F1"
                }
              ]
            },
            {
              "task": {
                "type": "question-answering",
                "name": "Question Answering"
              },
              "dataset": {
                "name": "squad",
                "type": "squad",
                "config": "plain_text",
                "split": "validation"
              },
              "metrics": [
                {
                  "type": "exact_match",
                  "value": 89.357,
                  "name": "Exact Match"
                },
                {
                  "type": "f1",
                  "value": 95.06,
                  "name": "F1"
                }
              ]
            },
            {
              "task": {
                "type": "question-answering",
                "name": "Question Answering"
              },
              "dataset": {
                "name": "adversarial_qa",
                "type": "adversarial_qa",
                "config": "adversarialQA",
                "split": "validation"
              },
              "metrics": [
                {
                  "type": "exact_match",
                  "value": 48.833,
                  "name": "Exact Match"
                },
                {
                  "type": "f1",
                  "value": 62.555,
                  "name": "F1"
                }
              ]
            },
            {
              "task": {
                "type": "question-answering",
                "name": "Question Answering"
              },
              "dataset": {
                "name": "squad_adversarial",
                "type": "squad_adversarial",
                "config": "AddOneSent",
                "split": "validation"
              },
              "metrics": [
                {
                  "type": "exact_match",
                  "value": 84.835,
                  "name": "Exact Match"
                },
                {
                  "type": "f1",
                  "value": 90.245,
                  "name": "F1"
                }
              ]
            },
            {
              "task": {
                "type": "question-answering",
                "name": "Question Answering"
              },
              "dataset": {
                "name": "squadshifts amazon",
                "type": "squadshifts",
                "config": "amazon",
                "split": "test"
              },
              "metrics": [
                {
                  "type": "exact_match",
                  "value": 76.722,
                  "name": "Exact Match"
                },
                {
                  "type": "f1",
                  "value": 89.68,
                  "name": "F1"
                }
              ]
            },
            {
              "task": {
                "type": "question-answering",
                "name": "Question Answering"
              },
              "dataset": {
                "name": "squadshifts new_wiki",
                "type": "squadshifts",
                "config": "new_wiki",
                "split": "test"
              },
              "metrics": [
                {
                  "type": "exact_match",
                  "value": 84.316,
                  "name": "Exact Match"
                },
                {
                  "type": "f1",
                  "value": 92.967,
                  "name": "F1"
                }
              ]
            },
            {
              "task": {
                "type": "question-answering",
                "name": "Question Answering"
              },
              "dataset": {
                "name": "squadshifts nyt",
                "type": "squadshifts",
                "config": "nyt",
                "split": "test"
              },
              "metrics": [
                {
                  "type": "exact_match",
                  "value": 86.925,
                  "name": "Exact Match"
                },
                {
                  "type": "f1",
                  "value": 94.064,
                  "name": "F1"
                }
              ]
            },
            {
              "task": {
                "type": "question-answering",
                "name": "Question Answering"
              },
              "dataset": {
                "name": "squadshifts reddit",
                "type": "squadshifts",
                "config": "reddit",
                "split": "test"
              },
              "metrics": [
                {
                  "type": "exact_match",
                  "value": 78.241,
                  "name": "Exact Match"
                },
                {
                  "type": "f1",
                  "value": 89.243,
                  "name": "F1"
                }
              ]
            }
          ]
        }
      ]
    },
    "card_text": "\n# flan-t5-large for Extractive QA\n\nThis is the [flan-t5-large](https://huggingface.co/google/flan-t5-large) model, fine-tuned using the [SQuAD2.0](https://huggingface.co/datasets/squad_v2) dataset. It's been trained on question-answer pairs, including unanswerable questions, for the task of Extractive Question Answering.\n\n**UPDATE:** With transformers version 4.31.0 the `use_remote_code=True` is no longer necessary.\n\nThis model was trained using LoRA available through the [PEFT library](https://github.com/huggingface/peft).\n\n**NOTE:** The `<cls>` token must be manually added to the beginning of the question for this model to work properly. It uses the `<cls>` token to be able to make \"no answer\" predictions. The t5 tokenizer does not automatically add this special token which is why it is added manually.\n\n## Overview\n**Language model:** flan-t5-large  \n**Language:** English  \n**Downstream-task:** Extractive QA  \n**Training data:** SQuAD 2.0  \n**Eval data:** SQuAD 2.0  \n**Infrastructure**: 1x NVIDIA 3070  \n\n## Model Usage\n\n### Using Transformers\nThis uses the merged weights (base model weights + LoRA weights) to allow for simple use in Transformers pipelines. It has the same performance as using the weights separately when using the PEFT library.\n```python\nimport torch\nfrom transformers import(\n  AutoModelForQuestionAnswering,\n  AutoTokenizer,\n  pipeline\n)\nmodel_name = \"sjrhuschlee/flan-t5-large-squad2\"\n\n# a) Using pipelines\nnlp = pipeline(\n  'question-answering',\n  model=model_name,\n  tokenizer=model_name,\n  # trust_remote_code=True, # Do not use if version transformers>=4.31.0\n)\nqa_input = {\n'question': f'{nlp.tokenizer.cls_token}Where do I live?',  # '<cls>Where do I live?'\n'context': 'My name is Sarah and I live in London'\n}\nres = nlp(qa_input)\n# {'score': 0.984, 'start': 30, 'end': 37, 'answer': ' London'}\n\n# b) Load model & tokenizer\nmodel = AutoModelForQuestionAnswering.from_pretrained(\n  model_name,\n  # trust_remote_code=True # Do not use if version transformers>=4.31.0\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nquestion = f'{tokenizer.cls_token}Where do I live?'  # '<cls>Where do I live?'\ncontext = 'My name is Sarah and I live in London'\nencoding = tokenizer(question, context, return_tensors=\"pt\")\noutput = model(\n  encoding[\"input_ids\"],\n  attention_mask=encoding[\"attention_mask\"]\n)\n\nall_tokens = tokenizer.convert_ids_to_tokens(encoding[\"input_ids\"][0].tolist())\nanswer_tokens = all_tokens[torch.argmax(output[\"start_logits\"]):torch.argmax(output[\"end_logits\"]) + 1]\nanswer = tokenizer.decode(tokenizer.convert_tokens_to_ids(answer_tokens))\n# 'London'\n```\n\n## Metrics\n\n```bash\n# Squad v2\n{\n    \"eval_HasAns_exact\": 85.08771929824562,\n    \"eval_HasAns_f1\": 90.598422845031,\n    \"eval_HasAns_total\": 5928,\n    \"eval_NoAns_exact\": 88.47771236333053,\n    \"eval_NoAns_f1\": 88.47771236333053,\n    \"eval_NoAns_total\": 5945,\n    \"eval_best_exact\": 86.78514276088605,\n    \"eval_best_exact_thresh\": 0.0,\n    \"eval_best_f1\": 89.53654936623764,\n    \"eval_best_f1_thresh\": 0.0,\n    \"eval_exact\": 86.78514276088605,\n    \"eval_f1\": 89.53654936623776,\n    \"eval_runtime\": 1908.3189,\n    \"eval_samples\": 12001,\n    \"eval_samples_per_second\": 6.289,\n    \"eval_steps_per_second\": 0.787,\n    \"eval_total\": 11873\n}\n\n# Squad\n{\n    \"eval_HasAns_exact\": 85.99810785241249,\n    \"eval_HasAns_f1\": 91.296119057944,\n    \"eval_HasAns_total\": 10570,\n    \"eval_best_exact\": 85.99810785241249,\n    \"eval_best_exact_thresh\": 0.0,\n    \"eval_best_f1\": 91.296119057944,\n    \"eval_best_f1_thresh\": 0.0,\n    \"eval_exact\": 85.99810785241249,\n    \"eval_f1\": 91.296119057944,\n    \"eval_runtime\": 1508.9596,\n    \"eval_samples\": 10657,\n    \"eval_samples_per_second\": 7.062,\n    \"eval_steps_per_second\": 0.883,\n    \"eval_total\": 10570\n}\n```\n\n### Using with Peft\n**NOTE**: This requires code in the PR https://github.com/huggingface/peft/pull/473 for the PEFT library.\n```python\n#!pip install peft\n\nfrom peft import LoraConfig, PeftModelForQuestionAnswering\nfrom transformers import AutoModelForQuestionAnswering, AutoTokenizer\nmodel_name = \"sjrhuschlee/flan-t5-large-squad2\"\n```",
    "card_content": "---\nlanguage:\n- en\nlicense: mit\nlibrary_name: transformers\ntags:\n- question-answering\n- squad\n- squad_v2\n- t5\n- lora\n- peft\ndatasets:\n- squad_v2\n- squad\nbase_model: google/flan-t5-large\nmodel-index:\n- name: sjrhuschlee/flan-t5-large-squad2\n  results:\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squad_v2\n      type: squad_v2\n      config: squad_v2\n      split: validation\n    metrics:\n    - type: exact_match\n      value: 86.819\n      name: Exact Match\n    - type: f1\n      value: 89.569\n      name: F1\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squad\n      type: squad\n      config: plain_text\n      split: validation\n    metrics:\n    - type: exact_match\n      value: 89.357\n      name: Exact Match\n    - type: f1\n      value: 95.06\n      name: F1\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: adversarial_qa\n      type: adversarial_qa\n      config: adversarialQA\n      split: validation\n    metrics:\n    - type: exact_match\n      value: 48.833\n      name: Exact Match\n    - type: f1\n      value: 62.555\n      name: F1\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squad_adversarial\n      type: squad_adversarial\n      config: AddOneSent\n      split: validation\n    metrics:\n    - type: exact_match\n      value: 84.835\n      name: Exact Match\n    - type: f1\n      value: 90.245\n      name: F1\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squadshifts amazon\n      type: squadshifts\n      config: amazon\n      split: test\n    metrics:\n    - type: exact_match\n      value: 76.722\n      name: Exact Match\n    - type: f1\n      value: 89.68\n      name: F1\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squadshifts new_wiki\n      type: squadshifts\n      config: new_wiki\n      split: test\n    metrics:\n    - type: exact_match\n      value: 84.316\n      name: Exact Match\n    - type: f1\n      value: 92.967\n      name: F1\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squadshifts nyt\n      type: squadshifts\n      config: nyt\n      split: test\n    metrics:\n    - type: exact_match\n      value: 86.925\n      name: Exact Match\n    - type: f1\n      value: 94.064\n      name: F1\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squadshifts reddit\n      type: squadshifts\n      config: reddit\n      split: test\n    metrics:\n    - type: exact_match\n      value: 78.241\n      name: Exact Match\n    - type: f1\n      value: 89.243\n      name: F1\n---\n\n# flan-t5-large for Extractive QA\n\nThis is the [flan-t5-large](https://huggingface.co/google/flan-t5-large) model, fine-tuned using the [SQuAD2.0](https://huggingface.co/datasets/squad_v2) dataset. It's been trained on question-answer pairs, including unanswerable questions, for the task of Extractive Question Answering.\n\n**UPDATE:** With transformers version 4.31.0 the `use_remote_code=True` is no longer necessary.\n\nThis model was trained using LoRA available through the [PEFT library](https://github.com/huggingface/peft).\n\n**NOTE:** The `<cls>` token must be manually added to the beginning of the question for this model to work properly. It uses the `<cls>` token to be able to make \"no answer\" predictions. The t5 tokenizer does not automatically add this special token which is why it is added manually.\n\n## Overview\n**Language model:** flan-t5-large  \n**Language:** English  \n**Downstream-task:** Extractive QA  \n**Training data:** SQuAD 2.0  \n**Eval data:** SQuAD 2.0  \n**Infrastructure**: 1x NVIDIA 3070  \n\n## Model Usage\n\n### Using Transformers\nThis uses the merged weights (base model weights + LoRA weights) to allow for simple use in Transformers pipelines. It has the same performance as using the weights separately when using the PEFT library.\n```python\nimport torch\nfrom transformers import(\n  AutoModelForQuestionAnswering,\n  AutoTokenizer,\n  pipeline\n)\nmodel_name = \"sjrhuschlee/flan-t5-large-squad2\"\n\n# a) Using pipelines\nnlp = pipeline(\n  'question-answering',\n  model=model_name,\n  tokenizer=model_name,\n  # trust_remote_code=True, # Do not use if version transformers>=4.31.0\n)\nqa_input = {\n'question': f'{nlp.tokenizer.cls_token}Where do I live?',  # '<cls>Where do I live?'\n'context': 'My name is Sarah and I live in London'\n}\nres = nlp(qa_input)\n# {'score': 0.984, 'start': 30, 'end': 37, 'answer': ' London'}\n\n# b) Load model & tokenizer\nmodel = AutoModelForQuestionAnswering.from_pretrained(\n  model_name,\n  # trust_remote_code=True # Do not use if version transformers>=4.31.0\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nquestion = f'{tokenizer.cls_token}Where do I live?'  # '<cls>Where do I live?'\ncontext = 'My name is Sarah and I live in London'\nencoding = tokenizer(question, context, return_tensors=\"pt\")\noutput = model(\n  encoding[\"input_ids\"],\n  attention_mask=encoding[\"attention_mask\"]\n)\n\nall_tokens = tokenizer.convert_ids_to_tokens(encoding[\"input_ids\"][0].tolist())\nanswer_tokens = all_tokens[torch.argmax(output[\"start_logits\"]):torch.argmax(output[\"end_logits\"]) + 1]\nanswer = tokenizer.decode(tokenizer.convert_tokens_to_ids(answer_tokens))\n# 'London'\n```\n\n## Metrics\n\n```bash\n# Squad v2\n{\n    \"eval_HasAns_exact\": 85.08771929824562,\n    \"eval_HasAns_f1\": 90.598422845031,\n    \"eval_HasAns_total\": 5928,\n    \"eval_NoAns_exact\": 88.47771236333053,\n    \"eval_NoAns_f1\": 88.47771236333053,\n    \"eval_NoAns_total\": 5945,\n    \"eval_best_exact\": 86.78514276088605,\n    \"eval_best_exact_thresh\": 0.0,\n    \"eval_best_f1\": 89.53654936623764,\n    \"eval_best_f1_thresh\": 0.0,\n    \"eval_exact\": 86.78514276088605,\n    \"eval_f1\": 89.53654936623776,\n    \"eval_runtime\": 1908.3189,\n    \"eval_samples\": 12001,\n    \"eval_samples_per_second\": 6.289,\n    \"eval_steps_per_second\": 0.787,\n    \"eval_total\": 11873\n}\n\n# Squad\n{\n    \"eval_HasAns_exact\": 85.99810785241249,\n    \"eval_HasAns_f1\": 91.296119057944,\n    \"eval_HasAns_total\": 10570,\n    \"eval_best_exact\": 85.99810785241249,\n    \"eval_best_exact_thresh\": 0.0,\n    \"eval_best_f1\": 91.296119057944,\n    \"eval_best_f1_thresh\": 0.0,\n    \"eval_exact\": 85.99810785241249,\n    \"eval_f1\": 91.296119057944,\n    \"eval_runtime\": 1508.9596,\n    \"eval_samples\": 10657,\n    \"eval_samples_per_second\": 7.062,\n    \"eval_steps_per_second\": 0.883,\n    \"eval_total\": 10570\n}\n```\n\n### Using with Peft\n**NOTE**: This requires code in the PR https://github.com/huggingface/peft/pull/473 for the PEFT library.\n```python\n#!pip install peft\n\nfrom peft import LoraConfig, PeftModelForQuestionAnswering\nfrom transformers import AutoModelForQuestionAnswering, AutoTokenizer\nmodel_name = \"sjrhuschlee/flan-t5-large-squad2\"\n```",
    "library_name": "transformers"
  },
  {
    "model_id": "distilbert/distilbert-base-uncased-distilled-squad",
    "model_name": "distilbert/distilbert-base-uncased-distilled-squad",
    "author": "distilbert",
    "downloads": 131759,
    "likes": 111,
    "tags": [
      "transformers",
      "pytorch",
      "tf",
      "tflite",
      "coreml",
      "safetensors",
      "distilbert",
      "question-answering",
      "en",
      "dataset:squad",
      "arxiv:1910.01108",
      "arxiv:1910.09700",
      "license:apache-2.0",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/distilbert/distilbert-base-uncased-distilled-squad",
    "dependencies": [
      [
        "transformers",
        "4.36.2"
      ],
      [
        "torch",
        "2.5.1"
      ]
    ],
    "analysis_date": "2025-03-26T00:49:45.300735",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "distilbert",
    "pipeline_tag": "question-answering",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": "en",
      "license": "apache-2.0",
      "datasets": [
        "squad"
      ],
      "widget": [
        {
          "text": "Which name is also used to describe the Amazon rainforest in English?",
          "context": "The Amazon rainforest (Portuguese: Floresta Amaz\u00f4nica or Amaz\u00f4nia; Spanish: Selva Amaz\u00f3nica, Amazon\u00eda or usually Amazonia; French: For\u00eat amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet's remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species."
        },
        {
          "text": "How many square kilometers of rainforest is covered in the basin?",
          "context": "The Amazon rainforest (Portuguese: Floresta Amaz\u00f4nica or Amaz\u00f4nia; Spanish: Selva Amaz\u00f3nica, Amazon\u00eda or usually Amazonia; French: For\u00eat amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet's remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species."
        }
      ]
    },
    "card_text": "\n# DistilBERT base uncased distilled SQuAD\n\n## Table of Contents\n- [Model Details](#model-details)\n- [How To Get Started With the Model](#how-to-get-started-with-the-model)\n- [Uses](#uses)\n- [Risks, Limitations and Biases](#risks-limitations-and-biases)\n- [Training](#training)\n- [Evaluation](#evaluation)\n- [Environmental Impact](#environmental-impact)\n- [Technical Specifications](#technical-specifications)\n- [Citation Information](#citation-information)\n- [Model Card Authors](#model-card-authors)\n\n## Model Details\n\n**Model Description:** The DistilBERT model was proposed in the blog post [Smaller, faster, cheaper, lighter: Introducing DistilBERT, adistilled version of BERT](https://medium.com/huggingface/distilbert-8cf3380435b5), and the paper [DistilBERT, adistilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/abs/1910.01108). DistilBERT is a small, fast, cheap and light Transformer model trained by distilling BERT base. It has 40% less parameters than *bert-base-uncased*, runs 60% faster while preserving over 95% of BERT's performances as measured on the GLUE language understanding benchmark.\n\nThis model is a fine-tune checkpoint of [DistilBERT-base-uncased](https://huggingface.co/distilbert-base-uncased), fine-tuned using (a second step of) knowledge distillation on [SQuAD v1.1](https://huggingface.co/datasets/squad). \n\n- **Developed by:** Hugging Face\n- **Model Type:** Transformer-based language model\n- **Language(s):** English \n- **License:** Apache 2.0\n- **Related Models:** [DistilBERT-base-uncased](https://huggingface.co/distilbert-base-uncased)\n- **Resources for more information:**\n  - See [this repository](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation) for more about Distil\\* (a class of compressed models including this model)\n  - See [Sanh et al. (2019)](https://arxiv.org/abs/1910.01108) for more information about knowledge distillation and the training procedure\n\n## How to Get Started with the Model \n\nUse the code below to get started with the model. \n\n```python\n>>> from transformers import pipeline\n>>> question_answerer = pipeline(\"question-answering\", model='distilbert-base-uncased-distilled-squad')\n\n>>> context = r\"\"\"\n... Extractive Question Answering is the task of extracting an answer from a text given a question. An example     of a\n... question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\n... a model on a SQuAD task, you may leverage the examples/pytorch/question-answering/run_squad.py script.\n... \"\"\"\n\n>>> result = question_answerer(question=\"What is a good example of a question answering dataset?\",     context=context)\n>>> print(\n... f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\"\n...)\n\nAnswer: 'SQuAD dataset', score: 0.4704, start: 147, end: 160\n```\n\nHere is how to use this model in PyTorch:\n\n```python\nfrom transformers import DistilBertTokenizer, DistilBertForQuestionAnswering\nimport torch\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased-distilled-squad')\nmodel = DistilBertForQuestionAnswering.from_pretrained('distilbert-base-uncased-distilled-squad')\n\nquestion, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"\n\ninputs = tokenizer(question, text, return_tensors=\"pt\")\nwith torch.no_grad():\n    outputs = model(**inputs)\n\nanswer_start_index = torch.argmax(outputs.start_logits)\nanswer_end_index = torch.argmax(outputs.end_logits)\n\npredict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\ntokenizer.decode(predict_answer_tokens)\n```\n\nAnd in TensorFlow: \n\n```python\nfrom transformers import DistilBertTokenizer, TFDistilBertForQuestionAnswering\nimport tensorflow as tf\n\ntokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased-distilled-squad\")\nmodel = TFDistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-uncased-distilled-squad\")\n\nquestion, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"\n\ninputs = tokenizer(question, text, return_tensors=\"tf\")\noutputs = model(**inputs)\n\nanswer_start_index = int(tf.math.argmax(outputs.start_logits, axis=-1)[0])\nanswer_end_index = int(tf.math.argmax(outputs.end_logits, axis=-1)[0])\n\npredict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\ntokenizer.decode(predict_answer_tokens)\n```\n\n## Uses\n\nThis model can be used for question answering.\n\n#### Misuse and Out-of-scope Use\n\nThe model should not be used to intentionally create hostile or alienating environments for people. In addition, the model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model.\n\n## Risks, Limitations and Biases\n\n**CONTENT WARNING: Readers should be aware that language generated by this model can be disturbing or offensive to some and can propagate historical and current stereotypes.**\n\nSignificant research has explored bias and fairness issues with language models (see, e.g., [Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)). Predictions generated by the model can include disturbing and harmful stereotypes across protected classes; identity characteristics; and sensitive, social, and occupational groups. For example:\n\n\n```python\n>>> from transformers import pipeline\n>>> question_answerer = pipeline(\"question-answering\", model='distilbert-base-uncased-distilled-squad')\n\n>>> context = r\"\"\"\n... Alice is sitting on the bench. Bob is sitting next to her.\n... \"\"\"\n\n>>> result = question_answerer(question=\"Who is the CEO?\", context=context)\n>>> print(\n... f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\"\n...)\n\nAnswer: 'Bob', score: 0.4183, start: 32, end: 35\n```\n\nUsers (both direct and downstream) should be made aware of the risks, biases and limitations of the model.\n\n## Training\n\n#### Training Data\n\nThe [distilbert-base-uncased model](https://huggingface.co/distilbert-base-uncased) model describes it's training data as: \n\n> DistilBERT pretrained on the same data as BERT, which is [BookCorpus](https://yknzhu.wixsite.com/mbweb), a dataset consisting of 11,038 unpublished books and [English Wikipedia](https://en.wikipedia.org/wiki/English_Wikipedia) (excluding lists, tables and headers).\n\nTo learn more about the SQuAD v1.1 dataset, see the [SQuAD v1.1 data card](https://huggingface.co/datasets/squad).\n\n#### Training Procedure\n\n##### Preprocessing\n\nSee the [distilbert-base-uncased model card](https://huggingface.co/distilbert-base-uncased) for further details.\n\n##### Pretraining\n\nSee the [distilbert-base-uncased model card](https://huggingface.co/distilbert-base-uncased) for further details. \n\n## Evaluation\n\nAs discussed in the [model repository](https://github.com/huggingface/transformers/blob/main/examples/research_projects/distillation/README.md)\n\n> This model reaches a F1 score of 86.9 on the [SQuAD v1.1] dev set (for comparison, Bert bert-base-uncased version reaches a F1 score of 88.5).\n\n## Environmental Impact\n\nCarbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700). We present the hardware type and hours used based on the [associated paper](https://arxiv.org/pdf/1910.01108.pdf). Note that these details are just for training DistilBERT, not including the fine-tuning with SQuAD.\n\n- **Hardware Type:** 8 16GB V100 GPUs\n- **Hours used:** 90 hours\n- **Cloud Provider:** Unknown\n- **Compute Region:** Unknown\n- **Carbon Emitted:** Unknown\n\n## Technical Specifications\n\nSee the [associated paper](https://arxiv.org/abs/1910.01108) for details on the modeling architecture, objective, compute infrastructure, and training details.\n\n## Citation Information\n\n```bibtex\n@inproceedings{sanh2019distilbert,\n  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},\n  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},\n  booktitle={NeurIPS EMC^2 Workshop},\n  year={2019}\n}\n```\n\nAPA: \n- Sanh, V., Debut, L., Chaumond, J., & Wolf, T. (2019). DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108.\n\n## Model Card Authors\n\nThis model card was written by the Hugging Face team. \n",
    "card_content": "---\nlanguage: en\nlicense: apache-2.0\ndatasets:\n- squad\nwidget:\n- text: Which name is also used to describe the Amazon rainforest in English?\n  context: 'The Amazon rainforest (Portuguese: Floresta Amaz\u00f4nica or Amaz\u00f4nia; Spanish:\n    Selva Amaz\u00f3nica, Amazon\u00eda or usually Amazonia; French: For\u00eat amazonienne; Dutch:\n    Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is\n    a moist broadleaf forest that covers most of the Amazon basin of South America.\n    This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which\n    5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This\n    region includes territory belonging to nine nations. The majority of the forest\n    is contained within Brazil, with 60% of the rainforest, followed by Peru with\n    13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia,\n    Guyana, Suriname and French Guiana. States or departments in four nations contain\n    \"Amazonas\" in their names. The Amazon represents over half of the planet''s remaining\n    rainforests, and comprises the largest and most biodiverse tract of tropical rainforest\n    in the world, with an estimated 390 billion individual trees divided into 16,000\n    species.'\n- text: How many square kilometers of rainforest is covered in the basin?\n  context: 'The Amazon rainforest (Portuguese: Floresta Amaz\u00f4nica or Amaz\u00f4nia; Spanish:\n    Selva Amaz\u00f3nica, Amazon\u00eda or usually Amazonia; French: For\u00eat amazonienne; Dutch:\n    Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is\n    a moist broadleaf forest that covers most of the Amazon basin of South America.\n    This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which\n    5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This\n    region includes territory belonging to nine nations. The majority of the forest\n    is contained within Brazil, with 60% of the rainforest, followed by Peru with\n    13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia,\n    Guyana, Suriname and French Guiana. States or departments in four nations contain\n    \"Amazonas\" in their names. The Amazon represents over half of the planet''s remaining\n    rainforests, and comprises the largest and most biodiverse tract of tropical rainforest\n    in the world, with an estimated 390 billion individual trees divided into 16,000\n    species.'\n---\n\n# DistilBERT base uncased distilled SQuAD\n\n## Table of Contents\n- [Model Details](#model-details)\n- [How To Get Started With the Model](#how-to-get-started-with-the-model)\n- [Uses](#uses)\n- [Risks, Limitations and Biases](#risks-limitations-and-biases)\n- [Training](#training)\n- [Evaluation](#evaluation)\n- [Environmental Impact](#environmental-impact)\n- [Technical Specifications](#technical-specifications)\n- [Citation Information](#citation-information)\n- [Model Card Authors](#model-card-authors)\n\n## Model Details\n\n**Model Description:** The DistilBERT model was proposed in the blog post [Smaller, faster, cheaper, lighter: Introducing DistilBERT, adistilled version of BERT](https://medium.com/huggingface/distilbert-8cf3380435b5), and the paper [DistilBERT, adistilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/abs/1910.01108). DistilBERT is a small, fast, cheap and light Transformer model trained by distilling BERT base. It has 40% less parameters than *bert-base-uncased*, runs 60% faster while preserving over 95% of BERT's performances as measured on the GLUE language understanding benchmark.\n\nThis model is a fine-tune checkpoint of [DistilBERT-base-uncased](https://huggingface.co/distilbert-base-uncased), fine-tuned using (a second step of) knowledge distillation on [SQuAD v1.1](https://huggingface.co/datasets/squad). \n\n- **Developed by:** Hugging Face\n- **Model Type:** Transformer-based language model\n- **Language(s):** English \n- **License:** Apache 2.0\n- **Related Models:** [DistilBERT-base-uncased](https://huggingface.co/distilbert-base-uncased)\n- **Resources for more information:**\n  - See [this repository](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation) for more about Distil\\* (a class of compressed models including this model)\n  - See [Sanh et al. (2019)](https://arxiv.org/abs/1910.01108) for more information about knowledge distillation and the training procedure\n\n## How to Get Started with the Model \n\nUse the code below to get started with the model. \n\n```python\n>>> from transformers import pipeline\n>>> question_answerer = pipeline(\"question-answering\", model='distilbert-base-uncased-distilled-squad')\n\n>>> context = r\"\"\"\n... Extractive Question Answering is the task of extracting an answer from a text given a question. An example     of a\n... question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\n... a model on a SQuAD task, you may leverage the examples/pytorch/question-answering/run_squad.py script.\n... \"\"\"\n\n>>> result = question_answerer(question=\"What is a good example of a question answering dataset?\",     context=context)\n>>> print(\n... f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\"\n...)\n\nAnswer: 'SQuAD dataset', score: 0.4704, start: 147, end: 160\n```\n\nHere is how to use this model in PyTorch:\n\n```python\nfrom transformers import DistilBertTokenizer, DistilBertForQuestionAnswering\nimport torch\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased-distilled-squad')\nmodel = DistilBertForQuestionAnswering.from_pretrained('distilbert-base-uncased-distilled-squad')\n\nquestion, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"\n\ninputs = tokenizer(question, text, return_tensors=\"pt\")\nwith torch.no_grad():\n    outputs = model(**inputs)\n\nanswer_start_index = torch.argmax(outputs.start_logits)\nanswer_end_index = torch.argmax(outputs.end_logits)\n\npredict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\ntokenizer.decode(predict_answer_tokens)\n```\n\nAnd in TensorFlow: \n\n```python\nfrom transformers import DistilBertTokenizer, TFDistilBertForQuestionAnswering\nimport tensorflow as tf\n\ntokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased-distilled-squad\")\nmodel = TFDistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-uncased-distilled-squad\")\n\nquestion, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"\n\ninputs = tokenizer(question, text, return_tensors=\"tf\")\noutputs = model(**inputs)\n\nanswer_start_index = int(tf.math.argmax(outputs.start_logits, axis=-1)[0])\nanswer_end_index = int(tf.math.argmax(outputs.end_logits, axis=-1)[0])\n\npredict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\ntokenizer.decode(predict_answer_tokens)\n```\n\n## Uses\n\nThis model can be used for question answering.\n\n#### Misuse and Out-of-scope Use\n\nThe model should not be used to intentionally create hostile or alienating environments for people. In addition, the model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model.\n\n## Risks, Limitations and Biases\n\n**CONTENT WARNING: Readers should be aware that language generated by this model can be disturbing or offensive to some and can propagate historical and current stereotypes.**\n\nSignificant research has explored bias and fairness issues with language models (see, e.g., [Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)). Predictions generated by the model can include disturbing and harmful stereotypes across protected classes; identity characteristics; and sensitive, social, and occupational groups. For example:\n\n\n```python\n>>> from transformers import pipeline\n>>> question_answerer = pipeline(\"question-answering\", model='distilbert-base-uncased-distilled-squad')\n\n>>> context = r\"\"\"\n... Alice is sitting on the bench. Bob is sitting next to her.\n... \"\"\"\n\n>>> result = question_answerer(question=\"Who is the CEO?\", context=context)\n>>> print(\n... f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\"\n...)\n\nAnswer: 'Bob', score: 0.4183, start: 32, end: 35\n```\n\nUsers (both direct and downstream) should be made aware of the risks, biases and limitations of the model.\n\n## Training\n\n#### Training Data\n\nThe [distilbert-base-uncased model](https://huggingface.co/distilbert-base-uncased) model describes it's training data as: \n\n> DistilBERT pretrained on the same data as BERT, which is [BookCorpus](https://yknzhu.wixsite.com/mbweb), a dataset consisting of 11,038 unpublished books and [English Wikipedia](https://en.wikipedia.org/wiki/English_Wikipedia) (excluding lists, tables and headers).\n\nTo learn more about the SQuAD v1.1 dataset, see the [SQuAD v1.1 data card](https://huggingface.co/datasets/squad).\n\n#### Training Procedure\n\n##### Preprocessing\n\nSee the [distilbert-base-uncased model card](https://huggingface.co/distilbert-base-uncased) for further details.\n\n##### Pretraining\n\nSee the [distilbert-base-uncased model card](https://huggingface.co/distilbert-base-uncased) for further details. \n\n## Evaluation\n\nAs discussed in the [model repository](https://github.com/huggingface/transformers/blob/main/examples/research_projects/distillation/README.md)\n\n> This model reaches a F1 score of 86.9 on the [SQuAD v1.1] dev set (for comparison, Bert bert-base-uncased version reaches a F1 score of 88.5).\n\n## Environmental Impact\n\nCarbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700). We present the hardware type and hours used based on the [associated paper](https://arxiv.org/pdf/1910.01108.pdf). Note that these details are just for training DistilBERT, not including the fine-tuning with SQuAD.\n\n- **Hardware Type:** 8 16GB V100 GPUs\n- **Hours used:** 90 hours\n- **Cloud Provider:** Unknown\n- **Compute Region:** Unknown\n- **Carbon Emitted:** Unknown\n\n## Technical Specifications\n\nSee the [associated paper](https://arxiv.org/abs/1910.01108) for details on the modeling architecture, objective, compute infrastructure, and training details.\n\n## Citation Information\n\n```bibtex\n@inproceedings{sanh2019distilbert,\n  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},\n  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},\n  booktitle={NeurIPS EMC^2 Workshop},\n  year={2019}\n}\n```\n\nAPA: \n- Sanh, V., Debut, L., Chaumond, J., & Wolf, T. (2019). DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108.\n\n## Model Card Authors\n\nThis model card was written by the Hugging Face team. \n",
    "library_name": "transformers"
  },
  {
    "model_id": "google-bert/bert-large-cased-whole-word-masking-finetuned-squad",
    "model_name": "google-bert/bert-large-cased-whole-word-masking-finetuned-squad",
    "author": "google-bert",
    "downloads": 107165,
    "likes": 1,
    "tags": [
      "transformers",
      "pytorch",
      "tf",
      "jax",
      "rust",
      "safetensors",
      "bert",
      "question-answering",
      "en",
      "dataset:bookcorpus",
      "dataset:wikipedia",
      "arxiv:1810.04805",
      "license:apache-2.0",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/google-bert/bert-large-cased-whole-word-masking-finetuned-squad",
    "dependencies": [
      [
        "torch",
        "2.5.1"
      ],
      [
        "transformers",
        "4.36.2"
      ],
      [
        "numpy",
        "1.26.2"
      ]
    ],
    "analysis_date": "2025-03-26T00:49:46.431355",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "bert",
    "pipeline_tag": "question-answering",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": "en",
      "license": "apache-2.0",
      "datasets": [
        "bookcorpus",
        "wikipedia"
      ]
    },
    "card_text": "\n# BERT large model (cased) whole word masking finetuned on SQuAD\n\nPretrained model on English language using a masked language modeling (MLM) objective. It was introduced in\n[this paper](https://arxiv.org/abs/1810.04805) and first released in\n[this repository](https://github.com/google-research/bert). This model is cased: it makes a difference between english and English.\n\nDifferently to other BERT models, this model was trained with a new technique: Whole Word Masking. In this case, all of the tokens corresponding to a word are masked at once. The overall masking rate remains the same.\n\nThe training is identical -- each masked WordPiece token is predicted independently. \n\nAfter pre-training, this model was fine-tuned on the SQuAD dataset with one of our fine-tuning scripts. See below for more information regarding this fine-tuning.\n\nDisclaimer: The team releasing BERT did not write a model card for this model so this model card has been written by\nthe Hugging Face team.\n\n## Model description\n\nBERT is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. This means it\nwas pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of\npublicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it\nwas pretrained with two objectives:\n\n- Masked language modeling (MLM): taking a sentence, the model randomly masks 15% of the words in the input then run\n  the entire masked sentence through the model and has to predict the masked words. This is different from traditional\n  recurrent neural networks (RNNs) that usually see the words one after the other, or from autoregressive models like\n  GPT which internally mask the future tokens. It allows the model to learn a bidirectional representation of the\n  sentence.\n- Next sentence prediction (NSP): the models concatenates two masked sentences as inputs during pretraining. Sometimes\n  they correspond to sentences that were next to each other in the original text, sometimes not. The model then has to\n  predict if the two sentences were following each other or not.\n\nThis way, the model learns an inner representation of the English language that can then be used to extract features\nuseful for downstream tasks: if you have a dataset of labeled sentences for instance, you can train a standard\nclassifier using the features produced by the BERT model as inputs.\n\nThis model has the following configuration:\n\n- 24-layer\n- 1024 hidden dimension\n- 16 attention heads\n- 336M parameters.\n\n## Intended uses & limitations\nThis model should be used as a question-answering model. You may use it in a question answering pipeline, or use it to output raw results given a query and a context. You may see other use cases in the [task summary](https://huggingface.co/transformers/task_summary.html#extractive-question-answering) of the transformers documentation.## Training data\n\nThe BERT model was pretrained on [BookCorpus](https://yknzhu.wixsite.com/mbweb), a dataset consisting of 11,038\nunpublished books and [English Wikipedia](https://en.wikipedia.org/wiki/English_Wikipedia) (excluding lists, tables and\nheaders).\n\n## Training procedure\n\n### Preprocessing\n\nThe texts are lowercased and tokenized using WordPiece and a vocabulary size of 30,000. The inputs of the model are\nthen of the form:\n\n```\n[CLS] Sentence A [SEP] Sentence B [SEP]\n```\n\nWith probability 0.5, sentence A and sentence B correspond to two consecutive sentences in the original corpus and in\nthe other cases, it's another random sentence in the corpus. Note that what is considered a sentence here is a\nconsecutive span of text usually longer than a single sentence. The only constrain is that the result with the two\n\"sentences\" has a combined length of less than 512 tokens.\n\nThe details of the masking procedure for each sentence are the following:\n- 15% of the tokens are masked.\n- In 80% of the cases, the masked tokens are replaced by `[MASK]`.\n- In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace.\n- In the 10% remaining cases, the masked tokens are left as is.\n\n### Pretraining\n\nThe model was trained on 4 cloud TPUs in Pod configuration (16 TPU chips total) for one million steps with a batch size\nof 256. The sequence length was limited to 128 tokens for 90% of the steps and 512 for the remaining 10%. The optimizer\nused is Adam with a learning rate of 1e-4, \\\\(\\beta_{1} = 0.9\\\\) and \\\\(\\beta_{2} = 0.999\\\\), a weight decay of 0.01,\nlearning rate warmup for 10,000 steps and linear decay of the learning rate after.\n\n### Fine-tuning\n\nAfter pre-training, this model was fine-tuned on the SQuAD dataset with one of our fine-tuning scripts. In order to reproduce the training, you may use the following command:\n```\npython -m torch.distributed.launch --nproc_per_node=8 ./examples/question-answering/run_qa.py \\\n    --model_name_or_path bert-large-cased-whole-word-masking \\\n    --dataset_name squad \\\n    --do_train \\\n    --do_eval \\\n    --learning_rate 3e-5 \\\n    --num_train_epochs 2 \\\n    --max_seq_length 384 \\\n    --doc_stride 128 \\\n    --output_dir ./examples/models/wwm_cased_finetuned_squad/ \\\n    --per_device_eval_batch_size=3   \\\n    --per_device_train_batch_size=3   \\\n```\n\n### BibTeX entry and citation info\n\n```bibtex\n@article{DBLP:journals/corr/abs-1810-04805,\n  author    = {Jacob Devlin and\n               Ming{-}Wei Chang and\n               Kenton Lee and\n               Kristina Toutanova},\n  title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language\n               Understanding},\n  journal   = {CoRR},\n  volume    = {abs/1810.04805},\n  year      = {2018},\n  url       = {http://arxiv.org/abs/1810.04805},\n  archivePrefix = {arXiv},\n  eprint    = {1810.04805},\n  timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n```",
    "card_content": "---\nlanguage: en\nlicense: apache-2.0\ndatasets:\n- bookcorpus\n- wikipedia\n---\n\n# BERT large model (cased) whole word masking finetuned on SQuAD\n\nPretrained model on English language using a masked language modeling (MLM) objective. It was introduced in\n[this paper](https://arxiv.org/abs/1810.04805) and first released in\n[this repository](https://github.com/google-research/bert). This model is cased: it makes a difference between english and English.\n\nDifferently to other BERT models, this model was trained with a new technique: Whole Word Masking. In this case, all of the tokens corresponding to a word are masked at once. The overall masking rate remains the same.\n\nThe training is identical -- each masked WordPiece token is predicted independently. \n\nAfter pre-training, this model was fine-tuned on the SQuAD dataset with one of our fine-tuning scripts. See below for more information regarding this fine-tuning.\n\nDisclaimer: The team releasing BERT did not write a model card for this model so this model card has been written by\nthe Hugging Face team.\n\n## Model description\n\nBERT is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. This means it\nwas pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of\npublicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it\nwas pretrained with two objectives:\n\n- Masked language modeling (MLM): taking a sentence, the model randomly masks 15% of the words in the input then run\n  the entire masked sentence through the model and has to predict the masked words. This is different from traditional\n  recurrent neural networks (RNNs) that usually see the words one after the other, or from autoregressive models like\n  GPT which internally mask the future tokens. It allows the model to learn a bidirectional representation of the\n  sentence.\n- Next sentence prediction (NSP): the models concatenates two masked sentences as inputs during pretraining. Sometimes\n  they correspond to sentences that were next to each other in the original text, sometimes not. The model then has to\n  predict if the two sentences were following each other or not.\n\nThis way, the model learns an inner representation of the English language that can then be used to extract features\nuseful for downstream tasks: if you have a dataset of labeled sentences for instance, you can train a standard\nclassifier using the features produced by the BERT model as inputs.\n\nThis model has the following configuration:\n\n- 24-layer\n- 1024 hidden dimension\n- 16 attention heads\n- 336M parameters.\n\n## Intended uses & limitations\nThis model should be used as a question-answering model. You may use it in a question answering pipeline, or use it to output raw results given a query and a context. You may see other use cases in the [task summary](https://huggingface.co/transformers/task_summary.html#extractive-question-answering) of the transformers documentation.## Training data\n\nThe BERT model was pretrained on [BookCorpus](https://yknzhu.wixsite.com/mbweb), a dataset consisting of 11,038\nunpublished books and [English Wikipedia](https://en.wikipedia.org/wiki/English_Wikipedia) (excluding lists, tables and\nheaders).\n\n## Training procedure\n\n### Preprocessing\n\nThe texts are lowercased and tokenized using WordPiece and a vocabulary size of 30,000. The inputs of the model are\nthen of the form:\n\n```\n[CLS] Sentence A [SEP] Sentence B [SEP]\n```\n\nWith probability 0.5, sentence A and sentence B correspond to two consecutive sentences in the original corpus and in\nthe other cases, it's another random sentence in the corpus. Note that what is considered a sentence here is a\nconsecutive span of text usually longer than a single sentence. The only constrain is that the result with the two\n\"sentences\" has a combined length of less than 512 tokens.\n\nThe details of the masking procedure for each sentence are the following:\n- 15% of the tokens are masked.\n- In 80% of the cases, the masked tokens are replaced by `[MASK]`.\n- In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace.\n- In the 10% remaining cases, the masked tokens are left as is.\n\n### Pretraining\n\nThe model was trained on 4 cloud TPUs in Pod configuration (16 TPU chips total) for one million steps with a batch size\nof 256. The sequence length was limited to 128 tokens for 90% of the steps and 512 for the remaining 10%. The optimizer\nused is Adam with a learning rate of 1e-4, \\\\(\\beta_{1} = 0.9\\\\) and \\\\(\\beta_{2} = 0.999\\\\), a weight decay of 0.01,\nlearning rate warmup for 10,000 steps and linear decay of the learning rate after.\n\n### Fine-tuning\n\nAfter pre-training, this model was fine-tuned on the SQuAD dataset with one of our fine-tuning scripts. In order to reproduce the training, you may use the following command:\n```\npython -m torch.distributed.launch --nproc_per_node=8 ./examples/question-answering/run_qa.py \\\n    --model_name_or_path bert-large-cased-whole-word-masking \\\n    --dataset_name squad \\\n    --do_train \\\n    --do_eval \\\n    --learning_rate 3e-5 \\\n    --num_train_epochs 2 \\\n    --max_seq_length 384 \\\n    --doc_stride 128 \\\n    --output_dir ./examples/models/wwm_cased_finetuned_squad/ \\\n    --per_device_eval_batch_size=3   \\\n    --per_device_train_batch_size=3   \\\n```\n\n### BibTeX entry and citation info\n\n```bibtex\n@article{DBLP:journals/corr/abs-1810-04805,\n  author    = {Jacob Devlin and\n               Ming{-}Wei Chang and\n               Kenton Lee and\n               Kristina Toutanova},\n  title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language\n               Understanding},\n  journal   = {CoRR},\n  volume    = {abs/1810.04805},\n  year      = {2018},\n  url       = {http://arxiv.org/abs/1810.04805},\n  archivePrefix = {arXiv},\n  eprint    = {1810.04805},\n  timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n```",
    "library_name": "transformers"
  },
  {
    "model_id": "csarron/mobilebert-uncased-squad-v2",
    "model_name": "csarron/mobilebert-uncased-squad-v2",
    "author": "csarron",
    "downloads": 80697,
    "likes": 7,
    "tags": [
      "transformers",
      "pytorch",
      "onnx",
      "safetensors",
      "mobilebert",
      "question-answering",
      "en",
      "dataset:squad_v2",
      "arxiv:2004.02984",
      "license:mit",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/csarron/mobilebert-uncased-squad-v2",
    "dependencies": [
      [
        "transformers",
        "4.36.2"
      ],
      [
        "torch",
        "2.5.1"
      ]
    ],
    "analysis_date": "2025-03-26T00:49:49.572989",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "mobilebert",
    "pipeline_tag": "question-answering",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": "en",
      "license": "mit",
      "tags": [
        "question-answering",
        "mobilebert"
      ],
      "datasets": [
        "squad_v2"
      ],
      "metrics": [
        "squad_v2"
      ],
      "widget": [
        {
          "text": "Which name is also used to describe the Amazon rainforest in English?",
          "context": "The Amazon rainforest (Portuguese: Floresta Amaz\u00f4nica or Amaz\u00f4nia; Spanish: Selva Amaz\u00f3nica, Amazon\u00eda or usually Amazonia; French: For\u00eat amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet's remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species."
        },
        {
          "text": "How many square kilometers of rainforest is covered in the basin?",
          "context": "The Amazon rainforest (Portuguese: Floresta Amaz\u00f4nica or Amaz\u00f4nia; Spanish: Selva Amaz\u00f3nica, Amazon\u00eda or usually Amazonia; French: For\u00eat amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet's remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species."
        }
      ]
    },
    "card_text": "\n## MobileBERT fine-tuned on SQuAD v2\n\n[MobileBERT](https://arxiv.org/abs/2004.02984) is a thin version of BERT_LARGE, while equipped with bottleneck structures and a carefully designed balance\nbetween self-attentions and feed-forward networks.\n\nThis model was fine-tuned from the HuggingFace checkpoint `google/mobilebert-uncased` on [SQuAD2.0](https://rajpurkar.github.io/SQuAD-explorer).\n\n## Details\n\n| Dataset  | Split | # samples |\n| -------- | ----- | --------- |\n| SQuAD2.0 | train | 130k      |\n| SQuAD2.0 | eval  | 12.3k     |\n\n\n### Fine-tuning\n- Python: `3.7.5`\n\n- Machine specs: \n\n  `CPU: Intel(R) Core(TM) i7-6800K CPU @ 3.40GHz`\n  \n  `Memory: 32 GiB`\n\n  `GPUs: 2 GeForce GTX 1070, each with 8GiB memory`\n  \n  `GPU driver: 418.87.01, CUDA: 10.1`\n\n- script:\n\n  ```shell\n  # after install https://github.com/huggingface/transformers\n\n  cd examples/question-answering\n  mkdir -p data\n\n  wget -O data/train-v2.0.json https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\n\n  wget -O data/dev-v2.0.json  https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json\n\n  export SQUAD_DIR=`pwd`/data\n\n  python run_squad.py \\\n    --model_type mobilebert \\\n    --model_name_or_path google/mobilebert-uncased \\\n    --do_train \\\n    --do_eval \\\n    --do_lower_case \\\n    --version_2_with_negative \\\n    --train_file $SQUAD_DIR/train-v2.0.json \\\n    --predict_file $SQUAD_DIR/dev-v2.0.json \\\n    --per_gpu_train_batch_size 16 \\\n    --per_gpu_eval_batch_size 16 \\\n    --learning_rate 4e-5 \\\n    --num_train_epochs 5.0 \\\n    --max_seq_length 320 \\\n    --doc_stride 128 \\\n    --warmup_steps 1400 \\\n    --save_steps 2000 \\\n    --output_dir $SQUAD_DIR/mobilebert-uncased-warmup-squad_v2 2>&1 | tee train-mobilebert-warmup-squad_v2.log\n  ```\n\nIt took about 3.5 hours to finish.\n\n### Results\n\n**Model size**: `95M`\n\n| Metric | # Value   | # Original ([Table 5](https://arxiv.org/pdf/2004.02984.pdf))|\n| ------ | --------- | --------- |\n| **EM** | **75.2** | **76.2** |\n| **F1** | **78.8** | **79.2** |\n\nNote that the above results didn't involve any hyperparameter search.\n\n## Example Usage\n\n\n```python\nfrom transformers import pipeline\n\nqa_pipeline = pipeline(\n    \"question-answering\",\n    model=\"csarron/mobilebert-uncased-squad-v2\",\n    tokenizer=\"csarron/mobilebert-uncased-squad-v2\"\n)\n\npredictions = qa_pipeline({\n    'context': \"The game was played on February 7, 2016 at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California.\",\n    'question': \"What day was the game played on?\"\n})\n\nprint(predictions)\n# output:\n# {'score': 0.71434086561203, 'start': 23, 'end': 39, 'answer': 'February 7, 2016'}\n```\n\n> Created by [Qingqing Cao](https://awk.ai/) | [GitHub](https://github.com/csarron) | [Twitter](https://twitter.com/sysnlp) \n\n> Made with \u2764\ufe0f in New York.",
    "card_content": "---\nlanguage: en\nlicense: mit\ntags:\n- question-answering\n- mobilebert\ndatasets:\n- squad_v2\nmetrics:\n- squad_v2\nwidget:\n- text: Which name is also used to describe the Amazon rainforest in English?\n  context: 'The Amazon rainforest (Portuguese: Floresta Amaz\u00f4nica or Amaz\u00f4nia; Spanish:\n    Selva Amaz\u00f3nica, Amazon\u00eda or usually Amazonia; French: For\u00eat amazonienne; Dutch:\n    Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is\n    a moist broadleaf forest that covers most of the Amazon basin of South America.\n    This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which\n    5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This\n    region includes territory belonging to nine nations. The majority of the forest\n    is contained within Brazil, with 60% of the rainforest, followed by Peru with\n    13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia,\n    Guyana, Suriname and French Guiana. States or departments in four nations contain\n    \"Amazonas\" in their names. The Amazon represents over half of the planet''s remaining\n    rainforests, and comprises the largest and most biodiverse tract of tropical rainforest\n    in the world, with an estimated 390 billion individual trees divided into 16,000\n    species.'\n- text: How many square kilometers of rainforest is covered in the basin?\n  context: 'The Amazon rainforest (Portuguese: Floresta Amaz\u00f4nica or Amaz\u00f4nia; Spanish:\n    Selva Amaz\u00f3nica, Amazon\u00eda or usually Amazonia; French: For\u00eat amazonienne; Dutch:\n    Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is\n    a moist broadleaf forest that covers most of the Amazon basin of South America.\n    This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which\n    5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This\n    region includes territory belonging to nine nations. The majority of the forest\n    is contained within Brazil, with 60% of the rainforest, followed by Peru with\n    13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia,\n    Guyana, Suriname and French Guiana. States or departments in four nations contain\n    \"Amazonas\" in their names. The Amazon represents over half of the planet''s remaining\n    rainforests, and comprises the largest and most biodiverse tract of tropical rainforest\n    in the world, with an estimated 390 billion individual trees divided into 16,000\n    species.'\n---\n\n## MobileBERT fine-tuned on SQuAD v2\n\n[MobileBERT](https://arxiv.org/abs/2004.02984) is a thin version of BERT_LARGE, while equipped with bottleneck structures and a carefully designed balance\nbetween self-attentions and feed-forward networks.\n\nThis model was fine-tuned from the HuggingFace checkpoint `google/mobilebert-uncased` on [SQuAD2.0](https://rajpurkar.github.io/SQuAD-explorer).\n\n## Details\n\n| Dataset  | Split | # samples |\n| -------- | ----- | --------- |\n| SQuAD2.0 | train | 130k      |\n| SQuAD2.0 | eval  | 12.3k     |\n\n\n### Fine-tuning\n- Python: `3.7.5`\n\n- Machine specs: \n\n  `CPU: Intel(R) Core(TM) i7-6800K CPU @ 3.40GHz`\n  \n  `Memory: 32 GiB`\n\n  `GPUs: 2 GeForce GTX 1070, each with 8GiB memory`\n  \n  `GPU driver: 418.87.01, CUDA: 10.1`\n\n- script:\n\n  ```shell\n  # after install https://github.com/huggingface/transformers\n\n  cd examples/question-answering\n  mkdir -p data\n\n  wget -O data/train-v2.0.json https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\n\n  wget -O data/dev-v2.0.json  https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json\n\n  export SQUAD_DIR=`pwd`/data\n\n  python run_squad.py \\\n    --model_type mobilebert \\\n    --model_name_or_path google/mobilebert-uncased \\\n    --do_train \\\n    --do_eval \\\n    --do_lower_case \\\n    --version_2_with_negative \\\n    --train_file $SQUAD_DIR/train-v2.0.json \\\n    --predict_file $SQUAD_DIR/dev-v2.0.json \\\n    --per_gpu_train_batch_size 16 \\\n    --per_gpu_eval_batch_size 16 \\\n    --learning_rate 4e-5 \\\n    --num_train_epochs 5.0 \\\n    --max_seq_length 320 \\\n    --doc_stride 128 \\\n    --warmup_steps 1400 \\\n    --save_steps 2000 \\\n    --output_dir $SQUAD_DIR/mobilebert-uncased-warmup-squad_v2 2>&1 | tee train-mobilebert-warmup-squad_v2.log\n  ```\n\nIt took about 3.5 hours to finish.\n\n### Results\n\n**Model size**: `95M`\n\n| Metric | # Value   | # Original ([Table 5](https://arxiv.org/pdf/2004.02984.pdf))|\n| ------ | --------- | --------- |\n| **EM** | **75.2** | **76.2** |\n| **F1** | **78.8** | **79.2** |\n\nNote that the above results didn't involve any hyperparameter search.\n\n## Example Usage\n\n\n```python\nfrom transformers import pipeline\n\nqa_pipeline = pipeline(\n    \"question-answering\",\n    model=\"csarron/mobilebert-uncased-squad-v2\",\n    tokenizer=\"csarron/mobilebert-uncased-squad-v2\"\n)\n\npredictions = qa_pipeline({\n    'context': \"The game was played on February 7, 2016 at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California.\",\n    'question': \"What day was the game played on?\"\n})\n\nprint(predictions)\n# output:\n# {'score': 0.71434086561203, 'start': 23, 'end': 39, 'answer': 'February 7, 2016'}\n```\n\n> Created by [Qingqing Cao](https://awk.ai/) | [GitHub](https://github.com/csarron) | [Twitter](https://twitter.com/sysnlp) \n\n> Made with \u2764\ufe0f in New York.",
    "library_name": "transformers"
  },
  {
    "model_id": "deepset/bert-base-cased-squad2",
    "model_name": "deepset/bert-base-cased-squad2",
    "author": "deepset",
    "downloads": 65765,
    "likes": 20,
    "tags": [
      "transformers",
      "pytorch",
      "jax",
      "safetensors",
      "bert",
      "question-answering",
      "en",
      "dataset:squad_v2",
      "license:cc-by-4.0",
      "model-index",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/deepset/bert-base-cased-squad2",
    "dependencies": [
      [
        "haystack-ai",
        "2.11.2"
      ],
      [
        "transformers",
        "4.36.2"
      ],
      [
        "torch",
        "2.5.1"
      ]
    ],
    "analysis_date": "2025-03-26T00:49:50.947071",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "bert",
    "pipeline_tag": "question-answering",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": "en",
      "license": "cc-by-4.0",
      "datasets": [
        "squad_v2"
      ],
      "model-index": [
        {
          "name": "deepset/bert-base-cased-squad2",
          "results": [
            {
              "task": {
                "type": "question-answering",
                "name": "Question Answering"
              },
              "dataset": {
                "name": "squad_v2",
                "type": "squad_v2",
                "config": "squad_v2",
                "split": "validation"
              },
              "metrics": [
                {
                  "type": "exact_match",
                  "value": 71.1517,
                  "name": "Exact Match",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZGZlNmQ1YzIzMWUzNTg4YmI4NWVhYThiMzE2ZGZmNWUzNDM3NWI0ZGJkNzliNGUxNTY2MDA5MWVkYjAwYWZiMCIsInZlcnNpb24iOjF9.iUvVdy5c4hoXkwlThJankQqG9QXzNilvfF1_4P0oL8X-jkY5Q6YSsZx6G6cpgXogqFpn7JlE_lP6_OT0VIamCg"
                },
                {
                  "type": "f1",
                  "value": 74.6714,
                  "name": "F1",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMWE5OGNjODhmY2Y0NWIyZDIzMmQ2NmRjZGYyYTYzOWMxZDUzYzg4YjBhNTRiNTY4NTc0M2IxNjI5NWI5ZDM0NCIsInZlcnNpb24iOjF9.IqU9rbzUcKmDEoLkwCUZTKSH0ZFhtqgnhOaEDKKnaRMGBJLj98D5V4VirYT6jLh8FlR0FiwvMTMjReBcfTisAQ"
                }
              ]
            }
          ]
        }
      ]
    },
    "card_text": "\nThis is a BERT base cased model trained on SQuAD v2\n\n## Overview\n**Language model:** bert-base-cased\n**Language:** English  \n**Downstream-task:** Extractive QA  \n**Training data:** SQuAD 2.0  \n**Eval data:** SQuAD 2.0  \n**Code:**  See [an example extractive QA pipeline built with Haystack](https://haystack.deepset.ai/tutorials/34_extractive_qa_pipeline)  \n\n## Usage\n\n### In Haystack\nHaystack is an AI orchestration framework to build customizable, production-ready LLM applications. You can use this model in Haystack to do extractive question answering on documents. \nTo load and run the model with [Haystack](https://github.com/deepset-ai/haystack/):\n```python\n# After running pip install haystack-ai \"transformers[torch,sentencepiece]\"\n\nfrom haystack import Document\nfrom haystack.components.readers import ExtractiveReader\n\ndocs = [\n    Document(content=\"Python is a popular programming language\"),\n    Document(content=\"python ist eine beliebte Programmiersprache\"),\n]\n\nreader = ExtractiveReader(model=\"deepset/bert-base-cased-squad2\")\nreader.warm_up()\n\nquestion = \"What is a popular programming language?\"\nresult = reader.run(query=question, documents=docs)\n# {'answers': [ExtractedAnswer(query='What is a popular programming language?', score=0.5740374326705933, data='python', document=Document(id=..., content: '...'), context=None, document_offset=ExtractedAnswer.Span(start=0, end=6),...)]}\n```\nFor a complete example with an extractive question answering pipeline that scales over many documents, check out the [corresponding Haystack tutorial](https://haystack.deepset.ai/tutorials/34_extractive_qa_pipeline).\n\n### In Transformers\n```python\nfrom transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n\nmodel_name = \"deepset/bert-base-cased-squad2\"\n\n# a) Get predictions\nnlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\nQA_input = {\n    'question': 'Why is model conversion important?',\n    'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\n}\nres = nlp(QA_input)\n\n# b) Load model & tokenizer\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n```\n\n## About us\n\n<div class=\"grid lg:grid-cols-2 gap-x-4 gap-y-3\">\n    <div class=\"w-full h-40 object-cover mb-2 rounded-lg flex items-center justify-center\">\n         <img alt=\"\" src=\"https://raw.githubusercontent.com/deepset-ai/.github/main/deepset-logo-colored.png\" class=\"w-40\"/>\n     </div>\n     <div class=\"w-full h-40 object-cover mb-2 rounded-lg flex items-center justify-center\">\n         <img alt=\"\" src=\"https://raw.githubusercontent.com/deepset-ai/.github/main/haystack-logo-colored.png\" class=\"w-40\"/>\n     </div>\n</div>\n\n[deepset](http://deepset.ai/) is the company behind the production-ready open-source AI framework [Haystack](https://haystack.deepset.ai/).\n\nSome of our other work: \n- [Distilled roberta-base-squad2 (aka \"tinyroberta-squad2\")](https://huggingface.co/deepset/tinyroberta-squad2)\n- [German BERT (aka \"bert-base-german-cased\")](https://deepset.ai/german-bert)\n- [GermanQuAD and GermanDPR datasets and models (aka \"gelectra-base-germanquad\", \"gbert-base-germandpr\")](https://deepset.ai/germanquad)\n\n## Get in touch and join the Haystack community\n\n<p>For more info on Haystack, visit our <strong><a href=\"https://github.com/deepset-ai/haystack\">GitHub</a></strong> repo and <strong><a href=\"https://docs.haystack.deepset.ai\">Documentation</a></strong>. \n\nWe also have a <strong><a class=\"h-7\" href=\"https://haystack.deepset.ai/community\">Discord community open to everyone!</a></strong></p>\n\n[Twitter](https://twitter.com/Haystack_AI) | [LinkedIn](https://www.linkedin.com/company/deepset-ai/) | [Discord](https://haystack.deepset.ai/community) | [GitHub Discussions](https://github.com/deepset-ai/haystack/discussions) | [Website](https://haystack.deepset.ai/) | [YouTube](https://www.youtube.com/@deepset_ai)\n\nBy the way: [we're hiring!](http://www.deepset.ai/jobs)",
    "card_content": "---\nlanguage: en\nlicense: cc-by-4.0\ndatasets:\n- squad_v2\nmodel-index:\n- name: deepset/bert-base-cased-squad2\n  results:\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squad_v2\n      type: squad_v2\n      config: squad_v2\n      split: validation\n    metrics:\n    - type: exact_match\n      value: 71.1517\n      name: Exact Match\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZGZlNmQ1YzIzMWUzNTg4YmI4NWVhYThiMzE2ZGZmNWUzNDM3NWI0ZGJkNzliNGUxNTY2MDA5MWVkYjAwYWZiMCIsInZlcnNpb24iOjF9.iUvVdy5c4hoXkwlThJankQqG9QXzNilvfF1_4P0oL8X-jkY5Q6YSsZx6G6cpgXogqFpn7JlE_lP6_OT0VIamCg\n    - type: f1\n      value: 74.6714\n      name: F1\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMWE5OGNjODhmY2Y0NWIyZDIzMmQ2NmRjZGYyYTYzOWMxZDUzYzg4YjBhNTRiNTY4NTc0M2IxNjI5NWI5ZDM0NCIsInZlcnNpb24iOjF9.IqU9rbzUcKmDEoLkwCUZTKSH0ZFhtqgnhOaEDKKnaRMGBJLj98D5V4VirYT6jLh8FlR0FiwvMTMjReBcfTisAQ\n---\n\nThis is a BERT base cased model trained on SQuAD v2\n\n## Overview\n**Language model:** bert-base-cased\n**Language:** English  \n**Downstream-task:** Extractive QA  \n**Training data:** SQuAD 2.0  \n**Eval data:** SQuAD 2.0  \n**Code:**  See [an example extractive QA pipeline built with Haystack](https://haystack.deepset.ai/tutorials/34_extractive_qa_pipeline)  \n\n## Usage\n\n### In Haystack\nHaystack is an AI orchestration framework to build customizable, production-ready LLM applications. You can use this model in Haystack to do extractive question answering on documents. \nTo load and run the model with [Haystack](https://github.com/deepset-ai/haystack/):\n```python\n# After running pip install haystack-ai \"transformers[torch,sentencepiece]\"\n\nfrom haystack import Document\nfrom haystack.components.readers import ExtractiveReader\n\ndocs = [\n    Document(content=\"Python is a popular programming language\"),\n    Document(content=\"python ist eine beliebte Programmiersprache\"),\n]\n\nreader = ExtractiveReader(model=\"deepset/bert-base-cased-squad2\")\nreader.warm_up()\n\nquestion = \"What is a popular programming language?\"\nresult = reader.run(query=question, documents=docs)\n# {'answers': [ExtractedAnswer(query='What is a popular programming language?', score=0.5740374326705933, data='python', document=Document(id=..., content: '...'), context=None, document_offset=ExtractedAnswer.Span(start=0, end=6),...)]}\n```\nFor a complete example with an extractive question answering pipeline that scales over many documents, check out the [corresponding Haystack tutorial](https://haystack.deepset.ai/tutorials/34_extractive_qa_pipeline).\n\n### In Transformers\n```python\nfrom transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n\nmodel_name = \"deepset/bert-base-cased-squad2\"\n\n# a) Get predictions\nnlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\nQA_input = {\n    'question': 'Why is model conversion important?',\n    'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\n}\nres = nlp(QA_input)\n\n# b) Load model & tokenizer\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n```\n\n## About us\n\n<div class=\"grid lg:grid-cols-2 gap-x-4 gap-y-3\">\n    <div class=\"w-full h-40 object-cover mb-2 rounded-lg flex items-center justify-center\">\n         <img alt=\"\" src=\"https://raw.githubusercontent.com/deepset-ai/.github/main/deepset-logo-colored.png\" class=\"w-40\"/>\n     </div>\n     <div class=\"w-full h-40 object-cover mb-2 rounded-lg flex items-center justify-center\">\n         <img alt=\"\" src=\"https://raw.githubusercontent.com/deepset-ai/.github/main/haystack-logo-colored.png\" class=\"w-40\"/>\n     </div>\n</div>\n\n[deepset](http://deepset.ai/) is the company behind the production-ready open-source AI framework [Haystack](https://haystack.deepset.ai/).\n\nSome of our other work: \n- [Distilled roberta-base-squad2 (aka \"tinyroberta-squad2\")](https://huggingface.co/deepset/tinyroberta-squad2)\n- [German BERT (aka \"bert-base-german-cased\")](https://deepset.ai/german-bert)\n- [GermanQuAD and GermanDPR datasets and models (aka \"gelectra-base-germanquad\", \"gbert-base-germandpr\")](https://deepset.ai/germanquad)\n\n## Get in touch and join the Haystack community\n\n<p>For more info on Haystack, visit our <strong><a href=\"https://github.com/deepset-ai/haystack\">GitHub</a></strong> repo and <strong><a href=\"https://docs.haystack.deepset.ai\">Documentation</a></strong>. \n\nWe also have a <strong><a class=\"h-7\" href=\"https://haystack.deepset.ai/community\">Discord community open to everyone!</a></strong></p>\n\n[Twitter](https://twitter.com/Haystack_AI) | [LinkedIn](https://www.linkedin.com/company/deepset-ai/) | [Discord](https://haystack.deepset.ai/community) | [GitHub Discussions](https://github.com/deepset-ai/haystack/discussions) | [Website](https://haystack.deepset.ai/) | [YouTube](https://www.youtube.com/@deepset_ai)\n\nBy the way: [we're hiring!](http://www.deepset.ai/jobs)",
    "library_name": "transformers"
  },
  {
    "model_id": "deepset/roberta-large-squad2",
    "model_name": "deepset/roberta-large-squad2",
    "author": "deepset",
    "downloads": 41397,
    "likes": 27,
    "tags": [
      "transformers",
      "pytorch",
      "jax",
      "safetensors",
      "roberta",
      "question-answering",
      "en",
      "dataset:squad_v2",
      "base_model:FacebookAI/roberta-large",
      "base_model:finetune:FacebookAI/roberta-large",
      "license:cc-by-4.0",
      "model-index",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/deepset/roberta-large-squad2",
    "dependencies": [
      [
        "transformers",
        "4.36.2"
      ],
      [
        "torch",
        "2.5.1"
      ]
    ],
    "analysis_date": "2025-03-26T00:49:52.499660",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "roberta",
    "pipeline_tag": "question-answering",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": "en",
      "license": "cc-by-4.0",
      "datasets": [
        "squad_v2"
      ],
      "base_model": "roberta-large",
      "model-index": [
        {
          "name": "deepset/roberta-large-squad2",
          "results": [
            {
              "task": {
                "type": "question-answering",
                "name": "Question Answering"
              },
              "dataset": {
                "name": "squad_v2",
                "type": "squad_v2",
                "config": "squad_v2",
                "split": "validation"
              },
              "metrics": [
                {
                  "type": "exact_match",
                  "value": 85.168,
                  "name": "Exact Match"
                },
                {
                  "type": "f1",
                  "value": 88.349,
                  "name": "F1"
                }
              ]
            },
            {
              "task": {
                "type": "question-answering",
                "name": "Question Answering"
              },
              "dataset": {
                "name": "squad",
                "type": "squad",
                "config": "plain_text",
                "split": "validation"
              },
              "metrics": [
                {
                  "type": "exact_match",
                  "value": 87.162,
                  "name": "Exact Match"
                },
                {
                  "type": "f1",
                  "value": 93.603,
                  "name": "F1"
                }
              ]
            },
            {
              "task": {
                "type": "question-answering",
                "name": "Question Answering"
              },
              "dataset": {
                "name": "adversarial_qa",
                "type": "adversarial_qa",
                "config": "adversarialQA",
                "split": "validation"
              },
              "metrics": [
                {
                  "type": "exact_match",
                  "value": 35.9,
                  "name": "Exact Match"
                },
                {
                  "type": "f1",
                  "value": 48.923,
                  "name": "F1"
                }
              ]
            },
            {
              "task": {
                "type": "question-answering",
                "name": "Question Answering"
              },
              "dataset": {
                "name": "squad_adversarial",
                "type": "squad_adversarial",
                "config": "AddOneSent",
                "split": "validation"
              },
              "metrics": [
                {
                  "type": "exact_match",
                  "value": 81.142,
                  "name": "Exact Match"
                },
                {
                  "type": "f1",
                  "value": 87.099,
                  "name": "F1"
                }
              ]
            },
            {
              "task": {
                "type": "question-answering",
                "name": "Question Answering"
              },
              "dataset": {
                "name": "squadshifts amazon",
                "type": "squadshifts",
                "config": "amazon",
                "split": "test"
              },
              "metrics": [
                {
                  "type": "exact_match",
                  "value": 72.453,
                  "name": "Exact Match"
                },
                {
                  "type": "f1",
                  "value": 86.325,
                  "name": "F1"
                }
              ]
            },
            {
              "task": {
                "type": "question-answering",
                "name": "Question Answering"
              },
              "dataset": {
                "name": "squadshifts new_wiki",
                "type": "squadshifts",
                "config": "new_wiki",
                "split": "test"
              },
              "metrics": [
                {
                  "type": "exact_match",
                  "value": 82.338,
                  "name": "Exact Match"
                },
                {
                  "type": "f1",
                  "value": 91.974,
                  "name": "F1"
                }
              ]
            },
            {
              "task": {
                "type": "question-answering",
                "name": "Question Answering"
              },
              "dataset": {
                "name": "squadshifts nyt",
                "type": "squadshifts",
                "config": "nyt",
                "split": "test"
              },
              "metrics": [
                {
                  "type": "exact_match",
                  "value": 84.352,
                  "name": "Exact Match"
                },
                {
                  "type": "f1",
                  "value": 92.645,
                  "name": "F1"
                }
              ]
            },
            {
              "task": {
                "type": "question-answering",
                "name": "Question Answering"
              },
              "dataset": {
                "name": "squadshifts reddit",
                "type": "squadshifts",
                "config": "reddit",
                "split": "test"
              },
              "metrics": [
                {
                  "type": "exact_match",
                  "value": 74.722,
                  "name": "Exact Match"
                },
                {
                  "type": "f1",
                  "value": 86.86,
                  "name": "F1"
                }
              ]
            }
          ]
        }
      ]
    },
    "card_text": "\n# roberta-large for Extractive QA \n\nThis is the [roberta-large](https://huggingface.co/roberta-large) model, fine-tuned using the [SQuAD2.0](https://huggingface.co/datasets/squad_v2) dataset. It's been trained on question-answer pairs, including unanswerable questions, for the task of Question Answering. \n\n\n## Overview\n**Language model:** roberta-large  \n**Language:** English  \n**Downstream-task:** Extractive QA  \n**Training data:** SQuAD 2.0  \n**Eval data:** SQuAD 2.0  \n**Code:**  See [an example extractive QA pipeline built with Haystack](https://haystack.deepset.ai/tutorials/34_extractive_qa_pipeline)  \n**Infrastructure**: 4x Tesla v100\n\n## Hyperparameters\n\n```\nbase_LM_model = \"roberta-large\"\n``` \n\n## Using a distilled model instead\nPlease note that we have also released a distilled version of this model called [deepset/roberta-base-squad2-distilled](https://huggingface.co/deepset/roberta-base-squad2-distilled). The distilled model has a comparable prediction quality and runs at twice the speed of the large model.\n\n## Usage\n\n### In Haystack\nHaystack is an AI orchestration framework to build customizable, production-ready LLM applications. You can use this model in Haystack to do extractive question answering on documents. \nTo load and run the model with [Haystack](https://github.com/deepset-ai/haystack/):\n```python\n# After running pip install haystack-ai \"transformers[torch,sentencepiece]\"\n\nfrom haystack import Document\nfrom haystack.components.readers import ExtractiveReader\n\ndocs = [\n    Document(content=\"Python is a popular programming language\"),\n    Document(content=\"python ist eine beliebte Programmiersprache\"),\n]\n\nreader = ExtractiveReader(model=\"deepset/roberta-large-squad2\")\nreader.warm_up()\n\nquestion = \"What is a popular programming language?\"\nresult = reader.run(query=question, documents=docs)\n# {'answers': [ExtractedAnswer(query='What is a popular programming language?', score=0.5740374326705933, data='python', document=Document(id=..., content: '...'), context=None, document_offset=ExtractedAnswer.Span(start=0, end=6),...)]}\n```\nFor a complete example with an extractive question answering pipeline that scales over many documents, check out the [corresponding Haystack tutorial](https://haystack.deepset.ai/tutorials/34_extractive_qa_pipeline).\n\n### In Transformers\n```python\nfrom transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n\nmodel_name = \"deepset/roberta-large-squad2\"\n\n# a) Get predictions\nnlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\nQA_input = {\n    'question': 'Why is model conversion important?',\n    'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\n}\nres = nlp(QA_input)\n\n# b) Load model & tokenizer\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n```\n\n## Authors\n**Branden Chan:** branden.chan@deepset.ai  \n**Timo M\u00f6ller:** timo.moeller@deepset.ai  \n**Malte Pietsch:** malte.pietsch@deepset.ai  \n**Tanay Soni:**  tanay.soni@deepset.ai \n\n## About us\n\n<div class=\"grid lg:grid-cols-2 gap-x-4 gap-y-3\">\n    <div class=\"w-full h-40 object-cover mb-2 rounded-lg flex items-center justify-center\">\n         <img alt=\"\" src=\"https://raw.githubusercontent.com/deepset-ai/.github/main/deepset-logo-colored.png\" class=\"w-40\"/>\n     </div>\n     <div class=\"w-full h-40 object-cover mb-2 rounded-lg flex items-center justify-center\">\n         <img alt=\"\" src=\"https://raw.githubusercontent.com/deepset-ai/.github/main/haystack-logo-colored.png\" class=\"w-40\"/>\n     </div>\n</div>\n\n[deepset](http://deepset.ai/) is the company behind the production-ready open-source AI framework [Haystack](https://haystack.deepset.ai/).\n\nSome of our other work: \n- [Distilled roberta-base-squad2 (aka \"tinyroberta-squad2\")](https://huggingface.co/deepset/tinyroberta-squad2)\n- [German BERT](https://deepset.ai/german-bert), [GermanQuAD and GermanDPR](https://deepset.ai/germanquad), [German embedding model](https://huggingface.co/mixedbread-ai/deepset-mxbai-embed-de-large-v1)\n- [deepset Cloud](https://www.deepset.ai/deepset-cloud-product), [deepset Studio](https://www.deepset.ai/deepset-studio)\n\n## Get in touch and join the Haystack community\n\n<p>For more info on Haystack, visit our <strong><a href=\"https://github.com/deepset-ai/haystack\">GitHub</a></strong> repo and <strong><a href=\"https://docs.haystack.deepset.ai\">Documentation</a></strong>. \n\nWe also have a <strong><a class=\"h-7\" href=\"https://haystack.deepset.ai/community\">Discord community open to everyone!</a></strong></p>\n\n[Twitter](https://twitter.com/Haystack_AI) | [LinkedIn](https://www.linkedin.com/company/deepset-ai/) | [Discord](https://haystack.deepset.ai/community) | [GitHub Discussions](https://github.com/deepset-ai/haystack/discussions) | [Website](https://haystack.deepset.ai/) | [YouTube](https://www.youtube.com/@deepset_ai)\n\nBy the way: [we're hiring!](http://www.deepset.ai/jobs)",
    "card_content": "---\nlanguage: en\nlicense: cc-by-4.0\ndatasets:\n- squad_v2\nbase_model: roberta-large\nmodel-index:\n- name: deepset/roberta-large-squad2\n  results:\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squad_v2\n      type: squad_v2\n      config: squad_v2\n      split: validation\n    metrics:\n    - type: exact_match\n      value: 85.168\n      name: Exact Match\n    - type: f1\n      value: 88.349\n      name: F1\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squad\n      type: squad\n      config: plain_text\n      split: validation\n    metrics:\n    - type: exact_match\n      value: 87.162\n      name: Exact Match\n    - type: f1\n      value: 93.603\n      name: F1\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: adversarial_qa\n      type: adversarial_qa\n      config: adversarialQA\n      split: validation\n    metrics:\n    - type: exact_match\n      value: 35.9\n      name: Exact Match\n    - type: f1\n      value: 48.923\n      name: F1\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squad_adversarial\n      type: squad_adversarial\n      config: AddOneSent\n      split: validation\n    metrics:\n    - type: exact_match\n      value: 81.142\n      name: Exact Match\n    - type: f1\n      value: 87.099\n      name: F1\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squadshifts amazon\n      type: squadshifts\n      config: amazon\n      split: test\n    metrics:\n    - type: exact_match\n      value: 72.453\n      name: Exact Match\n    - type: f1\n      value: 86.325\n      name: F1\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squadshifts new_wiki\n      type: squadshifts\n      config: new_wiki\n      split: test\n    metrics:\n    - type: exact_match\n      value: 82.338\n      name: Exact Match\n    - type: f1\n      value: 91.974\n      name: F1\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squadshifts nyt\n      type: squadshifts\n      config: nyt\n      split: test\n    metrics:\n    - type: exact_match\n      value: 84.352\n      name: Exact Match\n    - type: f1\n      value: 92.645\n      name: F1\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squadshifts reddit\n      type: squadshifts\n      config: reddit\n      split: test\n    metrics:\n    - type: exact_match\n      value: 74.722\n      name: Exact Match\n    - type: f1\n      value: 86.86\n      name: F1\n---\n\n# roberta-large for Extractive QA \n\nThis is the [roberta-large](https://huggingface.co/roberta-large) model, fine-tuned using the [SQuAD2.0](https://huggingface.co/datasets/squad_v2) dataset. It's been trained on question-answer pairs, including unanswerable questions, for the task of Question Answering. \n\n\n## Overview\n**Language model:** roberta-large  \n**Language:** English  \n**Downstream-task:** Extractive QA  \n**Training data:** SQuAD 2.0  \n**Eval data:** SQuAD 2.0  \n**Code:**  See [an example extractive QA pipeline built with Haystack](https://haystack.deepset.ai/tutorials/34_extractive_qa_pipeline)  \n**Infrastructure**: 4x Tesla v100\n\n## Hyperparameters\n\n```\nbase_LM_model = \"roberta-large\"\n``` \n\n## Using a distilled model instead\nPlease note that we have also released a distilled version of this model called [deepset/roberta-base-squad2-distilled](https://huggingface.co/deepset/roberta-base-squad2-distilled). The distilled model has a comparable prediction quality and runs at twice the speed of the large model.\n\n## Usage\n\n### In Haystack\nHaystack is an AI orchestration framework to build customizable, production-ready LLM applications. You can use this model in Haystack to do extractive question answering on documents. \nTo load and run the model with [Haystack](https://github.com/deepset-ai/haystack/):\n```python\n# After running pip install haystack-ai \"transformers[torch,sentencepiece]\"\n\nfrom haystack import Document\nfrom haystack.components.readers import ExtractiveReader\n\ndocs = [\n    Document(content=\"Python is a popular programming language\"),\n    Document(content=\"python ist eine beliebte Programmiersprache\"),\n]\n\nreader = ExtractiveReader(model=\"deepset/roberta-large-squad2\")\nreader.warm_up()\n\nquestion = \"What is a popular programming language?\"\nresult = reader.run(query=question, documents=docs)\n# {'answers': [ExtractedAnswer(query='What is a popular programming language?', score=0.5740374326705933, data='python', document=Document(id=..., content: '...'), context=None, document_offset=ExtractedAnswer.Span(start=0, end=6),...)]}\n```\nFor a complete example with an extractive question answering pipeline that scales over many documents, check out the [corresponding Haystack tutorial](https://haystack.deepset.ai/tutorials/34_extractive_qa_pipeline).\n\n### In Transformers\n```python\nfrom transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n\nmodel_name = \"deepset/roberta-large-squad2\"\n\n# a) Get predictions\nnlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\nQA_input = {\n    'question': 'Why is model conversion important?',\n    'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\n}\nres = nlp(QA_input)\n\n# b) Load model & tokenizer\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n```\n\n## Authors\n**Branden Chan:** branden.chan@deepset.ai  \n**Timo M\u00f6ller:** timo.moeller@deepset.ai  \n**Malte Pietsch:** malte.pietsch@deepset.ai  \n**Tanay Soni:**  tanay.soni@deepset.ai \n\n## About us\n\n<div class=\"grid lg:grid-cols-2 gap-x-4 gap-y-3\">\n    <div class=\"w-full h-40 object-cover mb-2 rounded-lg flex items-center justify-center\">\n         <img alt=\"\" src=\"https://raw.githubusercontent.com/deepset-ai/.github/main/deepset-logo-colored.png\" class=\"w-40\"/>\n     </div>\n     <div class=\"w-full h-40 object-cover mb-2 rounded-lg flex items-center justify-center\">\n         <img alt=\"\" src=\"https://raw.githubusercontent.com/deepset-ai/.github/main/haystack-logo-colored.png\" class=\"w-40\"/>\n     </div>\n</div>\n\n[deepset](http://deepset.ai/) is the company behind the production-ready open-source AI framework [Haystack](https://haystack.deepset.ai/).\n\nSome of our other work: \n- [Distilled roberta-base-squad2 (aka \"tinyroberta-squad2\")](https://huggingface.co/deepset/tinyroberta-squad2)\n- [German BERT](https://deepset.ai/german-bert), [GermanQuAD and GermanDPR](https://deepset.ai/germanquad), [German embedding model](https://huggingface.co/mixedbread-ai/deepset-mxbai-embed-de-large-v1)\n- [deepset Cloud](https://www.deepset.ai/deepset-cloud-product), [deepset Studio](https://www.deepset.ai/deepset-studio)\n\n## Get in touch and join the Haystack community\n\n<p>For more info on Haystack, visit our <strong><a href=\"https://github.com/deepset-ai/haystack\">GitHub</a></strong> repo and <strong><a href=\"https://docs.haystack.deepset.ai\">Documentation</a></strong>. \n\nWe also have a <strong><a class=\"h-7\" href=\"https://haystack.deepset.ai/community\">Discord community open to everyone!</a></strong></p>\n\n[Twitter](https://twitter.com/Haystack_AI) | [LinkedIn](https://www.linkedin.com/company/deepset-ai/) | [Discord](https://haystack.deepset.ai/community) | [GitHub Discussions](https://github.com/deepset-ai/haystack/discussions) | [Website](https://haystack.deepset.ai/) | [YouTube](https://www.youtube.com/@deepset_ai)\n\nBy the way: [we're hiring!](http://www.deepset.ai/jobs)",
    "library_name": "transformers"
  },
  {
    "model_id": "Rifky/Indobert-QA",
    "model_name": "Rifky/Indobert-QA",
    "author": "Rifky",
    "downloads": 40547,
    "likes": 14,
    "tags": [
      "transformers",
      "pytorch",
      "safetensors",
      "bert",
      "question-answering",
      "id",
      "dataset:rajpurkar/squad_v2",
      "base_model:indolem/indobert-base-uncased",
      "base_model:finetune:indolem/indobert-base-uncased",
      "license:apache-2.0",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/Rifky/Indobert-QA",
    "dependencies": [
      [
        "transformers",
        "4.36.2"
      ],
      [
        "torch",
        "2.5.1"
      ]
    ],
    "analysis_date": "2025-03-26T00:49:53.613792",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "bert",
    "pipeline_tag": "question-answering",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": [
        "id"
      ],
      "license": "apache-2.0",
      "datasets": [
        "rajpurkar/squad_v2"
      ],
      "metrics": [
        "f1"
      ],
      "base_model": [
        "indolem/indobert-base-uncased"
      ]
    },
    "card_text": "[Github](https://github.com/rifkybujana/IndoBERT-QA)\n\n*Notice of Attribution Clarification*\n\nThis is to clarify that *Muhammad Fajrin Buyang Daffa* is not, and has never been, a part of this project. They have made no contributions to this repository, and as such, shall not be given any attribution in relation to this work.\n\nFor further inquiries, please contact the rifky@genta.tech.\n\n> This project is part of my research entitled \"Teman Belajar : Asisten Digital Pelajar SMA Negeri 28 Jakarta dalam Membaca\" for KOPSI (Kompetisi Penelitian Siswa Indonesia/Indonesian Student Research Competition).\n\n## indoBERT Base-Uncased fine-tuned on Translated Squad v2.0\n[IndoBERT](https://huggingface.co/indolem/indobert-base-uncased) trained by [IndoLEM](https://indolem.github.io/) and fine-tuned on [Translated SQuAD 2.0](https://github.com/Wikidepia/indonesian_datasets/tree/master/question-answering/squad) for **Q&A** downstream task.\n\n**Model Size** (after training): 420mb\n\n## Details of indoBERT (from their documentation)\n[IndoBERT](https://huggingface.co/indolem/indobert-base-uncased) is the Indonesian version of BERT model. We train the model using over 220M words, aggregated from three main sources:\n\n- Indonesian Wikipedia (74M words)\n- news articles from Kompas, Tempo (Tala et al., 2003), and Liputan6 (55M words in total)\n- an Indonesian Web Corpus (Medved and Suchomel, 2017) (90M words).\n\nWe trained the model for 2.4M steps (180 epochs) with the final perplexity over the development set being 3.97 (similar to English BERT-base).\n\nThis IndoBERT was used to examine IndoLEM - an Indonesian benchmark that comprises of seven tasks for the Indonesian language, spanning morpho-syntax, semantics, and discourse.[[1]](#1)\n\n## Details of the downstream task (Q&A) - Dataset\nSQuAD2.0 combines the 100,000 questions in SQuAD1.1 with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuAD2.0, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering.\n\n| Dataset  | Split | # samples |\n| -------- | ----- | --------- |\n| SQuAD2.0 | train | 130k      |\n| SQuAD2.0 | eval  | 12.3k     |\n\n## Model Training\nThe model was trained on a Tesla T4 GPU and 12GB of RAM.\n\n## Results:\n| Metric | # Value   |\n| ------ | --------- |\n| **EM** | **51.61** |\n| **F1** | **69.09** |\n\n## Simple Usage\n```py\nfrom transformers import pipeline\n\nqa_pipeline = pipeline(\n    \"question-answering\",\n    model=\"Rifky/Indobert-QA\",\n    tokenizer=\"Rifky/Indobert-QA\"\n)\n\nqa_pipeline({\n    'context': \"\"\"Pangeran Harya Dipanegara (atau biasa dikenal dengan nama Pangeran Diponegoro, lahir di Ngayogyakarta Hadiningrat, 11 November 1785 \u2013 meninggal di Makassar, Hindia Belanda, 8 Januari 1855 pada umur 69 tahun) adalah salah seorang pahlawan nasional Republik Indonesia, yang memimpin Perang Diponegoro atau Perang Jawa selama periode tahun 1825 hingga 1830 melawan pemerintah Hindia Belanda. Sejarah mencatat, Perang Diponegoro atau Perang Jawa dikenal sebagai perang yang menelan korban terbanyak dalam sejarah Indonesia, yakni 8.000 korban serdadu Hindia Belanda, 7.000 pribumi, dan 200 ribu orang Jawa serta kerugian materi 25 juta Gulden.\"\"\",\n    'question': \"kapan pangeran diponegoro lahir?\"\n})\n```\n*output:*\n```py\n{\n  'answer': '11 November 1785',\n  'end': 131,\n  'score': 0.9272009134292603,\n  'start': 115\n}\n```\n\n### Reference\n<a id=\"1\">[1]</a>Fajri Koto and Afshin Rahimi and Jey Han Lau and Timothy Baldwin. 2020. IndoLEM and IndoBERT: A Benchmark Dataset and Pre-trained Language Model for Indonesian NLP. Proceedings of the 28th COLING.",
    "card_content": "---\nlanguage:\n- id\nlicense: apache-2.0\ndatasets:\n- rajpurkar/squad_v2\nmetrics:\n- f1\nbase_model:\n- indolem/indobert-base-uncased\n---\n[Github](https://github.com/rifkybujana/IndoBERT-QA)\n\n*Notice of Attribution Clarification*\n\nThis is to clarify that *Muhammad Fajrin Buyang Daffa* is not, and has never been, a part of this project. They have made no contributions to this repository, and as such, shall not be given any attribution in relation to this work.\n\nFor further inquiries, please contact the rifky@genta.tech.\n\n> This project is part of my research entitled \"Teman Belajar : Asisten Digital Pelajar SMA Negeri 28 Jakarta dalam Membaca\" for KOPSI (Kompetisi Penelitian Siswa Indonesia/Indonesian Student Research Competition).\n\n## indoBERT Base-Uncased fine-tuned on Translated Squad v2.0\n[IndoBERT](https://huggingface.co/indolem/indobert-base-uncased) trained by [IndoLEM](https://indolem.github.io/) and fine-tuned on [Translated SQuAD 2.0](https://github.com/Wikidepia/indonesian_datasets/tree/master/question-answering/squad) for **Q&A** downstream task.\n\n**Model Size** (after training): 420mb\n\n## Details of indoBERT (from their documentation)\n[IndoBERT](https://huggingface.co/indolem/indobert-base-uncased) is the Indonesian version of BERT model. We train the model using over 220M words, aggregated from three main sources:\n\n- Indonesian Wikipedia (74M words)\n- news articles from Kompas, Tempo (Tala et al., 2003), and Liputan6 (55M words in total)\n- an Indonesian Web Corpus (Medved and Suchomel, 2017) (90M words).\n\nWe trained the model for 2.4M steps (180 epochs) with the final perplexity over the development set being 3.97 (similar to English BERT-base).\n\nThis IndoBERT was used to examine IndoLEM - an Indonesian benchmark that comprises of seven tasks for the Indonesian language, spanning morpho-syntax, semantics, and discourse.[[1]](#1)\n\n## Details of the downstream task (Q&A) - Dataset\nSQuAD2.0 combines the 100,000 questions in SQuAD1.1 with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuAD2.0, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering.\n\n| Dataset  | Split | # samples |\n| -------- | ----- | --------- |\n| SQuAD2.0 | train | 130k      |\n| SQuAD2.0 | eval  | 12.3k     |\n\n## Model Training\nThe model was trained on a Tesla T4 GPU and 12GB of RAM.\n\n## Results:\n| Metric | # Value   |\n| ------ | --------- |\n| **EM** | **51.61** |\n| **F1** | **69.09** |\n\n## Simple Usage\n```py\nfrom transformers import pipeline\n\nqa_pipeline = pipeline(\n    \"question-answering\",\n    model=\"Rifky/Indobert-QA\",\n    tokenizer=\"Rifky/Indobert-QA\"\n)\n\nqa_pipeline({\n    'context': \"\"\"Pangeran Harya Dipanegara (atau biasa dikenal dengan nama Pangeran Diponegoro, lahir di Ngayogyakarta Hadiningrat, 11 November 1785 \u2013 meninggal di Makassar, Hindia Belanda, 8 Januari 1855 pada umur 69 tahun) adalah salah seorang pahlawan nasional Republik Indonesia, yang memimpin Perang Diponegoro atau Perang Jawa selama periode tahun 1825 hingga 1830 melawan pemerintah Hindia Belanda. Sejarah mencatat, Perang Diponegoro atau Perang Jawa dikenal sebagai perang yang menelan korban terbanyak dalam sejarah Indonesia, yakni 8.000 korban serdadu Hindia Belanda, 7.000 pribumi, dan 200 ribu orang Jawa serta kerugian materi 25 juta Gulden.\"\"\",\n    'question': \"kapan pangeran diponegoro lahir?\"\n})\n```\n*output:*\n```py\n{\n  'answer': '11 November 1785',\n  'end': 131,\n  'score': 0.9272009134292603,\n  'start': 115\n}\n```\n\n### Reference\n<a id=\"1\">[1]</a>Fajri Koto and Afshin Rahimi and Jey Han Lau and Timothy Baldwin. 2020. IndoLEM and IndoBERT: A Benchmark Dataset and Pre-trained Language Model for Indonesian NLP. Proceedings of the 28th COLING.",
    "library_name": "transformers"
  },
  {
    "model_id": "SmallDoge/Doge-160M-Instruct",
    "model_name": "SmallDoge/Doge-160M-Instruct",
    "author": "SmallDoge",
    "downloads": 40006,
    "likes": 11,
    "tags": [
      "transformers",
      "safetensors",
      "doge",
      "text-generation",
      "trl",
      "sft",
      "dpo",
      "question-answering",
      "custom_code",
      "en",
      "dataset:HuggingFaceTB/smoltalk",
      "dataset:HuggingFaceH4/ultrafeedback_binarized",
      "arxiv:2412.11834",
      "base_model:SmallDoge/Doge-160M",
      "base_model:finetune:SmallDoge/Doge-160M",
      "license:apache-2.0",
      "autotrain_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/SmallDoge/Doge-160M-Instruct",
    "dependencies": [
      [
        "transformers",
        "4.36.2"
      ],
      [
        "torch",
        "2.2.2"
      ],
      [
        "numpy",
        "1.26.3"
      ]
    ],
    "analysis_date": "2025-03-26T00:49:55.053561",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "doge",
    "pipeline_tag": "question-answering",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": [
        "en"
      ],
      "license": "apache-2.0",
      "library_name": "transformers",
      "tags": [
        "trl",
        "sft",
        "dpo",
        "doge"
      ],
      "datasets": [
        "HuggingFaceTB/smoltalk",
        "HuggingFaceH4/ultrafeedback_binarized"
      ],
      "base_model": [
        "SmallDoge/Doge-160M"
      ],
      "pipeline_tag": "question-answering"
    },
    "card_text": "\n\n# **Doge 160M Instruct**\n\n<div align=\"center\">\n  <img src=\"https://huggingface.co/spaces/SmallDoge/README/resolve/main/org_icon.png\" width=\"100%\" alt=\"SmallDoge\" />\n</div>\n<hr>\n<div align=\"center\">\n  <a href=\"https://discord.gg/P2yYH95N\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-Small%20Doges-7289da?logo=discord&logoColor=white&color=7289da\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://arxiv.org/abs/2412.11834\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"arXiv\" src=\"https://img.shields.io/static/v1?label=arXiv&message=2412.11834&color=B31B1B&logo=arXiv\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://github.com/SmallDoges/small-doge\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"GitHub\" src=\"https://img.shields.io/badge/GitHub-SmallDoge-181717?logo=github\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://github.com/SmallDoges/small-doge/blob/main/LICENSE\" style=\"margin: 2px;\">\n    <img alt=\"License\" src=\"https://img.shields.io/badge/License-Apache--2.0-blue.svg\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\nDoge uses Dynamic Mask Attention as sequence transformation and can use Multi-Layer Perceptron or Cross Domain Mixture of Experts as state transformation. Dynamic Mask Attention allows the Transformer to use self-attention during training and state space during inference, and Cross Domain Mixture of Experts can directly inherit the weights of Multi-Layer Perceptron for further training. This model is trained by [SmallDoge](https://huggingface.co/SmallDoge) community, for detailed algorithm and model architecture, please refer to [Wonderful Matrices](https://arxiv.org/abs/2412.11834), all training details and code are publicly available on the [small-doge](https://github.com/SmallDoges/small-doge) repository.\n\n\n## Uses\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig, TextStreamer\n\ntokenizer = AutoTokenizer.from_pretrained(\"SmallDoge/Doge-160M-Instruct\")\nmodel = AutoModelForCausalLM.from_pretrained(\"SmallDoge/Doge-160M-Instruct\", trust_remote_code=True)\n\ngeneration_config = GenerationConfig(\n      max_new_tokens=100, \n      use_cache=True, \n      do_sample=True, \n      temperature=0.8, \n      top_p=0.9,\n      repetition_penalty=1.0\n)\nsteamer = TextStreamer(\n      tokenizer=tokenizer, \n      skip_prompt=True\n)\n\nprompt = \"Hi, how are you doing today?\"\nconversation = [\n      {\"role\": \"user\", \"content\": prompt}\n]\ninputs = tokenizer.apply_chat_template(\n    conversation=conversation,\n    tokenize=True,\n    return_tensors=\"pt\",\n)\n\noutputs = model.generate(\n    inputs, \n    tokenizer=tokenizer,\n    generation_config=generation_config, \n    streamer=steamer\n)\n```\n\n\n## Model Details\n\nWe build the Doge-Instruct by first SFT on [SmolTalk](https://huggingface.co/datasets/HuggingFaceTB/smoltalk) and then DPO on [UltraFeedback Binarized](https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized).\n\n> TODO: The larger model is under training and will be uploaded soon.\n\n**SFT**:\n| Model | Training Data | Epochs | Content Length | LR | Batch Size | Precision |\n|---|---|---|---|---|---|---|\n| [Doge-20M-Instruct-SFT](https://huggingface.co/SmallDoge/Doge-20M-Instruct-SFT) | [HuggingFaceTB/smoltalk](https://huggingface.co/datasets/HuggingFaceTB/smoltalk) | 2 | 2048 | 8e-4 | 0.25M | bfloat16 |\n| [Doge-60M-Instruct-SFT](https://huggingface.co/SmallDoge/Doge-60M-Instruct-SFT) | [HuggingFaceTB/smoltalk](https://huggingface.co/datasets/HuggingFaceTB/smoltalk) | 2 | 2048 | 6e-4 | 0.25M | bfloat16 |\n| [Doge-160M-Instruct-SFT](https://huggingface.co/SmallDoge/Doge-160M-Instruct-SFT) | [HuggingFaceTB/smoltalk](https://huggingface.co/datasets/HuggingFaceTB/smoltalk) | 2 | 2048 | 4e-4 | 0.25M | bfloat16 |\n\n**DPO**:\n| Model | Training Data | Epochs | Content Length | LR | Batch Size | Precision |\n|---|---|---|---|---|---|---|\n| [Doge-20M-Instruct](https://huggingface.co/SmallDoge/Doge-20M-Instruct) | [HuggingFaceH4/ultrafeedback_binarized](https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized) | 2 | 1024 | 8e-5 | 0.125M | bfloat16 |\n| [Doge-60M-Instruct](https://huggingface.co/SmallDoge/Doge-60M-Instruct) | [HuggingFaceH4/ultrafeedback_binarized](https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized) | 2 | 1024 | 6e-5 | 0.125M | bfloat16 |\n| [Doge-160M-Instruct](https://huggingface.co/SmallDoge/Doge-160M-Instruct) | [HuggingFaceH4/ultrafeedback_binarized](https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized) | 2 | 1024 | 4e-5 | 0.125M | bfloat16 |\n\n\n**Evaluation**:\n\n| Model | IFEval (Prompt Strict Acc) | MMLU | BBH | ARC | PIQA | HellaSwag | tokens / s on i7-11 CPU |\n|---|---|---|---|---|---|---|---|\n| [Doge-20M-Instruct](https://huggingface.co/SmallDoge/Doge-20M-Instruct) | 7.3 | 26.3 | 18.3 | 29.2 | 57.8 | 27.8 | 142 |\n| [Doge-60M-Instruct](https://huggingface.co/SmallDoge/Doge-60M-Instruct) | 7.4 | 27.5 | 27.7 | 37.5 | 61.4 | 32.1 | 62 |\n| [SmolLM-135M-Instruct](https://huggingface.co/HuggingFaceTB/SmolLM-135M-Instruct) | 12.2 | 28.3 | 25.2 | 33.9 | 64.0 | 38.9 | 30 |\n| [Doge-160M-Instruct](https://huggingface.co/SmallDoge/Doge-160M-Instruct) | 16.8 | 29.7 | 29.1 | 42.8 | 64.1 | 37.1 | 28 |\n\n\n**Procedure**:\n\n**SFT**:\n[<img src=\"https://raw.githubusercontent.com/wandb/assets/main/wandb-github-badge-28.svg\" alt=\"Visualize in Weights & Biases\" width=\"150\" height=\"24\"/>](https://wandb.ai/loser_cheems/huggingface/runs/0jht5dro) \n\n**DPO**:\n[<img src=\"https://raw.githubusercontent.com/wandb/assets/main/wandb-github-badge-28.svg\" alt=\"Visualize in Weights & Biases\" width=\"150\" height=\"24\"/>](https://wandb.ai/loser_cheems/huggingface/runs/m5onn07v)\n\n\n**Environment**:\n- Image: nvcr.io/nvidia/pytorch:24.12-py3\n- Hardware: 1x NVIDIA RTX 4090\n- Software: Transformers, TRL\n\n\n## Citation\n\n```bibtex\n@misc{shi2024wonderfulmatrices,\n      title={Wonderful Matrices: Combining for a More Efficient and Effective Foundation Model Architecture}, \n      author={Jingze Shi and Bingheng Wu},\n      year={2024},\n      eprint={2412.11834},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG},\n      url={https://arxiv.org/abs/2412.11834}, \n}\n```",
    "card_content": "---\nlanguage:\n- en\nlicense: apache-2.0\nlibrary_name: transformers\ntags:\n- trl\n- sft\n- dpo\n- doge\ndatasets:\n- HuggingFaceTB/smoltalk\n- HuggingFaceH4/ultrafeedback_binarized\nbase_model:\n- SmallDoge/Doge-160M\npipeline_tag: question-answering\n---\n\n\n# **Doge 160M Instruct**\n\n<div align=\"center\">\n  <img src=\"https://huggingface.co/spaces/SmallDoge/README/resolve/main/org_icon.png\" width=\"100%\" alt=\"SmallDoge\" />\n</div>\n<hr>\n<div align=\"center\">\n  <a href=\"https://discord.gg/P2yYH95N\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-Small%20Doges-7289da?logo=discord&logoColor=white&color=7289da\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://arxiv.org/abs/2412.11834\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"arXiv\" src=\"https://img.shields.io/static/v1?label=arXiv&message=2412.11834&color=B31B1B&logo=arXiv\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://github.com/SmallDoges/small-doge\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"GitHub\" src=\"https://img.shields.io/badge/GitHub-SmallDoge-181717?logo=github\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://github.com/SmallDoges/small-doge/blob/main/LICENSE\" style=\"margin: 2px;\">\n    <img alt=\"License\" src=\"https://img.shields.io/badge/License-Apache--2.0-blue.svg\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\nDoge uses Dynamic Mask Attention as sequence transformation and can use Multi-Layer Perceptron or Cross Domain Mixture of Experts as state transformation. Dynamic Mask Attention allows the Transformer to use self-attention during training and state space during inference, and Cross Domain Mixture of Experts can directly inherit the weights of Multi-Layer Perceptron for further training. This model is trained by [SmallDoge](https://huggingface.co/SmallDoge) community, for detailed algorithm and model architecture, please refer to [Wonderful Matrices](https://arxiv.org/abs/2412.11834), all training details and code are publicly available on the [small-doge](https://github.com/SmallDoges/small-doge) repository.\n\n\n## Uses\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig, TextStreamer\n\ntokenizer = AutoTokenizer.from_pretrained(\"SmallDoge/Doge-160M-Instruct\")\nmodel = AutoModelForCausalLM.from_pretrained(\"SmallDoge/Doge-160M-Instruct\", trust_remote_code=True)\n\ngeneration_config = GenerationConfig(\n      max_new_tokens=100, \n      use_cache=True, \n      do_sample=True, \n      temperature=0.8, \n      top_p=0.9,\n      repetition_penalty=1.0\n)\nsteamer = TextStreamer(\n      tokenizer=tokenizer, \n      skip_prompt=True\n)\n\nprompt = \"Hi, how are you doing today?\"\nconversation = [\n      {\"role\": \"user\", \"content\": prompt}\n]\ninputs = tokenizer.apply_chat_template(\n    conversation=conversation,\n    tokenize=True,\n    return_tensors=\"pt\",\n)\n\noutputs = model.generate(\n    inputs, \n    tokenizer=tokenizer,\n    generation_config=generation_config, \n    streamer=steamer\n)\n```\n\n\n## Model Details\n\nWe build the Doge-Instruct by first SFT on [SmolTalk](https://huggingface.co/datasets/HuggingFaceTB/smoltalk) and then DPO on [UltraFeedback Binarized](https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized).\n\n> TODO: The larger model is under training and will be uploaded soon.\n\n**SFT**:\n| Model | Training Data | Epochs | Content Length | LR | Batch Size | Precision |\n|---|---|---|---|---|---|---|\n| [Doge-20M-Instruct-SFT](https://huggingface.co/SmallDoge/Doge-20M-Instruct-SFT) | [HuggingFaceTB/smoltalk](https://huggingface.co/datasets/HuggingFaceTB/smoltalk) | 2 | 2048 | 8e-4 | 0.25M | bfloat16 |\n| [Doge-60M-Instruct-SFT](https://huggingface.co/SmallDoge/Doge-60M-Instruct-SFT) | [HuggingFaceTB/smoltalk](https://huggingface.co/datasets/HuggingFaceTB/smoltalk) | 2 | 2048 | 6e-4 | 0.25M | bfloat16 |\n| [Doge-160M-Instruct-SFT](https://huggingface.co/SmallDoge/Doge-160M-Instruct-SFT) | [HuggingFaceTB/smoltalk](https://huggingface.co/datasets/HuggingFaceTB/smoltalk) | 2 | 2048 | 4e-4 | 0.25M | bfloat16 |\n\n**DPO**:\n| Model | Training Data | Epochs | Content Length | LR | Batch Size | Precision |\n|---|---|---|---|---|---|---|\n| [Doge-20M-Instruct](https://huggingface.co/SmallDoge/Doge-20M-Instruct) | [HuggingFaceH4/ultrafeedback_binarized](https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized) | 2 | 1024 | 8e-5 | 0.125M | bfloat16 |\n| [Doge-60M-Instruct](https://huggingface.co/SmallDoge/Doge-60M-Instruct) | [HuggingFaceH4/ultrafeedback_binarized](https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized) | 2 | 1024 | 6e-5 | 0.125M | bfloat16 |\n| [Doge-160M-Instruct](https://huggingface.co/SmallDoge/Doge-160M-Instruct) | [HuggingFaceH4/ultrafeedback_binarized](https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized) | 2 | 1024 | 4e-5 | 0.125M | bfloat16 |\n\n\n**Evaluation**:\n\n| Model | IFEval (Prompt Strict Acc) | MMLU | BBH | ARC | PIQA | HellaSwag | tokens / s on i7-11 CPU |\n|---|---|---|---|---|---|---|---|\n| [Doge-20M-Instruct](https://huggingface.co/SmallDoge/Doge-20M-Instruct) | 7.3 | 26.3 | 18.3 | 29.2 | 57.8 | 27.8 | 142 |\n| [Doge-60M-Instruct](https://huggingface.co/SmallDoge/Doge-60M-Instruct) | 7.4 | 27.5 | 27.7 | 37.5 | 61.4 | 32.1 | 62 |\n| [SmolLM-135M-Instruct](https://huggingface.co/HuggingFaceTB/SmolLM-135M-Instruct) | 12.2 | 28.3 | 25.2 | 33.9 | 64.0 | 38.9 | 30 |\n| [Doge-160M-Instruct](https://huggingface.co/SmallDoge/Doge-160M-Instruct) | 16.8 | 29.7 | 29.1 | 42.8 | 64.1 | 37.1 | 28 |\n\n\n**Procedure**:\n\n**SFT**:\n[<img src=\"https://raw.githubusercontent.com/wandb/assets/main/wandb-github-badge-28.svg\" alt=\"Visualize in Weights & Biases\" width=\"150\" height=\"24\"/>](https://wandb.ai/loser_cheems/huggingface/runs/0jht5dro) \n\n**DPO**:\n[<img src=\"https://raw.githubusercontent.com/wandb/assets/main/wandb-github-badge-28.svg\" alt=\"Visualize in Weights & Biases\" width=\"150\" height=\"24\"/>](https://wandb.ai/loser_cheems/huggingface/runs/m5onn07v)\n\n\n**Environment**:\n- Image: nvcr.io/nvidia/pytorch:24.12-py3\n- Hardware: 1x NVIDIA RTX 4090\n- Software: Transformers, TRL\n\n\n## Citation\n\n```bibtex\n@misc{shi2024wonderfulmatrices,\n      title={Wonderful Matrices: Combining for a More Efficient and Effective Foundation Model Architecture}, \n      author={Jingze Shi and Bingheng Wu},\n      year={2024},\n      eprint={2412.11834},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG},\n      url={https://arxiv.org/abs/2412.11834}, \n}\n```",
    "library_name": "transformers"
  },
  {
    "model_id": "AgentPublic/camembert-base-squadFR-fquad-piaf",
    "model_name": "AgentPublic/camembert-base-squadFR-fquad-piaf",
    "author": "AgentPublic",
    "downloads": 37081,
    "likes": 28,
    "tags": [
      "transformers",
      "pytorch",
      "tf",
      "safetensors",
      "camembert",
      "question-answering",
      "fr",
      "dataset:piaf",
      "dataset:FQuAD",
      "dataset:SQuAD-FR",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/AgentPublic/camembert-base-squadFR-fquad-piaf",
    "dependencies": [
      [
        "transformers",
        "4.36.2"
      ],
      [
        "torch",
        "2.2.2"
      ],
      [
        "sentencepiece",
        "0.1.99"
      ],
      [
        "numpy",
        "1.26.3"
      ],
      [
        "cmake",
        "3.28.1"
      ]
    ],
    "analysis_date": "2025-03-26T00:49:56.216425",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "camembert",
    "pipeline_tag": "question-answering",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": "fr",
      "datasets": [
        "piaf",
        "FQuAD",
        "SQuAD-FR"
      ],
      "widget": [
        {
          "text": "Comment s'appelle le portail open data du gouvernement ?",
          "context": "Etalab est une administration publique fran\u00e7aise qui fait notamment office de Chief Data Officer de l'\u00c9tat et coordonne la conception et la mise en \u0153uvre de sa strat\u00e9gie dans le domaine de la donn\u00e9e (ouverture et partage des donn\u00e9es publiques ou open data, exploitation des donn\u00e9es et intelligence artificielle...). Ainsi, Etalab d\u00e9veloppe et maintient le portail des donn\u00e9es ouvertes du gouvernement fran\u00e7ais data.gouv.fr. Etalab promeut \u00e9galement une plus grande ouverture l'administration sur la soci\u00e9t\u00e9 (gouvernement ouvert) : transparence de l'action publique, innovation ouverte, participation citoyenne... elle promeut l\u2019innovation, l\u2019exp\u00e9rimentation, les m\u00e9thodes de travail ouvertes, agiles et it\u00e9ratives, ainsi que les synergies avec la soci\u00e9t\u00e9 civile pour d\u00e9cloisonner l\u2019administration et favoriser l\u2019adoption des meilleures pratiques professionnelles dans le domaine du num\u00e9rique. \u00c0 ce titre elle \u00e9tudie notamment l\u2019opportunit\u00e9 de recourir \u00e0 des technologies en voie de maturation issues du monde de la recherche. Cette entit\u00e9 charg\u00e9e de l'innovation au sein de l'administration doit contribuer \u00e0 l'am\u00e9lioration du service public gr\u00e2ce au num\u00e9rique. Elle est rattach\u00e9e \u00e0 la Direction interminist\u00e9rielle du num\u00e9rique, dont les missions et l\u2019organisation ont \u00e9t\u00e9 fix\u00e9es par le d\u00e9cret du 30 octobre 2019.\u2009 Dirig\u00e9 par Laure Lucchesi depuis 2016, elle rassemble une \u00e9quipe pluridisciplinaire d'une trentaine de personnes."
        }
      ]
    },
    "card_text": "\n# camembert-base-squadFR-fquad-piaf\n\n## Description\n\nQuestion-answering French model, using base [CamemBERT](https://camembert-model.fr/) fine-tuned on a combo of three French Q&A datasets:\n\n1. [PIAFv1.1](https://www.data.gouv.fr/en/datasets/piaf-le-dataset-francophone-de-questions-reponses/)\n2. [FQuADv1.0](https://fquad.illuin.tech/)\n3. [SQuAD-FR (SQuAD automatically translated to French)](https://github.com/Alikabbadj/French-SQuAD)\n\n## Training hyperparameters\n\n```shell\npython run_squad.py \\\n--model_type camembert \\\n--model_name_or_path camembert-base \\\n--do_train --do_eval \\\n--train_file data/SQuAD+fquad+piaf.json \\\n--predict_file data/fquad_valid.json \\\n--per_gpu_train_batch_size 12 \\ \n--learning_rate 3e-5 \\ \n--num_train_epochs 4 \\  \n--max_seq_length 384 \\ \n--doc_stride 128 \\\n--save_steps 10000 \n``` \n\n## Evaluation results\n### FQuAD v1.0 Evaluation\n```shell\n{\"f1\": 79.81, \"exact_match\": 55.14}\n```\n### SQuAD-FR Evaluation\n```shell\n{\"f1\": 80.61, \"exact_match\": 59.54}\n```\n\n## Usage\n\n```python\nfrom transformers import pipeline\n\nnlp = pipeline('question-answering', model='etalab-ia/camembert-base-squadFR-fquad-piaf', tokenizer='etalab-ia/camembert-base-squadFR-fquad-piaf')\n\nnlp({\n    'question': \"Qui est Claude Monet?\",\n    'context': \"Claude Monet, n\u00e9 le 14 novembre 1840 \u00e0 Paris et mort le 5 d\u00e9cembre 1926 \u00e0 Giverny, est un peintre fran\u00e7ais et l\u2019un des fondateurs de l'impressionnisme.\"\n})\n```\n## Acknowledgments\n\nThis work was performed using HPC resources from GENCI\u2013IDRIS (Grant 2020-AD011011224). \n\n## Citations\n\n### PIAF\n```\n@inproceedings{KeraronLBAMSSS20,\n  author    = {Rachel Keraron and\n               Guillaume Lancrenon and\n               Mathilde Bras and\n               Fr{\\'{e}}d{\\'{e}}ric Allary and\n               Gilles Moyse and\n               Thomas Scialom and\n               Edmundo{-}Pavel Soriano{-}Morales and\n               Jacopo Staiano},\n  title     = {Project {PIAF:} Building a Native French Question-Answering Dataset},\n  booktitle = {{LREC}},\n  pages     = {5481--5490},\n  publisher = {European Language Resources Association},\n  year      = {2020}\n}\n\n```\n\n### FQuAD\n```\n@article{dHoffschmidt2020FQuADFQ,\n  title={FQuAD: French Question Answering Dataset},\n  author={Martin d'Hoffschmidt and Maxime Vidal and Wacim Belblidia and Tom Brendl'e and Quentin Heinrich},\n  journal={ArXiv},\n  year={2020},\n  volume={abs/2002.06071}\n}\n```\n\n### SQuAD-FR\n```\n @MISC{kabbadj2018,\n   author =       \"Kabbadj, Ali\",\n   title =        \"Something new in French Text Mining and Information Extraction (Universal Chatbot): Largest Q&A French training dataset (110 000+) \",\n   editor =       \"linkedin.com\",\n   month =        \"November\",\n   year =         \"2018\",\n   url =          \"\\url{https://www.linkedin.com/pulse/something-new-french-text-mining-information-chatbot-largest-kabbadj/}\",\n   note =         \"[Online; posted 11-November-2018]\",\n }\n ```\n\n### CamemBERT\nHF model card : [https://huggingface.co/camembert-base](https://huggingface.co/camembert-base)\n\n```\n@inproceedings{martin2020camembert,\n  title={CamemBERT: a Tasty French Language Model},\n  author={Martin, Louis and Muller, Benjamin and Su{\\'a}rez, Pedro Javier Ortiz and Dupont, Yoann and Romary, Laurent and de la Clergerie, {\\'E}ric Villemonte and Seddah, Djam{\\'e} and Sagot, Beno{\\^\\i}t},\n  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},\n  year={2020}\n}\n```\n\n",
    "card_content": "---\nlanguage: fr\ndatasets:\n- piaf\n- FQuAD\n- SQuAD-FR\nwidget:\n- text: Comment s'appelle le portail open data du gouvernement ?\n  context: 'Etalab est une administration publique fran\u00e7aise qui fait notamment office\n    de Chief Data Officer de l''\u00c9tat et coordonne la conception et la mise en \u0153uvre\n    de sa strat\u00e9gie dans le domaine de la donn\u00e9e (ouverture et partage des donn\u00e9es\n    publiques ou open data, exploitation des donn\u00e9es et intelligence artificielle...).\n    Ainsi, Etalab d\u00e9veloppe et maintient le portail des donn\u00e9es ouvertes du gouvernement\n    fran\u00e7ais data.gouv.fr. Etalab promeut \u00e9galement une plus grande ouverture l''administration\n    sur la soci\u00e9t\u00e9 (gouvernement ouvert) : transparence de l''action publique, innovation\n    ouverte, participation citoyenne... elle promeut l\u2019innovation, l\u2019exp\u00e9rimentation,\n    les m\u00e9thodes de travail ouvertes, agiles et it\u00e9ratives, ainsi que les synergies\n    avec la soci\u00e9t\u00e9 civile pour d\u00e9cloisonner l\u2019administration et favoriser l\u2019adoption\n    des meilleures pratiques professionnelles dans le domaine du num\u00e9rique. \u00c0 ce titre\n    elle \u00e9tudie notamment l\u2019opportunit\u00e9 de recourir \u00e0 des technologies en voie de\n    maturation issues du monde de la recherche. Cette entit\u00e9 charg\u00e9e de l''innovation\n    au sein de l''administration doit contribuer \u00e0 l''am\u00e9lioration du service public\n    gr\u00e2ce au num\u00e9rique. Elle est rattach\u00e9e \u00e0 la Direction interminist\u00e9rielle du num\u00e9rique,\n    dont les missions et l\u2019organisation ont \u00e9t\u00e9 fix\u00e9es par le d\u00e9cret du 30 octobre\n    2019.\u2009 Dirig\u00e9 par Laure Lucchesi depuis 2016, elle rassemble une \u00e9quipe pluridisciplinaire\n    d''une trentaine de personnes.'\n---\n\n# camembert-base-squadFR-fquad-piaf\n\n## Description\n\nQuestion-answering French model, using base [CamemBERT](https://camembert-model.fr/) fine-tuned on a combo of three French Q&A datasets:\n\n1. [PIAFv1.1](https://www.data.gouv.fr/en/datasets/piaf-le-dataset-francophone-de-questions-reponses/)\n2. [FQuADv1.0](https://fquad.illuin.tech/)\n3. [SQuAD-FR (SQuAD automatically translated to French)](https://github.com/Alikabbadj/French-SQuAD)\n\n## Training hyperparameters\n\n```shell\npython run_squad.py \\\n--model_type camembert \\\n--model_name_or_path camembert-base \\\n--do_train --do_eval \\\n--train_file data/SQuAD+fquad+piaf.json \\\n--predict_file data/fquad_valid.json \\\n--per_gpu_train_batch_size 12 \\ \n--learning_rate 3e-5 \\ \n--num_train_epochs 4 \\  \n--max_seq_length 384 \\ \n--doc_stride 128 \\\n--save_steps 10000 \n``` \n\n## Evaluation results\n### FQuAD v1.0 Evaluation\n```shell\n{\"f1\": 79.81, \"exact_match\": 55.14}\n```\n### SQuAD-FR Evaluation\n```shell\n{\"f1\": 80.61, \"exact_match\": 59.54}\n```\n\n## Usage\n\n```python\nfrom transformers import pipeline\n\nnlp = pipeline('question-answering', model='etalab-ia/camembert-base-squadFR-fquad-piaf', tokenizer='etalab-ia/camembert-base-squadFR-fquad-piaf')\n\nnlp({\n    'question': \"Qui est Claude Monet?\",\n    'context': \"Claude Monet, n\u00e9 le 14 novembre 1840 \u00e0 Paris et mort le 5 d\u00e9cembre 1926 \u00e0 Giverny, est un peintre fran\u00e7ais et l\u2019un des fondateurs de l'impressionnisme.\"\n})\n```\n## Acknowledgments\n\nThis work was performed using HPC resources from GENCI\u2013IDRIS (Grant 2020-AD011011224). \n\n## Citations\n\n### PIAF\n```\n@inproceedings{KeraronLBAMSSS20,\n  author    = {Rachel Keraron and\n               Guillaume Lancrenon and\n               Mathilde Bras and\n               Fr{\\'{e}}d{\\'{e}}ric Allary and\n               Gilles Moyse and\n               Thomas Scialom and\n               Edmundo{-}Pavel Soriano{-}Morales and\n               Jacopo Staiano},\n  title     = {Project {PIAF:} Building a Native French Question-Answering Dataset},\n  booktitle = {{LREC}},\n  pages     = {5481--5490},\n  publisher = {European Language Resources Association},\n  year      = {2020}\n}\n\n```\n\n### FQuAD\n```\n@article{dHoffschmidt2020FQuADFQ,\n  title={FQuAD: French Question Answering Dataset},\n  author={Martin d'Hoffschmidt and Maxime Vidal and Wacim Belblidia and Tom Brendl'e and Quentin Heinrich},\n  journal={ArXiv},\n  year={2020},\n  volume={abs/2002.06071}\n}\n```\n\n### SQuAD-FR\n```\n @MISC{kabbadj2018,\n   author =       \"Kabbadj, Ali\",\n   title =        \"Something new in French Text Mining and Information Extraction (Universal Chatbot): Largest Q&A French training dataset (110 000+) \",\n   editor =       \"linkedin.com\",\n   month =        \"November\",\n   year =         \"2018\",\n   url =          \"\\url{https://www.linkedin.com/pulse/something-new-french-text-mining-information-chatbot-largest-kabbadj/}\",\n   note =         \"[Online; posted 11-November-2018]\",\n }\n ```\n\n### CamemBERT\nHF model card : [https://huggingface.co/camembert-base](https://huggingface.co/camembert-base)\n\n```\n@inproceedings{martin2020camembert,\n  title={CamemBERT: a Tasty French Language Model},\n  author={Martin, Louis and Muller, Benjamin and Su{\\'a}rez, Pedro Javier Ortiz and Dupont, Yoann and Romary, Laurent and de la Clergerie, {\\'E}ric Villemonte and Seddah, Djam{\\'e} and Sagot, Beno{\\^\\i}t},\n  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},\n  year={2020}\n}\n```\n\n",
    "library_name": "transformers"
  },
  {
    "model_id": "philschmid/distilbert-onnx",
    "model_name": "philschmid/distilbert-onnx",
    "author": "philschmid",
    "downloads": 36536,
    "likes": 2,
    "tags": [
      "transformers",
      "onnx",
      "distilbert",
      "question-answering",
      "en",
      "dataset:squad",
      "license:apache-2.0",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/philschmid/distilbert-onnx",
    "dependencies": [
      [
        "onnx",
        "1.15.0"
      ],
      [
        "transformers",
        null
      ]
    ],
    "analysis_date": "2025-03-26T00:49:57.408374",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "distilbert",
    "pipeline_tag": "question-answering",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": "en",
      "license": "apache-2.0",
      "datasets": [
        "squad"
      ],
      "metrics": [
        "squad"
      ]
    },
    "card_text": "\n# ONNX Conversion of [distilbert-base-cased-distilled-squad](https://huggingface.co/distilbert-base-cased-distilled-squad)\n\n# DistilBERT base cased distilled SQuAD\n\nThis model is a fine-tune checkpoint of [DistilBERT-base-cased](https://huggingface.co/distilbert-base-cased), fine-tuned using (a second step of) knowledge distillation on SQuAD v1.1.\nThis model reaches a F1 score of 87.1 on the dev set (for comparison, BERT bert-base-cased version reaches a F1 score of 88.7).\n",
    "card_content": "---\nlanguage: en\nlicense: apache-2.0\ndatasets:\n- squad\nmetrics:\n- squad\n---\n\n# ONNX Conversion of [distilbert-base-cased-distilled-squad](https://huggingface.co/distilbert-base-cased-distilled-squad)\n\n# DistilBERT base cased distilled SQuAD\n\nThis model is a fine-tune checkpoint of [DistilBERT-base-cased](https://huggingface.co/distilbert-base-cased), fine-tuned using (a second step of) knowledge distillation on SQuAD v1.1.\nThis model reaches a F1 score of 87.1 on the dev set (for comparison, BERT bert-base-cased version reaches a F1 score of 88.7).\n",
    "library_name": "transformers"
  },
  {
    "model_id": "furiosa-ai/mlperf-bert-large",
    "model_name": "furiosa-ai/mlperf-bert-large",
    "author": "furiosa-ai",
    "downloads": 31972,
    "likes": 0,
    "tags": [
      "transformers",
      "pytorch",
      "bert",
      "question-answering",
      "arxiv:1910.09700",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/furiosa-ai/mlperf-bert-large",
    "dependencies": [
      [
        "transformers",
        "4.36.2"
      ],
      [
        "torch",
        "2.5.1"
      ]
    ],
    "analysis_date": "2025-03-26T00:49:58.859446",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "bert",
    "pipeline_tag": "question-answering",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "library_name": "transformers",
      "tags": []
    },
    "card_text": "\n# Model Card for Model ID\n\n<!-- Provide a quick summary of what the model is/does. -->\n\n\n\n## Model Details\n\n### Model Description\n\n<!-- Provide a longer summary of what this model is. -->\n\nThis is the model card of a \ud83e\udd17 transformers model that has been pushed on the Hub. This model card has been automatically generated.\n\n- **Developed by:** [More Information Needed]\n- **Funded by [optional]:** [More Information Needed]\n- **Shared by [optional]:** [More Information Needed]\n- **Model type:** [More Information Needed]\n- **Language(s) (NLP):** [More Information Needed]\n- **License:** [More Information Needed]\n- **Finetuned from model [optional]:** [More Information Needed]\n\n### Model Sources [optional]\n\n<!-- Provide the basic links for the model. -->\n\n- **Repository:** [More Information Needed]\n- **Paper [optional]:** [More Information Needed]\n- **Demo [optional]:** [More Information Needed]\n\n## Uses\n\n<!-- Address questions around how the model is intended to be used, including the foreseeable users of the model and those affected by the model. -->\n\n### Direct Use\n\n<!-- This section is for the model use without fine-tuning or plugging into a larger ecosystem/app. -->\n\n[More Information Needed]\n\n### Downstream Use [optional]\n\n<!-- This section is for the model use when fine-tuned for a task, or when plugged into a larger ecosystem/app -->\n\n[More Information Needed]\n\n### Out-of-Scope Use\n\n<!-- This section addresses misuse, malicious use, and uses that the model will not work well for. -->\n\n[More Information Needed]\n\n## Bias, Risks, and Limitations\n\n<!-- This section is meant to convey both technical and sociotechnical limitations. -->\n\n[More Information Needed]\n\n### Recommendations\n\n<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->\n\nUsers (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.\n\n## How to Get Started with the Model\n\nUse the code below to get started with the model.\n\n[More Information Needed]\n\n## Training Details\n\n### Training Data\n\n<!-- This should link to a Dataset Card, perhaps with a short stub of information on what the training data is all about as well as documentation related to data pre-processing or additional filtering. -->\n\n[More Information Needed]\n\n### Training Procedure\n\n<!-- This relates heavily to the Technical Specifications. Content here should link to that section when it is relevant to the training procedure. -->\n\n#### Preprocessing [optional]\n\n[More Information Needed]\n\n\n#### Training Hyperparameters\n\n- **Training regime:** [More Information Needed] <!--fp32, fp16 mixed precision, bf16 mixed precision, bf16 non-mixed precision, fp16 non-mixed precision, fp8 mixed precision -->\n\n#### Speeds, Sizes, Times [optional]\n\n<!-- This section provides information about throughput, start/end time, checkpoint size if relevant, etc. -->\n\n[More Information Needed]\n\n## Evaluation\n\n<!-- This section describes the evaluation protocols and provides the results. -->\n\n### Testing Data, Factors & Metrics\n\n#### Testing Data\n\n<!-- This should link to a Dataset Card if possible. -->\n\n[More Information Needed]\n\n#### Factors\n\n<!-- These are the things the evaluation is disaggregating by, e.g., subpopulations or domains. -->\n\n[More Information Needed]\n\n#### Metrics\n\n<!-- These are the evaluation metrics being used, ideally with a description of why. -->\n\n[More Information Needed]\n\n### Results\n\n[More Information Needed]\n\n#### Summary\n\n\n\n## Model Examination [optional]\n\n<!-- Relevant interpretability work for the model goes here -->\n\n[More Information Needed]\n\n## Environmental Impact\n\n<!-- Total emissions (in grams of CO2eq) and additional considerations, such as electricity usage, go here. Edit the suggested text below accordingly -->\n\nCarbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).\n\n- **Hardware Type:** [More Information Needed]\n- **Hours used:** [More Information Needed]\n- **Cloud Provider:** [More Information Needed]\n- **Compute Region:** [More Information Needed]\n- **Carbon Emitted:** [More Information Needed]\n\n## Technical Specifications [optional]\n\n### Model Architecture and Objective\n\n[More Information Needed]\n\n### Compute Infrastructure\n\n[More Information Needed]\n\n#### Hardware\n\n[More Information Needed]\n\n#### Software\n\n[More Information Needed]\n\n## Citation [optional]\n\n<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->\n\n**BibTeX:**\n\n[More Information Needed]\n\n**APA:**\n\n[More Information Needed]\n\n## Glossary [optional]\n\n<!-- If relevant, include terms and calculations in this section that can help readers understand the model or model card. -->\n\n[More Information Needed]\n\n## More Information [optional]\n\n[More Information Needed]\n\n## Model Card Authors [optional]\n\n[More Information Needed]\n\n## Model Card Contact\n\n[More Information Needed]",
    "card_content": "---\nlibrary_name: transformers\ntags: []\n---\n\n# Model Card for Model ID\n\n<!-- Provide a quick summary of what the model is/does. -->\n\n\n\n## Model Details\n\n### Model Description\n\n<!-- Provide a longer summary of what this model is. -->\n\nThis is the model card of a \ud83e\udd17 transformers model that has been pushed on the Hub. This model card has been automatically generated.\n\n- **Developed by:** [More Information Needed]\n- **Funded by [optional]:** [More Information Needed]\n- **Shared by [optional]:** [More Information Needed]\n- **Model type:** [More Information Needed]\n- **Language(s) (NLP):** [More Information Needed]\n- **License:** [More Information Needed]\n- **Finetuned from model [optional]:** [More Information Needed]\n\n### Model Sources [optional]\n\n<!-- Provide the basic links for the model. -->\n\n- **Repository:** [More Information Needed]\n- **Paper [optional]:** [More Information Needed]\n- **Demo [optional]:** [More Information Needed]\n\n## Uses\n\n<!-- Address questions around how the model is intended to be used, including the foreseeable users of the model and those affected by the model. -->\n\n### Direct Use\n\n<!-- This section is for the model use without fine-tuning or plugging into a larger ecosystem/app. -->\n\n[More Information Needed]\n\n### Downstream Use [optional]\n\n<!-- This section is for the model use when fine-tuned for a task, or when plugged into a larger ecosystem/app -->\n\n[More Information Needed]\n\n### Out-of-Scope Use\n\n<!-- This section addresses misuse, malicious use, and uses that the model will not work well for. -->\n\n[More Information Needed]\n\n## Bias, Risks, and Limitations\n\n<!-- This section is meant to convey both technical and sociotechnical limitations. -->\n\n[More Information Needed]\n\n### Recommendations\n\n<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->\n\nUsers (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.\n\n## How to Get Started with the Model\n\nUse the code below to get started with the model.\n\n[More Information Needed]\n\n## Training Details\n\n### Training Data\n\n<!-- This should link to a Dataset Card, perhaps with a short stub of information on what the training data is all about as well as documentation related to data pre-processing or additional filtering. -->\n\n[More Information Needed]\n\n### Training Procedure\n\n<!-- This relates heavily to the Technical Specifications. Content here should link to that section when it is relevant to the training procedure. -->\n\n#### Preprocessing [optional]\n\n[More Information Needed]\n\n\n#### Training Hyperparameters\n\n- **Training regime:** [More Information Needed] <!--fp32, fp16 mixed precision, bf16 mixed precision, bf16 non-mixed precision, fp16 non-mixed precision, fp8 mixed precision -->\n\n#### Speeds, Sizes, Times [optional]\n\n<!-- This section provides information about throughput, start/end time, checkpoint size if relevant, etc. -->\n\n[More Information Needed]\n\n## Evaluation\n\n<!-- This section describes the evaluation protocols and provides the results. -->\n\n### Testing Data, Factors & Metrics\n\n#### Testing Data\n\n<!-- This should link to a Dataset Card if possible. -->\n\n[More Information Needed]\n\n#### Factors\n\n<!-- These are the things the evaluation is disaggregating by, e.g., subpopulations or domains. -->\n\n[More Information Needed]\n\n#### Metrics\n\n<!-- These are the evaluation metrics being used, ideally with a description of why. -->\n\n[More Information Needed]\n\n### Results\n\n[More Information Needed]\n\n#### Summary\n\n\n\n## Model Examination [optional]\n\n<!-- Relevant interpretability work for the model goes here -->\n\n[More Information Needed]\n\n## Environmental Impact\n\n<!-- Total emissions (in grams of CO2eq) and additional considerations, such as electricity usage, go here. Edit the suggested text below accordingly -->\n\nCarbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).\n\n- **Hardware Type:** [More Information Needed]\n- **Hours used:** [More Information Needed]\n- **Cloud Provider:** [More Information Needed]\n- **Compute Region:** [More Information Needed]\n- **Carbon Emitted:** [More Information Needed]\n\n## Technical Specifications [optional]\n\n### Model Architecture and Objective\n\n[More Information Needed]\n\n### Compute Infrastructure\n\n[More Information Needed]\n\n#### Hardware\n\n[More Information Needed]\n\n#### Software\n\n[More Information Needed]\n\n## Citation [optional]\n\n<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->\n\n**BibTeX:**\n\n[More Information Needed]\n\n**APA:**\n\n[More Information Needed]\n\n## Glossary [optional]\n\n<!-- If relevant, include terms and calculations in this section that can help readers understand the model or model card. -->\n\n[More Information Needed]\n\n## More Information [optional]\n\n[More Information Needed]\n\n## Model Card Authors [optional]\n\n[More Information Needed]\n\n## Model Card Contact\n\n[More Information Needed]",
    "library_name": "transformers"
  },
  {
    "model_id": "Rakib/roberta-base-on-cuad",
    "model_name": "Rakib/roberta-base-on-cuad",
    "author": "Rakib",
    "downloads": 18030,
    "likes": 8,
    "tags": [
      "transformers",
      "pytorch",
      "roberta",
      "question-answering",
      "legal-contract-review",
      "cuad",
      "en",
      "dataset:cuad",
      "license:mit",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/Rakib/roberta-base-on-cuad",
    "dependencies": [
      [
        "transformers",
        "4.36.2"
      ],
      [
        "torch",
        "2.5.1"
      ]
    ],
    "analysis_date": "2025-03-26T00:49:59.950932",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "roberta",
    "pipeline_tag": "question-answering",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": [
        "en"
      ],
      "license": "mit",
      "library_name": "transformers",
      "tags": [
        "legal-contract-review",
        "roberta",
        "cuad"
      ],
      "datasets": [
        "cuad"
      ],
      "pipeline_tag": "question-answering"
    },
    "card_text": "# Model Card for roberta-base-on-cuad\n  \n# Model Details\n \n## Model Description\n \n- **Developed by:** Mohammed Rakib\n- **Shared by [Optional]:** More information needed\n- **Model type:** Question Answering \n- **Language(s) (NLP):** en\n- **License:** MIT\n- **Related Models:**\n  - **Parent Model:** RoBERTa \n- **Resources for more information:** \n    - GitHub Repo: [defactolaw](https://github.com/afra-tech/defactolaw)\n    - Associated Paper: [An Open Source Contractual Language Understanding Application Using Machine Learning](https://aclanthology.org/2022.lateraisse-1.6/)\n \n \n \n \n# Uses\n \n \n## Direct Use\n \nThis model can be used for the task of  Question Answering  on Legal Documents.\n \n# Training Details\n\nRead: [An Open Source Contractual Language Understanding Application Using Machine Learning](https://aclanthology.org/2022.lateraisse-1.6/) \nfor detailed information on training procedure, dataset preprocessing and evaluation.\n \n## Training Data\n \nSee [CUAD dataset card](https://huggingface.co/datasets/cuad) for more information.\n \n## Training Procedure\n\n\n### Preprocessing\n \nMore information needed\n \n### Speeds, Sizes, Times\n \nMore information needed\n \n# Evaluation\n \n \n## Testing Data, Factors & Metrics\n \n### Testing Data\n \nSee [CUAD dataset card](https://huggingface.co/datasets/cuad) for more information.\n \n### Factors\n \n \n### Metrics\n \nMore information needed\n## Results \n \nMore information needed\n \n# Model Examination\n \nMore information needed\n  \n- **Hardware Type:** More information needed\n- **Hours used:** More information needed\n- **Cloud Provider:** More information needed\n- **Compute Region:** More information needed\n- **Carbon Emitted:** More information needed\n \n# Technical Specifications [optional]\n \n## Model Architecture and Objective\n \nMore information needed\n \n## Compute Infrastructure\n \nMore information needed\n \n### Hardware\n \nUsed V100/P100 from Google Colab Pro\n \n### Software\n\nPython, Transformers\n \n# Citation\n \n \n**BibTeX:**\n ```\n@inproceedings{nawar-etal-2022-open,\n    title = \"An Open Source Contractual Language Understanding Application Using Machine Learning\",\n    author = \"Nawar, Afra  and\n      Rakib, Mohammed  and\n      Hai, Salma Abdul  and\n      Haq, Sanaulla\",\n    booktitle = \"Proceedings of the First Workshop on Language Technology and Resources for a Fair, Inclusive, and Safe Society within the 13th Language Resources and Evaluation Conference\",\n    month = jun,\n    year = \"2022\",\n    address = \"Marseille, France\",\n    publisher = \"European Language Resources Association\",\n    url = \"https://aclanthology.org/2022.lateraisse-1.6\",\n    pages = \"42--50\",\n    abstract = \"Legal field is characterized by its exclusivity and non-transparency. Despite the frequency and relevance of legal dealings, legal documents like contracts remains elusive to non-legal professionals for the copious usage of legal jargon. There has been little advancement in making legal contracts more comprehensible. This paper presents how Machine Learning and NLP can be applied to solve this problem, further considering the challenges of applying ML to the high length of contract documents and training in a low resource environment. The largest open-source contract dataset so far, the Contract Understanding Atticus Dataset (CUAD) is utilized. Various pre-processing experiments and hyperparameter tuning have been carried out and we successfully managed to eclipse SOTA results presented for models in the CUAD dataset trained on RoBERTa-base. Our model, A-type-RoBERTa-base achieved an AUPR score of 46.6{\\%} compared to 42.6{\\%} on the original RoBERT-base. This model is utilized in our end to end contract understanding application which is able to take a contract and highlight the clauses a user is looking to find along with it{'}s descriptions to aid due diligence before signing. Alongside digital, i.e. searchable, contracts the system is capable of processing scanned, i.e. non-searchable, contracts using tesseract OCR. This application is aimed to not only make contract review a comprehensible process to non-legal professionals, but also to help lawyers and attorneys more efficiently review contracts.\",\n}\n```\n \n \n# Glossary [optional]\nMore information needed\n \n# More Information [optional]\n \nMore information needed\n \n# Model Card Authors [optional]\n  \nMohammed Rakib in collaboration with Ezi Ozoani and the Hugging Face team\n \n# Model Card Contact\n \nMore information needed\n \n# How to Get Started with the Model\n \nUse the code below to get started with the model.\n \n<details>\n<summary> Click to expand </summary>\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForQuestionAnswering\n \ntokenizer = AutoTokenizer.from_pretrained(\"Rakib/roberta-base-on-cuad\")\n \nmodel = AutoModelForQuestionAnswering.from_pretrained(\"Rakib/roberta-base-on-cuad\")\n```\n</details>",
    "card_content": "---\nlanguage:\n- en\nlicense: mit\nlibrary_name: transformers\ntags:\n- legal-contract-review\n- roberta\n- cuad\ndatasets:\n- cuad\npipeline_tag: question-answering\n---\n# Model Card for roberta-base-on-cuad\n  \n# Model Details\n \n## Model Description\n \n- **Developed by:** Mohammed Rakib\n- **Shared by [Optional]:** More information needed\n- **Model type:** Question Answering \n- **Language(s) (NLP):** en\n- **License:** MIT\n- **Related Models:**\n  - **Parent Model:** RoBERTa \n- **Resources for more information:** \n    - GitHub Repo: [defactolaw](https://github.com/afra-tech/defactolaw)\n    - Associated Paper: [An Open Source Contractual Language Understanding Application Using Machine Learning](https://aclanthology.org/2022.lateraisse-1.6/)\n \n \n \n \n# Uses\n \n \n## Direct Use\n \nThis model can be used for the task of  Question Answering  on Legal Documents.\n \n# Training Details\n\nRead: [An Open Source Contractual Language Understanding Application Using Machine Learning](https://aclanthology.org/2022.lateraisse-1.6/) \nfor detailed information on training procedure, dataset preprocessing and evaluation.\n \n## Training Data\n \nSee [CUAD dataset card](https://huggingface.co/datasets/cuad) for more information.\n \n## Training Procedure\n\n\n### Preprocessing\n \nMore information needed\n \n### Speeds, Sizes, Times\n \nMore information needed\n \n# Evaluation\n \n \n## Testing Data, Factors & Metrics\n \n### Testing Data\n \nSee [CUAD dataset card](https://huggingface.co/datasets/cuad) for more information.\n \n### Factors\n \n \n### Metrics\n \nMore information needed\n## Results \n \nMore information needed\n \n# Model Examination\n \nMore information needed\n  \n- **Hardware Type:** More information needed\n- **Hours used:** More information needed\n- **Cloud Provider:** More information needed\n- **Compute Region:** More information needed\n- **Carbon Emitted:** More information needed\n \n# Technical Specifications [optional]\n \n## Model Architecture and Objective\n \nMore information needed\n \n## Compute Infrastructure\n \nMore information needed\n \n### Hardware\n \nUsed V100/P100 from Google Colab Pro\n \n### Software\n\nPython, Transformers\n \n# Citation\n \n \n**BibTeX:**\n ```\n@inproceedings{nawar-etal-2022-open,\n    title = \"An Open Source Contractual Language Understanding Application Using Machine Learning\",\n    author = \"Nawar, Afra  and\n      Rakib, Mohammed  and\n      Hai, Salma Abdul  and\n      Haq, Sanaulla\",\n    booktitle = \"Proceedings of the First Workshop on Language Technology and Resources for a Fair, Inclusive, and Safe Society within the 13th Language Resources and Evaluation Conference\",\n    month = jun,\n    year = \"2022\",\n    address = \"Marseille, France\",\n    publisher = \"European Language Resources Association\",\n    url = \"https://aclanthology.org/2022.lateraisse-1.6\",\n    pages = \"42--50\",\n    abstract = \"Legal field is characterized by its exclusivity and non-transparency. Despite the frequency and relevance of legal dealings, legal documents like contracts remains elusive to non-legal professionals for the copious usage of legal jargon. There has been little advancement in making legal contracts more comprehensible. This paper presents how Machine Learning and NLP can be applied to solve this problem, further considering the challenges of applying ML to the high length of contract documents and training in a low resource environment. The largest open-source contract dataset so far, the Contract Understanding Atticus Dataset (CUAD) is utilized. Various pre-processing experiments and hyperparameter tuning have been carried out and we successfully managed to eclipse SOTA results presented for models in the CUAD dataset trained on RoBERTa-base. Our model, A-type-RoBERTa-base achieved an AUPR score of 46.6{\\%} compared to 42.6{\\%} on the original RoBERT-base. This model is utilized in our end to end contract understanding application which is able to take a contract and highlight the clauses a user is looking to find along with it{'}s descriptions to aid due diligence before signing. Alongside digital, i.e. searchable, contracts the system is capable of processing scanned, i.e. non-searchable, contracts using tesseract OCR. This application is aimed to not only make contract review a comprehensible process to non-legal professionals, but also to help lawyers and attorneys more efficiently review contracts.\",\n}\n```\n \n \n# Glossary [optional]\nMore information needed\n \n# More Information [optional]\n \nMore information needed\n \n# Model Card Authors [optional]\n  \nMohammed Rakib in collaboration with Ezi Ozoani and the Hugging Face team\n \n# Model Card Contact\n \nMore information needed\n \n# How to Get Started with the Model\n \nUse the code below to get started with the model.\n \n<details>\n<summary> Click to expand </summary>\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForQuestionAnswering\n \ntokenizer = AutoTokenizer.from_pretrained(\"Rakib/roberta-base-on-cuad\")\n \nmodel = AutoModelForQuestionAnswering.from_pretrained(\"Rakib/roberta-base-on-cuad\")\n```\n</details>",
    "library_name": "transformers"
  },
  {
    "model_id": "qiyuw/WSPAlign-ft-kftt",
    "model_name": "qiyuw/WSPAlign-ft-kftt",
    "author": "qiyuw",
    "downloads": 16154,
    "likes": 0,
    "tags": [
      "transformers",
      "pytorch",
      "bert",
      "question-answering",
      "word alignment",
      "multilingual",
      "translation",
      "en",
      "de",
      "fr",
      "zh",
      "ja",
      "ro",
      "license:cc-by-nc-sa-4.0",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/qiyuw/WSPAlign-ft-kftt",
    "dependencies": [
      [
        "transformers",
        "4.36.2"
      ],
      [
        "SpaCy",
        null
      ],
      [
        "torch",
        null
      ],
      [
        "spacy",
        "3.7.2"
      ]
    ],
    "analysis_date": "2025-03-26T00:50:00.975345",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "bert",
    "pipeline_tag": "translation",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": [
        "en",
        "de",
        "fr",
        "zh",
        "ja",
        "ro"
      ],
      "license": "cc-by-nc-sa-4.0",
      "tags": [
        "word alignment",
        "multilingual",
        "translation"
      ]
    },
    "card_text": "# Model Description \nRefer to [https://github.com/qiyuw/WSPAlign](https://github.com/qiyuw/WSPAlign) and [https://github.com/qiyuw/WSPAlign.InferEval](https://github.com/qiyuw/WSPAlign.InferEval) for details.\n\n# Qucik Usage\n\nFirst clone inference repository:\n```\ngit clone https://github.com/qiyuw/WSPAlign.InferEval.git\n```\nThen install the requirements following [https://github.com/qiyuw/WSPAlign.InferEval](https://github.com/qiyuw/WSPAlign.InferEval). For inference only `transformers`, `SpaCy` and `torch` are required.\n\nFinally, run the following example:\n```\npython inference.py --model_name_or_path qiyuw/WSPAlign-ft-kftt --src_lang ja --src_text=\"\u79c1\u306f\u732b\u304c\u597d\u304d\u3067\u3059\u3002\" --tgt_lang en --tgt_text=\"I like cats.\"\n```\nCheck `inference.py` for details usage.\n\n# Citation\nCite our paper if WSPAlign helps your work:\n\n```bibtex\n@inproceedings{wu-etal-2023-wspalign,\n    title = \"{WSPA}lign: Word Alignment Pre-training via Large-Scale Weakly Supervised Span Prediction\",\n    author = \"Wu, Qiyu  and Nagata, Masaaki  and Tsuruoka, Yoshimasa\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.621\",\n    pages = \"11084--11099\",\n}\n```",
    "card_content": "---\nlanguage:\n- en\n- de\n- fr\n- zh\n- ja\n- ro\nlicense: cc-by-nc-sa-4.0\ntags:\n- word alignment\n- multilingual\n- translation\n---\n# Model Description \nRefer to [https://github.com/qiyuw/WSPAlign](https://github.com/qiyuw/WSPAlign) and [https://github.com/qiyuw/WSPAlign.InferEval](https://github.com/qiyuw/WSPAlign.InferEval) for details.\n\n# Qucik Usage\n\nFirst clone inference repository:\n```\ngit clone https://github.com/qiyuw/WSPAlign.InferEval.git\n```\nThen install the requirements following [https://github.com/qiyuw/WSPAlign.InferEval](https://github.com/qiyuw/WSPAlign.InferEval). For inference only `transformers`, `SpaCy` and `torch` are required.\n\nFinally, run the following example:\n```\npython inference.py --model_name_or_path qiyuw/WSPAlign-ft-kftt --src_lang ja --src_text=\"\u79c1\u306f\u732b\u304c\u597d\u304d\u3067\u3059\u3002\" --tgt_lang en --tgt_text=\"I like cats.\"\n```\nCheck `inference.py` for details usage.\n\n# Citation\nCite our paper if WSPAlign helps your work:\n\n```bibtex\n@inproceedings{wu-etal-2023-wspalign,\n    title = \"{WSPA}lign: Word Alignment Pre-training via Large-Scale Weakly Supervised Span Prediction\",\n    author = \"Wu, Qiyu  and Nagata, Masaaki  and Tsuruoka, Yoshimasa\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.621\",\n    pages = \"11084--11099\",\n}\n```",
    "library_name": "transformers"
  },
  {
    "model_id": "HPAI-BSC/Llama3-Aloe-8B-Alpha",
    "model_name": "HPAI-BSC/Llama3-Aloe-8B-Alpha",
    "author": "HPAI-BSC",
    "downloads": 15619,
    "likes": 59,
    "tags": [
      "transformers",
      "pytorch",
      "safetensors",
      "llama",
      "text-generation",
      "biology",
      "medical",
      "question-answering",
      "en",
      "dataset:argilla/dpo-mix-7k",
      "dataset:nvidia/HelpSteer",
      "dataset:jondurbin/airoboros-3.2",
      "dataset:hkust-nlp/deita-10k-v0",
      "dataset:LDJnr/Capybara",
      "dataset:HPAI-BSC/CareQA",
      "dataset:GBaker/MedQA-USMLE-4-options",
      "dataset:lukaemon/mmlu",
      "dataset:bigbio/pubmed_qa",
      "dataset:openlifescienceai/medmcqa",
      "dataset:bigbio/med_qa",
      "dataset:HPAI-BSC/better-safe-than-sorry",
      "dataset:HPAI-BSC/pubmedqa-cot",
      "dataset:HPAI-BSC/medmcqa-cot",
      "dataset:HPAI-BSC/medqa-cot",
      "arxiv:2405.01886",
      "license:cc-by-nc-4.0",
      "autotrain_compatible",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/HPAI-BSC/Llama3-Aloe-8B-Alpha",
    "dependencies": [
      [
        "transformers",
        "4.36.2"
      ],
      [
        "torch",
        "2.2.1"
      ],
      [
        "numpy",
        "1.26.3"
      ]
    ],
    "analysis_date": "2025-03-26T00:50:02.673327",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "llama",
    "pipeline_tag": "question-answering",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": [
        "en"
      ],
      "license": "cc-by-nc-4.0",
      "library_name": "transformers",
      "tags": [
        "biology",
        "medical"
      ],
      "datasets": [
        "argilla/dpo-mix-7k",
        "nvidia/HelpSteer",
        "jondurbin/airoboros-3.2",
        "hkust-nlp/deita-10k-v0",
        "LDJnr/Capybara",
        "HPAI-BSC/CareQA",
        "GBaker/MedQA-USMLE-4-options",
        "lukaemon/mmlu",
        "bigbio/pubmed_qa",
        "openlifescienceai/medmcqa",
        "bigbio/med_qa",
        "HPAI-BSC/better-safe-than-sorry",
        "HPAI-BSC/pubmedqa-cot",
        "HPAI-BSC/medmcqa-cot",
        "HPAI-BSC/medqa-cot"
      ],
      "pipeline_tag": "question-answering"
    },
    "card_text": "\nAVAILABLE NOW THE LATEST ITERATION OF THE ALOE FAMILY! [ALOE BETA 8B](https://huggingface.co/HPAI-BSC/Llama3.1-Aloe-Beta-8B) AND [ALOE BETA 70B](https://huggingface.co/HPAI-BSC/Llama3.1-Aloe-Beta-70B) VERSIONS. These include:\n* Better overall performance\n* More thorough alignment and safety\n* License compatible with more uses\n\n\n# Aloe: A New Family of Healthcare LLMs\n\nAloe is a new family of healthcare LLMs that is highly competitive with all previous open models of its range and reaches state-of-the-art results at its size by using model merging and advanced prompting strategies. Aloe scores high in metrics measuring ethics and factuality, thanks to a combined red teaming and alignment effort. Complete training details, model merging configurations, and all training data (including synthetically generated data) will be shared. Additionally, the prompting repository used in this work to produce state-of-the-art results during inference will also be shared. Aloe comes with a healthcare-specific risk assessment to contribute to the safe use and deployment of such systems.\n\n<img src=\"https://cdn-uploads.huggingface.co/production/uploads/62972c4979f193515da1d38e/xlssx5_3_kLQlJlmE-aya.png\" width=\"95%\">\n\n## Model Details\n\n### [](https://huggingface.co/templates/model-card-example#model-description)Model Description\n\n- **Developed by:**\u00a0[HPAI](https://hpai.bsc.es/)\n- **Model type:**\u00a0Causal decoder-only transformer language model\n- **Language(s) (NLP):**\u00a0English (mainly)\n- **License:**\u00a0This model is based on Meta Llama 3 8B and is governed by the [Meta Llama 3 License](https://llama.meta.com/llama3/license/). All our modifications are available with a [CC BY-NC 4.0](https://creativecommons.org/licenses/by-nc/4.0/) license.\n- **Finetuned from model :** [meta-llama/Meta-Llama-3-8B](https://huggingface.co/meta-llama/Meta-Llama-3-8B)\n\n### [](https://huggingface.co/templates/model-card-example#model-sources-optional)Model Sources [optional]\n\n- **Repository:**\u00a0https://github.com/HPAI-BSC/prompt_engine (more coming soon)\n- **Paper:** https://arxiv.org/abs/2405.01886 (more coming soon)\n\n## Model Performance\n\nAloe has been tested on the most popular healthcare QA datasets, with and without medprompting inference technique. Results show competitive performance, even against bigger models.\n\n<img src=\"https://cdn-uploads.huggingface.co/production/uploads/62f7a16192950415b637e201/rQ4z-qXzKN44oAcFDbHi2.png\" width=\"95%\">\n\nResults using advanced prompting methods (aka Medprompt) are achieved through a [repo](https://github.com/HPAI-BSC/prompt_engine) made public with this work.\n\n## Uses\n\n### Direct Use\n\nWe encourage the use of Aloe for research purposes, as a stepping stone to build better foundational models for healthcare.\n\n### Out-of-Scope Use\n\nThese models are not to be used for clinical practice, medical diagnosis, or any other form of direct or indirect healthcare advice. Models are prone to error and can produce toxic content. The use of Aloe models for activities harmful for individuals, such as spam, fraud, or impersonation, is prohibited.\n\n## Bias, Risks, and Limitations\n\nWe consider three risk cases:\n- Healthcare professional impersonation, a fraudulent behaviour which currently generates billions of dollars in [profit](https://www.justice.gov/opa/pr/justice-department-charges-dozens-12-billion-health-care-fraud). A model such as Aloe could be used to increase the efficacy of such deceiving activities, making them more widespread. The main preventive actions are public literacy on the unreliability of digitised information and the importance of medical registration, and legislation enforcing AI-generated content disclaimers. \n- Medical decision-making without professional supervision. While this is already an issue in modern societies (eg self-medication) a model such as Aloe, capable of producing high-quality conversational data, can facilitate self-delusion, particularly in the presence of sycophancy. By producing tailored responses, it can also be used to generate actionable answers. Public literacy on the dangers of self-diagnosis is one of the main defences, together with the introduction of disclaimers and warnings on the models' outputs. \n- Access to information on dangerous substances or procedures. While the literature on sensitive content can already be found on different sources (eg libraries, internet, dark web), LLMs can centralize such access, making it nearly impossible to control the flow of such information. Model alignment can help in that regard, but so far the effects remain insufficient, as jailbreaking methods still overcome it.\n\nTable below shows the performance of Aloe at several AI safety tasks:\n\n<img src=\"https://cdn-uploads.huggingface.co/production/uploads/62972c4979f193515da1d38e/T6Jblpf1kmTkM04K716rM.png\" width=\"95%\">\n\n### Recommendations\n\nWe avoid the use of all personal data in our training. Model safety cannot be guaranteed. Aloe can produce toxic content under the appropriate prompts. For these reasons, minors should not be left alone to interact with Aloe without supervision.\n\n## How to Get Started with the Model\n\nUse the code below to get started with the model. You can run conversational inference using the Transformers pipeline abstraction, or by leveraging the Auto classes with the `generate()` function. Let's see examples of both.\n\n#### Transformers pipeline\n\n```python\nimport transformers\nimport torch\n\nmodel_id = \"HPAI-BSC/Llama3-Aloe-8B-Alpha\"\n\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model_id,\n    model_kwargs={\"torch_dtype\": torch.bfloat16},\n    device_map=\"auto\",\n)\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are an expert medical assistant named Aloe, developed by the High Performance Artificial Intelligence Group at Barcelona Supercomputing Center(BSC). You are to be a helpful, respectful, and honest assistant.\"},\n    {\"role\": \"user\", \"content\": \"Hello.\"},\n]\n\nprompt = pipeline.tokenizer.apply_chat_template(\n\t\tmessages, \n\t\ttokenize=False, \n\t\tadd_generation_prompt=True\n)\n\nterminators = [\n    pipeline.tokenizer.eos_token_id,\n    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n]\n\noutputs = pipeline(\n    prompt,\n    max_new_tokens=256,\n    eos_token_id=terminators,\n    do_sample=True,\n    temperature=0.6,\n    top_p=0.9,\n)\nprint(outputs[0][\"generated_text\"][len(prompt):])\n```\n\n#### Transformers AutoModelForCausalLM\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\nmodel_id = \"HPAI-BSC/Llama3-Aloe-8B-Alpha\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\",\n)\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are an expert medical assistant named Aloe, developed by the High Performance Artificial Intelligence Group at Barcelona Supercomputing Center(BSC). You are to be a helpful, respectful, and honest assistant.\"},\n    {\"role\": \"user\", \"content\": \"Hello\"},\n]\n\ninput_ids = tokenizer.apply_chat_template(\n    messages,\n    add_generation_prompt=True,\n    return_tensors=\"pt\"\n).to(model.device)\n\nterminators = [\n    tokenizer.eos_token_id,\n    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n]\n\noutputs = model.generate(\n    input_ids,\n    max_new_tokens=256,\n    eos_token_id=terminators,\n    do_sample=True,\n    temperature=0.6,\n    top_p=0.9,\n)\nresponse = outputs[0][input_ids.shape[-1]:]\nprint(tokenizer.decode(response, skip_special_tokens=True))\n```\n\n## Training Details\n\nSupervised fine-tuning on top of Llama 3 8B using medical and general domain datasets, model merging using DARE-TIES process, two-stage DPO process for human preference alignment. More details coming soon.\n\n### Training Data\n\n- Medical domain datasets, including synthetic data generated using Mixtral-8x7B and Genstruct\n  - HPAI-BSC/pubmedqa-cot\n  - HPAI-BSC/medqa-cot\n  - HPAI-BSC/medmcqa-cot\n- LDJnr/Capybara\n- hkust-nlp/deita-10k-v0\n- jondurbin/airoboros-3.2\n- argilla/dpo-mix-7k\n- nvidia/HelpSteer\n- Custom preference data with adversarial prompts generated from Anthropic Harmless, Chen et al., and original prompts\n\n## Evaluation\n\n### Testing Data, Factors & Metrics\n\n#### Testing Data\n\n- [MedQA (USMLE)](https://huggingface.co/datasets/bigbio/med_qa)\n- [MedMCQA](https://huggingface.co/datasets/medmcqa)\n- [PubMedQA](https://huggingface.co/datasets/bigbio/pubmed_qa)\n- [MMLU-Medical](https://huggingface.co/datasets/lukaemon/mmlu)\n- [MedQA-4-Option](https://huggingface.co/datasets/GBaker/MedQA-USMLE-4-options)\n- [CareQA](https://huggingface.co/datasets/HPAI-BSC/CareQA)  \n\n#### Metrics\n\n- Accuracy: suite the evaluation of multiple-choice question-answering tasks.\n\n### Results\n\n<img src=\"https://cdn-uploads.huggingface.co/production/uploads/62972c4979f193515da1d38e/STlPSggXr9P9JeWAvmAsi.png\" width=\"90%\">\n\n#### Summary\n\nTo compare Aloe with the most competitive open models (both general purpose and healthcare-specific) we use popular healthcare datasets (PubMedQA, MedMCQA, MedQA and MMLU for six medical tasks only), together with the new and highly reliable CareQA. We produce the standard MultiMedQA score for reference, by computing the weighted average accuracy on all scores except CareQA. Additionally, we calculate the arithmetic mean across all datasets. The Medical MMLU is calculated by averaging the six medical subtasks: Anatomy, Clinical knowledge, College Biology, College medicine, Medical genetics, and Professional medicine.\n\nBenchmark results indicate the training conducted on Aloe has boosted its performance above Llama3-8B-Instruct. Llama3-Aloe-8B-Alpha outperforms larger models like Meditron 70B, and is close to larger base models, like Yi-34. For the former, this gain is consistent even when using SC-CoT, using their best-reported variant. All these results make Llama3-Aloe-8B-Alpha the best healthcare LLM of its size.\n\nWith the help of prompting techniques the performance of Llama3-Aloe-8B-Alpha is significantly improved. Medprompting in particular provides a 7% increase in reported accuracy, after which Llama3-Aloe-8B-Alpha only lags behind the ten times bigger Llama-3-70B-Instruct. This improvement is mostly consistent across medical fields. Llama3-Aloe-8B-Alpha with medprompting beats the performance of Meditron 70B with their self reported 20 shot SC-CoT in MMLU med and is slightly worse in the other benchmarks. \n\n## Environmental Impact\n\n- **Hardware Type:**\u00a04xH100\n- **Hours used:**\u00a07,000\n- **Hardware Provider:**\u00a0Barcelona Supercomputing Center\n- **Compute Region:**\u00a0Spain\n- **Carbon Emitted:**\u00a0439.25kg\n\n## Model Card Authors\n[Ashwin Kumar Gururajan](https://huggingface.co/G-AshwinKumar)\n\n## Model Card Contact\n\nmailto:hpai@bsc.es\n\n## Citations\n\nIf you use this repository in a published work, please cite the following papers as source:\n\n```\n@misc{gururajan2024aloe,\n      title={Aloe: A Family of Fine-tuned Open Healthcare LLMs}, \n      author={Ashwin Kumar Gururajan and Enrique Lopez-Cuena and Jordi Bayarri-Planas and Adrian Tormos and Daniel Hinjos and Pablo Bernabeu-Perez and Anna Arias-Duart and Pablo Agustin Martin-Torres and Lucia Urcelay-Ganzabal and Marta Gonzalez-Mallo and Sergio Alvarez-Napagao and Eduard Ayguad\u00e9-Parra and Ulises Cort\u00e9s Dario Garcia-Gasulla},\n      year={2024},\n      eprint={2405.01886},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n",
    "card_content": "---\nlanguage:\n- en\nlicense: cc-by-nc-4.0\nlibrary_name: transformers\ntags:\n- biology\n- medical\ndatasets:\n- argilla/dpo-mix-7k\n- nvidia/HelpSteer\n- jondurbin/airoboros-3.2\n- hkust-nlp/deita-10k-v0\n- LDJnr/Capybara\n- HPAI-BSC/CareQA\n- GBaker/MedQA-USMLE-4-options\n- lukaemon/mmlu\n- bigbio/pubmed_qa\n- openlifescienceai/medmcqa\n- bigbio/med_qa\n- HPAI-BSC/better-safe-than-sorry\n- HPAI-BSC/pubmedqa-cot\n- HPAI-BSC/medmcqa-cot\n- HPAI-BSC/medqa-cot\npipeline_tag: question-answering\n---\n\nAVAILABLE NOW THE LATEST ITERATION OF THE ALOE FAMILY! [ALOE BETA 8B](https://huggingface.co/HPAI-BSC/Llama3.1-Aloe-Beta-8B) AND [ALOE BETA 70B](https://huggingface.co/HPAI-BSC/Llama3.1-Aloe-Beta-70B) VERSIONS. These include:\n* Better overall performance\n* More thorough alignment and safety\n* License compatible with more uses\n\n\n# Aloe: A New Family of Healthcare LLMs\n\nAloe is a new family of healthcare LLMs that is highly competitive with all previous open models of its range and reaches state-of-the-art results at its size by using model merging and advanced prompting strategies. Aloe scores high in metrics measuring ethics and factuality, thanks to a combined red teaming and alignment effort. Complete training details, model merging configurations, and all training data (including synthetically generated data) will be shared. Additionally, the prompting repository used in this work to produce state-of-the-art results during inference will also be shared. Aloe comes with a healthcare-specific risk assessment to contribute to the safe use and deployment of such systems.\n\n<img src=\"https://cdn-uploads.huggingface.co/production/uploads/62972c4979f193515da1d38e/xlssx5_3_kLQlJlmE-aya.png\" width=\"95%\">\n\n## Model Details\n\n### [](https://huggingface.co/templates/model-card-example#model-description)Model Description\n\n- **Developed by:**\u00a0[HPAI](https://hpai.bsc.es/)\n- **Model type:**\u00a0Causal decoder-only transformer language model\n- **Language(s) (NLP):**\u00a0English (mainly)\n- **License:**\u00a0This model is based on Meta Llama 3 8B and is governed by the [Meta Llama 3 License](https://llama.meta.com/llama3/license/). All our modifications are available with a [CC BY-NC 4.0](https://creativecommons.org/licenses/by-nc/4.0/) license.\n- **Finetuned from model :** [meta-llama/Meta-Llama-3-8B](https://huggingface.co/meta-llama/Meta-Llama-3-8B)\n\n### [](https://huggingface.co/templates/model-card-example#model-sources-optional)Model Sources [optional]\n\n- **Repository:**\u00a0https://github.com/HPAI-BSC/prompt_engine (more coming soon)\n- **Paper:** https://arxiv.org/abs/2405.01886 (more coming soon)\n\n## Model Performance\n\nAloe has been tested on the most popular healthcare QA datasets, with and without medprompting inference technique. Results show competitive performance, even against bigger models.\n\n<img src=\"https://cdn-uploads.huggingface.co/production/uploads/62f7a16192950415b637e201/rQ4z-qXzKN44oAcFDbHi2.png\" width=\"95%\">\n\nResults using advanced prompting methods (aka Medprompt) are achieved through a [repo](https://github.com/HPAI-BSC/prompt_engine) made public with this work.\n\n## Uses\n\n### Direct Use\n\nWe encourage the use of Aloe for research purposes, as a stepping stone to build better foundational models for healthcare.\n\n### Out-of-Scope Use\n\nThese models are not to be used for clinical practice, medical diagnosis, or any other form of direct or indirect healthcare advice. Models are prone to error and can produce toxic content. The use of Aloe models for activities harmful for individuals, such as spam, fraud, or impersonation, is prohibited.\n\n## Bias, Risks, and Limitations\n\nWe consider three risk cases:\n- Healthcare professional impersonation, a fraudulent behaviour which currently generates billions of dollars in [profit](https://www.justice.gov/opa/pr/justice-department-charges-dozens-12-billion-health-care-fraud). A model such as Aloe could be used to increase the efficacy of such deceiving activities, making them more widespread. The main preventive actions are public literacy on the unreliability of digitised information and the importance of medical registration, and legislation enforcing AI-generated content disclaimers. \n- Medical decision-making without professional supervision. While this is already an issue in modern societies (eg self-medication) a model such as Aloe, capable of producing high-quality conversational data, can facilitate self-delusion, particularly in the presence of sycophancy. By producing tailored responses, it can also be used to generate actionable answers. Public literacy on the dangers of self-diagnosis is one of the main defences, together with the introduction of disclaimers and warnings on the models' outputs. \n- Access to information on dangerous substances or procedures. While the literature on sensitive content can already be found on different sources (eg libraries, internet, dark web), LLMs can centralize such access, making it nearly impossible to control the flow of such information. Model alignment can help in that regard, but so far the effects remain insufficient, as jailbreaking methods still overcome it.\n\nTable below shows the performance of Aloe at several AI safety tasks:\n\n<img src=\"https://cdn-uploads.huggingface.co/production/uploads/62972c4979f193515da1d38e/T6Jblpf1kmTkM04K716rM.png\" width=\"95%\">\n\n### Recommendations\n\nWe avoid the use of all personal data in our training. Model safety cannot be guaranteed. Aloe can produce toxic content under the appropriate prompts. For these reasons, minors should not be left alone to interact with Aloe without supervision.\n\n## How to Get Started with the Model\n\nUse the code below to get started with the model. You can run conversational inference using the Transformers pipeline abstraction, or by leveraging the Auto classes with the `generate()` function. Let's see examples of both.\n\n#### Transformers pipeline\n\n```python\nimport transformers\nimport torch\n\nmodel_id = \"HPAI-BSC/Llama3-Aloe-8B-Alpha\"\n\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model_id,\n    model_kwargs={\"torch_dtype\": torch.bfloat16},\n    device_map=\"auto\",\n)\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are an expert medical assistant named Aloe, developed by the High Performance Artificial Intelligence Group at Barcelona Supercomputing Center(BSC). You are to be a helpful, respectful, and honest assistant.\"},\n    {\"role\": \"user\", \"content\": \"Hello.\"},\n]\n\nprompt = pipeline.tokenizer.apply_chat_template(\n\t\tmessages, \n\t\ttokenize=False, \n\t\tadd_generation_prompt=True\n)\n\nterminators = [\n    pipeline.tokenizer.eos_token_id,\n    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n]\n\noutputs = pipeline(\n    prompt,\n    max_new_tokens=256,\n    eos_token_id=terminators,\n    do_sample=True,\n    temperature=0.6,\n    top_p=0.9,\n)\nprint(outputs[0][\"generated_text\"][len(prompt):])\n```\n\n#### Transformers AutoModelForCausalLM\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\nmodel_id = \"HPAI-BSC/Llama3-Aloe-8B-Alpha\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\",\n)\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are an expert medical assistant named Aloe, developed by the High Performance Artificial Intelligence Group at Barcelona Supercomputing Center(BSC). You are to be a helpful, respectful, and honest assistant.\"},\n    {\"role\": \"user\", \"content\": \"Hello\"},\n]\n\ninput_ids = tokenizer.apply_chat_template(\n    messages,\n    add_generation_prompt=True,\n    return_tensors=\"pt\"\n).to(model.device)\n\nterminators = [\n    tokenizer.eos_token_id,\n    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n]\n\noutputs = model.generate(\n    input_ids,\n    max_new_tokens=256,\n    eos_token_id=terminators,\n    do_sample=True,\n    temperature=0.6,\n    top_p=0.9,\n)\nresponse = outputs[0][input_ids.shape[-1]:]\nprint(tokenizer.decode(response, skip_special_tokens=True))\n```\n\n## Training Details\n\nSupervised fine-tuning on top of Llama 3 8B using medical and general domain datasets, model merging using DARE-TIES process, two-stage DPO process for human preference alignment. More details coming soon.\n\n### Training Data\n\n- Medical domain datasets, including synthetic data generated using Mixtral-8x7B and Genstruct\n  - HPAI-BSC/pubmedqa-cot\n  - HPAI-BSC/medqa-cot\n  - HPAI-BSC/medmcqa-cot\n- LDJnr/Capybara\n- hkust-nlp/deita-10k-v0\n- jondurbin/airoboros-3.2\n- argilla/dpo-mix-7k\n- nvidia/HelpSteer\n- Custom preference data with adversarial prompts generated from Anthropic Harmless, Chen et al., and original prompts\n\n## Evaluation\n\n### Testing Data, Factors & Metrics\n\n#### Testing Data\n\n- [MedQA (USMLE)](https://huggingface.co/datasets/bigbio/med_qa)\n- [MedMCQA](https://huggingface.co/datasets/medmcqa)\n- [PubMedQA](https://huggingface.co/datasets/bigbio/pubmed_qa)\n- [MMLU-Medical](https://huggingface.co/datasets/lukaemon/mmlu)\n- [MedQA-4-Option](https://huggingface.co/datasets/GBaker/MedQA-USMLE-4-options)\n- [CareQA](https://huggingface.co/datasets/HPAI-BSC/CareQA)  \n\n#### Metrics\n\n- Accuracy: suite the evaluation of multiple-choice question-answering tasks.\n\n### Results\n\n<img src=\"https://cdn-uploads.huggingface.co/production/uploads/62972c4979f193515da1d38e/STlPSggXr9P9JeWAvmAsi.png\" width=\"90%\">\n\n#### Summary\n\nTo compare Aloe with the most competitive open models (both general purpose and healthcare-specific) we use popular healthcare datasets (PubMedQA, MedMCQA, MedQA and MMLU for six medical tasks only), together with the new and highly reliable CareQA. We produce the standard MultiMedQA score for reference, by computing the weighted average accuracy on all scores except CareQA. Additionally, we calculate the arithmetic mean across all datasets. The Medical MMLU is calculated by averaging the six medical subtasks: Anatomy, Clinical knowledge, College Biology, College medicine, Medical genetics, and Professional medicine.\n\nBenchmark results indicate the training conducted on Aloe has boosted its performance above Llama3-8B-Instruct. Llama3-Aloe-8B-Alpha outperforms larger models like Meditron 70B, and is close to larger base models, like Yi-34. For the former, this gain is consistent even when using SC-CoT, using their best-reported variant. All these results make Llama3-Aloe-8B-Alpha the best healthcare LLM of its size.\n\nWith the help of prompting techniques the performance of Llama3-Aloe-8B-Alpha is significantly improved. Medprompting in particular provides a 7% increase in reported accuracy, after which Llama3-Aloe-8B-Alpha only lags behind the ten times bigger Llama-3-70B-Instruct. This improvement is mostly consistent across medical fields. Llama3-Aloe-8B-Alpha with medprompting beats the performance of Meditron 70B with their self reported 20 shot SC-CoT in MMLU med and is slightly worse in the other benchmarks. \n\n## Environmental Impact\n\n- **Hardware Type:**\u00a04xH100\n- **Hours used:**\u00a07,000\n- **Hardware Provider:**\u00a0Barcelona Supercomputing Center\n- **Compute Region:**\u00a0Spain\n- **Carbon Emitted:**\u00a0439.25kg\n\n## Model Card Authors\n[Ashwin Kumar Gururajan](https://huggingface.co/G-AshwinKumar)\n\n## Model Card Contact\n\nmailto:hpai@bsc.es\n\n## Citations\n\nIf you use this repository in a published work, please cite the following papers as source:\n\n```\n@misc{gururajan2024aloe,\n      title={Aloe: A Family of Fine-tuned Open Healthcare LLMs}, \n      author={Ashwin Kumar Gururajan and Enrique Lopez-Cuena and Jordi Bayarri-Planas and Adrian Tormos and Daniel Hinjos and Pablo Bernabeu-Perez and Anna Arias-Duart and Pablo Agustin Martin-Torres and Lucia Urcelay-Ganzabal and Marta Gonzalez-Mallo and Sergio Alvarez-Napagao and Eduard Ayguad\u00e9-Parra and Ulises Cort\u00e9s Dario Garcia-Gasulla},\n      year={2024},\n      eprint={2405.01886},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n",
    "library_name": "transformers"
  },
  {
    "model_id": "deepset/roberta-base-squad2-distilled",
    "model_name": "deepset/roberta-base-squad2-distilled",
    "author": "deepset",
    "downloads": 15128,
    "likes": 15,
    "tags": [
      "transformers",
      "pytorch",
      "safetensors",
      "roberta",
      "question-answering",
      "exbert",
      "en",
      "dataset:squad_v2",
      "license:mit",
      "model-index",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/deepset/roberta-base-squad2-distilled",
    "dependencies": [
      [
        "haystack-ai",
        "2.11.2"
      ],
      [
        "transformers",
        "4.36.2"
      ],
      [
        "torch",
        "2.5.1"
      ]
    ],
    "analysis_date": "2025-03-26T00:50:04.350715",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "roberta",
    "pipeline_tag": "question-answering",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": "en",
      "license": "mit",
      "tags": [
        "exbert"
      ],
      "datasets": [
        "squad_v2"
      ],
      "thumbnail": "https://thumb.tildacdn.com/tild3433-3637-4830-a533-353833613061/-/resize/720x/-/format/webp/germanquad.jpg",
      "model-index": [
        {
          "name": "deepset/roberta-base-squad2-distilled",
          "results": [
            {
              "task": {
                "type": "question-answering",
                "name": "Question Answering"
              },
              "dataset": {
                "name": "squad_v2",
                "type": "squad_v2",
                "config": "squad_v2",
                "split": "validation"
              },
              "metrics": [
                {
                  "type": "exact_match",
                  "value": 80.8593,
                  "name": "Exact Match",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMzVjNzkxNmNiNDkzNzdiYjJjZGM3ZTViMGJhOGM2ZjFmYjg1MjYxMDM2YzM5NWMwNDIyYzNlN2QwNGYyNDMzZSIsInZlcnNpb24iOjF9.Rgww8tf8D7nF2dh2U_DMrFzmp87k8s7RFibrDXSvQyA66PGWXwjlsd1552lzjHnNV5hvHUM1-h3PTuY_5p64BA"
                },
                {
                  "type": "f1",
                  "value": 84.0104,
                  "name": "F1",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiNTAyZDViNWYzNjA4OWQ5MzgyYmQ2ZDlhNWRhMTIzYTYxYzViMmI4NWE4ZGU5MzVhZTAwNTRlZmRlNWUwMjI0ZSIsInZlcnNpb24iOjF9.Er21BNgJ3jJXLuZtpubTYq9wCwO1i_VLQFwS5ET0e4eAYVVj0aOA40I5FvP5pZac3LjkCnVacxzsFWGCYVmnDA"
                }
              ]
            },
            {
              "task": {
                "type": "question-answering",
                "name": "Question Answering"
              },
              "dataset": {
                "name": "squad",
                "type": "squad",
                "config": "plain_text",
                "split": "validation"
              },
              "metrics": [
                {
                  "type": "exact_match",
                  "value": 86.225,
                  "name": "Exact Match"
                },
                {
                  "type": "f1",
                  "value": 92.483,
                  "name": "F1"
                }
              ]
            },
            {
              "task": {
                "type": "question-answering",
                "name": "Question Answering"
              },
              "dataset": {
                "name": "adversarial_qa",
                "type": "adversarial_qa",
                "config": "adversarialQA",
                "split": "validation"
              },
              "metrics": [
                {
                  "type": "exact_match",
                  "value": 29.9,
                  "name": "Exact Match"
                },
                {
                  "type": "f1",
                  "value": 41.183,
                  "name": "F1"
                }
              ]
            },
            {
              "task": {
                "type": "question-answering",
                "name": "Question Answering"
              },
              "dataset": {
                "name": "squad_adversarial",
                "type": "squad_adversarial",
                "config": "AddOneSent",
                "split": "validation"
              },
              "metrics": [
                {
                  "type": "exact_match",
                  "value": 79.071,
                  "name": "Exact Match"
                },
                {
                  "type": "f1",
                  "value": 84.472,
                  "name": "F1"
                }
              ]
            },
            {
              "task": {
                "type": "question-answering",
                "name": "Question Answering"
              },
              "dataset": {
                "name": "squadshifts amazon",
                "type": "squadshifts",
                "config": "amazon",
                "split": "test"
              },
              "metrics": [
                {
                  "type": "exact_match",
                  "value": 70.733,
                  "name": "Exact Match"
                },
                {
                  "type": "f1",
                  "value": 83.958,
                  "name": "F1"
                }
              ]
            },
            {
              "task": {
                "type": "question-answering",
                "name": "Question Answering"
              },
              "dataset": {
                "name": "squadshifts new_wiki",
                "type": "squadshifts",
                "config": "new_wiki",
                "split": "test"
              },
              "metrics": [
                {
                  "type": "exact_match",
                  "value": 82.011,
                  "name": "Exact Match"
                },
                {
                  "type": "f1",
                  "value": 91.092,
                  "name": "F1"
                }
              ]
            },
            {
              "task": {
                "type": "question-answering",
                "name": "Question Answering"
              },
              "dataset": {
                "name": "squadshifts nyt",
                "type": "squadshifts",
                "config": "nyt",
                "split": "test"
              },
              "metrics": [
                {
                  "type": "exact_match",
                  "value": 84.203,
                  "name": "Exact Match"
                },
                {
                  "type": "f1",
                  "value": 91.521,
                  "name": "F1"
                }
              ]
            },
            {
              "task": {
                "type": "question-answering",
                "name": "Question Answering"
              },
              "dataset": {
                "name": "squadshifts reddit",
                "type": "squadshifts",
                "config": "reddit",
                "split": "test"
              },
              "metrics": [
                {
                  "type": "exact_match",
                  "value": 72.029,
                  "name": "Exact Match"
                },
                {
                  "type": "f1",
                  "value": 83.454,
                  "name": "F1"
                }
              ]
            }
          ]
        }
      ]
    },
    "card_text": "\n# roberta-base distilled for Extractive QA \n\n## Overview\n**Language model:** deepset/roberta-base-squad2-distilled   \n**Language:** English  \n**Training data:** SQuAD 2.0 training set   \n**Eval data:** SQuAD 2.0 dev set   \n**Code:**  See [an example extractive QA pipeline built with Haystack](https://haystack.deepset.ai/tutorials/34_extractive_qa_pipeline)  \n**Infrastructure**: 4x V100 GPU  \n**Published**: Dec 8th, 2021\n\n## Details\n- haystack's distillation feature was used for training. deepset/roberta-large-squad2 was used as the teacher model.\n\n## Hyperparameters\n```\nbatch_size = 80\nn_epochs = 4\nmax_seq_len = 384\nlearning_rate = 3e-5\nlr_schedule = LinearWarmup\nembeds_dropout_prob = 0.1\ntemperature = 1.5\ndistillation_loss_weight = 0.75\n```\n\n## Usage\n\n### In Haystack\nHaystack is an AI orchestration framework to build customizable, production-ready LLM applications. You can use this model in Haystack to do extractive question answering on documents. \nTo load and run the model with [Haystack](https://github.com/deepset-ai/haystack/):\n```python\n# After running pip install haystack-ai \"transformers[torch,sentencepiece]\"\n\nfrom haystack import Document\nfrom haystack.components.readers import ExtractiveReader\n\ndocs = [\n    Document(content=\"Python is a popular programming language\"),\n    Document(content=\"python ist eine beliebte Programmiersprache\"),\n]\n\nreader = ExtractiveReader(model=\"deepset/roberta-base-squad2-distilled\")\nreader.warm_up()\n\nquestion = \"What is a popular programming language?\"\nresult = reader.run(query=question, documents=docs)\n# {'answers': [ExtractedAnswer(query='What is a popular programming language?', score=0.5740374326705933, data='python', document=Document(id=..., content: '...'), context=None, document_offset=ExtractedAnswer.Span(start=0, end=6),...)]}\n```\nFor a complete example with an extractive question answering pipeline that scales over many documents, check out the [corresponding Haystack tutorial](https://haystack.deepset.ai/tutorials/34_extractive_qa_pipeline).\n\n### In Transformers\n```python\nfrom transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n\nmodel_name = \"deepset/roberta-base-squad2-distilled\"\n\n# a) Get predictions\nnlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\nQA_input = {\n    'question': 'Why is model conversion important?',\n    'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\n}\nres = nlp(QA_input)\n\n# b) Load model & tokenizer\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n```\n\n## Performance\n```\n\"exact\": 79.8366040596311\n\"f1\": 83.916407079888\n```\n\n## Authors\n**Timo M\u00f6ller:** timo.moeller@deepset.ai    \n**Julian Risch:** julian.risch@deepset.ai    \n**Malte Pietsch:** malte.pietsch@deepset.ai    \n**Michel Bartels:** michel.bartels@deepset.ai    \n\n## About us\n\n<div class=\"grid lg:grid-cols-2 gap-x-4 gap-y-3\">\n    <div class=\"w-full h-40 object-cover mb-2 rounded-lg flex items-center justify-center\">\n         <img alt=\"\" src=\"https://raw.githubusercontent.com/deepset-ai/.github/main/deepset-logo-colored.png\" class=\"w-40\"/>\n     </div>\n     <div class=\"w-full h-40 object-cover mb-2 rounded-lg flex items-center justify-center\">\n         <img alt=\"\" src=\"https://raw.githubusercontent.com/deepset-ai/.github/main/haystack-logo-colored.png\" class=\"w-40\"/>\n     </div>\n</div>\n\n[deepset](http://deepset.ai/) is the company behind the production-ready open-source AI framework [Haystack](https://haystack.deepset.ai/).\n\nSome of our other work: \n- [Distilled roberta-base-squad2 (aka \"tinyroberta-squad2\")](https://huggingface.co/deepset/tinyroberta-squad2)\n- [German BERT](https://deepset.ai/german-bert), [GermanQuAD and GermanDPR](https://deepset.ai/germanquad), [German embedding model](https://huggingface.co/mixedbread-ai/deepset-mxbai-embed-de-large-v1)\n- [deepset Cloud](https://www.deepset.ai/deepset-cloud-product), [deepset Studio](https://www.deepset.ai/deepset-studio)\n\n## Get in touch and join the Haystack community\n\n<p>For more info on Haystack, visit our <strong><a href=\"https://github.com/deepset-ai/haystack\">GitHub</a></strong> repo and <strong><a href=\"https://docs.haystack.deepset.ai\">Documentation</a></strong>. \n\nWe also have a <strong><a class=\"h-7\" href=\"https://haystack.deepset.ai/community\">Discord community open to everyone!</a></strong></p>\n\n[Twitter](https://twitter.com/Haystack_AI) | [LinkedIn](https://www.linkedin.com/company/deepset-ai/) | [Discord](https://haystack.deepset.ai/community) | [GitHub Discussions](https://github.com/deepset-ai/haystack/discussions) | [Website](https://haystack.deepset.ai/) | [YouTube](https://www.youtube.com/@deepset_ai)\n\nBy the way: [we're hiring!](http://www.deepset.ai/jobs)",
    "card_content": "---\nlanguage: en\nlicense: mit\ntags:\n- exbert\ndatasets:\n- squad_v2\nthumbnail: https://thumb.tildacdn.com/tild3433-3637-4830-a533-353833613061/-/resize/720x/-/format/webp/germanquad.jpg\nmodel-index:\n- name: deepset/roberta-base-squad2-distilled\n  results:\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squad_v2\n      type: squad_v2\n      config: squad_v2\n      split: validation\n    metrics:\n    - type: exact_match\n      value: 80.8593\n      name: Exact Match\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMzVjNzkxNmNiNDkzNzdiYjJjZGM3ZTViMGJhOGM2ZjFmYjg1MjYxMDM2YzM5NWMwNDIyYzNlN2QwNGYyNDMzZSIsInZlcnNpb24iOjF9.Rgww8tf8D7nF2dh2U_DMrFzmp87k8s7RFibrDXSvQyA66PGWXwjlsd1552lzjHnNV5hvHUM1-h3PTuY_5p64BA\n    - type: f1\n      value: 84.0104\n      name: F1\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiNTAyZDViNWYzNjA4OWQ5MzgyYmQ2ZDlhNWRhMTIzYTYxYzViMmI4NWE4ZGU5MzVhZTAwNTRlZmRlNWUwMjI0ZSIsInZlcnNpb24iOjF9.Er21BNgJ3jJXLuZtpubTYq9wCwO1i_VLQFwS5ET0e4eAYVVj0aOA40I5FvP5pZac3LjkCnVacxzsFWGCYVmnDA\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squad\n      type: squad\n      config: plain_text\n      split: validation\n    metrics:\n    - type: exact_match\n      value: 86.225\n      name: Exact Match\n    - type: f1\n      value: 92.483\n      name: F1\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: adversarial_qa\n      type: adversarial_qa\n      config: adversarialQA\n      split: validation\n    metrics:\n    - type: exact_match\n      value: 29.9\n      name: Exact Match\n    - type: f1\n      value: 41.183\n      name: F1\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squad_adversarial\n      type: squad_adversarial\n      config: AddOneSent\n      split: validation\n    metrics:\n    - type: exact_match\n      value: 79.071\n      name: Exact Match\n    - type: f1\n      value: 84.472\n      name: F1\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squadshifts amazon\n      type: squadshifts\n      config: amazon\n      split: test\n    metrics:\n    - type: exact_match\n      value: 70.733\n      name: Exact Match\n    - type: f1\n      value: 83.958\n      name: F1\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squadshifts new_wiki\n      type: squadshifts\n      config: new_wiki\n      split: test\n    metrics:\n    - type: exact_match\n      value: 82.011\n      name: Exact Match\n    - type: f1\n      value: 91.092\n      name: F1\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squadshifts nyt\n      type: squadshifts\n      config: nyt\n      split: test\n    metrics:\n    - type: exact_match\n      value: 84.203\n      name: Exact Match\n    - type: f1\n      value: 91.521\n      name: F1\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squadshifts reddit\n      type: squadshifts\n      config: reddit\n      split: test\n    metrics:\n    - type: exact_match\n      value: 72.029\n      name: Exact Match\n    - type: f1\n      value: 83.454\n      name: F1\n---\n\n# roberta-base distilled for Extractive QA \n\n## Overview\n**Language model:** deepset/roberta-base-squad2-distilled   \n**Language:** English  \n**Training data:** SQuAD 2.0 training set   \n**Eval data:** SQuAD 2.0 dev set   \n**Code:**  See [an example extractive QA pipeline built with Haystack](https://haystack.deepset.ai/tutorials/34_extractive_qa_pipeline)  \n**Infrastructure**: 4x V100 GPU  \n**Published**: Dec 8th, 2021\n\n## Details\n- haystack's distillation feature was used for training. deepset/roberta-large-squad2 was used as the teacher model.\n\n## Hyperparameters\n```\nbatch_size = 80\nn_epochs = 4\nmax_seq_len = 384\nlearning_rate = 3e-5\nlr_schedule = LinearWarmup\nembeds_dropout_prob = 0.1\ntemperature = 1.5\ndistillation_loss_weight = 0.75\n```\n\n## Usage\n\n### In Haystack\nHaystack is an AI orchestration framework to build customizable, production-ready LLM applications. You can use this model in Haystack to do extractive question answering on documents. \nTo load and run the model with [Haystack](https://github.com/deepset-ai/haystack/):\n```python\n# After running pip install haystack-ai \"transformers[torch,sentencepiece]\"\n\nfrom haystack import Document\nfrom haystack.components.readers import ExtractiveReader\n\ndocs = [\n    Document(content=\"Python is a popular programming language\"),\n    Document(content=\"python ist eine beliebte Programmiersprache\"),\n]\n\nreader = ExtractiveReader(model=\"deepset/roberta-base-squad2-distilled\")\nreader.warm_up()\n\nquestion = \"What is a popular programming language?\"\nresult = reader.run(query=question, documents=docs)\n# {'answers': [ExtractedAnswer(query='What is a popular programming language?', score=0.5740374326705933, data='python', document=Document(id=..., content: '...'), context=None, document_offset=ExtractedAnswer.Span(start=0, end=6),...)]}\n```\nFor a complete example with an extractive question answering pipeline that scales over many documents, check out the [corresponding Haystack tutorial](https://haystack.deepset.ai/tutorials/34_extractive_qa_pipeline).\n\n### In Transformers\n```python\nfrom transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n\nmodel_name = \"deepset/roberta-base-squad2-distilled\"\n\n# a) Get predictions\nnlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\nQA_input = {\n    'question': 'Why is model conversion important?',\n    'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\n}\nres = nlp(QA_input)\n\n# b) Load model & tokenizer\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n```\n\n## Performance\n```\n\"exact\": 79.8366040596311\n\"f1\": 83.916407079888\n```\n\n## Authors\n**Timo M\u00f6ller:** timo.moeller@deepset.ai    \n**Julian Risch:** julian.risch@deepset.ai    \n**Malte Pietsch:** malte.pietsch@deepset.ai    \n**Michel Bartels:** michel.bartels@deepset.ai    \n\n## About us\n\n<div class=\"grid lg:grid-cols-2 gap-x-4 gap-y-3\">\n    <div class=\"w-full h-40 object-cover mb-2 rounded-lg flex items-center justify-center\">\n         <img alt=\"\" src=\"https://raw.githubusercontent.com/deepset-ai/.github/main/deepset-logo-colored.png\" class=\"w-40\"/>\n     </div>\n     <div class=\"w-full h-40 object-cover mb-2 rounded-lg flex items-center justify-center\">\n         <img alt=\"\" src=\"https://raw.githubusercontent.com/deepset-ai/.github/main/haystack-logo-colored.png\" class=\"w-40\"/>\n     </div>\n</div>\n\n[deepset](http://deepset.ai/) is the company behind the production-ready open-source AI framework [Haystack](https://haystack.deepset.ai/).\n\nSome of our other work: \n- [Distilled roberta-base-squad2 (aka \"tinyroberta-squad2\")](https://huggingface.co/deepset/tinyroberta-squad2)\n- [German BERT](https://deepset.ai/german-bert), [GermanQuAD and GermanDPR](https://deepset.ai/germanquad), [German embedding model](https://huggingface.co/mixedbread-ai/deepset-mxbai-embed-de-large-v1)\n- [deepset Cloud](https://www.deepset.ai/deepset-cloud-product), [deepset Studio](https://www.deepset.ai/deepset-studio)\n\n## Get in touch and join the Haystack community\n\n<p>For more info on Haystack, visit our <strong><a href=\"https://github.com/deepset-ai/haystack\">GitHub</a></strong> repo and <strong><a href=\"https://docs.haystack.deepset.ai\">Documentation</a></strong>. \n\nWe also have a <strong><a class=\"h-7\" href=\"https://haystack.deepset.ai/community\">Discord community open to everyone!</a></strong></p>\n\n[Twitter](https://twitter.com/Haystack_AI) | [LinkedIn](https://www.linkedin.com/company/deepset-ai/) | [Discord](https://haystack.deepset.ai/community) | [GitHub Discussions](https://github.com/deepset-ai/haystack/discussions) | [Website](https://haystack.deepset.ai/) | [YouTube](https://www.youtube.com/@deepset_ai)\n\nBy the way: [we're hiring!](http://www.deepset.ai/jobs)",
    "library_name": "transformers"
  },
  {
    "model_id": "timpal0l/mdeberta-v3-base-squad2",
    "model_name": "timpal0l/mdeberta-v3-base-squad2",
    "author": "timpal0l",
    "downloads": 15121,
    "likes": 241,
    "tags": [
      "transformers",
      "pytorch",
      "safetensors",
      "deberta-v2",
      "question-answering",
      "deberta",
      "deberta-v3",
      "mdeberta",
      "qa",
      "multilingual",
      "af",
      "am",
      "ar",
      "as",
      "az",
      "be",
      "bg",
      "bn",
      "br",
      "bs",
      "ca",
      "cs",
      "cy",
      "da",
      "de",
      "el",
      "en",
      "eo",
      "es",
      "et",
      "eu",
      "fa",
      "fi",
      "fr",
      "fy",
      "ga",
      "gd",
      "gl",
      "gu",
      "ha",
      "he",
      "hi",
      "hr",
      "hu",
      "hy",
      "id",
      "is",
      "it",
      "ja",
      "jv",
      "ka",
      "kk",
      "km",
      "kn",
      "ko",
      "ku",
      "ky",
      "la",
      "lo",
      "lt",
      "lv",
      "mg",
      "mk",
      "ml",
      "mn",
      "mr",
      "ms",
      "my",
      "ne",
      "nl",
      "no",
      "om",
      "or",
      "pa",
      "pl",
      "ps",
      "pt",
      "ro",
      "ru",
      "sa",
      "sd",
      "si",
      "sk",
      "sl",
      "so",
      "sq",
      "sr",
      "su",
      "sv",
      "sw",
      "ta",
      "te",
      "th",
      "tl",
      "tr",
      "ug",
      "uk",
      "ur",
      "uz",
      "vi",
      "xh",
      "yi",
      "zh",
      "dataset:squad_v2",
      "arxiv:2006.03654",
      "arxiv:2111.09543",
      "base_model:microsoft/mdeberta-v3-base",
      "base_model:finetune:microsoft/mdeberta-v3-base",
      "license:mit",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/timpal0l/mdeberta-v3-base-squad2",
    "dependencies": [
      [
        "transformers",
        "4.36.2"
      ],
      [
        "torch",
        "2.5.1"
      ]
    ],
    "analysis_date": "2025-03-26T00:50:05.473589",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "deberta-v2",
    "pipeline_tag": "question-answering",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": [
        "multilingual",
        "af",
        "am",
        "ar",
        "as",
        "az",
        "be",
        "bg",
        "bn",
        "br",
        "bs",
        "ca",
        "cs",
        "cy",
        "da",
        "de",
        "el",
        "en",
        "eo",
        "es",
        "et",
        "eu",
        "fa",
        "fi",
        "fr",
        "fy",
        "ga",
        "gd",
        "gl",
        "gu",
        "ha",
        "he",
        "hi",
        "hr",
        "hu",
        "hy",
        "id",
        "is",
        "it",
        "ja",
        "jv",
        "ka",
        "kk",
        "km",
        "kn",
        "ko",
        "ku",
        "ky",
        "la",
        "lo",
        "lt",
        "lv",
        "mg",
        "mk",
        "ml",
        "mn",
        "mr",
        "ms",
        "my",
        "ne",
        "nl",
        "no",
        "om",
        "or",
        "pa",
        "pl",
        "ps",
        "pt",
        "ro",
        "ru",
        "sa",
        "sd",
        "si",
        "sk",
        "sl",
        "so",
        "sq",
        "sr",
        "su",
        "sv",
        "sw",
        "ta",
        "te",
        "th",
        "tl",
        "tr",
        "ug",
        "uk",
        "ur",
        "uz",
        "vi",
        "xh",
        "yi",
        "zh"
      ],
      "license": "mit",
      "tags": [
        "deberta",
        "deberta-v3",
        "mdeberta",
        "question-answering",
        "qa",
        "multilingual"
      ],
      "datasets": [
        "squad_v2"
      ],
      "thumbnail": "https://huggingface.co/front/thumbnails/microsoft.png",
      "base_model": [
        "microsoft/mdeberta-v3-base"
      ]
    },
    "card_text": "## This model can be used for Extractive QA\nIt has been finetuned for 3 epochs on [SQuAD2.0](https://rajpurkar.github.io/SQuAD-explorer/).\n\n## Usage\n```python\nfrom transformers import pipeline\n\nqa_model = pipeline(\"question-answering\", \"timpal0l/mdeberta-v3-base-squad2\")\nquestion = \"Where do I live?\"\ncontext = \"My name is Tim and I live in Sweden.\"\nqa_model(question = question, context = context)\n# {'score': 0.975547730922699, 'start': 28, 'end': 36, 'answer': ' Sweden.'}\n```\n\n## Evaluation on SQuAD2.0 dev set\n```bash\n{\n    \"epoch\": 3.0,\n    \"eval_HasAns_exact\": 79.65587044534414,\n    \"eval_HasAns_f1\": 85.91387795001529,\n    \"eval_HasAns_total\": 5928,\n    \"eval_NoAns_exact\": 82.10260723296888,\n    \"eval_NoAns_f1\": 82.10260723296888,\n    \"eval_NoAns_total\": 5945,\n    \"eval_best_exact\": 80.8809904826076,\n    \"eval_best_exact_thresh\": 0.0,\n    \"eval_best_f1\": 84.00551406448994,\n    \"eval_best_f1_thresh\": 0.0,\n    \"eval_exact\": 80.8809904826076,\n    \"eval_f1\": 84.00551406449004,\n    \"eval_samples\": 12508,\n    \"eval_total\": 11873,\n    \"train_loss\": 0.7729689576483615,\n    \"train_runtime\": 9118.953,\n    \"train_samples\": 134891,\n    \"train_samples_per_second\": 44.377,\n    \"train_steps_per_second\": 0.925\n}\n``` \n## DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing\n\n[DeBERTa](https://arxiv.org/abs/2006.03654) improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder. With those two improvements, DeBERTa out perform RoBERTa on a majority of NLU tasks with 80GB training data. \n\nIn [DeBERTa V3](https://arxiv.org/abs/2111.09543), we further improved the efficiency of DeBERTa using ELECTRA-Style pre-training with Gradient Disentangled Embedding Sharing. Compared to DeBERTa,  our V3 version significantly improves the model performance on downstream tasks.  You can find more technique details about the new model from our [paper](https://arxiv.org/abs/2111.09543).\n\nPlease check the [official repository](https://github.com/microsoft/DeBERTa) for more implementation details and updates.\n\nmDeBERTa is multilingual version of DeBERTa which use the same structure as DeBERTa and was trained with CC100 multilingual data.\nThe mDeBERTa V3 base model comes with 12 layers and a hidden size of 768. It has 86M backbone parameters  with a vocabulary containing 250K tokens which introduces 190M parameters in the Embedding layer.  This model was trained using the 2.5T CC100 data as XLM-R.",
    "card_content": "---\nlanguage:\n- multilingual\n- af\n- am\n- ar\n- as\n- az\n- be\n- bg\n- bn\n- br\n- bs\n- ca\n- cs\n- cy\n- da\n- de\n- el\n- en\n- eo\n- es\n- et\n- eu\n- fa\n- fi\n- fr\n- fy\n- ga\n- gd\n- gl\n- gu\n- ha\n- he\n- hi\n- hr\n- hu\n- hy\n- id\n- is\n- it\n- ja\n- jv\n- ka\n- kk\n- km\n- kn\n- ko\n- ku\n- ky\n- la\n- lo\n- lt\n- lv\n- mg\n- mk\n- ml\n- mn\n- mr\n- ms\n- my\n- ne\n- nl\n- 'no'\n- om\n- or\n- pa\n- pl\n- ps\n- pt\n- ro\n- ru\n- sa\n- sd\n- si\n- sk\n- sl\n- so\n- sq\n- sr\n- su\n- sv\n- sw\n- ta\n- te\n- th\n- tl\n- tr\n- ug\n- uk\n- ur\n- uz\n- vi\n- xh\n- yi\n- zh\nlicense: mit\ntags:\n- deberta\n- deberta-v3\n- mdeberta\n- question-answering\n- qa\n- multilingual\ndatasets:\n- squad_v2\nthumbnail: https://huggingface.co/front/thumbnails/microsoft.png\nbase_model:\n- microsoft/mdeberta-v3-base\n---\n## This model can be used for Extractive QA\nIt has been finetuned for 3 epochs on [SQuAD2.0](https://rajpurkar.github.io/SQuAD-explorer/).\n\n## Usage\n```python\nfrom transformers import pipeline\n\nqa_model = pipeline(\"question-answering\", \"timpal0l/mdeberta-v3-base-squad2\")\nquestion = \"Where do I live?\"\ncontext = \"My name is Tim and I live in Sweden.\"\nqa_model(question = question, context = context)\n# {'score': 0.975547730922699, 'start': 28, 'end': 36, 'answer': ' Sweden.'}\n```\n\n## Evaluation on SQuAD2.0 dev set\n```bash\n{\n    \"epoch\": 3.0,\n    \"eval_HasAns_exact\": 79.65587044534414,\n    \"eval_HasAns_f1\": 85.91387795001529,\n    \"eval_HasAns_total\": 5928,\n    \"eval_NoAns_exact\": 82.10260723296888,\n    \"eval_NoAns_f1\": 82.10260723296888,\n    \"eval_NoAns_total\": 5945,\n    \"eval_best_exact\": 80.8809904826076,\n    \"eval_best_exact_thresh\": 0.0,\n    \"eval_best_f1\": 84.00551406448994,\n    \"eval_best_f1_thresh\": 0.0,\n    \"eval_exact\": 80.8809904826076,\n    \"eval_f1\": 84.00551406449004,\n    \"eval_samples\": 12508,\n    \"eval_total\": 11873,\n    \"train_loss\": 0.7729689576483615,\n    \"train_runtime\": 9118.953,\n    \"train_samples\": 134891,\n    \"train_samples_per_second\": 44.377,\n    \"train_steps_per_second\": 0.925\n}\n``` \n## DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing\n\n[DeBERTa](https://arxiv.org/abs/2006.03654) improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder. With those two improvements, DeBERTa out perform RoBERTa on a majority of NLU tasks with 80GB training data. \n\nIn [DeBERTa V3](https://arxiv.org/abs/2111.09543), we further improved the efficiency of DeBERTa using ELECTRA-Style pre-training with Gradient Disentangled Embedding Sharing. Compared to DeBERTa,  our V3 version significantly improves the model performance on downstream tasks.  You can find more technique details about the new model from our [paper](https://arxiv.org/abs/2111.09543).\n\nPlease check the [official repository](https://github.com/microsoft/DeBERTa) for more implementation details and updates.\n\nmDeBERTa is multilingual version of DeBERTa which use the same structure as DeBERTa and was trained with CC100 multilingual data.\nThe mDeBERTa V3 base model comes with 12 layers and a hidden size of 768. It has 86M backbone parameters  with a vocabulary containing 250K tokens which introduces 190M parameters in the Embedding layer.  This model was trained using the 2.5T CC100 data as XLM-R.",
    "library_name": "transformers"
  },
  {
    "model_id": "allenai/longformer-large-4096-finetuned-triviaqa",
    "model_name": "allenai/longformer-large-4096-finetuned-triviaqa",
    "author": "allenai",
    "downloads": 14398,
    "likes": 7,
    "tags": [
      "transformers",
      "pytorch",
      "tf",
      "longformer",
      "question-answering",
      "en",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/allenai/longformer-large-4096-finetuned-triviaqa",
    "dependencies": null,
    "analysis_date": "2025-03-26T00:50:06.631038",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "longformer",
    "pipeline_tag": "question-answering",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": "en"
    },
    "card_text": "",
    "card_content": "---\nlanguage: en\n---\n",
    "library_name": "transformers"
  },
  {
    "model_id": "deepset/tinyroberta-squad2",
    "model_name": "deepset/tinyroberta-squad2",
    "author": "deepset",
    "downloads": 14149,
    "likes": 105,
    "tags": [
      "transformers",
      "pytorch",
      "safetensors",
      "roberta",
      "question-answering",
      "en",
      "dataset:squad_v2",
      "arxiv:1909.10351",
      "license:cc-by-4.0",
      "model-index",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/deepset/tinyroberta-squad2",
    "dependencies": [
      [
        "haystack-ai",
        "2.11.2"
      ],
      [
        "transformers",
        "4.36.2"
      ],
      [
        "torch",
        "2.5.1"
      ]
    ],
    "analysis_date": "2025-03-26T00:50:08.340537",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "roberta",
    "pipeline_tag": "question-answering",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": "en",
      "license": "cc-by-4.0",
      "datasets": [
        "squad_v2"
      ],
      "model-index": [
        {
          "name": "deepset/tinyroberta-squad2",
          "results": [
            {
              "task": {
                "type": "question-answering",
                "name": "Question Answering"
              },
              "dataset": {
                "name": "squad_v2",
                "type": "squad_v2",
                "config": "squad_v2",
                "split": "validation"
              },
              "metrics": [
                {
                  "type": "exact_match",
                  "value": 78.8627,
                  "name": "Exact Match",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiNDNlZDU4ODAxMzY5NGFiMTMyZmQ1M2ZhZjMyODA1NmFlOGMxNzYxNTA4OGE5YTBkZWViZjBkNGQ2ZmMxZjVlMCIsInZlcnNpb24iOjF9.Wgu599r6TvgMLTrHlLMVAbUtKD_3b70iJ5QSeDQ-bRfUsVk6Sz9OsJCp47riHJVlmSYzcDj_z_3jTcUjCFFXBg"
                },
                {
                  "type": "f1",
                  "value": 82.0355,
                  "name": "F1",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiOTFkMzEzMWNiZDRhMGZlODhkYzcwZTZiMDFjZDg2YjllZmUzYWM5NTgwNGQ2NGYyMDk2ZGQwN2JmMTE5NTc3YiIsInZlcnNpb24iOjF9.ChgaYpuRHd5WeDFjtiAHUyczxtoOD_M5WR8834jtbf7wXhdGOnZKdZ1KclmhoI5NuAGc1NptX-G0zQ5FTHEcBA"
                }
              ]
            },
            {
              "task": {
                "type": "question-answering",
                "name": "Question Answering"
              },
              "dataset": {
                "name": "squad",
                "type": "squad",
                "config": "plain_text",
                "split": "validation"
              },
              "metrics": [
                {
                  "type": "exact_match",
                  "value": 83.86,
                  "name": "Exact Match"
                },
                {
                  "type": "f1",
                  "value": 90.752,
                  "name": "F1"
                }
              ]
            },
            {
              "task": {
                "type": "question-answering",
                "name": "Question Answering"
              },
              "dataset": {
                "name": "adversarial_qa",
                "type": "adversarial_qa",
                "config": "adversarialQA",
                "split": "validation"
              },
              "metrics": [
                {
                  "type": "exact_match",
                  "value": 25.967,
                  "name": "Exact Match"
                },
                {
                  "type": "f1",
                  "value": 37.006,
                  "name": "F1"
                }
              ]
            },
            {
              "task": {
                "type": "question-answering",
                "name": "Question Answering"
              },
              "dataset": {
                "name": "squad_adversarial",
                "type": "squad_adversarial",
                "config": "AddOneSent",
                "split": "validation"
              },
              "metrics": [
                {
                  "type": "exact_match",
                  "value": 76.329,
                  "name": "Exact Match"
                },
                {
                  "type": "f1",
                  "value": 83.292,
                  "name": "F1"
                }
              ]
            },
            {
              "task": {
                "type": "question-answering",
                "name": "Question Answering"
              },
              "dataset": {
                "name": "squadshifts amazon",
                "type": "squadshifts",
                "config": "amazon",
                "split": "test"
              },
              "metrics": [
                {
                  "type": "exact_match",
                  "value": 63.915,
                  "name": "Exact Match"
                },
                {
                  "type": "f1",
                  "value": 78.395,
                  "name": "F1"
                }
              ]
            },
            {
              "task": {
                "type": "question-answering",
                "name": "Question Answering"
              },
              "dataset": {
                "name": "squadshifts new_wiki",
                "type": "squadshifts",
                "config": "new_wiki",
                "split": "test"
              },
              "metrics": [
                {
                  "type": "exact_match",
                  "value": 80.297,
                  "name": "Exact Match"
                },
                {
                  "type": "f1",
                  "value": 89.808,
                  "name": "F1"
                }
              ]
            },
            {
              "task": {
                "type": "question-answering",
                "name": "Question Answering"
              },
              "dataset": {
                "name": "squadshifts nyt",
                "type": "squadshifts",
                "config": "nyt",
                "split": "test"
              },
              "metrics": [
                {
                  "type": "exact_match",
                  "value": 80.149,
                  "name": "Exact Match"
                },
                {
                  "type": "f1",
                  "value": 88.321,
                  "name": "F1"
                }
              ]
            },
            {
              "task": {
                "type": "question-answering",
                "name": "Question Answering"
              },
              "dataset": {
                "name": "squadshifts reddit",
                "type": "squadshifts",
                "config": "reddit",
                "split": "test"
              },
              "metrics": [
                {
                  "type": "exact_match",
                  "value": 66.959,
                  "name": "Exact Match"
                },
                {
                  "type": "f1",
                  "value": 79.3,
                  "name": "F1"
                }
              ]
            }
          ]
        }
      ]
    },
    "card_text": "\n# tinyroberta for Extractive QA\n\nThis is the *distilled* version of the [deepset/roberta-base-squad2](https://huggingface.co/deepset/roberta-base-squad2) model. This model has a comparable prediction quality and runs at twice the speed of the base model.\n\n## Overview\n**Language model:** tinyroberta-squad2  \n**Language:** English  \n**Downstream-task:** Extractive QA  \n**Training data:** SQuAD 2.0  \n**Eval data:** SQuAD 2.0  \n**Code:**  See [an example extractive QA pipeline built with Haystack](https://haystack.deepset.ai/tutorials/34_extractive_qa_pipeline)  \n**Infrastructure**: 4x Tesla v100\n\n## Hyperparameters\n\n```\nbatch_size = 96\nn_epochs = 4\nbase_LM_model = \"deepset/tinyroberta-squad2-step1\"\nmax_seq_len = 384\nlearning_rate = 3e-5\nlr_schedule = LinearWarmup\nwarmup_proportion = 0.2\ndoc_stride = 128\nmax_query_length = 64\ndistillation_loss_weight = 0.75\ntemperature = 1.5\nteacher = \"deepset/robert-large-squad2\"\n``` \n\n## Distillation\nThis model was distilled using the TinyBERT approach described in [this paper](https://arxiv.org/pdf/1909.10351.pdf) and implemented in [haystack](https://github.com/deepset-ai/haystack).\nFirstly, we have performed intermediate layer distillation with roberta-base as the teacher which resulted in [deepset/tinyroberta-6l-768d](https://huggingface.co/deepset/tinyroberta-6l-768d).\nSecondly, we have performed task-specific distillation with [deepset/roberta-base-squad2](https://huggingface.co/deepset/roberta-base-squad2) as the teacher for further intermediate layer distillation on an augmented version of SQuADv2 and then with [deepset/roberta-large-squad2](https://huggingface.co/deepset/roberta-large-squad2) as the teacher for prediction layer distillation. \n\n## Usage\n\n### In Haystack\nHaystack is an AI orchestration framework to build customizable, production-ready LLM applications. You can use this model in Haystack to do extractive question answering on documents. \nTo load and run the model with [Haystack](https://github.com/deepset-ai/haystack/):\n```python\n# After running pip install haystack-ai \"transformers[torch,sentencepiece]\"\n\nfrom haystack import Document\nfrom haystack.components.readers import ExtractiveReader\n\ndocs = [\n    Document(content=\"Python is a popular programming language\"),\n    Document(content=\"python ist eine beliebte Programmiersprache\"),\n]\n\nreader = ExtractiveReader(model=\"deepset/tinyroberta-squad2\")\nreader.warm_up()\n\nquestion = \"What is a popular programming language?\"\nresult = reader.run(query=question, documents=docs)\n# {'answers': [ExtractedAnswer(query='What is a popular programming language?', score=0.5740374326705933, data='python', document=Document(id=..., content: '...'), context=None, document_offset=ExtractedAnswer.Span(start=0, end=6),...)]}\n```\nFor a complete example with an extractive question answering pipeline that scales over many documents, check out the [corresponding Haystack tutorial](https://haystack.deepset.ai/tutorials/34_extractive_qa_pipeline).\n\n### In Transformers\n```python\nfrom transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n\nmodel_name = \"deepset/tinyroberta-squad2\"\n\n# a) Get predictions\nnlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\nQA_input = {\n    'question': 'Why is model conversion important?',\n    'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\n}\nres = nlp(QA_input)\n\n# b) Load model & tokenizer\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n```\n\n## Performance\nEvaluated on the SQuAD 2.0 dev set with the [official eval script](https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/contents/blob/).\n\n```\n\"exact\": 78.69114798281817,\n\"f1\": 81.9198998536977,\n\n\"total\": 11873,\n\"HasAns_exact\": 76.19770580296895,\n\"HasAns_f1\": 82.66446878592329,\n\"HasAns_total\": 5928,\n\"NoAns_exact\": 81.17746005046257,\n\"NoAns_f1\": 81.17746005046257,\n\"NoAns_total\": 5945\n```\n\n## Authors\n**Branden Chan:** branden.chan@deepset.ai  \n**Timo M\u00f6ller:** timo.moeller@deepset.ai  \n**Malte Pietsch:** malte.pietsch@deepset.ai  \n**Tanay Soni:** tanay.soni@deepset.ai  \n**Michel Bartels:** michel.bartels@deepset.ai\n\n## About us\n\n<div class=\"grid lg:grid-cols-2 gap-x-4 gap-y-3\">\n    <div class=\"w-full h-40 object-cover mb-2 rounded-lg flex items-center justify-center\">\n         <img alt=\"\" src=\"https://raw.githubusercontent.com/deepset-ai/.github/main/deepset-logo-colored.png\" class=\"w-40\"/>\n     </div>\n     <div class=\"w-full h-40 object-cover mb-2 rounded-lg flex items-center justify-center\">\n         <img alt=\"\" src=\"https://raw.githubusercontent.com/deepset-ai/.github/main/haystack-logo-colored.png\" class=\"w-40\"/>\n     </div>\n</div>\n\n[deepset](http://deepset.ai/) is the company behind the production-ready open-source AI framework [Haystack](https://haystack.deepset.ai/).\n\nSome of our other work: \n- [Distilled roberta-base-squad2 (aka \"tinyroberta-squad2\")](https://huggingface.co/deepset/tinyroberta-squad2)\n- [German BERT](https://deepset.ai/german-bert), [GermanQuAD and GermanDPR](https://deepset.ai/germanquad), [German embedding model](https://huggingface.co/mixedbread-ai/deepset-mxbai-embed-de-large-v1)\n- [deepset Cloud](https://www.deepset.ai/deepset-cloud-product), [deepset Studio](https://www.deepset.ai/deepset-studio)\n\n## Get in touch and join the Haystack community\n\n<p>For more info on Haystack, visit our <strong><a href=\"https://github.com/deepset-ai/haystack\">GitHub</a></strong> repo and <strong><a href=\"https://docs.haystack.deepset.ai\">Documentation</a></strong>. \n\nWe also have a <strong><a class=\"h-7\" href=\"https://haystack.deepset.ai/community\">Discord community open to everyone!</a></strong></p>\n\n[Twitter](https://twitter.com/Haystack_AI) | [LinkedIn](https://www.linkedin.com/company/deepset-ai/) | [Discord](https://haystack.deepset.ai/community) | [GitHub Discussions](https://github.com/deepset-ai/haystack/discussions) | [Website](https://haystack.deepset.ai/) | [YouTube](https://www.youtube.com/@deepset_ai)\n\nBy the way: [we're hiring!](http://www.deepset.ai/jobs)",
    "card_content": "---\nlanguage: en\nlicense: cc-by-4.0\ndatasets:\n- squad_v2\nmodel-index:\n- name: deepset/tinyroberta-squad2\n  results:\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squad_v2\n      type: squad_v2\n      config: squad_v2\n      split: validation\n    metrics:\n    - type: exact_match\n      value: 78.8627\n      name: Exact Match\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiNDNlZDU4ODAxMzY5NGFiMTMyZmQ1M2ZhZjMyODA1NmFlOGMxNzYxNTA4OGE5YTBkZWViZjBkNGQ2ZmMxZjVlMCIsInZlcnNpb24iOjF9.Wgu599r6TvgMLTrHlLMVAbUtKD_3b70iJ5QSeDQ-bRfUsVk6Sz9OsJCp47riHJVlmSYzcDj_z_3jTcUjCFFXBg\n    - type: f1\n      value: 82.0355\n      name: F1\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiOTFkMzEzMWNiZDRhMGZlODhkYzcwZTZiMDFjZDg2YjllZmUzYWM5NTgwNGQ2NGYyMDk2ZGQwN2JmMTE5NTc3YiIsInZlcnNpb24iOjF9.ChgaYpuRHd5WeDFjtiAHUyczxtoOD_M5WR8834jtbf7wXhdGOnZKdZ1KclmhoI5NuAGc1NptX-G0zQ5FTHEcBA\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squad\n      type: squad\n      config: plain_text\n      split: validation\n    metrics:\n    - type: exact_match\n      value: 83.86\n      name: Exact Match\n    - type: f1\n      value: 90.752\n      name: F1\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: adversarial_qa\n      type: adversarial_qa\n      config: adversarialQA\n      split: validation\n    metrics:\n    - type: exact_match\n      value: 25.967\n      name: Exact Match\n    - type: f1\n      value: 37.006\n      name: F1\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squad_adversarial\n      type: squad_adversarial\n      config: AddOneSent\n      split: validation\n    metrics:\n    - type: exact_match\n      value: 76.329\n      name: Exact Match\n    - type: f1\n      value: 83.292\n      name: F1\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squadshifts amazon\n      type: squadshifts\n      config: amazon\n      split: test\n    metrics:\n    - type: exact_match\n      value: 63.915\n      name: Exact Match\n    - type: f1\n      value: 78.395\n      name: F1\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squadshifts new_wiki\n      type: squadshifts\n      config: new_wiki\n      split: test\n    metrics:\n    - type: exact_match\n      value: 80.297\n      name: Exact Match\n    - type: f1\n      value: 89.808\n      name: F1\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squadshifts nyt\n      type: squadshifts\n      config: nyt\n      split: test\n    metrics:\n    - type: exact_match\n      value: 80.149\n      name: Exact Match\n    - type: f1\n      value: 88.321\n      name: F1\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squadshifts reddit\n      type: squadshifts\n      config: reddit\n      split: test\n    metrics:\n    - type: exact_match\n      value: 66.959\n      name: Exact Match\n    - type: f1\n      value: 79.3\n      name: F1\n---\n\n# tinyroberta for Extractive QA\n\nThis is the *distilled* version of the [deepset/roberta-base-squad2](https://huggingface.co/deepset/roberta-base-squad2) model. This model has a comparable prediction quality and runs at twice the speed of the base model.\n\n## Overview\n**Language model:** tinyroberta-squad2  \n**Language:** English  \n**Downstream-task:** Extractive QA  \n**Training data:** SQuAD 2.0  \n**Eval data:** SQuAD 2.0  \n**Code:**  See [an example extractive QA pipeline built with Haystack](https://haystack.deepset.ai/tutorials/34_extractive_qa_pipeline)  \n**Infrastructure**: 4x Tesla v100\n\n## Hyperparameters\n\n```\nbatch_size = 96\nn_epochs = 4\nbase_LM_model = \"deepset/tinyroberta-squad2-step1\"\nmax_seq_len = 384\nlearning_rate = 3e-5\nlr_schedule = LinearWarmup\nwarmup_proportion = 0.2\ndoc_stride = 128\nmax_query_length = 64\ndistillation_loss_weight = 0.75\ntemperature = 1.5\nteacher = \"deepset/robert-large-squad2\"\n``` \n\n## Distillation\nThis model was distilled using the TinyBERT approach described in [this paper](https://arxiv.org/pdf/1909.10351.pdf) and implemented in [haystack](https://github.com/deepset-ai/haystack).\nFirstly, we have performed intermediate layer distillation with roberta-base as the teacher which resulted in [deepset/tinyroberta-6l-768d](https://huggingface.co/deepset/tinyroberta-6l-768d).\nSecondly, we have performed task-specific distillation with [deepset/roberta-base-squad2](https://huggingface.co/deepset/roberta-base-squad2) as the teacher for further intermediate layer distillation on an augmented version of SQuADv2 and then with [deepset/roberta-large-squad2](https://huggingface.co/deepset/roberta-large-squad2) as the teacher for prediction layer distillation. \n\n## Usage\n\n### In Haystack\nHaystack is an AI orchestration framework to build customizable, production-ready LLM applications. You can use this model in Haystack to do extractive question answering on documents. \nTo load and run the model with [Haystack](https://github.com/deepset-ai/haystack/):\n```python\n# After running pip install haystack-ai \"transformers[torch,sentencepiece]\"\n\nfrom haystack import Document\nfrom haystack.components.readers import ExtractiveReader\n\ndocs = [\n    Document(content=\"Python is a popular programming language\"),\n    Document(content=\"python ist eine beliebte Programmiersprache\"),\n]\n\nreader = ExtractiveReader(model=\"deepset/tinyroberta-squad2\")\nreader.warm_up()\n\nquestion = \"What is a popular programming language?\"\nresult = reader.run(query=question, documents=docs)\n# {'answers': [ExtractedAnswer(query='What is a popular programming language?', score=0.5740374326705933, data='python', document=Document(id=..., content: '...'), context=None, document_offset=ExtractedAnswer.Span(start=0, end=6),...)]}\n```\nFor a complete example with an extractive question answering pipeline that scales over many documents, check out the [corresponding Haystack tutorial](https://haystack.deepset.ai/tutorials/34_extractive_qa_pipeline).\n\n### In Transformers\n```python\nfrom transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n\nmodel_name = \"deepset/tinyroberta-squad2\"\n\n# a) Get predictions\nnlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\nQA_input = {\n    'question': 'Why is model conversion important?',\n    'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\n}\nres = nlp(QA_input)\n\n# b) Load model & tokenizer\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n```\n\n## Performance\nEvaluated on the SQuAD 2.0 dev set with the [official eval script](https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/contents/blob/).\n\n```\n\"exact\": 78.69114798281817,\n\"f1\": 81.9198998536977,\n\n\"total\": 11873,\n\"HasAns_exact\": 76.19770580296895,\n\"HasAns_f1\": 82.66446878592329,\n\"HasAns_total\": 5928,\n\"NoAns_exact\": 81.17746005046257,\n\"NoAns_f1\": 81.17746005046257,\n\"NoAns_total\": 5945\n```\n\n## Authors\n**Branden Chan:** branden.chan@deepset.ai  \n**Timo M\u00f6ller:** timo.moeller@deepset.ai  \n**Malte Pietsch:** malte.pietsch@deepset.ai  \n**Tanay Soni:** tanay.soni@deepset.ai  \n**Michel Bartels:** michel.bartels@deepset.ai\n\n## About us\n\n<div class=\"grid lg:grid-cols-2 gap-x-4 gap-y-3\">\n    <div class=\"w-full h-40 object-cover mb-2 rounded-lg flex items-center justify-center\">\n         <img alt=\"\" src=\"https://raw.githubusercontent.com/deepset-ai/.github/main/deepset-logo-colored.png\" class=\"w-40\"/>\n     </div>\n     <div class=\"w-full h-40 object-cover mb-2 rounded-lg flex items-center justify-center\">\n         <img alt=\"\" src=\"https://raw.githubusercontent.com/deepset-ai/.github/main/haystack-logo-colored.png\" class=\"w-40\"/>\n     </div>\n</div>\n\n[deepset](http://deepset.ai/) is the company behind the production-ready open-source AI framework [Haystack](https://haystack.deepset.ai/).\n\nSome of our other work: \n- [Distilled roberta-base-squad2 (aka \"tinyroberta-squad2\")](https://huggingface.co/deepset/tinyroberta-squad2)\n- [German BERT](https://deepset.ai/german-bert), [GermanQuAD and GermanDPR](https://deepset.ai/germanquad), [German embedding model](https://huggingface.co/mixedbread-ai/deepset-mxbai-embed-de-large-v1)\n- [deepset Cloud](https://www.deepset.ai/deepset-cloud-product), [deepset Studio](https://www.deepset.ai/deepset-studio)\n\n## Get in touch and join the Haystack community\n\n<p>For more info on Haystack, visit our <strong><a href=\"https://github.com/deepset-ai/haystack\">GitHub</a></strong> repo and <strong><a href=\"https://docs.haystack.deepset.ai\">Documentation</a></strong>. \n\nWe also have a <strong><a class=\"h-7\" href=\"https://haystack.deepset.ai/community\">Discord community open to everyone!</a></strong></p>\n\n[Twitter](https://twitter.com/Haystack_AI) | [LinkedIn](https://www.linkedin.com/company/deepset-ai/) | [Discord](https://haystack.deepset.ai/community) | [GitHub Discussions](https://github.com/deepset-ai/haystack/discussions) | [Website](https://haystack.deepset.ai/) | [YouTube](https://www.youtube.com/@deepset_ai)\n\nBy the way: [we're hiring!](http://www.deepset.ai/jobs)",
    "library_name": "transformers"
  },
  {
    "model_id": "ahotrod/electra_large_discriminator_squad2_512",
    "model_name": "ahotrod/electra_large_discriminator_squad2_512",
    "author": "ahotrod",
    "downloads": 11976,
    "likes": 6,
    "tags": [
      "transformers",
      "pytorch",
      "tf",
      "electra",
      "question-answering",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/ahotrod/electra_large_discriminator_squad2_512",
    "dependencies": [
      [
        "transformers",
        "2.11.0"
      ],
      [
        "torch",
        "1.5.0"
      ],
      [
        "tensorflow",
        "2.2.0"
      ],
      [
        "tokenizers",
        "0.13.3"
      ]
    ],
    "analysis_date": "2025-03-26T00:50:09.810007",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "electra",
    "pipeline_tag": "question-answering",
    "task_categories": null,
    "license": null,
    "card_metadata": {},
    "card_text": "## ELECTRA_large_discriminator language model fine-tuned on SQuAD2.0\n\n### with the following results:\n\n```\n  \"exact\": 87.09677419354838,\n  \"f1\": 89.98343832723452,\n  \"total\": 11873,\n  \"HasAns_exact\": 84.66599190283401,\n  \"HasAns_f1\": 90.44759839056285,\n  \"HasAns_total\": 5928,\n  \"NoAns_exact\": 89.52060555088309,\n  \"NoAns_f1\": 89.52060555088309,\n  \"NoAns_total\": 5945,\n  \"best_exact\": 87.09677419354838,\n  \"best_exact_thresh\": 0.0,\n  \"best_f1\": 89.98343832723432,\n  \"best_f1_thresh\": 0.0\n```\n### from script:\n```\npython ${EXAMPLES}/run_squad.py \\\n  --model_type electra \\\n  --model_name_or_path google/electra-large-discriminator \\\n  --do_train \\\n  --do_eval \\\n  --train_file ${SQUAD}/train-v2.0.json \\\n  --predict_file ${SQUAD}/dev-v2.0.json \\\n  --version_2_with_negative \\\n  --do_lower_case \\\n  --num_train_epochs 3 \\\n  --warmup_steps 306 \\\n  --weight_decay 0.01 \\\n  --learning_rate 3e-5 \\\n  --max_grad_norm 0.5 \\\n  --adam_epsilon 1e-6 \\\n  --max_seq_length 512 \\\n  --doc_stride 128 \\\n  --per_gpu_train_batch_size 8 \\\n  --gradient_accumulation_steps 16 \\\n  --per_gpu_eval_batch_size 128 \\\n  --fp16 \\\n  --fp16_opt_level O1 \\\n  --threads 12 \\\n  --logging_steps 50 \\\n  --save_steps 1000 \\\n  --overwrite_output_dir \\\n  --output_dir ${MODEL_PATH}\n```\n### using the following system & software:\n```\nTransformers: 2.11.0\nPyTorch: 1.5.0\nTensorFlow: 2.2.0\nPython: 3.8.1\nOS/Platform: Linux-5.3.0-59-generic-x86_64-with-glibc2.10\nCPU/GPU: Intel i9-9900K / NVIDIA Titan RTX 24GB\n```\n",
    "card_content": "---\n{}\n---\n## ELECTRA_large_discriminator language model fine-tuned on SQuAD2.0\n\n### with the following results:\n\n```\n  \"exact\": 87.09677419354838,\n  \"f1\": 89.98343832723452,\n  \"total\": 11873,\n  \"HasAns_exact\": 84.66599190283401,\n  \"HasAns_f1\": 90.44759839056285,\n  \"HasAns_total\": 5928,\n  \"NoAns_exact\": 89.52060555088309,\n  \"NoAns_f1\": 89.52060555088309,\n  \"NoAns_total\": 5945,\n  \"best_exact\": 87.09677419354838,\n  \"best_exact_thresh\": 0.0,\n  \"best_f1\": 89.98343832723432,\n  \"best_f1_thresh\": 0.0\n```\n### from script:\n```\npython ${EXAMPLES}/run_squad.py \\\n  --model_type electra \\\n  --model_name_or_path google/electra-large-discriminator \\\n  --do_train \\\n  --do_eval \\\n  --train_file ${SQUAD}/train-v2.0.json \\\n  --predict_file ${SQUAD}/dev-v2.0.json \\\n  --version_2_with_negative \\\n  --do_lower_case \\\n  --num_train_epochs 3 \\\n  --warmup_steps 306 \\\n  --weight_decay 0.01 \\\n  --learning_rate 3e-5 \\\n  --max_grad_norm 0.5 \\\n  --adam_epsilon 1e-6 \\\n  --max_seq_length 512 \\\n  --doc_stride 128 \\\n  --per_gpu_train_batch_size 8 \\\n  --gradient_accumulation_steps 16 \\\n  --per_gpu_eval_batch_size 128 \\\n  --fp16 \\\n  --fp16_opt_level O1 \\\n  --threads 12 \\\n  --logging_steps 50 \\\n  --save_steps 1000 \\\n  --overwrite_output_dir \\\n  --output_dir ${MODEL_PATH}\n```\n### using the following system & software:\n```\nTransformers: 2.11.0\nPyTorch: 1.5.0\nTensorFlow: 2.2.0\nPython: 3.8.1\nOS/Platform: Linux-5.3.0-59-generic-x86_64-with-glibc2.10\nCPU/GPU: Intel i9-9900K / NVIDIA Titan RTX 24GB\n```\n",
    "library_name": "transformers"
  },
  {
    "model_id": "deepset/deberta-v3-base-squad2",
    "model_name": "deepset/deberta-v3-base-squad2",
    "author": "deepset",
    "downloads": 9667,
    "likes": 20,
    "tags": [
      "transformers",
      "pytorch",
      "safetensors",
      "deberta-v2",
      "question-answering",
      "deberta",
      "deberta-v3",
      "en",
      "dataset:squad_v2",
      "base_model:microsoft/deberta-v3-base",
      "base_model:finetune:microsoft/deberta-v3-base",
      "license:cc-by-4.0",
      "model-index",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/deepset/deberta-v3-base-squad2",
    "dependencies": [
      [
        "haystack-ai",
        "2.11.2"
      ],
      [
        "transformers",
        "4.36.2"
      ],
      [
        "torch",
        "2.5.1"
      ]
    ],
    "analysis_date": "2025-03-26T00:50:11.558254",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "deberta-v2",
    "pipeline_tag": "question-answering",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": "en",
      "license": "cc-by-4.0",
      "tags": [
        "deberta",
        "deberta-v3"
      ],
      "datasets": [
        "squad_v2"
      ],
      "base_model": "microsoft/deberta-v3-base",
      "model-index": [
        {
          "name": "deepset/deberta-v3-base-squad2",
          "results": [
            {
              "task": {
                "type": "question-answering",
                "name": "Question Answering"
              },
              "dataset": {
                "name": "squad_v2",
                "type": "squad_v2",
                "config": "squad_v2",
                "split": "validation"
              },
              "metrics": [
                {
                  "type": "exact_match",
                  "value": 83.8248,
                  "name": "Exact Match",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiY2IyZTEyYzNlOTAwZmFlNWRiZTdiNzQzMTUyM2FmZTQ3ZWQwNWZmMzc2ZDVhYWYyMzkxOTUyMGNlMWY0M2E5MiIsInZlcnNpb24iOjF9.y8KvfefMLI977BYun0X1rAq5qudmezW_UJe9mh6sYBoiWaBosDO5TRnEGR1BHzdxmv2EgPK_PSomtZvb043jBQ"
                },
                {
                  "type": "f1",
                  "value": 87.41,
                  "name": "F1",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiOWVhNjAwM2Q5N2Y3MGU4ZWY3N2Y0MmNjYWYwYmQzNTdiYWExODhkYmQ1YjIwM2I1ODEzNWIxZDI1ZWQ1YWRjNSIsInZlcnNpb24iOjF9.Jk0v1ZheLRFz6k9iNAgCMMZtPYj5eVwUCku4E76wRYc-jHPmiUuxvNiNkn6NW-jkBD8bJGMqDSjJyVpVMn9pBA"
                }
              ]
            },
            {
              "task": {
                "type": "question-answering",
                "name": "Question Answering"
              },
              "dataset": {
                "name": "squad",
                "type": "squad",
                "config": "plain_text",
                "split": "validation"
              },
              "metrics": [
                {
                  "type": "exact_match",
                  "value": 84.9678,
                  "name": "Exact Match",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiOWUxYTg4MzU3YTdmMDRmMGM0NjFjMTcwNGM3YzljM2RkMTc1ZGNhMDQwMTgwNGI0ZDE4ZGMxZTE3YjY5YzQ0ZiIsInZlcnNpb24iOjF9.KKaJ1UtikNe2g6T8XhLoWNtL9X4dHHyl_O4VZ5LreBT9nXneGc21lI1AW3n8KXTFGemzRpRMvmCDyKVDHucdDQ"
                },
                {
                  "type": "f1",
                  "value": 92.2777,
                  "name": "F1",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiNDU0ZTQwMzg4ZDY1ZWYxOGIxMzY2ODljZTBkMTNlYjA0ODBjNjcxNTg3ZDliYWU1YTdkYTM2NTIxOTg1MGM4OCIsInZlcnNpb24iOjF9.8VHg1BXx6gLw_K7MUK2QSE80Y9guiVR8n8K8nX4laGsLibxv5u_yDv9F3ahbUa1eZG_bbidl93TY2qFUiYHtAQ"
                }
              ]
            },
            {
              "task": {
                "type": "question-answering",
                "name": "Question Answering"
              },
              "dataset": {
                "name": "adversarial_qa",
                "type": "adversarial_qa",
                "config": "adversarialQA",
                "split": "validation"
              },
              "metrics": [
                {
                  "type": "exact_match",
                  "value": 30.733,
                  "name": "Exact Match"
                },
                {
                  "type": "f1",
                  "value": 44.099,
                  "name": "F1"
                }
              ]
            },
            {
              "task": {
                "type": "question-answering",
                "name": "Question Answering"
              },
              "dataset": {
                "name": "squad_adversarial",
                "type": "squad_adversarial",
                "config": "AddOneSent",
                "split": "validation"
              },
              "metrics": [
                {
                  "type": "exact_match",
                  "value": 79.295,
                  "name": "Exact Match"
                },
                {
                  "type": "f1",
                  "value": 86.609,
                  "name": "F1"
                }
              ]
            },
            {
              "task": {
                "type": "question-answering",
                "name": "Question Answering"
              },
              "dataset": {
                "name": "squadshifts amazon",
                "type": "squadshifts",
                "config": "amazon",
                "split": "test"
              },
              "metrics": [
                {
                  "type": "exact_match",
                  "value": 68.68,
                  "name": "Exact Match"
                },
                {
                  "type": "f1",
                  "value": 83.832,
                  "name": "F1"
                }
              ]
            },
            {
              "task": {
                "type": "question-answering",
                "name": "Question Answering"
              },
              "dataset": {
                "name": "squadshifts new_wiki",
                "type": "squadshifts",
                "config": "new_wiki",
                "split": "test"
              },
              "metrics": [
                {
                  "type": "exact_match",
                  "value": 80.171,
                  "name": "Exact Match"
                },
                {
                  "type": "f1",
                  "value": 90.452,
                  "name": "F1"
                }
              ]
            },
            {
              "task": {
                "type": "question-answering",
                "name": "Question Answering"
              },
              "dataset": {
                "name": "squadshifts nyt",
                "type": "squadshifts",
                "config": "nyt",
                "split": "test"
              },
              "metrics": [
                {
                  "type": "exact_match",
                  "value": 81.57,
                  "name": "Exact Match"
                },
                {
                  "type": "f1",
                  "value": 90.644,
                  "name": "F1"
                }
              ]
            },
            {
              "task": {
                "type": "question-answering",
                "name": "Question Answering"
              },
              "dataset": {
                "name": "squadshifts reddit",
                "type": "squadshifts",
                "config": "reddit",
                "split": "test"
              },
              "metrics": [
                {
                  "type": "exact_match",
                  "value": 66.99,
                  "name": "Exact Match"
                },
                {
                  "type": "f1",
                  "value": 80.231,
                  "name": "F1"
                }
              ]
            }
          ]
        }
      ]
    },
    "card_text": "\n# deberta-v3-base for Extractive QA \n\nThis is the [deberta-v3-base](https://huggingface.co/microsoft/deberta-v3-base) model, fine-tuned using the [SQuAD2.0](https://huggingface.co/datasets/squad_v2) dataset. It's been trained on question-answer pairs, including unanswerable questions, for the task of Question Answering. \n\n\n## Overview\n**Language model:** deberta-v3-base  \n**Language:** English  \n**Downstream-task:** Extractive QA  \n**Training data:** SQuAD 2.0  \n**Eval data:** SQuAD 2.0  \n**Code:**  See [an example extractive QA pipeline built with Haystack](https://haystack.deepset.ai/tutorials/34_extractive_qa_pipeline)  \n**Infrastructure**: 1x NVIDIA A10G\n\n## Hyperparameters\n\n```\nbatch_size = 12\nn_epochs = 4\nbase_LM_model = \"deberta-v3-base\"\nmax_seq_len = 512\nlearning_rate = 2e-5\nlr_schedule = LinearWarmup\nwarmup_proportion = 0.2\ndoc_stride = 128\nmax_query_length = 64\n``` \n\n## Usage\n\n### In Haystack\nHaystack is an AI orchestration framework to build customizable, production-ready LLM applications. You can use this model in Haystack to do extractive question answering on documents. \nTo load and run the model with [Haystack](https://github.com/deepset-ai/haystack/):\n```python\n# After running pip install haystack-ai \"transformers[torch,sentencepiece]\"\n\nfrom haystack import Document\nfrom haystack.components.readers import ExtractiveReader\n\ndocs = [\n    Document(content=\"Python is a popular programming language\"),\n    Document(content=\"python ist eine beliebte Programmiersprache\"),\n]\n\nreader = ExtractiveReader(model=\"deepset/roberta-base-squad2\")\nreader.warm_up()\n\nquestion = \"What is a popular programming language?\"\nresult = reader.run(query=question, documents=docs)\n# {'answers': [ExtractedAnswer(query='What is a popular programming language?', score=0.5740374326705933, data='python', document=Document(id=..., content: '...'), context=None, document_offset=ExtractedAnswer.Span(start=0, end=6),...)]}\n```\nFor a complete example with an extractive question answering pipeline that scales over many documents, check out the [corresponding Haystack tutorial](https://haystack.deepset.ai/tutorials/34_extractive_qa_pipeline).\n\n### In Transformers\n```python\nfrom transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n\nmodel_name = \"deepset/roberta-base-squad2\"\n\n# a) Get predictions\nnlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\nQA_input = {\n    'question': 'Why is model conversion important?',\n    'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\n}\nres = nlp(QA_input)\n\n# b) Load model & tokenizer\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n```\n\n\n## Authors\n**Sebastian Lee:** sebastian.lee [at] deepset.ai  \n**Timo M\u00f6ller:** timo.moeller [at] deepset.ai  \n**Malte Pietsch:** malte.pietsch [at] deepset.ai  \n\n## About us\n\n<div class=\"grid lg:grid-cols-2 gap-x-4 gap-y-3\">\n    <div class=\"w-full h-40 object-cover mb-2 rounded-lg flex items-center justify-center\">\n         <img alt=\"\" src=\"https://raw.githubusercontent.com/deepset-ai/.github/main/deepset-logo-colored.png\" class=\"w-40\"/>\n     </div>\n     <div class=\"w-full h-40 object-cover mb-2 rounded-lg flex items-center justify-center\">\n         <img alt=\"\" src=\"https://raw.githubusercontent.com/deepset-ai/.github/main/haystack-logo-colored.png\" class=\"w-40\"/>\n     </div>\n</div>\n\n[deepset](http://deepset.ai/) is the company behind the production-ready open-source AI framework [Haystack](https://haystack.deepset.ai/).\n\nSome of our other work: \n- [Distilled roberta-base-squad2 (aka \"tinyroberta-squad2\")](https://huggingface.co/deepset/tinyroberta-squad2)\n- [German BERT](https://deepset.ai/german-bert), [GermanQuAD and GermanDPR](https://deepset.ai/germanquad), [German embedding model](https://huggingface.co/mixedbread-ai/deepset-mxbai-embed-de-large-v1)\n- [deepset Cloud](https://www.deepset.ai/deepset-cloud-product), [deepset Studio](https://www.deepset.ai/deepset-studio)\n\n## Get in touch and join the Haystack community\n\n<p>For more info on Haystack, visit our <strong><a href=\"https://github.com/deepset-ai/haystack\">GitHub</a></strong> repo and <strong><a href=\"https://docs.haystack.deepset.ai\">Documentation</a></strong>. \n\nWe also have a <strong><a class=\"h-7\" href=\"https://haystack.deepset.ai/community\">Discord community open to everyone!</a></strong></p>\n\n[Twitter](https://twitter.com/Haystack_AI) | [LinkedIn](https://www.linkedin.com/company/deepset-ai/) | [Discord](https://haystack.deepset.ai/community) | [GitHub Discussions](https://github.com/deepset-ai/haystack/discussions) | [Website](https://haystack.deepset.ai/) | [YouTube](https://www.youtube.com/@deepset_ai)\n\nBy the way: [we're hiring!](http://www.deepset.ai/jobs)",
    "card_content": "---\nlanguage: en\nlicense: cc-by-4.0\ntags:\n- deberta\n- deberta-v3\ndatasets:\n- squad_v2\nbase_model: microsoft/deberta-v3-base\nmodel-index:\n- name: deepset/deberta-v3-base-squad2\n  results:\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squad_v2\n      type: squad_v2\n      config: squad_v2\n      split: validation\n    metrics:\n    - type: exact_match\n      value: 83.8248\n      name: Exact Match\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiY2IyZTEyYzNlOTAwZmFlNWRiZTdiNzQzMTUyM2FmZTQ3ZWQwNWZmMzc2ZDVhYWYyMzkxOTUyMGNlMWY0M2E5MiIsInZlcnNpb24iOjF9.y8KvfefMLI977BYun0X1rAq5qudmezW_UJe9mh6sYBoiWaBosDO5TRnEGR1BHzdxmv2EgPK_PSomtZvb043jBQ\n    - type: f1\n      value: 87.41\n      name: F1\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiOWVhNjAwM2Q5N2Y3MGU4ZWY3N2Y0MmNjYWYwYmQzNTdiYWExODhkYmQ1YjIwM2I1ODEzNWIxZDI1ZWQ1YWRjNSIsInZlcnNpb24iOjF9.Jk0v1ZheLRFz6k9iNAgCMMZtPYj5eVwUCku4E76wRYc-jHPmiUuxvNiNkn6NW-jkBD8bJGMqDSjJyVpVMn9pBA\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squad\n      type: squad\n      config: plain_text\n      split: validation\n    metrics:\n    - type: exact_match\n      value: 84.9678\n      name: Exact Match\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiOWUxYTg4MzU3YTdmMDRmMGM0NjFjMTcwNGM3YzljM2RkMTc1ZGNhMDQwMTgwNGI0ZDE4ZGMxZTE3YjY5YzQ0ZiIsInZlcnNpb24iOjF9.KKaJ1UtikNe2g6T8XhLoWNtL9X4dHHyl_O4VZ5LreBT9nXneGc21lI1AW3n8KXTFGemzRpRMvmCDyKVDHucdDQ\n    - type: f1\n      value: 92.2777\n      name: F1\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiNDU0ZTQwMzg4ZDY1ZWYxOGIxMzY2ODljZTBkMTNlYjA0ODBjNjcxNTg3ZDliYWU1YTdkYTM2NTIxOTg1MGM4OCIsInZlcnNpb24iOjF9.8VHg1BXx6gLw_K7MUK2QSE80Y9guiVR8n8K8nX4laGsLibxv5u_yDv9F3ahbUa1eZG_bbidl93TY2qFUiYHtAQ\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: adversarial_qa\n      type: adversarial_qa\n      config: adversarialQA\n      split: validation\n    metrics:\n    - type: exact_match\n      value: 30.733\n      name: Exact Match\n    - type: f1\n      value: 44.099\n      name: F1\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squad_adversarial\n      type: squad_adversarial\n      config: AddOneSent\n      split: validation\n    metrics:\n    - type: exact_match\n      value: 79.295\n      name: Exact Match\n    - type: f1\n      value: 86.609\n      name: F1\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squadshifts amazon\n      type: squadshifts\n      config: amazon\n      split: test\n    metrics:\n    - type: exact_match\n      value: 68.68\n      name: Exact Match\n    - type: f1\n      value: 83.832\n      name: F1\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squadshifts new_wiki\n      type: squadshifts\n      config: new_wiki\n      split: test\n    metrics:\n    - type: exact_match\n      value: 80.171\n      name: Exact Match\n    - type: f1\n      value: 90.452\n      name: F1\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squadshifts nyt\n      type: squadshifts\n      config: nyt\n      split: test\n    metrics:\n    - type: exact_match\n      value: 81.57\n      name: Exact Match\n    - type: f1\n      value: 90.644\n      name: F1\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squadshifts reddit\n      type: squadshifts\n      config: reddit\n      split: test\n    metrics:\n    - type: exact_match\n      value: 66.99\n      name: Exact Match\n    - type: f1\n      value: 80.231\n      name: F1\n---\n\n# deberta-v3-base for Extractive QA \n\nThis is the [deberta-v3-base](https://huggingface.co/microsoft/deberta-v3-base) model, fine-tuned using the [SQuAD2.0](https://huggingface.co/datasets/squad_v2) dataset. It's been trained on question-answer pairs, including unanswerable questions, for the task of Question Answering. \n\n\n## Overview\n**Language model:** deberta-v3-base  \n**Language:** English  \n**Downstream-task:** Extractive QA  \n**Training data:** SQuAD 2.0  \n**Eval data:** SQuAD 2.0  \n**Code:**  See [an example extractive QA pipeline built with Haystack](https://haystack.deepset.ai/tutorials/34_extractive_qa_pipeline)  \n**Infrastructure**: 1x NVIDIA A10G\n\n## Hyperparameters\n\n```\nbatch_size = 12\nn_epochs = 4\nbase_LM_model = \"deberta-v3-base\"\nmax_seq_len = 512\nlearning_rate = 2e-5\nlr_schedule = LinearWarmup\nwarmup_proportion = 0.2\ndoc_stride = 128\nmax_query_length = 64\n``` \n\n## Usage\n\n### In Haystack\nHaystack is an AI orchestration framework to build customizable, production-ready LLM applications. You can use this model in Haystack to do extractive question answering on documents. \nTo load and run the model with [Haystack](https://github.com/deepset-ai/haystack/):\n```python\n# After running pip install haystack-ai \"transformers[torch,sentencepiece]\"\n\nfrom haystack import Document\nfrom haystack.components.readers import ExtractiveReader\n\ndocs = [\n    Document(content=\"Python is a popular programming language\"),\n    Document(content=\"python ist eine beliebte Programmiersprache\"),\n]\n\nreader = ExtractiveReader(model=\"deepset/roberta-base-squad2\")\nreader.warm_up()\n\nquestion = \"What is a popular programming language?\"\nresult = reader.run(query=question, documents=docs)\n# {'answers': [ExtractedAnswer(query='What is a popular programming language?', score=0.5740374326705933, data='python', document=Document(id=..., content: '...'), context=None, document_offset=ExtractedAnswer.Span(start=0, end=6),...)]}\n```\nFor a complete example with an extractive question answering pipeline that scales over many documents, check out the [corresponding Haystack tutorial](https://haystack.deepset.ai/tutorials/34_extractive_qa_pipeline).\n\n### In Transformers\n```python\nfrom transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n\nmodel_name = \"deepset/roberta-base-squad2\"\n\n# a) Get predictions\nnlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\nQA_input = {\n    'question': 'Why is model conversion important?',\n    'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\n}\nres = nlp(QA_input)\n\n# b) Load model & tokenizer\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n```\n\n\n## Authors\n**Sebastian Lee:** sebastian.lee [at] deepset.ai  \n**Timo M\u00f6ller:** timo.moeller [at] deepset.ai  \n**Malte Pietsch:** malte.pietsch [at] deepset.ai  \n\n## About us\n\n<div class=\"grid lg:grid-cols-2 gap-x-4 gap-y-3\">\n    <div class=\"w-full h-40 object-cover mb-2 rounded-lg flex items-center justify-center\">\n         <img alt=\"\" src=\"https://raw.githubusercontent.com/deepset-ai/.github/main/deepset-logo-colored.png\" class=\"w-40\"/>\n     </div>\n     <div class=\"w-full h-40 object-cover mb-2 rounded-lg flex items-center justify-center\">\n         <img alt=\"\" src=\"https://raw.githubusercontent.com/deepset-ai/.github/main/haystack-logo-colored.png\" class=\"w-40\"/>\n     </div>\n</div>\n\n[deepset](http://deepset.ai/) is the company behind the production-ready open-source AI framework [Haystack](https://haystack.deepset.ai/).\n\nSome of our other work: \n- [Distilled roberta-base-squad2 (aka \"tinyroberta-squad2\")](https://huggingface.co/deepset/tinyroberta-squad2)\n- [German BERT](https://deepset.ai/german-bert), [GermanQuAD and GermanDPR](https://deepset.ai/germanquad), [German embedding model](https://huggingface.co/mixedbread-ai/deepset-mxbai-embed-de-large-v1)\n- [deepset Cloud](https://www.deepset.ai/deepset-cloud-product), [deepset Studio](https://www.deepset.ai/deepset-studio)\n\n## Get in touch and join the Haystack community\n\n<p>For more info on Haystack, visit our <strong><a href=\"https://github.com/deepset-ai/haystack\">GitHub</a></strong> repo and <strong><a href=\"https://docs.haystack.deepset.ai\">Documentation</a></strong>. \n\nWe also have a <strong><a class=\"h-7\" href=\"https://haystack.deepset.ai/community\">Discord community open to everyone!</a></strong></p>\n\n[Twitter](https://twitter.com/Haystack_AI) | [LinkedIn](https://www.linkedin.com/company/deepset-ai/) | [Discord](https://haystack.deepset.ai/community) | [GitHub Discussions](https://github.com/deepset-ai/haystack/discussions) | [Website](https://haystack.deepset.ai/) | [YouTube](https://www.youtube.com/@deepset_ai)\n\nBy the way: [we're hiring!](http://www.deepset.ai/jobs)",
    "library_name": "transformers"
  },
  {
    "model_id": "michiyasunaga/BioLinkBERT-large",
    "model_name": "michiyasunaga/BioLinkBERT-large",
    "author": "michiyasunaga",
    "downloads": 7731,
    "likes": 33,
    "tags": [
      "transformers",
      "pytorch",
      "bert",
      "feature-extraction",
      "exbert",
      "linkbert",
      "biolinkbert",
      "fill-mask",
      "question-answering",
      "text-classification",
      "token-classification",
      "en",
      "dataset:pubmed",
      "arxiv:2203.15827",
      "license:apache-2.0",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/michiyasunaga/BioLinkBERT-large",
    "dependencies": [
      [
        "transformers",
        "4.36.2"
      ],
      [
        "torch",
        "2.2.2"
      ],
      [
        "numpy",
        "1.26.3"
      ]
    ],
    "analysis_date": "2025-03-26T00:50:12.706791",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "bert",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": "en",
      "license": "apache-2.0",
      "tags": [
        "bert",
        "exbert",
        "linkbert",
        "biolinkbert",
        "feature-extraction",
        "fill-mask",
        "question-answering",
        "text-classification",
        "token-classification"
      ],
      "datasets": [
        "pubmed"
      ],
      "widget": [
        {
          "text": "Sunitinib is a tyrosine kinase inhibitor"
        }
      ]
    },
    "card_text": "\r\n## BioLinkBERT-large\r\n\r\nBioLinkBERT-large model pretrained on [PubMed](https://pubmed.ncbi.nlm.nih.gov/) abstracts along with citation link information. It is introduced in the paper [LinkBERT: Pretraining Language Models with Document Links (ACL 2022)](https://arxiv.org/abs/2203.15827). The code and data are available in [this repository](https://github.com/michiyasunaga/LinkBERT).\r\n\r\nThis model achieves state-of-the-art performance on several biomedical NLP benchmarks such as [BLURB](https://microsoft.github.io/BLURB/) and [MedQA-USMLE](https://github.com/jind11/MedQA).\r\n\r\n\r\n## Model description\r\n\r\nLinkBERT is a transformer encoder (BERT-like) model pretrained on a large corpus of documents. It is an improvement of BERT that newly captures **document links** such as hyperlinks and citation links to include knowledge that spans across multiple documents. Specifically, it was pretrained by feeding linked documents into the same language model context, besides a single document.\r\n\r\nLinkBERT can be used as a drop-in replacement for BERT. It achieves better performance for general language understanding tasks (e.g. text classification), and is also particularly effective for **knowledge-intensive** tasks (e.g. question answering) and **cross-document** tasks (e.g. reading comprehension, document retrieval).\r\n\r\n\r\n## Intended uses & limitations\r\n\r\nThe model can be used by fine-tuning on a downstream task, such as question answering, sequence classification, and token classification.\r\nYou can also use the raw model for feature extraction (i.e. obtaining embeddings for input text).\r\n\r\n\r\n### How to use\r\n\r\nTo use the model to get the features of a given text in PyTorch:\r\n\r\n```python\r\nfrom transformers import AutoTokenizer, AutoModel\r\ntokenizer = AutoTokenizer.from_pretrained('michiyasunaga/BioLinkBERT-large')\r\nmodel = AutoModel.from_pretrained('michiyasunaga/BioLinkBERT-large')\r\ninputs = tokenizer(\"Sunitinib is a tyrosine kinase inhibitor\", return_tensors=\"pt\")\r\noutputs = model(**inputs)\r\nlast_hidden_states = outputs.last_hidden_state\r\n```\r\n\r\nFor fine-tuning, you can use [this repository](https://github.com/michiyasunaga/LinkBERT) or follow any other BERT fine-tuning codebases.\r\n\r\n\r\n## Evaluation results\r\n\r\nWhen fine-tuned on downstream tasks, LinkBERT achieves the following results.\r\n\r\n**Biomedical benchmarks ([BLURB](https://microsoft.github.io/BLURB/), [MedQA](https://github.com/jind11/MedQA), [MMLU](https://github.com/hendrycks/test), etc.):** BioLinkBERT attains new state-of-the-art.\r\n\r\n|                         | BLURB score | PubMedQA | BioASQ   | MedQA-USMLE |\r\n| ----------------------  | --------    | -------- | -------  | --------    |\r\n| PubmedBERT-base         | 81.10       | 55.8     | 87.5     | 38.1        |\r\n| **BioLinkBERT-base**    | **83.39**   | **70.2** | **91.4** | **40.0** |\r\n| **BioLinkBERT-large**   | **84.30**   | **72.2** | **94.8** | **44.6** |\r\n\r\n|                         | MMLU-professional medicine     |\r\n| ----------------------  | --------  |\r\n| GPT-3 (175 params)      | 38.7      |\r\n| UnifiedQA (11B params)  | 43.2      |\r\n| **BioLinkBERT-large (340M params)** | **50.7**  |\r\n\r\n\r\n## Citation\r\n\r\nIf you find LinkBERT useful in your project, please cite the following:\r\n\r\n```bibtex\r\n@InProceedings{yasunaga2022linkbert,\r\n  author =  {Michihiro Yasunaga and Jure Leskovec and Percy Liang},\r\n  title =   {LinkBERT: Pretraining Language Models with Document Links},\r\n  year =    {2022},  \r\n  booktitle = {Association for Computational Linguistics (ACL)},  \r\n}\r\n```\r\n",
    "card_content": "---\r\nlanguage: en\r\nlicense: apache-2.0\r\ntags:\r\n- bert\r\n- exbert\r\n- linkbert\r\n- biolinkbert\r\n- feature-extraction\r\n- fill-mask\r\n- question-answering\r\n- text-classification\r\n- token-classification\r\ndatasets:\r\n- pubmed\r\nwidget:\r\n- text: Sunitinib is a tyrosine kinase inhibitor\r\n---\r\n\r\n## BioLinkBERT-large\r\n\r\nBioLinkBERT-large model pretrained on [PubMed](https://pubmed.ncbi.nlm.nih.gov/) abstracts along with citation link information. It is introduced in the paper [LinkBERT: Pretraining Language Models with Document Links (ACL 2022)](https://arxiv.org/abs/2203.15827). The code and data are available in [this repository](https://github.com/michiyasunaga/LinkBERT).\r\n\r\nThis model achieves state-of-the-art performance on several biomedical NLP benchmarks such as [BLURB](https://microsoft.github.io/BLURB/) and [MedQA-USMLE](https://github.com/jind11/MedQA).\r\n\r\n\r\n## Model description\r\n\r\nLinkBERT is a transformer encoder (BERT-like) model pretrained on a large corpus of documents. It is an improvement of BERT that newly captures **document links** such as hyperlinks and citation links to include knowledge that spans across multiple documents. Specifically, it was pretrained by feeding linked documents into the same language model context, besides a single document.\r\n\r\nLinkBERT can be used as a drop-in replacement for BERT. It achieves better performance for general language understanding tasks (e.g. text classification), and is also particularly effective for **knowledge-intensive** tasks (e.g. question answering) and **cross-document** tasks (e.g. reading comprehension, document retrieval).\r\n\r\n\r\n## Intended uses & limitations\r\n\r\nThe model can be used by fine-tuning on a downstream task, such as question answering, sequence classification, and token classification.\r\nYou can also use the raw model for feature extraction (i.e. obtaining embeddings for input text).\r\n\r\n\r\n### How to use\r\n\r\nTo use the model to get the features of a given text in PyTorch:\r\n\r\n```python\r\nfrom transformers import AutoTokenizer, AutoModel\r\ntokenizer = AutoTokenizer.from_pretrained('michiyasunaga/BioLinkBERT-large')\r\nmodel = AutoModel.from_pretrained('michiyasunaga/BioLinkBERT-large')\r\ninputs = tokenizer(\"Sunitinib is a tyrosine kinase inhibitor\", return_tensors=\"pt\")\r\noutputs = model(**inputs)\r\nlast_hidden_states = outputs.last_hidden_state\r\n```\r\n\r\nFor fine-tuning, you can use [this repository](https://github.com/michiyasunaga/LinkBERT) or follow any other BERT fine-tuning codebases.\r\n\r\n\r\n## Evaluation results\r\n\r\nWhen fine-tuned on downstream tasks, LinkBERT achieves the following results.\r\n\r\n**Biomedical benchmarks ([BLURB](https://microsoft.github.io/BLURB/), [MedQA](https://github.com/jind11/MedQA), [MMLU](https://github.com/hendrycks/test), etc.):** BioLinkBERT attains new state-of-the-art.\r\n\r\n|                         | BLURB score | PubMedQA | BioASQ   | MedQA-USMLE |\r\n| ----------------------  | --------    | -------- | -------  | --------    |\r\n| PubmedBERT-base         | 81.10       | 55.8     | 87.5     | 38.1        |\r\n| **BioLinkBERT-base**    | **83.39**   | **70.2** | **91.4** | **40.0** |\r\n| **BioLinkBERT-large**   | **84.30**   | **72.2** | **94.8** | **44.6** |\r\n\r\n|                         | MMLU-professional medicine     |\r\n| ----------------------  | --------  |\r\n| GPT-3 (175 params)      | 38.7      |\r\n| UnifiedQA (11B params)  | 43.2      |\r\n| **BioLinkBERT-large (340M params)** | **50.7**  |\r\n\r\n\r\n## Citation\r\n\r\nIf you find LinkBERT useful in your project, please cite the following:\r\n\r\n```bibtex\r\n@InProceedings{yasunaga2022linkbert,\r\n  author =  {Michihiro Yasunaga and Jure Leskovec and Percy Liang},\r\n  title =   {LinkBERT: Pretraining Language Models with Document Links},\r\n  year =    {2022},  \r\n  booktitle = {Association for Computational Linguistics (ACL)},  \r\n}\r\n```\r\n",
    "library_name": "transformers"
  },
  {
    "model_id": "SmallDoge/Doge-160M-Reason-Distill",
    "model_name": "SmallDoge/Doge-160M-Reason-Distill",
    "author": "SmallDoge",
    "downloads": 6360,
    "likes": 4,
    "tags": [
      "transformers",
      "safetensors",
      "doge",
      "text-generation",
      "trl",
      "sft",
      "grpo",
      "question-answering",
      "custom_code",
      "en",
      "dataset:open-thoughts/OpenThoughts-114k",
      "dataset:open-r1/OpenR1-Math-220k",
      "dataset:SmallDoge/Reason-Distill",
      "arxiv:2412.11834",
      "base_model:SmallDoge/Doge-160M",
      "base_model:finetune:SmallDoge/Doge-160M",
      "license:apache-2.0",
      "autotrain_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/SmallDoge/Doge-160M-Reason-Distill",
    "dependencies": [
      [
        "transformers",
        "4.36.2"
      ],
      [
        "torch",
        "2.2.0"
      ],
      [
        "numpy",
        "1.26.3"
      ]
    ],
    "analysis_date": "2025-03-26T00:50:14.016632",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "doge",
    "pipeline_tag": "question-answering",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": [
        "en"
      ],
      "license": "apache-2.0",
      "library_name": "transformers",
      "tags": [
        "trl",
        "sft",
        "grpo",
        "doge"
      ],
      "datasets": [
        "open-thoughts/OpenThoughts-114k",
        "open-r1/OpenR1-Math-220k",
        "SmallDoge/Reason-Distill"
      ],
      "base_model": [
        "SmallDoge/Doge-160M"
      ],
      "pipeline_tag": "question-answering"
    },
    "card_text": "\n\n# **Doge 160M Reason Distill**\n\n<div align=\"center\">\n  <img src=\"https://huggingface.co/spaces/SmallDoge/README/resolve/main/org_icon.png\" width=\"100%\" alt=\"SmallDoge\" />\n</div>\n<hr>\n<div align=\"center\">\n  <a href=\"https://discord.gg/P2yYH95N\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-Small%20Doges-7289da?logo=discord&logoColor=white&color=7289da\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://arxiv.org/abs/2412.11834\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"arXiv\" src=\"https://img.shields.io/static/v1?label=arXiv&message=2412.11834&color=B31B1B&logo=arXiv\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://github.com/SmallDoges/small-doge\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"GitHub\" src=\"https://img.shields.io/badge/GitHub-SmallDoge-181717?logo=github\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://github.com/SmallDoges/small-doge/blob/main/LICENSE\" style=\"margin: 2px;\">\n    <img alt=\"License\" src=\"https://img.shields.io/badge/License-Apache--2.0-blue.svg\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\nDoge uses Dynamic Mask Attention as sequence transformation and can use Multi-Layer Perceptron or Cross Domain Mixture of Experts as state transformation. Dynamic Mask Attention allows the Transformer to use self-attention during training and state space during inference, and Cross Domain Mixture of Experts can directly inherit the weights of Multi-Layer Perceptron for further training. This model is trained by [SmallDoge](https://huggingface.co/SmallDoge) community, for detailed algorithm and model architecture, please refer to [Wonderful Matrices](https://arxiv.org/abs/2412.11834), all training details and code are publicly available on the [small-doge](https://github.com/SmallDoges/small-doge) repository.\n\n\n## Uses\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig, TextStreamer\n\ntokenizer = AutoTokenizer.from_pretrained(\"SmallDoge/Doge-160M-Reason-Distill\")\nmodel = AutoModelForCausalLM.from_pretrained(\"SmallDoge/Doge-160M-Reason-Distill\", trust_remote_code=True)\n\ngeneration_config = GenerationConfig(\n      max_new_tokens=100, \n      use_cache=True, \n      do_sample=True, \n      temperature=0.8, \n      top_p=0.9,\n      repetition_penalty=1.0\n)\nsteamer = TextStreamer(\n      tokenizer=tokenizer, \n      skip_prompt=True\n)\n\nsystem_prompt = \"\"\"\nYour role as an assistant involves thoroughly exploring questions through a systematic long thinking process before providing the final precise and accurate solutions. This requires engaging in a comprehensive cycle of analysis, summarizing, exploration, reassessment, reflection, backtracing, and iteration to develop well-considered thinking process. Please structure your response into two main sections: Thought and Solution. In the Thought section, detail your reasoning process using the specified format: <|begin_of_thought|> {thought with steps separated with '\\n\\n'} <|end_of_thought|> Each step should include detailed considerations such as analisying questions, summarizing relevant findings, brainstorming new ideas, verifying the accuracy of the current steps, refining any errors, and revisiting previous steps. In the Solution section, based on various attempts, explorations, and reflections from the Thought section, systematically present the final solution that you deem correct. The solution should remain a logical, accurate, concise expression style and detail necessary step needed to reach the conclusion, formatted as follows: <|begin_of_solution|> {final formatted, precise, and clear solution} <|end_of_solution|> Now, try to solve the following question through the above guidelines:\n\"\"\".strip()\nprompt = \"Which number is bigger, 3.9 or 3.11?\"\nconversation = [\n    {\"role\": \"system\", \"content\": system_prompt},\n    {\"role\": \"user\", \"content\": prompt}\n]\ninputs = tokenizer.apply_chat_template(\n    conversation=conversation,\n    tokenize=True,\n    return_tensors=\"pt\",\n)\n\noutputs = model.generate(\n    inputs, \n    tokenizer=tokenizer,\n    generation_config=generation_config, \n    streamer=steamer\n)\n```\n\n\n## Model Details\n\nWe build the Doge-Reason-Distill by SFT on [Reason-Distill](https://huggingface.co/datasets/SmallDoge/Reason-Distill).\n\n> TODO: The larger model is under training and will be uploaded soon.\n\n**SFT**:\n| Model | Training Data | Epochs | Content Length | LR | Batch Size | Precision |\n|---|---|---|---|---|---|---|\n| [Doge-160M-Reason-Distil](https://huggingface.co/SmallDoge/Doge-160M-Reason-Distill) | [SmallDoge/Reason-Distill](https://huggingface.co/datasets/SmallDoge/Reason-Distill) | 2 | 4096 | 4e-4 | 0.5M | bfloat16 |\n\n\n**Procedure**:\n\n**SFT**:\n[<img src=\"https://raw.githubusercontent.com/wandb/assets/main/wandb-github-badge-28.svg\" alt=\"Visualize in Weights & Biases\" width=\"150\" height=\"24\"/>](https://wandb.ai/loser_cheems/huggingface/runs/zgk4eefz) \n\n\n**Environment**:\n- Image: nvcr.io/nvidia/pytorch:24.12-py3\n- Hardware: 1x NVIDIA RTX 4090\n- Software: Transformers, TRL\n\n\n## Citation\n\n```bibtex\n@misc{shi2024wonderfulmatrices,\n      title={Wonderful Matrices: Combining for a More Efficient and Effective Foundation Model Architecture}, \n      author={Jingze Shi and Bingheng Wu},\n      year={2024},\n      eprint={2412.11834},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG},\n      url={https://arxiv.org/abs/2412.11834}, \n}\n```",
    "card_content": "---\nlanguage:\n- en\nlicense: apache-2.0\nlibrary_name: transformers\ntags:\n- trl\n- sft\n- grpo\n- doge\ndatasets:\n- open-thoughts/OpenThoughts-114k\n- open-r1/OpenR1-Math-220k\n- SmallDoge/Reason-Distill\nbase_model:\n- SmallDoge/Doge-160M\npipeline_tag: question-answering\n---\n\n\n# **Doge 160M Reason Distill**\n\n<div align=\"center\">\n  <img src=\"https://huggingface.co/spaces/SmallDoge/README/resolve/main/org_icon.png\" width=\"100%\" alt=\"SmallDoge\" />\n</div>\n<hr>\n<div align=\"center\">\n  <a href=\"https://discord.gg/P2yYH95N\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-Small%20Doges-7289da?logo=discord&logoColor=white&color=7289da\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://arxiv.org/abs/2412.11834\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"arXiv\" src=\"https://img.shields.io/static/v1?label=arXiv&message=2412.11834&color=B31B1B&logo=arXiv\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://github.com/SmallDoges/small-doge\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"GitHub\" src=\"https://img.shields.io/badge/GitHub-SmallDoge-181717?logo=github\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://github.com/SmallDoges/small-doge/blob/main/LICENSE\" style=\"margin: 2px;\">\n    <img alt=\"License\" src=\"https://img.shields.io/badge/License-Apache--2.0-blue.svg\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\nDoge uses Dynamic Mask Attention as sequence transformation and can use Multi-Layer Perceptron or Cross Domain Mixture of Experts as state transformation. Dynamic Mask Attention allows the Transformer to use self-attention during training and state space during inference, and Cross Domain Mixture of Experts can directly inherit the weights of Multi-Layer Perceptron for further training. This model is trained by [SmallDoge](https://huggingface.co/SmallDoge) community, for detailed algorithm and model architecture, please refer to [Wonderful Matrices](https://arxiv.org/abs/2412.11834), all training details and code are publicly available on the [small-doge](https://github.com/SmallDoges/small-doge) repository.\n\n\n## Uses\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig, TextStreamer\n\ntokenizer = AutoTokenizer.from_pretrained(\"SmallDoge/Doge-160M-Reason-Distill\")\nmodel = AutoModelForCausalLM.from_pretrained(\"SmallDoge/Doge-160M-Reason-Distill\", trust_remote_code=True)\n\ngeneration_config = GenerationConfig(\n      max_new_tokens=100, \n      use_cache=True, \n      do_sample=True, \n      temperature=0.8, \n      top_p=0.9,\n      repetition_penalty=1.0\n)\nsteamer = TextStreamer(\n      tokenizer=tokenizer, \n      skip_prompt=True\n)\n\nsystem_prompt = \"\"\"\nYour role as an assistant involves thoroughly exploring questions through a systematic long thinking process before providing the final precise and accurate solutions. This requires engaging in a comprehensive cycle of analysis, summarizing, exploration, reassessment, reflection, backtracing, and iteration to develop well-considered thinking process. Please structure your response into two main sections: Thought and Solution. In the Thought section, detail your reasoning process using the specified format: <|begin_of_thought|> {thought with steps separated with '\\n\\n'} <|end_of_thought|> Each step should include detailed considerations such as analisying questions, summarizing relevant findings, brainstorming new ideas, verifying the accuracy of the current steps, refining any errors, and revisiting previous steps. In the Solution section, based on various attempts, explorations, and reflections from the Thought section, systematically present the final solution that you deem correct. The solution should remain a logical, accurate, concise expression style and detail necessary step needed to reach the conclusion, formatted as follows: <|begin_of_solution|> {final formatted, precise, and clear solution} <|end_of_solution|> Now, try to solve the following question through the above guidelines:\n\"\"\".strip()\nprompt = \"Which number is bigger, 3.9 or 3.11?\"\nconversation = [\n    {\"role\": \"system\", \"content\": system_prompt},\n    {\"role\": \"user\", \"content\": prompt}\n]\ninputs = tokenizer.apply_chat_template(\n    conversation=conversation,\n    tokenize=True,\n    return_tensors=\"pt\",\n)\n\noutputs = model.generate(\n    inputs, \n    tokenizer=tokenizer,\n    generation_config=generation_config, \n    streamer=steamer\n)\n```\n\n\n## Model Details\n\nWe build the Doge-Reason-Distill by SFT on [Reason-Distill](https://huggingface.co/datasets/SmallDoge/Reason-Distill).\n\n> TODO: The larger model is under training and will be uploaded soon.\n\n**SFT**:\n| Model | Training Data | Epochs | Content Length | LR | Batch Size | Precision |\n|---|---|---|---|---|---|---|\n| [Doge-160M-Reason-Distil](https://huggingface.co/SmallDoge/Doge-160M-Reason-Distill) | [SmallDoge/Reason-Distill](https://huggingface.co/datasets/SmallDoge/Reason-Distill) | 2 | 4096 | 4e-4 | 0.5M | bfloat16 |\n\n\n**Procedure**:\n\n**SFT**:\n[<img src=\"https://raw.githubusercontent.com/wandb/assets/main/wandb-github-badge-28.svg\" alt=\"Visualize in Weights & Biases\" width=\"150\" height=\"24\"/>](https://wandb.ai/loser_cheems/huggingface/runs/zgk4eefz) \n\n\n**Environment**:\n- Image: nvcr.io/nvidia/pytorch:24.12-py3\n- Hardware: 1x NVIDIA RTX 4090\n- Software: Transformers, TRL\n\n\n## Citation\n\n```bibtex\n@misc{shi2024wonderfulmatrices,\n      title={Wonderful Matrices: Combining for a More Efficient and Effective Foundation Model Architecture}, \n      author={Jingze Shi and Bingheng Wu},\n      year={2024},\n      eprint={2412.11834},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG},\n      url={https://arxiv.org/abs/2412.11834}, \n}\n```",
    "library_name": "transformers"
  },
  {
    "model_id": "SmallDoge/Doge-60M-Instruct",
    "model_name": "SmallDoge/Doge-60M-Instruct",
    "author": "SmallDoge",
    "downloads": 6236,
    "likes": 5,
    "tags": [
      "transformers",
      "safetensors",
      "doge",
      "text-generation",
      "trl",
      "sft",
      "dpo",
      "question-answering",
      "custom_code",
      "en",
      "dataset:HuggingFaceTB/smoltalk",
      "dataset:HuggingFaceH4/ultrafeedback_binarized",
      "arxiv:2412.11834",
      "base_model:SmallDoge/Doge-60M",
      "base_model:finetune:SmallDoge/Doge-60M",
      "license:apache-2.0",
      "autotrain_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/SmallDoge/Doge-60M-Instruct",
    "dependencies": [
      [
        "transformers",
        "4.36.2"
      ],
      [
        "torch",
        "2.2.0"
      ],
      [
        "numpy",
        "1.26.3"
      ]
    ],
    "analysis_date": "2025-03-26T00:50:15.129734",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "doge",
    "pipeline_tag": "question-answering",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": [
        "en"
      ],
      "license": "apache-2.0",
      "library_name": "transformers",
      "tags": [
        "trl",
        "sft",
        "dpo",
        "doge"
      ],
      "datasets": [
        "HuggingFaceTB/smoltalk",
        "HuggingFaceH4/ultrafeedback_binarized"
      ],
      "base_model": [
        "SmallDoge/Doge-60M"
      ],
      "pipeline_tag": "question-answering"
    },
    "card_text": "\n\n# **Doge 60M Instruct**\n\n<div align=\"center\">\n  <img src=\"https://huggingface.co/spaces/SmallDoge/README/resolve/main/org_icon.png\" width=\"100%\" alt=\"SmallDoge\" />\n</div>\n<hr>\n<div align=\"center\">\n  <a href=\"https://discord.gg/P2yYH95N\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-Small%20Doges-7289da?logo=discord&logoColor=white&color=7289da\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://arxiv.org/abs/2412.11834\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"arXiv\" src=\"https://img.shields.io/static/v1?label=arXiv&message=2412.11834&color=B31B1B&logo=arXiv\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://github.com/SmallDoges/small-doge\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"GitHub\" src=\"https://img.shields.io/badge/GitHub-SmallDoge-181717?logo=github\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://github.com/SmallDoges/small-doge/blob/main/LICENSE\" style=\"margin: 2px;\">\n    <img alt=\"License\" src=\"https://img.shields.io/badge/License-Apache--2.0-blue.svg\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\nDoge uses Dynamic Mask Attention as sequence transformation and can use Multi-Layer Perceptron or Cross Domain Mixture of Experts as state transformation. Dynamic Mask Attention allows the Transformer to use self-attention during training and state space during inference, and Cross Domain Mixture of Experts can directly inherit the weights of Multi-Layer Perceptron for further training. This model is trained by [SmallDoge](https://huggingface.co/SmallDoge) community, for detailed algorithm and model architecture, please refer to [Wonderful Matrices](https://arxiv.org/abs/2412.11834), all training details and code are publicly available on the [small-doge](https://github.com/SmallDoges/small-doge) repository.\n\n\n## Uses\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig, TextStreamer\n\ntokenizer = AutoTokenizer.from_pretrained(\"SmallDoge/Doge-60M-Instruct\")\nmodel = AutoModelForCausalLM.from_pretrained(\"SmallDoge/Doge-60M-Instruct\", trust_remote_code=True)\n\ngeneration_config = GenerationConfig(\n      max_new_tokens=100, \n      use_cache=True, \n      do_sample=True, \n      temperature=0.8, \n      top_p=0.9,\n      repetition_penalty=1.0\n)\nsteamer = TextStreamer(\n      tokenizer=tokenizer, \n      skip_prompt=True\n)\n\nprompt = \"Hi, how are you doing today?\"\nconversation = [\n      {\"role\": \"user\", \"content\": prompt}\n]\ninputs = tokenizer.apply_chat_template(\n    conversation=conversation,\n    tokenize=True,\n    return_tensors=\"pt\",\n)\n\noutputs = model.generate(\n    inputs, \n    tokenizer=tokenizer,\n    generation_config=generation_config, \n    streamer=steamer\n)\n```\n\n\n## Model Details\n\nWe build the Doge-Instruct by first SFT on [SmolTalk](https://huggingface.co/datasets/HuggingFaceTB/smoltalk) and then DPO on [UltraFeedback Binarized](https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized).\n\n> TODO: The larger model is under training and will be uploaded soon.\n\n**SFT**:\n| Model | Training Data | Epochs | Content Length | LR | Batch Size | Precision |\n|---|---|---|---|---|---|---|\n| [Doge-20M-Instruct-SFT](https://huggingface.co/SmallDoge/Doge-20M-Instruct-SFT) | [HuggingFaceTB/smoltalk](https://huggingface.co/datasets/HuggingFaceTB/smoltalk) | 2 | 2048 | 8e-4 | 0.25M | bfloat16 |\n| [Doge-60M-Instruct-SFT](https://huggingface.co/SmallDoge/Doge-60M-Instruct-SFT) | [HuggingFaceTB/smoltalk](https://huggingface.co/datasets/HuggingFaceTB/smoltalk) | 2 | 2048 | 6e-4 | 0.25M | bfloat16 |\n\n**DPO**:\n| Model | Training Data | Epochs | Content Length | LR | Batch Size | Precision |\n|---|---|---|---|---|---|---|\n| [Doge-20M-Instruct](https://huggingface.co/SmallDoge/Doge-20M-Instruct) | [HuggingFaceH4/ultrafeedback_binarized](https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized) | 2 | 1024 | 8e-5 | 0.125M | bfloat16 |\n| [Doge-60M-Instruct](https://huggingface.co/SmallDoge/Doge-60M-Instruct) | [HuggingFaceH4/ultrafeedback_binarized](https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized) | 2 | 1024 | 6e-5 | 0.125M | bfloat16 |\n\n\n**Procedure**:\n\n**SFT**:\n[<img src=\"https://raw.githubusercontent.com/wandb/assets/main/wandb-github-badge-28.svg\" alt=\"Visualize in Weights & Biases\" width=\"150\" height=\"24\"/>](https://wandb.ai/loser_cheems/huggingface/runs/ckbn4b5m) \n\n**DPO**:\n[<img src=\"https://raw.githubusercontent.com/wandb/assets/main/wandb-github-badge-28.svg\" alt=\"Visualize in Weights & Biases\" width=\"150\" height=\"24\"/>](https://wandb.ai/loser_cheems/huggingface/runs/3nk7mu5a)\n\n\n**Environment**:\n- Image: nvcr.io/nvidia/pytorch:24.12-py3\n- Hardware: 1x NVIDIA RTX 4090\n- Software: Transformers, TRL\n\n\n## Citation\n\n```bibtex\n@misc{shi2024wonderfulmatrices,\n      title={Wonderful Matrices: Combining for a More Efficient and Effective Foundation Model Architecture}, \n      author={Jingze Shi and Bingheng Wu},\n      year={2024},\n      eprint={2412.11834},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG},\n      url={https://arxiv.org/abs/2412.11834}, \n}\n```",
    "card_content": "---\nlanguage:\n- en\nlicense: apache-2.0\nlibrary_name: transformers\ntags:\n- trl\n- sft\n- dpo\n- doge\ndatasets:\n- HuggingFaceTB/smoltalk\n- HuggingFaceH4/ultrafeedback_binarized\nbase_model:\n- SmallDoge/Doge-60M\npipeline_tag: question-answering\n---\n\n\n# **Doge 60M Instruct**\n\n<div align=\"center\">\n  <img src=\"https://huggingface.co/spaces/SmallDoge/README/resolve/main/org_icon.png\" width=\"100%\" alt=\"SmallDoge\" />\n</div>\n<hr>\n<div align=\"center\">\n  <a href=\"https://discord.gg/P2yYH95N\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-Small%20Doges-7289da?logo=discord&logoColor=white&color=7289da\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://arxiv.org/abs/2412.11834\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"arXiv\" src=\"https://img.shields.io/static/v1?label=arXiv&message=2412.11834&color=B31B1B&logo=arXiv\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://github.com/SmallDoges/small-doge\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"GitHub\" src=\"https://img.shields.io/badge/GitHub-SmallDoge-181717?logo=github\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://github.com/SmallDoges/small-doge/blob/main/LICENSE\" style=\"margin: 2px;\">\n    <img alt=\"License\" src=\"https://img.shields.io/badge/License-Apache--2.0-blue.svg\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\nDoge uses Dynamic Mask Attention as sequence transformation and can use Multi-Layer Perceptron or Cross Domain Mixture of Experts as state transformation. Dynamic Mask Attention allows the Transformer to use self-attention during training and state space during inference, and Cross Domain Mixture of Experts can directly inherit the weights of Multi-Layer Perceptron for further training. This model is trained by [SmallDoge](https://huggingface.co/SmallDoge) community, for detailed algorithm and model architecture, please refer to [Wonderful Matrices](https://arxiv.org/abs/2412.11834), all training details and code are publicly available on the [small-doge](https://github.com/SmallDoges/small-doge) repository.\n\n\n## Uses\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig, TextStreamer\n\ntokenizer = AutoTokenizer.from_pretrained(\"SmallDoge/Doge-60M-Instruct\")\nmodel = AutoModelForCausalLM.from_pretrained(\"SmallDoge/Doge-60M-Instruct\", trust_remote_code=True)\n\ngeneration_config = GenerationConfig(\n      max_new_tokens=100, \n      use_cache=True, \n      do_sample=True, \n      temperature=0.8, \n      top_p=0.9,\n      repetition_penalty=1.0\n)\nsteamer = TextStreamer(\n      tokenizer=tokenizer, \n      skip_prompt=True\n)\n\nprompt = \"Hi, how are you doing today?\"\nconversation = [\n      {\"role\": \"user\", \"content\": prompt}\n]\ninputs = tokenizer.apply_chat_template(\n    conversation=conversation,\n    tokenize=True,\n    return_tensors=\"pt\",\n)\n\noutputs = model.generate(\n    inputs, \n    tokenizer=tokenizer,\n    generation_config=generation_config, \n    streamer=steamer\n)\n```\n\n\n## Model Details\n\nWe build the Doge-Instruct by first SFT on [SmolTalk](https://huggingface.co/datasets/HuggingFaceTB/smoltalk) and then DPO on [UltraFeedback Binarized](https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized).\n\n> TODO: The larger model is under training and will be uploaded soon.\n\n**SFT**:\n| Model | Training Data | Epochs | Content Length | LR | Batch Size | Precision |\n|---|---|---|---|---|---|---|\n| [Doge-20M-Instruct-SFT](https://huggingface.co/SmallDoge/Doge-20M-Instruct-SFT) | [HuggingFaceTB/smoltalk](https://huggingface.co/datasets/HuggingFaceTB/smoltalk) | 2 | 2048 | 8e-4 | 0.25M | bfloat16 |\n| [Doge-60M-Instruct-SFT](https://huggingface.co/SmallDoge/Doge-60M-Instruct-SFT) | [HuggingFaceTB/smoltalk](https://huggingface.co/datasets/HuggingFaceTB/smoltalk) | 2 | 2048 | 6e-4 | 0.25M | bfloat16 |\n\n**DPO**:\n| Model | Training Data | Epochs | Content Length | LR | Batch Size | Precision |\n|---|---|---|---|---|---|---|\n| [Doge-20M-Instruct](https://huggingface.co/SmallDoge/Doge-20M-Instruct) | [HuggingFaceH4/ultrafeedback_binarized](https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized) | 2 | 1024 | 8e-5 | 0.125M | bfloat16 |\n| [Doge-60M-Instruct](https://huggingface.co/SmallDoge/Doge-60M-Instruct) | [HuggingFaceH4/ultrafeedback_binarized](https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized) | 2 | 1024 | 6e-5 | 0.125M | bfloat16 |\n\n\n**Procedure**:\n\n**SFT**:\n[<img src=\"https://raw.githubusercontent.com/wandb/assets/main/wandb-github-badge-28.svg\" alt=\"Visualize in Weights & Biases\" width=\"150\" height=\"24\"/>](https://wandb.ai/loser_cheems/huggingface/runs/ckbn4b5m) \n\n**DPO**:\n[<img src=\"https://raw.githubusercontent.com/wandb/assets/main/wandb-github-badge-28.svg\" alt=\"Visualize in Weights & Biases\" width=\"150\" height=\"24\"/>](https://wandb.ai/loser_cheems/huggingface/runs/3nk7mu5a)\n\n\n**Environment**:\n- Image: nvcr.io/nvidia/pytorch:24.12-py3\n- Hardware: 1x NVIDIA RTX 4090\n- Software: Transformers, TRL\n\n\n## Citation\n\n```bibtex\n@misc{shi2024wonderfulmatrices,\n      title={Wonderful Matrices: Combining for a More Efficient and Effective Foundation Model Architecture}, \n      author={Jingze Shi and Bingheng Wu},\n      year={2024},\n      eprint={2412.11834},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG},\n      url={https://arxiv.org/abs/2412.11834}, \n}\n```",
    "library_name": "transformers"
  },
  {
    "model_id": "SmallDoge/Doge-20M-Instruct",
    "model_name": "SmallDoge/Doge-20M-Instruct",
    "author": "SmallDoge",
    "downloads": 6220,
    "likes": 4,
    "tags": [
      "transformers",
      "safetensors",
      "doge",
      "text-generation",
      "trl",
      "sft",
      "dpo",
      "question-answering",
      "custom_code",
      "en",
      "dataset:HuggingFaceTB/smoltalk",
      "dataset:HuggingFaceH4/ultrafeedback_binarized",
      "arxiv:2412.11834",
      "base_model:SmallDoge/Doge-20M",
      "base_model:finetune:SmallDoge/Doge-20M",
      "license:apache-2.0",
      "autotrain_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/SmallDoge/Doge-20M-Instruct",
    "dependencies": [
      [
        "transformers",
        "4.36.2"
      ],
      [
        "torch",
        "2.2.0"
      ],
      [
        "numpy",
        "1.26.3"
      ]
    ],
    "analysis_date": "2025-03-26T00:50:16.347429",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "doge",
    "pipeline_tag": "question-answering",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": [
        "en"
      ],
      "license": "apache-2.0",
      "library_name": "transformers",
      "tags": [
        "trl",
        "sft",
        "dpo",
        "doge"
      ],
      "datasets": [
        "HuggingFaceTB/smoltalk",
        "HuggingFaceH4/ultrafeedback_binarized"
      ],
      "base_model": [
        "SmallDoge/Doge-20M"
      ],
      "pipeline_tag": "question-answering"
    },
    "card_text": "\n\n# **Doge 20M Instruct**\n\n<div align=\"center\">\n  <img src=\"https://huggingface.co/spaces/SmallDoge/README/resolve/main/org_icon.png\" width=\"100%\" alt=\"SmallDoge\" />\n</div>\n<hr>\n<div align=\"center\">\n  <a href=\"https://discord.gg/P2yYH95N\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-Small%20Doges-7289da?logo=discord&logoColor=white&color=7289da\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://arxiv.org/abs/2412.11834\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"arXiv\" src=\"https://img.shields.io/static/v1?label=arXiv&message=2412.11834&color=B31B1B&logo=arXiv\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://github.com/SmallDoges/small-doge\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"GitHub\" src=\"https://img.shields.io/badge/GitHub-SmallDoge-181717?logo=github\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://github.com/SmallDoges/small-doge/blob/main/LICENSE\" style=\"margin: 2px;\">\n    <img alt=\"License\" src=\"https://img.shields.io/badge/License-Apache--2.0-blue.svg\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\nDoge uses Dynamic Mask Attention as sequence transformation and can use Multi-Layer Perceptron or Cross Domain Mixture of Experts as state transformation. Dynamic Mask Attention allows the Transformer to use self-attention during training and state space during inference, and Cross Domain Mixture of Experts can directly inherit the weights of Multi-Layer Perceptron for further training. This model is trained by [SmallDoge](https://huggingface.co/SmallDoge) community, for detailed algorithm and model architecture, please refer to [Wonderful Matrices](https://arxiv.org/abs/2412.11834), all training details and code are publicly available on the [small-doge](https://github.com/SmallDoges/small-doge) repository.\n\n\n## Uses\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig, TextStreamer\n\ntokenizer = AutoTokenizer.from_pretrained(\"SmallDoge/Doge-20M-Instruct\")\nmodel = AutoModelForCausalLM.from_pretrained(\"SmallDoge/Doge-20M-Instruct\", trust_remote_code=True)\n\ngeneration_config = GenerationConfig(\n      max_new_tokens=100, \n      use_cache=True, \n      do_sample=True, \n      temperature=0.8, \n      top_p=0.9,\n      repetition_penalty=1.0\n)\nsteamer = TextStreamer(\n      tokenizer=tokenizer, \n      skip_prompt=True\n)\n\nprompt = \"Hi, how are you doing today?\"\nconversation = [\n      {\"role\": \"user\", \"content\": prompt}\n]\ninputs = tokenizer.apply_chat_template(\n    conversation=conversation,\n    tokenize=True,\n    return_tensors=\"pt\",\n)\n\noutputs = model.generate(\n    inputs, \n    tokenizer=tokenizer,\n    generation_config=generation_config, \n    streamer=steamer\n)\n```\n\n\n## Model Details\n\nWe build the Doge-Instruct by first SFT on [SmolTalk](https://huggingface.co/datasets/HuggingFaceTB/smoltalk) and then DPO on [UltraFeedback Binarized](https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized).\n\n> TODO: The larger model is under training and will be uploaded soon.\n\n**SFT**:\n| Model | Training Data | Epochs | Content Length | LR | Batch Size | Precision |\n|---|---|---|---|---|---|---|\n| [Doge-20M-Instruct-SFT](https://huggingface.co/SmallDoge/Doge-20M-Instruct-SFT) | [HuggingFaceTB/smoltalk](https://huggingface.co/datasets/HuggingFaceTB/smoltalk) | 2 | 2048 | 8e-4 | 0.25M | bfloat16 |\n| [Doge-60M-Instruct-SFT](https://huggingface.co/SmallDoge/Doge-60M-Instruct-SFT) | [HuggingFaceTB/smoltalk](https://huggingface.co/datasets/HuggingFaceTB/smoltalk) | 2 | 2048 | 6e-4 | 0.25M | bfloat16 |\n\n**DPO**:\n| Model | Training Data | Epochs | Content Length | LR | Batch Size | Precision |\n|---|---|---|---|---|---|---|\n| [Doge-20M-Instruct](https://huggingface.co/SmallDoge/Doge-20M-Instruct) | [HuggingFaceH4/ultrafeedback_binarized](https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized) | 2 | 1024 | 8e-5 | 0.125M | bfloat16 |\n| [Doge-60M-Instruct](https://huggingface.co/SmallDoge/Doge-60M-Instruct) | [HuggingFaceH4/ultrafeedback_binarized](https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized) | 2 | 1024 | 6e-5 | 0.125M | bfloat16 |\n\n\n**Procedure**:\n\n**SFT**:\n[<img src=\"https://raw.githubusercontent.com/wandb/assets/main/wandb-github-badge-28.svg\" alt=\"Visualize in Weights & Biases\" width=\"150\" height=\"24\"/>](https://wandb.ai/loser_cheems/huggingface/runs/eohr6fuj) \n\n**DPO**:\n[<img src=\"https://raw.githubusercontent.com/wandb/assets/main/wandb-github-badge-28.svg\" alt=\"Visualize in Weights & Biases\" width=\"150\" height=\"24\"/>](https://wandb.ai/loser_cheems/huggingface/runs/h6c2p2fe)\n\n\n**Environment**:\n- Image: nvcr.io/nvidia/pytorch:24.12-py3\n- Hardware: 1x NVIDIA RTX 4090\n- Software: Transformers, TRL\n\n\n## Citation\n\n```bibtex\n@misc{shi2024wonderfulmatrices,\n      title={Wonderful Matrices: Combining for a More Efficient and Effective Foundation Model Architecture}, \n      author={Jingze Shi and Bingheng Wu},\n      year={2024},\n      eprint={2412.11834},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG},\n      url={https://arxiv.org/abs/2412.11834}, \n}\n```",
    "card_content": "---\nlanguage:\n- en\nlicense: apache-2.0\nlibrary_name: transformers\ntags:\n- trl\n- sft\n- dpo\n- doge\ndatasets:\n- HuggingFaceTB/smoltalk\n- HuggingFaceH4/ultrafeedback_binarized\nbase_model:\n- SmallDoge/Doge-20M\npipeline_tag: question-answering\n---\n\n\n# **Doge 20M Instruct**\n\n<div align=\"center\">\n  <img src=\"https://huggingface.co/spaces/SmallDoge/README/resolve/main/org_icon.png\" width=\"100%\" alt=\"SmallDoge\" />\n</div>\n<hr>\n<div align=\"center\">\n  <a href=\"https://discord.gg/P2yYH95N\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-Small%20Doges-7289da?logo=discord&logoColor=white&color=7289da\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://arxiv.org/abs/2412.11834\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"arXiv\" src=\"https://img.shields.io/static/v1?label=arXiv&message=2412.11834&color=B31B1B&logo=arXiv\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://github.com/SmallDoges/small-doge\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"GitHub\" src=\"https://img.shields.io/badge/GitHub-SmallDoge-181717?logo=github\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n  <a href=\"https://github.com/SmallDoges/small-doge/blob/main/LICENSE\" style=\"margin: 2px;\">\n    <img alt=\"License\" src=\"https://img.shields.io/badge/License-Apache--2.0-blue.svg\" style=\"display: inline-block; vertical-align: middle;\"/>\n  </a>\n</div>\n\nDoge uses Dynamic Mask Attention as sequence transformation and can use Multi-Layer Perceptron or Cross Domain Mixture of Experts as state transformation. Dynamic Mask Attention allows the Transformer to use self-attention during training and state space during inference, and Cross Domain Mixture of Experts can directly inherit the weights of Multi-Layer Perceptron for further training. This model is trained by [SmallDoge](https://huggingface.co/SmallDoge) community, for detailed algorithm and model architecture, please refer to [Wonderful Matrices](https://arxiv.org/abs/2412.11834), all training details and code are publicly available on the [small-doge](https://github.com/SmallDoges/small-doge) repository.\n\n\n## Uses\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig, TextStreamer\n\ntokenizer = AutoTokenizer.from_pretrained(\"SmallDoge/Doge-20M-Instruct\")\nmodel = AutoModelForCausalLM.from_pretrained(\"SmallDoge/Doge-20M-Instruct\", trust_remote_code=True)\n\ngeneration_config = GenerationConfig(\n      max_new_tokens=100, \n      use_cache=True, \n      do_sample=True, \n      temperature=0.8, \n      top_p=0.9,\n      repetition_penalty=1.0\n)\nsteamer = TextStreamer(\n      tokenizer=tokenizer, \n      skip_prompt=True\n)\n\nprompt = \"Hi, how are you doing today?\"\nconversation = [\n      {\"role\": \"user\", \"content\": prompt}\n]\ninputs = tokenizer.apply_chat_template(\n    conversation=conversation,\n    tokenize=True,\n    return_tensors=\"pt\",\n)\n\noutputs = model.generate(\n    inputs, \n    tokenizer=tokenizer,\n    generation_config=generation_config, \n    streamer=steamer\n)\n```\n\n\n## Model Details\n\nWe build the Doge-Instruct by first SFT on [SmolTalk](https://huggingface.co/datasets/HuggingFaceTB/smoltalk) and then DPO on [UltraFeedback Binarized](https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized).\n\n> TODO: The larger model is under training and will be uploaded soon.\n\n**SFT**:\n| Model | Training Data | Epochs | Content Length | LR | Batch Size | Precision |\n|---|---|---|---|---|---|---|\n| [Doge-20M-Instruct-SFT](https://huggingface.co/SmallDoge/Doge-20M-Instruct-SFT) | [HuggingFaceTB/smoltalk](https://huggingface.co/datasets/HuggingFaceTB/smoltalk) | 2 | 2048 | 8e-4 | 0.25M | bfloat16 |\n| [Doge-60M-Instruct-SFT](https://huggingface.co/SmallDoge/Doge-60M-Instruct-SFT) | [HuggingFaceTB/smoltalk](https://huggingface.co/datasets/HuggingFaceTB/smoltalk) | 2 | 2048 | 6e-4 | 0.25M | bfloat16 |\n\n**DPO**:\n| Model | Training Data | Epochs | Content Length | LR | Batch Size | Precision |\n|---|---|---|---|---|---|---|\n| [Doge-20M-Instruct](https://huggingface.co/SmallDoge/Doge-20M-Instruct) | [HuggingFaceH4/ultrafeedback_binarized](https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized) | 2 | 1024 | 8e-5 | 0.125M | bfloat16 |\n| [Doge-60M-Instruct](https://huggingface.co/SmallDoge/Doge-60M-Instruct) | [HuggingFaceH4/ultrafeedback_binarized](https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized) | 2 | 1024 | 6e-5 | 0.125M | bfloat16 |\n\n\n**Procedure**:\n\n**SFT**:\n[<img src=\"https://raw.githubusercontent.com/wandb/assets/main/wandb-github-badge-28.svg\" alt=\"Visualize in Weights & Biases\" width=\"150\" height=\"24\"/>](https://wandb.ai/loser_cheems/huggingface/runs/eohr6fuj) \n\n**DPO**:\n[<img src=\"https://raw.githubusercontent.com/wandb/assets/main/wandb-github-badge-28.svg\" alt=\"Visualize in Weights & Biases\" width=\"150\" height=\"24\"/>](https://wandb.ai/loser_cheems/huggingface/runs/h6c2p2fe)\n\n\n**Environment**:\n- Image: nvcr.io/nvidia/pytorch:24.12-py3\n- Hardware: 1x NVIDIA RTX 4090\n- Software: Transformers, TRL\n\n\n## Citation\n\n```bibtex\n@misc{shi2024wonderfulmatrices,\n      title={Wonderful Matrices: Combining for a More Efficient and Effective Foundation Model Architecture}, \n      author={Jingze Shi and Bingheng Wu},\n      year={2024},\n      eprint={2412.11834},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG},\n      url={https://arxiv.org/abs/2412.11834}, \n}\n```",
    "library_name": "transformers"
  },
  {
    "model_id": "deepset/xlm-roberta-large-squad2",
    "model_name": "deepset/xlm-roberta-large-squad2",
    "author": "deepset",
    "downloads": 6216,
    "likes": 49,
    "tags": [
      "transformers",
      "pytorch",
      "safetensors",
      "xlm-roberta",
      "question-answering",
      "multilingual",
      "dataset:squad_v2",
      "license:cc-by-4.0",
      "model-index",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/deepset/xlm-roberta-large-squad2",
    "dependencies": [
      [
        "haystack-ai",
        "2.11.2"
      ],
      [
        "transformers",
        "4.36.2"
      ],
      [
        "torch",
        "2.5.1"
      ],
      [
        "sentencepiece",
        "0.1.99"
      ],
      [
        "cmake",
        "3.28.1"
      ]
    ],
    "analysis_date": "2025-03-26T00:50:17.817789",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "xlm-roberta",
    "pipeline_tag": "question-answering",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": "multilingual",
      "license": "cc-by-4.0",
      "tags": [
        "question-answering"
      ],
      "datasets": [
        "squad_v2"
      ],
      "model-index": [
        {
          "name": "deepset/xlm-roberta-large-squad2",
          "results": [
            {
              "task": {
                "type": "question-answering",
                "name": "Question Answering"
              },
              "dataset": {
                "name": "squad_v2",
                "type": "squad_v2",
                "config": "squad_v2",
                "split": "validation"
              },
              "metrics": [
                {
                  "type": "exact_match",
                  "value": 81.8281,
                  "name": "Exact Match",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiNzVhZDE2NTg5NmUwOWRkMmI2MGUxYjFlZjIzNmMyNDQ2MDY2MDNhYzE0ZjY5YTkyY2U4ODc3ODFiZjQxZWQ2YSIsInZlcnNpb24iOjF9.f_rN3WPMAdv-OBPz0T7N7lOxYz9f1nEr_P-vwKhi3jNdRKp_JTy18MYR9eyJM2riKHC6_ge-8XwfyrUf51DSDA"
                },
                {
                  "type": "f1",
                  "value": 84.8886,
                  "name": "F1",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZGE5MWJmZGUxMGMwNWFhYzVhZjQwZGEwOWQ4N2Q2Yjg5NzdjNDFiNDhiYTQ1Y2E5ZWJkOTFhYmI1Y2Q2ZGYwOCIsInZlcnNpb24iOjF9.TIdH-tOx3kEMDs5wK1r6iwZqqSjNGlBrpawrsE917j1F3UFJVnQ7wJwaj0OIgmC4iw8OQeLZL56ucBcLApa-AQ"
                }
              ]
            }
          ]
        }
      ]
    },
    "card_text": "\n# Multilingual XLM-RoBERTa large for Extractive QA on various languages \n\n## Overview\n**Language model:** xlm-roberta-large  \n**Language:** Multilingual  \n**Downstream-task:** Extractive QA  \n**Training data:** SQuAD 2.0  \n**Eval data:** SQuAD dev set - German MLQA - German XQuAD   \n**Training run:** [MLFlow link](https://public-mlflow.deepset.ai/#/experiments/124/runs/3a540e3f3ecf4dd98eae8fc6d457ff20)  \n**Code:**  See [an example extractive QA pipeline built with Haystack](https://haystack.deepset.ai/tutorials/34_extractive_qa_pipeline)  \n**Infrastructure**: 4x Tesla v100\n\n## Hyperparameters\n\n```\nbatch_size = 32\nn_epochs = 3\nbase_LM_model = \"xlm-roberta-large\"\nmax_seq_len = 256\nlearning_rate = 1e-5\nlr_schedule = LinearWarmup\nwarmup_proportion = 0.2\ndoc_stride=128\nmax_query_length=64\n```\n\n## Usage\n\n### In Haystack\nHaystack is an AI orchestration framework to build customizable, production-ready LLM applications. You can use this model in Haystack to do extractive question answering on documents. \nTo load and run the model with [Haystack](https://github.com/deepset-ai/haystack/):\n```python\n# After running pip install haystack-ai \"transformers[torch,sentencepiece]\"\n\nfrom haystack import Document\nfrom haystack.components.readers import ExtractiveReader\n\ndocs = [\n    Document(content=\"Python is a popular programming language\"),\n    Document(content=\"python ist eine beliebte Programmiersprache\"),\n]\n\nreader = ExtractiveReader(model=\"deepset/xlm-roberta-large-squad2\")\nreader.warm_up()\n\nquestion = \"What is a popular programming language?\"\nresult = reader.run(query=question, documents=docs)\n# {'answers': [ExtractedAnswer(query='What is a popular programming language?', score=0.5740374326705933, data='python', document=Document(id=..., content: '...'), context=None, document_offset=ExtractedAnswer.Span(start=0, end=6),...)]}\n```\nFor a complete example with an extractive question answering pipeline that scales over many documents, check out the [corresponding Haystack tutorial](https://haystack.deepset.ai/tutorials/34_extractive_qa_pipeline).\n\n### In Transformers\n```python\nfrom transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n\nmodel_name = \"deepset/xlm-roberta-large-squad2\"\n\n# a) Get predictions\nnlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\nQA_input = {\n    'question': 'Why is model conversion important?',\n    'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\n}\nres = nlp(QA_input)\n\n# b) Load model & tokenizer\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n```\n\n## Performance\nEvaluated on the SQuAD 2.0 English dev set with the [official eval script](https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/contents/blob/).\n```\n  \"exact\": 79.45759285774446,\n  \"f1\": 83.79259828925511,\n  \"total\": 11873,\n  \"HasAns_exact\": 71.96356275303644,\n  \"HasAns_f1\": 80.6460053117963,\n  \"HasAns_total\": 5928,\n  \"NoAns_exact\": 86.93019343986543,\n  \"NoAns_f1\": 86.93019343986543,\n  \"NoAns_total\": 5945\n```\n\nEvaluated on German [MLQA: test-context-de-question-de.json](https://github.com/facebookresearch/MLQA)\n```\n\"exact\": 49.34691166703564,\n\"f1\": 66.15582561674236,\n\"total\": 4517,\n```\n\nEvaluated on German [XQuAD: xquad.de.json](https://github.com/deepmind/xquad)\n```\n\"exact\": 61.51260504201681,\n\"f1\": 78.80206098332569,\n\"total\": 1190,\n```\n\n## Usage\n\n### In Haystack\nFor doing QA at scale (i.e. many docs instead of single paragraph), you can load the model also in [haystack](https://github.com/deepset-ai/haystack/):\n```python\nreader = FARMReader(model_name_or_path=\"deepset/xlm-roberta-large-squad2\")\n# or \nreader = TransformersReader(model=\"deepset/xlm-roberta-large-squad2\",tokenizer=\"deepset/xlm-roberta-large-squad2\")\n```\n\n### In Transformers\n```python\nfrom transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n\nmodel_name = \"deepset/xlm-roberta-large-squad2\"\n\n# a) Get predictions\nnlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\nQA_input = {\n    'question': 'Why is model conversion important?',\n    'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\n}\nres = nlp(QA_input)\n\n# b) Load model & tokenizer\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n```\n\n## Authors\n**Branden Chan:** branden.chan@deepset.ai    \n**Timo M\u00f6ller:** timo.moeller@deepset.ai      \n**Malte Pietsch:** malte.pietsch@deepset.ai      \n**Tanay Soni:** tanay.soni@deepset.ai    \n\n## About us\n\n<div class=\"grid lg:grid-cols-2 gap-x-4 gap-y-3\">\n    <div class=\"w-full h-40 object-cover mb-2 rounded-lg flex items-center justify-center\">\n         <img alt=\"\" src=\"https://raw.githubusercontent.com/deepset-ai/.github/main/deepset-logo-colored.png\" class=\"w-40\"/>\n     </div>\n     <div class=\"w-full h-40 object-cover mb-2 rounded-lg flex items-center justify-center\">\n         <img alt=\"\" src=\"https://raw.githubusercontent.com/deepset-ai/.github/main/haystack-logo-colored.png\" class=\"w-40\"/>\n     </div>\n</div>\n\n[deepset](http://deepset.ai/) is the company behind the production-ready open-source AI framework [Haystack](https://haystack.deepset.ai/).\n\nSome of our other work: \n- [Distilled roberta-base-squad2 (aka \"tinyroberta-squad2\")](https://huggingface.co/deepset/tinyroberta-squad2)\n- [German BERT](https://deepset.ai/german-bert), [GermanQuAD and GermanDPR](https://deepset.ai/germanquad), [German embedding model](https://huggingface.co/mixedbread-ai/deepset-mxbai-embed-de-large-v1)\n- [deepset Cloud](https://www.deepset.ai/deepset-cloud-product), [deepset Studio](https://www.deepset.ai/deepset-studio)\n\n## Get in touch and join the Haystack community\n\n<p>For more info on Haystack, visit our <strong><a href=\"https://github.com/deepset-ai/haystack\">GitHub</a></strong> repo and <strong><a href=\"https://docs.haystack.deepset.ai\">Documentation</a></strong>. \n\nWe also have a <strong><a class=\"h-7\" href=\"https://haystack.deepset.ai/community\">Discord community open to everyone!</a></strong></p>\n\n[Twitter](https://twitter.com/Haystack_AI) | [LinkedIn](https://www.linkedin.com/company/deepset-ai/) | [Discord](https://haystack.deepset.ai/community) | [GitHub Discussions](https://github.com/deepset-ai/haystack/discussions) | [Website](https://haystack.deepset.ai/) | [YouTube](https://www.youtube.com/@deepset_ai)\n\nBy the way: [we're hiring!](http://www.deepset.ai/jobs)",
    "card_content": "---\nlanguage: multilingual\nlicense: cc-by-4.0\ntags:\n- question-answering\ndatasets:\n- squad_v2\nmodel-index:\n- name: deepset/xlm-roberta-large-squad2\n  results:\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squad_v2\n      type: squad_v2\n      config: squad_v2\n      split: validation\n    metrics:\n    - type: exact_match\n      value: 81.8281\n      name: Exact Match\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiNzVhZDE2NTg5NmUwOWRkMmI2MGUxYjFlZjIzNmMyNDQ2MDY2MDNhYzE0ZjY5YTkyY2U4ODc3ODFiZjQxZWQ2YSIsInZlcnNpb24iOjF9.f_rN3WPMAdv-OBPz0T7N7lOxYz9f1nEr_P-vwKhi3jNdRKp_JTy18MYR9eyJM2riKHC6_ge-8XwfyrUf51DSDA\n    - type: f1\n      value: 84.8886\n      name: F1\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZGE5MWJmZGUxMGMwNWFhYzVhZjQwZGEwOWQ4N2Q2Yjg5NzdjNDFiNDhiYTQ1Y2E5ZWJkOTFhYmI1Y2Q2ZGYwOCIsInZlcnNpb24iOjF9.TIdH-tOx3kEMDs5wK1r6iwZqqSjNGlBrpawrsE917j1F3UFJVnQ7wJwaj0OIgmC4iw8OQeLZL56ucBcLApa-AQ\n---\n\n# Multilingual XLM-RoBERTa large for Extractive QA on various languages \n\n## Overview\n**Language model:** xlm-roberta-large  \n**Language:** Multilingual  \n**Downstream-task:** Extractive QA  \n**Training data:** SQuAD 2.0  \n**Eval data:** SQuAD dev set - German MLQA - German XQuAD   \n**Training run:** [MLFlow link](https://public-mlflow.deepset.ai/#/experiments/124/runs/3a540e3f3ecf4dd98eae8fc6d457ff20)  \n**Code:**  See [an example extractive QA pipeline built with Haystack](https://haystack.deepset.ai/tutorials/34_extractive_qa_pipeline)  \n**Infrastructure**: 4x Tesla v100\n\n## Hyperparameters\n\n```\nbatch_size = 32\nn_epochs = 3\nbase_LM_model = \"xlm-roberta-large\"\nmax_seq_len = 256\nlearning_rate = 1e-5\nlr_schedule = LinearWarmup\nwarmup_proportion = 0.2\ndoc_stride=128\nmax_query_length=64\n```\n\n## Usage\n\n### In Haystack\nHaystack is an AI orchestration framework to build customizable, production-ready LLM applications. You can use this model in Haystack to do extractive question answering on documents. \nTo load and run the model with [Haystack](https://github.com/deepset-ai/haystack/):\n```python\n# After running pip install haystack-ai \"transformers[torch,sentencepiece]\"\n\nfrom haystack import Document\nfrom haystack.components.readers import ExtractiveReader\n\ndocs = [\n    Document(content=\"Python is a popular programming language\"),\n    Document(content=\"python ist eine beliebte Programmiersprache\"),\n]\n\nreader = ExtractiveReader(model=\"deepset/xlm-roberta-large-squad2\")\nreader.warm_up()\n\nquestion = \"What is a popular programming language?\"\nresult = reader.run(query=question, documents=docs)\n# {'answers': [ExtractedAnswer(query='What is a popular programming language?', score=0.5740374326705933, data='python', document=Document(id=..., content: '...'), context=None, document_offset=ExtractedAnswer.Span(start=0, end=6),...)]}\n```\nFor a complete example with an extractive question answering pipeline that scales over many documents, check out the [corresponding Haystack tutorial](https://haystack.deepset.ai/tutorials/34_extractive_qa_pipeline).\n\n### In Transformers\n```python\nfrom transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n\nmodel_name = \"deepset/xlm-roberta-large-squad2\"\n\n# a) Get predictions\nnlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\nQA_input = {\n    'question': 'Why is model conversion important?',\n    'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\n}\nres = nlp(QA_input)\n\n# b) Load model & tokenizer\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n```\n\n## Performance\nEvaluated on the SQuAD 2.0 English dev set with the [official eval script](https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/contents/blob/).\n```\n  \"exact\": 79.45759285774446,\n  \"f1\": 83.79259828925511,\n  \"total\": 11873,\n  \"HasAns_exact\": 71.96356275303644,\n  \"HasAns_f1\": 80.6460053117963,\n  \"HasAns_total\": 5928,\n  \"NoAns_exact\": 86.93019343986543,\n  \"NoAns_f1\": 86.93019343986543,\n  \"NoAns_total\": 5945\n```\n\nEvaluated on German [MLQA: test-context-de-question-de.json](https://github.com/facebookresearch/MLQA)\n```\n\"exact\": 49.34691166703564,\n\"f1\": 66.15582561674236,\n\"total\": 4517,\n```\n\nEvaluated on German [XQuAD: xquad.de.json](https://github.com/deepmind/xquad)\n```\n\"exact\": 61.51260504201681,\n\"f1\": 78.80206098332569,\n\"total\": 1190,\n```\n\n## Usage\n\n### In Haystack\nFor doing QA at scale (i.e. many docs instead of single paragraph), you can load the model also in [haystack](https://github.com/deepset-ai/haystack/):\n```python\nreader = FARMReader(model_name_or_path=\"deepset/xlm-roberta-large-squad2\")\n# or \nreader = TransformersReader(model=\"deepset/xlm-roberta-large-squad2\",tokenizer=\"deepset/xlm-roberta-large-squad2\")\n```\n\n### In Transformers\n```python\nfrom transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n\nmodel_name = \"deepset/xlm-roberta-large-squad2\"\n\n# a) Get predictions\nnlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\nQA_input = {\n    'question': 'Why is model conversion important?',\n    'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\n}\nres = nlp(QA_input)\n\n# b) Load model & tokenizer\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n```\n\n## Authors\n**Branden Chan:** branden.chan@deepset.ai    \n**Timo M\u00f6ller:** timo.moeller@deepset.ai      \n**Malte Pietsch:** malte.pietsch@deepset.ai      \n**Tanay Soni:** tanay.soni@deepset.ai    \n\n## About us\n\n<div class=\"grid lg:grid-cols-2 gap-x-4 gap-y-3\">\n    <div class=\"w-full h-40 object-cover mb-2 rounded-lg flex items-center justify-center\">\n         <img alt=\"\" src=\"https://raw.githubusercontent.com/deepset-ai/.github/main/deepset-logo-colored.png\" class=\"w-40\"/>\n     </div>\n     <div class=\"w-full h-40 object-cover mb-2 rounded-lg flex items-center justify-center\">\n         <img alt=\"\" src=\"https://raw.githubusercontent.com/deepset-ai/.github/main/haystack-logo-colored.png\" class=\"w-40\"/>\n     </div>\n</div>\n\n[deepset](http://deepset.ai/) is the company behind the production-ready open-source AI framework [Haystack](https://haystack.deepset.ai/).\n\nSome of our other work: \n- [Distilled roberta-base-squad2 (aka \"tinyroberta-squad2\")](https://huggingface.co/deepset/tinyroberta-squad2)\n- [German BERT](https://deepset.ai/german-bert), [GermanQuAD and GermanDPR](https://deepset.ai/germanquad), [German embedding model](https://huggingface.co/mixedbread-ai/deepset-mxbai-embed-de-large-v1)\n- [deepset Cloud](https://www.deepset.ai/deepset-cloud-product), [deepset Studio](https://www.deepset.ai/deepset-studio)\n\n## Get in touch and join the Haystack community\n\n<p>For more info on Haystack, visit our <strong><a href=\"https://github.com/deepset-ai/haystack\">GitHub</a></strong> repo and <strong><a href=\"https://docs.haystack.deepset.ai\">Documentation</a></strong>. \n\nWe also have a <strong><a class=\"h-7\" href=\"https://haystack.deepset.ai/community\">Discord community open to everyone!</a></strong></p>\n\n[Twitter](https://twitter.com/Haystack_AI) | [LinkedIn](https://www.linkedin.com/company/deepset-ai/) | [Discord](https://haystack.deepset.ai/community) | [GitHub Discussions](https://github.com/deepset-ai/haystack/discussions) | [Website](https://haystack.deepset.ai/) | [YouTube](https://www.youtube.com/@deepset_ai)\n\nBy the way: [we're hiring!](http://www.deepset.ai/jobs)",
    "library_name": "transformers"
  },
  {
    "model_id": "amara16/distilbert-extractive-qa-project",
    "model_name": "amara16/distilbert-extractive-qa-project",
    "author": "amara16",
    "downloads": 6004,
    "likes": 0,
    "tags": [
      "transformers",
      "safetensors",
      "distilbert",
      "question-answering",
      "arxiv:1910.09700",
      "license:mit",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/amara16/distilbert-extractive-qa-project",
    "dependencies": [
      [
        "transformers",
        "4.36.2"
      ],
      [
        "torch",
        "2.5.1"
      ]
    ],
    "analysis_date": "2025-03-26T00:50:18.964130",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "distilbert",
    "pipeline_tag": "question-answering",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "license": "mit",
      "library_name": "transformers"
    },
    "card_text": "\n# Model Card for Model ID\n\n<!-- Provide a quick summary of what the model is/does. -->\n\n\n\n## Model Details\n\n### Model Description\n\n<!-- Provide a longer summary of what this model is. -->\n\nThis is the model card of a \ud83e\udd17 transformers model that has been pushed on the Hub. This model card has been automatically generated.\n\n- **Developed by:** [More Information Needed]\n- **Funded by [optional]:** [More Information Needed]\n- **Shared by [optional]:** [More Information Needed]\n- **Model type:** [More Information Needed]\n- **Language(s) (NLP):** [More Information Needed]\n- **License:** [More Information Needed]\n- **Finetuned from model [optional]:** [More Information Needed]\n\n### Model Sources [optional]\n\n<!-- Provide the basic links for the model. -->\n\n- **Repository:** [More Information Needed]\n- **Paper [optional]:** [More Information Needed]\n- **Demo [optional]:** [More Information Needed]\n\n## Uses\n\n<!-- Address questions around how the model is intended to be used, including the foreseeable users of the model and those affected by the model. -->\n\n### Direct Use\n\n<!-- This section is for the model use without fine-tuning or plugging into a larger ecosystem/app. -->\n\n[More Information Needed]\n\n### Downstream Use [optional]\n\n<!-- This section is for the model use when fine-tuned for a task, or when plugged into a larger ecosystem/app -->\n\n[More Information Needed]\n\n### Out-of-Scope Use\n\n<!-- This section addresses misuse, malicious use, and uses that the model will not work well for. -->\n\n[More Information Needed]\n\n## Bias, Risks, and Limitations\n\n<!-- This section is meant to convey both technical and sociotechnical limitations. -->\n\n[More Information Needed]\n\n### Recommendations\n\n<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->\n\nUsers (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.\n\n## How to Get Started with the Model\n\nUse the code below to get started with the model.\n\n[More Information Needed]\n\n## Training Details\n\n### Training Data\n\n<!-- This should link to a Dataset Card, perhaps with a short stub of information on what the training data is all about as well as documentation related to data pre-processing or additional filtering. -->\n\n[More Information Needed]\n\n### Training Procedure\n\n<!-- This relates heavily to the Technical Specifications. Content here should link to that section when it is relevant to the training procedure. -->\n\n#### Preprocessing [optional]\n\n[More Information Needed]\n\n\n#### Training Hyperparameters\n\n- **Training regime:** [More Information Needed] <!--fp32, fp16 mixed precision, bf16 mixed precision, bf16 non-mixed precision, fp16 non-mixed precision, fp8 mixed precision -->\n\n#### Speeds, Sizes, Times [optional]\n\n<!-- This section provides information about throughput, start/end time, checkpoint size if relevant, etc. -->\n\n[More Information Needed]\n\n## Evaluation\n\n<!-- This section describes the evaluation protocols and provides the results. -->\n\n### Testing Data, Factors & Metrics\n\n#### Testing Data\n\n<!-- This should link to a Dataset Card if possible. -->\n\n[More Information Needed]\n\n#### Factors\n\n<!-- These are the things the evaluation is disaggregating by, e.g., subpopulations or domains. -->\n\n[More Information Needed]\n\n#### Metrics\n\n<!-- These are the evaluation metrics being used, ideally with a description of why. -->\n\n[More Information Needed]\n\n### Results\n\n[More Information Needed]\n\n#### Summary\n\n\n\n## Model Examination [optional]\n\n<!-- Relevant interpretability work for the model goes here -->\n\n[More Information Needed]\n\n## Environmental Impact\n\n<!-- Total emissions (in grams of CO2eq) and additional considerations, such as electricity usage, go here. Edit the suggested text below accordingly -->\n\nCarbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).\n\n- **Hardware Type:** [More Information Needed]\n- **Hours used:** [More Information Needed]\n- **Cloud Provider:** [More Information Needed]\n- **Compute Region:** [More Information Needed]\n- **Carbon Emitted:** [More Information Needed]\n\n## Technical Specifications [optional]\n\n### Model Architecture and Objective\n\n[More Information Needed]\n\n### Compute Infrastructure\n\n[More Information Needed]\n\n#### Hardware\n\n[More Information Needed]\n\n#### Software\n\n[More Information Needed]\n\n## Citation [optional]\n\n<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->\n\n**BibTeX:**\n\n[More Information Needed]\n\n**APA:**\n\n[More Information Needed]\n\n## Glossary [optional]\n\n<!-- If relevant, include terms and calculations in this section that can help readers understand the model or model card. -->\n\n[More Information Needed]\n\n## More Information [optional]\n\n[More Information Needed]\n\n## Model Card Authors [optional]\n\n[More Information Needed]\n\n## Model Card Contact\n\n[More Information Needed]",
    "card_content": "---\nlicense: mit\nlibrary_name: transformers\n---\n\n# Model Card for Model ID\n\n<!-- Provide a quick summary of what the model is/does. -->\n\n\n\n## Model Details\n\n### Model Description\n\n<!-- Provide a longer summary of what this model is. -->\n\nThis is the model card of a \ud83e\udd17 transformers model that has been pushed on the Hub. This model card has been automatically generated.\n\n- **Developed by:** [More Information Needed]\n- **Funded by [optional]:** [More Information Needed]\n- **Shared by [optional]:** [More Information Needed]\n- **Model type:** [More Information Needed]\n- **Language(s) (NLP):** [More Information Needed]\n- **License:** [More Information Needed]\n- **Finetuned from model [optional]:** [More Information Needed]\n\n### Model Sources [optional]\n\n<!-- Provide the basic links for the model. -->\n\n- **Repository:** [More Information Needed]\n- **Paper [optional]:** [More Information Needed]\n- **Demo [optional]:** [More Information Needed]\n\n## Uses\n\n<!-- Address questions around how the model is intended to be used, including the foreseeable users of the model and those affected by the model. -->\n\n### Direct Use\n\n<!-- This section is for the model use without fine-tuning or plugging into a larger ecosystem/app. -->\n\n[More Information Needed]\n\n### Downstream Use [optional]\n\n<!-- This section is for the model use when fine-tuned for a task, or when plugged into a larger ecosystem/app -->\n\n[More Information Needed]\n\n### Out-of-Scope Use\n\n<!-- This section addresses misuse, malicious use, and uses that the model will not work well for. -->\n\n[More Information Needed]\n\n## Bias, Risks, and Limitations\n\n<!-- This section is meant to convey both technical and sociotechnical limitations. -->\n\n[More Information Needed]\n\n### Recommendations\n\n<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->\n\nUsers (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.\n\n## How to Get Started with the Model\n\nUse the code below to get started with the model.\n\n[More Information Needed]\n\n## Training Details\n\n### Training Data\n\n<!-- This should link to a Dataset Card, perhaps with a short stub of information on what the training data is all about as well as documentation related to data pre-processing or additional filtering. -->\n\n[More Information Needed]\n\n### Training Procedure\n\n<!-- This relates heavily to the Technical Specifications. Content here should link to that section when it is relevant to the training procedure. -->\n\n#### Preprocessing [optional]\n\n[More Information Needed]\n\n\n#### Training Hyperparameters\n\n- **Training regime:** [More Information Needed] <!--fp32, fp16 mixed precision, bf16 mixed precision, bf16 non-mixed precision, fp16 non-mixed precision, fp8 mixed precision -->\n\n#### Speeds, Sizes, Times [optional]\n\n<!-- This section provides information about throughput, start/end time, checkpoint size if relevant, etc. -->\n\n[More Information Needed]\n\n## Evaluation\n\n<!-- This section describes the evaluation protocols and provides the results. -->\n\n### Testing Data, Factors & Metrics\n\n#### Testing Data\n\n<!-- This should link to a Dataset Card if possible. -->\n\n[More Information Needed]\n\n#### Factors\n\n<!-- These are the things the evaluation is disaggregating by, e.g., subpopulations or domains. -->\n\n[More Information Needed]\n\n#### Metrics\n\n<!-- These are the evaluation metrics being used, ideally with a description of why. -->\n\n[More Information Needed]\n\n### Results\n\n[More Information Needed]\n\n#### Summary\n\n\n\n## Model Examination [optional]\n\n<!-- Relevant interpretability work for the model goes here -->\n\n[More Information Needed]\n\n## Environmental Impact\n\n<!-- Total emissions (in grams of CO2eq) and additional considerations, such as electricity usage, go here. Edit the suggested text below accordingly -->\n\nCarbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).\n\n- **Hardware Type:** [More Information Needed]\n- **Hours used:** [More Information Needed]\n- **Cloud Provider:** [More Information Needed]\n- **Compute Region:** [More Information Needed]\n- **Carbon Emitted:** [More Information Needed]\n\n## Technical Specifications [optional]\n\n### Model Architecture and Objective\n\n[More Information Needed]\n\n### Compute Infrastructure\n\n[More Information Needed]\n\n#### Hardware\n\n[More Information Needed]\n\n#### Software\n\n[More Information Needed]\n\n## Citation [optional]\n\n<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->\n\n**BibTeX:**\n\n[More Information Needed]\n\n**APA:**\n\n[More Information Needed]\n\n## Glossary [optional]\n\n<!-- If relevant, include terms and calculations in this section that can help readers understand the model or model card. -->\n\n[More Information Needed]\n\n## More Information [optional]\n\n[More Information Needed]\n\n## Model Card Authors [optional]\n\n[More Information Needed]\n\n## Model Card Contact\n\n[More Information Needed]",
    "library_name": "transformers"
  },
  {
    "model_id": "umarigan/llama-3.1-openhermes-tr",
    "model_name": "umarigan/llama-3.1-openhermes-tr",
    "author": "umarigan",
    "downloads": 5575,
    "likes": 3,
    "tags": [
      "transformers",
      "safetensors",
      "llama",
      "text-generation",
      "text-generation-inference",
      "unsloth",
      "trl",
      "sft",
      "question-answering",
      "en",
      "tr",
      "base_model:unsloth/llama-3-8b-bnb-4bit",
      "base_model:finetune:unsloth/llama-3-8b-bnb-4bit",
      "license:apache-2.0",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/umarigan/llama-3.1-openhermes-tr",
    "dependencies": [
      [
        "transformers",
        "4.36.2"
      ],
      [
        "torch",
        "2.2.2"
      ],
      [
        "numpy",
        "1.26.3"
      ]
    ],
    "analysis_date": "2025-03-26T00:50:20.110718",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "llama",
    "pipeline_tag": "question-answering",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": [
        "en",
        "tr"
      ],
      "license": "apache-2.0",
      "tags": [
        "text-generation-inference",
        "transformers",
        "unsloth",
        "llama",
        "trl",
        "sft"
      ],
      "base_model": "unsloth/llama-3-8b-bnb-4bit",
      "pipeline_tag": "question-answering"
    },
    "card_text": "\n# Uploaded  model\n\n- **Developed by:** umarigan\n- **License:** apache-2.0\n- **Finetuned from model :** unsloth/llama-3-8b-bnb-4bit\n\nThis llama model was trained 2x faster with [Unsloth](https://github.com/unslothai/unsloth) and Huggingface's TRL library.\n\n[<img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20made%20with%20love.png\" width=\"200\"/>](https://github.com/unslothai/unsloth)\n\n## Usage Examples\n\n```python\n\n# Load model directly\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"umarigan/llama-3-openhermes-tr\")\nmodel = AutoModelForCausalLM.from_pretrained(\"umarigan/llama-3-openhermes-tr\")\nalpaca_prompt = \"\"\"\nG\u00f6rev:\n{}\n\nGirdi:\n{}\n\nCevap:\n{}\"\"\"\n\ninputs = tokenizer(\n[\n    alpaca_prompt.format(\n        \"bir haftada 3 kilo verebilece\u011fim 5 \u00f6neri sunabilir misin?\", # G\u00f6rev\n        \"\", # Girdi\n        \"\", # Cevap - bo\u015f b\u0131rak\u0131n!\n    )\n], return_tensors = \"pt\")\noutputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\ntokenizer.batch_decode(outputs)\n\nOutput:\n\n'<|begin_of_text|>\nG\u00f6rev: bir haftada 3 kilo verebilece\u011fim 5 \u00f6neri sunabilir misin?\nGirdi:\nCevap:\n1. Yeterli miktarda su i\u00e7mek: Su, v\u00fccuttaki toksinlerin at\u0131lmas\u0131n\u0131 ve sindirim sisteminin \u00e7al\u0131\u015fmas\u0131n\u0131 destekler. Su i\u00e7mek, v\u00fccuttaki su dengesini korumaya yard\u0131mc\u0131 olur ve kilo kayb\u0131na yard\u0131mc\u0131 olabilir. G\u00fcnl\u00fck su t\u00fcketiminizi 2-3 litre aras\u0131nda tutun.\n2. D\u00fczenli egzersiz yap\u0131n: Egzersiz, metabolizmay\u0131 h\u0131zland\u0131rmaya yard\u0131mc\u0131 olur ve v\u00fccuttaki ya\u011f yak\u0131m\u0131n\u0131 te\u015fvik eder. Haftada 3-5 g\u00fcn egzersiz yap\u0131n ve her g\u00fcn 30-60 dakika egzersiz yap\u0131n. Egzersizler, aerobik egzersizler (y\u00fcr\u00fcy\u00fc\u015f, y\u00fczme, bisiklet s\u00fcrmek) ve ana kas egzersizleri (kollar, bacaklar, kal\u00e7a) gibi \u00e7e\u015fitli egzersiz t\u00fcrlerini i\u00e7erebilir.\n3. Azalm\u0131\u015f kalori al\u0131m\u0131na odaklan\u0131n: Kilo verme hedefinize ula\u015fmak i\u00e7in kalori al\u0131m\u0131n\u0131z\u0131 azalt\u0131n. Bu, beslenme plan\u0131n\u0131zda azalm\u0131\u015f kalori i\u00e7eren yiyecekleri se\u00e7erek ve a\u015f\u0131r\u0131 kalori al\u0131m\u0131n\u0131 \u00f6nlemek i\u00e7in yemeklerinizi \u00f6l\u00e7erek veya a\u011f\u0131rla\u015ft\u0131rmaya \u00e7al\u0131\u015farak yap\u0131labilir.\n4. Yeterli uyku al: Uyku, metabolizmay\u0131 d\u00fczenleyen ve kilo kayb\u0131na yard\u0131mc\u0131 olan hormonlar\u0131n sal\u0131nmas\u0131n\u0131 etkileyen bir fakt\u00f6rd\u00fcr. G\u00fcnl\u00fck 7-9 saat uyku almay\u0131 hedefleyin.\n5. Stres y\u00f6netimi: Stres, hormonlar\u0131n sal\u0131nmas\u0131n\u0131 etkileyerek kilo al\u0131m\u0131na katk\u0131da bulunabilir. Stresi y\u00f6netmek i\u00e7in egzersiz, meditasyon veya derin nefes egzersizleri gibi \u00e7e\u015fitli y\u00f6ntemleri kullan\u0131n. Bu, stresi azaltmaya ve metabolizmay\u0131 d\u00fczenleme yetene\u011fini geli\u015ftirmeye yard\u0131mc\u0131 olabilir.\n<|end_of_text|>'\n```",
    "card_content": "---\nlanguage:\n- en\n- tr\nlicense: apache-2.0\ntags:\n- text-generation-inference\n- transformers\n- unsloth\n- llama\n- trl\n- sft\nbase_model: unsloth/llama-3-8b-bnb-4bit\npipeline_tag: question-answering\n---\n\n# Uploaded  model\n\n- **Developed by:** umarigan\n- **License:** apache-2.0\n- **Finetuned from model :** unsloth/llama-3-8b-bnb-4bit\n\nThis llama model was trained 2x faster with [Unsloth](https://github.com/unslothai/unsloth) and Huggingface's TRL library.\n\n[<img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20made%20with%20love.png\" width=\"200\"/>](https://github.com/unslothai/unsloth)\n\n## Usage Examples\n\n```python\n\n# Load model directly\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"umarigan/llama-3-openhermes-tr\")\nmodel = AutoModelForCausalLM.from_pretrained(\"umarigan/llama-3-openhermes-tr\")\nalpaca_prompt = \"\"\"\nG\u00f6rev:\n{}\n\nGirdi:\n{}\n\nCevap:\n{}\"\"\"\n\ninputs = tokenizer(\n[\n    alpaca_prompt.format(\n        \"bir haftada 3 kilo verebilece\u011fim 5 \u00f6neri sunabilir misin?\", # G\u00f6rev\n        \"\", # Girdi\n        \"\", # Cevap - bo\u015f b\u0131rak\u0131n!\n    )\n], return_tensors = \"pt\")\noutputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\ntokenizer.batch_decode(outputs)\n\nOutput:\n\n'<|begin_of_text|>\nG\u00f6rev: bir haftada 3 kilo verebilece\u011fim 5 \u00f6neri sunabilir misin?\nGirdi:\nCevap:\n1. Yeterli miktarda su i\u00e7mek: Su, v\u00fccuttaki toksinlerin at\u0131lmas\u0131n\u0131 ve sindirim sisteminin \u00e7al\u0131\u015fmas\u0131n\u0131 destekler. Su i\u00e7mek, v\u00fccuttaki su dengesini korumaya yard\u0131mc\u0131 olur ve kilo kayb\u0131na yard\u0131mc\u0131 olabilir. G\u00fcnl\u00fck su t\u00fcketiminizi 2-3 litre aras\u0131nda tutun.\n2. D\u00fczenli egzersiz yap\u0131n: Egzersiz, metabolizmay\u0131 h\u0131zland\u0131rmaya yard\u0131mc\u0131 olur ve v\u00fccuttaki ya\u011f yak\u0131m\u0131n\u0131 te\u015fvik eder. Haftada 3-5 g\u00fcn egzersiz yap\u0131n ve her g\u00fcn 30-60 dakika egzersiz yap\u0131n. Egzersizler, aerobik egzersizler (y\u00fcr\u00fcy\u00fc\u015f, y\u00fczme, bisiklet s\u00fcrmek) ve ana kas egzersizleri (kollar, bacaklar, kal\u00e7a) gibi \u00e7e\u015fitli egzersiz t\u00fcrlerini i\u00e7erebilir.\n3. Azalm\u0131\u015f kalori al\u0131m\u0131na odaklan\u0131n: Kilo verme hedefinize ula\u015fmak i\u00e7in kalori al\u0131m\u0131n\u0131z\u0131 azalt\u0131n. Bu, beslenme plan\u0131n\u0131zda azalm\u0131\u015f kalori i\u00e7eren yiyecekleri se\u00e7erek ve a\u015f\u0131r\u0131 kalori al\u0131m\u0131n\u0131 \u00f6nlemek i\u00e7in yemeklerinizi \u00f6l\u00e7erek veya a\u011f\u0131rla\u015ft\u0131rmaya \u00e7al\u0131\u015farak yap\u0131labilir.\n4. Yeterli uyku al: Uyku, metabolizmay\u0131 d\u00fczenleyen ve kilo kayb\u0131na yard\u0131mc\u0131 olan hormonlar\u0131n sal\u0131nmas\u0131n\u0131 etkileyen bir fakt\u00f6rd\u00fcr. G\u00fcnl\u00fck 7-9 saat uyku almay\u0131 hedefleyin.\n5. Stres y\u00f6netimi: Stres, hormonlar\u0131n sal\u0131nmas\u0131n\u0131 etkileyerek kilo al\u0131m\u0131na katk\u0131da bulunabilir. Stresi y\u00f6netmek i\u00e7in egzersiz, meditasyon veya derin nefes egzersizleri gibi \u00e7e\u015fitli y\u00f6ntemleri kullan\u0131n. Bu, stresi azaltmaya ve metabolizmay\u0131 d\u00fczenleme yetene\u011fini geli\u015ftirmeye yard\u0131mc\u0131 olabilir.\n<|end_of_text|>'\n```",
    "library_name": "transformers"
  },
  {
    "model_id": "Backedman/TriviaAnsweringMachineREAL",
    "model_name": "Backedman/TriviaAnsweringMachineREAL",
    "author": "Backedman",
    "downloads": 5266,
    "likes": 0,
    "tags": [
      "transformers",
      "pytorch",
      "TFIDF-QA",
      "question-answering",
      "custom_code",
      "en",
      "license:mit",
      "region:us"
    ],
    "card_url": "https://huggingface.co/Backedman/TriviaAnsweringMachineREAL",
    "dependencies": [
      [
        "icetk",
        "0.0.7"
      ],
      [
        "transformers",
        "4.36.2"
      ]
    ],
    "analysis_date": "2025-03-26T00:50:21.729706",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "TFIDF-QA",
    "pipeline_tag": "question-answering",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": [
        "en"
      ],
      "license": "mit",
      "pipeline_tag": "question-answering"
    },
    "card_text": "The evaluation of this project is to answer trivia questions.  You do\nnot need to do well at this task, but you should submit a system that\ncompletes the task or create adversarial questions in that setting.  This will help the whole class share data and\nresources.\n\nIf you focus on something other than predicting answers, *that's fine*!  \n\nAbout the Data\n==============\n\nQuiz bowl is an academic competition between schools in\nEnglish-speaking countries; hundreds of teams compete in dozens of\ntournaments each year. Quiz bowl is different from Jeopardy, a recent\napplication area.  While Jeopardy also uses signaling devices, these\nare only usable after a question is completed (interrupting Jeopardy's\nquestions would make for bad television).  Thus, Jeopardy is rapacious\nclassification followed by a race---among those who know the\nanswer---to punch a button first.\n\nHere's an example of a quiz bowl question:\n\nExpanding on a 1908 paper by Smoluchowski, he derived a formula for\nthe intensity of scattered light in media fluctuating densities that\nreduces to Rayleigh's law for ideal gases in The Theory of the\nOpalescence of Homogenous Fluids and Liquid Mixtures near the Critical\nState.  That research supported his theories of matter first developed\nwhen he calculated the diffusion constant in terms of fundamental\nparameters of the particles of a gas undergoing Brownian Motion.  In\nthat same year, 1905, he also published On a Heuristic Point of View\nConcerning the Production and Transformation of Light.  That\nexplication of the photoelectric effect won him 1921 Nobel in Physics.\nFor ten points, name this German physicist best known for his theory\nof Relativity.\n\n*ANSWER*: Albert _Einstein_\n\nTwo teams listen to the same question. Teams interrupt the question at\nany point by \"buzzing in\"; if the answer is correct, the team gets\npoints and the next question is read.  Otherwise, the team loses\npoints and the other team can answer.\n\nYou are welcome to use any *automatic* method to choose an answer.  It\nneed not be similar nor build on our provided systems.  In addition to\nthe data we provide, you are welcome to use any external data *except*\nour test quiz bowl questions (i.e., don't hack our server!).  You are\nwelcome (an encouraged) to use any publicly available software, but\nyou may want to check on Piazza for suggestions as many tools are\nbetter (or easier to use) than others.\n\nIf you don't like the interruptability of questions, you can also just answer entire questions.  However, you must also output a confidence.\n\nCompetition\n==================\nWe will use Dynabech website (https://dynabench.org/tasks/qa). If you remember the past workshop about Dynabench submission, this is the way to do it. The specific task name is \"Grounded QA\". Here, with the help of the video tutorial, you submit your QA model and assess how your QA model did compared to others. The assessment will take place by testing your QA model on several QA test datasets and the results of yours and your competitors will be visible on the leaderboard. Your goal is to rank the highest in terms of expected wins: you buzz in with probability proportional to your confidence, and if you're more right than the competition, you win. \n\nWriting Questions\n==================\n\nAlternatively, you can also *write* 50 adversarial questions that\nchallenge modern NLP systems. These questions must be diverse in the\nsubjects asked about, the skills computers need to answer the\nquestions, and the entities in those questions. Remember that your questions should be *factual* and\n*specific* enough for humans to answer, because your task is to stump\nthe computers relative to humans!\n\nIn addition to the raw questions, you will also need to create citations describing:\n* Why the question is difficult for computers: include citations from the NLP/AI/ML literature\n* Why the information in the question is correct: include citations from the sources you drew on the write the question\n* Why the question is interesting: include scholarly / popular culture artifacts to prove that people care about this\n* Why the question is pyramidal: discuss why your first clues are harder than your later clues\n\n**Category**\n\nWe want questions from many domains such as Art, Literature, Geography, History,\nScience, TV and Film, Music, Lifestyle, and Sport. The questions\nshould be written using all topics above (5 questions for each\ncategory and 5 more for the remaining categories). Indicate in your\nwriteup which category you chose to write on for each question.\n\n\nArt:\n\n* Questions about works: Mona Lisa, Raft of the Medussa\n\n* Questions about forms: color, contour, texture\n\n* Questions about artists: Picasso, Monet, Leonardo da Vinci\n\n* Questions about context: Renaissance, post-modernism, expressionism, surrealism\n\n\nLiterature: \n\n*\tQuestions about works: novels (1984), plays (The Lion and the Jewel), poems (Rubaiyat), criticism (Poetics)\n\n*\tQuestions about major characters or events in literature: The Death of Anna Karenina, Noboru Wataya, the Marriage of Hippolyta and Theseus\n\n*\tQuestions about literary movements (Sturm und Drang)\n\n*\tQuestions about translations\n\n*\tCross-cutting questions (appearances of Overcoats in novels)\n\n*\tCommon link questions (the literary output of a country/region)\n\n\nGeography: \n\n*\tQuestions about location: names of capital, state, river\n\n*\tQuestions about the place: temperature, wind flow, humidity\n\n\nHistory: \n\n*\tWhen: When did the First World war start? \n\n*\tWho: Who is called Napoleon of Iran? \n\n*\tWhere: Where was the first Summer Olympics held?\n\n*\tWhich: Which is the oldest civilization in the world?\n\n\nScience: \n\n*\tQuestions about terminology: The concept of gravity was discovered by which famous physicist?\n\n*\tQuestions about the experiment\n\n*\tQuestions about theory: The social action theory believes that individuals are influenced by this theory.\n\n\nTV and Film: \n\n*\tQuotes: What are the dying words of Charles Foster Kane in Citizen Kane?\n\n*\tTitle: What 1927 musical was the first \"talkie\"?\n\n*\tPlot: In The Matrix, does Neo take the blue pill or the red pill?\n\n\nMusic: \n\n*\tSinger: What singer has had a Billboard No. 1 hit in each of the last four decades?\n\n*\tBand: Before Bleachers and fun., Jack Antonoff fronted what band?\n\n*\tTitle: What was Madonna's first top 10 hit?\n\n*\tHistory: Which classical composer was deaf?\n\n\nLifestyle: \n\n*\tClothes: What clothing company, founded by a tennis player, has an alligator logo?\n\n*\tDecoration: What was the first perfume sold by Coco Chanel?\n\n\nSport: \n\n*\tKnown facts: What sport is best known as the \u2018king of sports\u2019?\n\n*\tNationality: What\u2019s the national sport of Canada?\n\n*\tSport player: The classic 1980 movie called Raging Bull is about which real-life boxer?\n\n*\tCountry: What country has competed the most times in the Summer Olympics yet hasn\u2019t won any kind of medal?\n\n\n**Diversity** \n\nOther than category diversity, if you find an ingenious way of writing questions about underrepresented countries, you will get bonus points (indicate which questions you included the diversity component in your writeup). You may decide which are underrepresented countries with your own reasonable reason (etc., less population may indicate underrepresented), but make sure to articulate this in your writeup. \n\n* Run state of the art QA systems on the questions to show they struggle, give individual results for each question and a summary over all questions\n\nFor an example of what the writeup for a single question should look like, see the adversarial HW:\nhttps://github.com/Pinafore/nlp-hw/blob/master/adversarial/question.tex\n\nProposal\n==================\n\nThe project proposal is a one page PDF document that describes:\n\n* Who is on your team (team sizes can be between three and six\n  students, but six is really too big to be effective; my suggestion\n  is that most groups should be between four or five).\n\n* What techniques you will explore \n\n* Your timeline for completing the project (be realistic; you should\n  have your first submission in a week or two)\n\nSubmit the proposal on Gradescope, but make sure to include all group\nmembers.  If all group members are not included, you will lose points.  Late days cannot be used on this\nassignment.\n\nMilestone 1\n====================== \n\nYou'll have to update how things are going: what's\nworking, what isn't, and how does it change your timeline?  How does it change your division of labor?\n\n*Question Writing*: You'll need to have answers selected for all of\nyour questions and first drafts of at least 15 questions.  This must\nbe submitted as a JSON file so that we run computer QA systems on it.\n\n*Project*: You'll need to have made a submission to the leaderboard with something that satisfies the API.\n\nSubmit a PDF updating on your progress to Gradescope.  If all team\nmembers are not on the submission, you will lose points.\n\nMilestone 2\n===================\n\nAs before, provide an updated timeline / division of labor, provide your intermediary results.  \n\n*Question Writing*: You'll need to have reflected the feedback from the first questions and completed a first draft of at least 30 questions.  You'll also need machine results to your questions and an overall evaluation of your human/computer accuracy.\n\n*Project*: You'll need to have a made a submission to the leaderboard with a working system (e.g., not just obey the API, but actually get reasonable answers).\n\nSubmit a PDF updating on your progress.\n\nFinal Presentation\n======================\n\nThe final presentation will be virtual (uploading a video).  In\nthe final presentation you will:\n\n* Explain what you did\n\n* Who did what.  For example, for the question writing project a team of five people might write: A wrote the first draft of questions.  B and C verified they were initially answerable by a human.  B ran computer systems to verify they were challenging to a computer.  C edited the questions and increased the computer difficulty.  D and E verified that the edited questions were still answerable by a human.  D and E checked all of the questions for factual accuracy and created citations and the writeup.\n\n* What challenges you had\n\n* Review how well you did (based on the competition or your own metrics).  If you do not use the course infrastructure to evaluate your project's work, you should talk about what alternative evaluations you used, why they're appropriate/fair, and how well you did on them.\n\n* Provide an error analysis.  An error analysis must contain examples from the\n  development set that you get wrong.  You should show those sentences\n  and explain why (in terms of features or the model) they have the\n  wrong answer.  You should have been doing this all along as you\n  derive new features, but this is your final inspection of\n  your errors. The feature or model problems you discover should not\n  be trivial features you could add easily.  Instead, these should be\n  features or models that are difficult to correct.  An error analysis\n  is not the same thing as simply presenting the error matrix, as it\n  does not inspect any individual examples.  If you're writing questions, talk about examples of questions that didn't work out as intended.\n\n* The linguistic motivation for your features / how your wrote the questions.  This is a\n  computational linguistics class, so you should give precedence to\n  features / techniques that we use in this class (e.g., syntax,\n  morphology, part of speech, word sense, etc.).  Given two features\n  that work equally well and one that is linguistically motivated,\n  we'll prefer the linguistically motivated one.\n\n* Presumably you did many different things; how did they each\n  individually contribute to your final result?\n\nEach group has 10 minutes to deliver their presentation. Please record the video, and upload it to Google Drive, and include the link in your writeup submission.\n\nFinal Question Submission\n======================\n\nBecause we need to get the questions ready for the systems, upload your raw questions on May 10.  This doesn't include the citations or other parts of the writeup.\n\nSystem Submission\n======================\n\nYou must submit a version of your system by May 12. It may not be perfect, but this what the question writing teams will use to test their results.\n\nYour system should be sent directly to the professor and TAs in zip files, including the correct dependencies and a working inference code. Your inference code should run successfully in the root folder (extracted from zip folder) directory with the command:\n\n```\n> python3 inference.py --data=evaluation_set.json \n\n```\n\nThe input will be in the form of a .json file () in the same format as the file the adversarial question writing team submits. The output format should also be in string.\n\nIf you have any notes or comments that we should be aware of while running your code, please include them in the folder as a .txt file. Also, dependency information should be included as a .txt file.\u00a0\n\nPlease prepend your email title with [2024-CMSC 470 System Submission].\n\nProject Writeup and JSON file\n======================\n\nBy May 17, submit your project writeup explaining what\nyou did and what results you achieved.  This document should\nmake it clear:\n\n* Why this is a good idea\n* What you did\n* Who did what\n* Whether your technique worked or not\n\nFor systems, please do not go over 2500 words unless you have a really good reason.\nImages are a much better use of space than words, usually (there's no\nlimit on including images, but use judgement and be selective).\n\nFor question writing, you have one page (single spaced, two column) per question plus a two page summary of results. Talk about how you organized the question writing, how you evaluated the questions, and a summary of the results.  Along with your writeup, turn in a json including the raw text of the question and answer and category. The json file is included in this directory. Make sure your json file is in the correct format and is callable via below code. Your submission will not be graded if it does not follow the format of the example json file.\n\n```\nwith open('path to your json file', 'r') as f:\n    data = json.load(f)\n```\n\n\n\nGrade\n======================\n\nThe grade will be out of 25 points, broken into five areas:\n\n* _Presentation_: For your oral presentation, do you highlight what\n  you did and make people care?  Did you use time well during the\n  presentation?\n\n* _Writeup_: Does the writeup explain what you did in a way that is\n  clear and effective?\n\nThe final three areas are different between the system and the questions.\n\n|    |      System      |  Questions |\n|----------|:-------------:|------:|\n| _Technical Soundness_ |  Did you use the right tools for the job, and did you use them correctly?  Were they relevant to this class? | Were your questions correct and accurately cited. |\n| _Effort_ |  Did you do what you say you would, and was it the right ammount of effort.  | Are the questions well-written, interesting, and thoroughly edited? |\n| _Performance_ | How did your techniques perform in terms of accuracy, recall, etc.? | Is the human accuracy substantially higher than the computer accuracy? |\n\nAll members of the group will receive the same grade.  It's impossible for the course staff to adjudicate Rashomon-style accounts of who did what, and the goal of a group project is for all team members to work together to create a cohesive project that works well together.  While it makes sense to divide the work into distinct areas of responsibility, at grading time we have now way to know who really did what, so it's the groups responsibility to create a piece of output that reflects well on the whole group.\n",
    "card_content": "---\nlanguage:\n- en\nlicense: mit\npipeline_tag: question-answering\n---\nThe evaluation of this project is to answer trivia questions.  You do\nnot need to do well at this task, but you should submit a system that\ncompletes the task or create adversarial questions in that setting.  This will help the whole class share data and\nresources.\n\nIf you focus on something other than predicting answers, *that's fine*!  \n\nAbout the Data\n==============\n\nQuiz bowl is an academic competition between schools in\nEnglish-speaking countries; hundreds of teams compete in dozens of\ntournaments each year. Quiz bowl is different from Jeopardy, a recent\napplication area.  While Jeopardy also uses signaling devices, these\nare only usable after a question is completed (interrupting Jeopardy's\nquestions would make for bad television).  Thus, Jeopardy is rapacious\nclassification followed by a race---among those who know the\nanswer---to punch a button first.\n\nHere's an example of a quiz bowl question:\n\nExpanding on a 1908 paper by Smoluchowski, he derived a formula for\nthe intensity of scattered light in media fluctuating densities that\nreduces to Rayleigh's law for ideal gases in The Theory of the\nOpalescence of Homogenous Fluids and Liquid Mixtures near the Critical\nState.  That research supported his theories of matter first developed\nwhen he calculated the diffusion constant in terms of fundamental\nparameters of the particles of a gas undergoing Brownian Motion.  In\nthat same year, 1905, he also published On a Heuristic Point of View\nConcerning the Production and Transformation of Light.  That\nexplication of the photoelectric effect won him 1921 Nobel in Physics.\nFor ten points, name this German physicist best known for his theory\nof Relativity.\n\n*ANSWER*: Albert _Einstein_\n\nTwo teams listen to the same question. Teams interrupt the question at\nany point by \"buzzing in\"; if the answer is correct, the team gets\npoints and the next question is read.  Otherwise, the team loses\npoints and the other team can answer.\n\nYou are welcome to use any *automatic* method to choose an answer.  It\nneed not be similar nor build on our provided systems.  In addition to\nthe data we provide, you are welcome to use any external data *except*\nour test quiz bowl questions (i.e., don't hack our server!).  You are\nwelcome (an encouraged) to use any publicly available software, but\nyou may want to check on Piazza for suggestions as many tools are\nbetter (or easier to use) than others.\n\nIf you don't like the interruptability of questions, you can also just answer entire questions.  However, you must also output a confidence.\n\nCompetition\n==================\nWe will use Dynabech website (https://dynabench.org/tasks/qa). If you remember the past workshop about Dynabench submission, this is the way to do it. The specific task name is \"Grounded QA\". Here, with the help of the video tutorial, you submit your QA model and assess how your QA model did compared to others. The assessment will take place by testing your QA model on several QA test datasets and the results of yours and your competitors will be visible on the leaderboard. Your goal is to rank the highest in terms of expected wins: you buzz in with probability proportional to your confidence, and if you're more right than the competition, you win. \n\nWriting Questions\n==================\n\nAlternatively, you can also *write* 50 adversarial questions that\nchallenge modern NLP systems. These questions must be diverse in the\nsubjects asked about, the skills computers need to answer the\nquestions, and the entities in those questions. Remember that your questions should be *factual* and\n*specific* enough for humans to answer, because your task is to stump\nthe computers relative to humans!\n\nIn addition to the raw questions, you will also need to create citations describing:\n* Why the question is difficult for computers: include citations from the NLP/AI/ML literature\n* Why the information in the question is correct: include citations from the sources you drew on the write the question\n* Why the question is interesting: include scholarly / popular culture artifacts to prove that people care about this\n* Why the question is pyramidal: discuss why your first clues are harder than your later clues\n\n**Category**\n\nWe want questions from many domains such as Art, Literature, Geography, History,\nScience, TV and Film, Music, Lifestyle, and Sport. The questions\nshould be written using all topics above (5 questions for each\ncategory and 5 more for the remaining categories). Indicate in your\nwriteup which category you chose to write on for each question.\n\n\nArt:\n\n* Questions about works: Mona Lisa, Raft of the Medussa\n\n* Questions about forms: color, contour, texture\n\n* Questions about artists: Picasso, Monet, Leonardo da Vinci\n\n* Questions about context: Renaissance, post-modernism, expressionism, surrealism\n\n\nLiterature: \n\n*\tQuestions about works: novels (1984), plays (The Lion and the Jewel), poems (Rubaiyat), criticism (Poetics)\n\n*\tQuestions about major characters or events in literature: The Death of Anna Karenina, Noboru Wataya, the Marriage of Hippolyta and Theseus\n\n*\tQuestions about literary movements (Sturm und Drang)\n\n*\tQuestions about translations\n\n*\tCross-cutting questions (appearances of Overcoats in novels)\n\n*\tCommon link questions (the literary output of a country/region)\n\n\nGeography: \n\n*\tQuestions about location: names of capital, state, river\n\n*\tQuestions about the place: temperature, wind flow, humidity\n\n\nHistory: \n\n*\tWhen: When did the First World war start? \n\n*\tWho: Who is called Napoleon of Iran? \n\n*\tWhere: Where was the first Summer Olympics held?\n\n*\tWhich: Which is the oldest civilization in the world?\n\n\nScience: \n\n*\tQuestions about terminology: The concept of gravity was discovered by which famous physicist?\n\n*\tQuestions about the experiment\n\n*\tQuestions about theory: The social action theory believes that individuals are influenced by this theory.\n\n\nTV and Film: \n\n*\tQuotes: What are the dying words of Charles Foster Kane in Citizen Kane?\n\n*\tTitle: What 1927 musical was the first \"talkie\"?\n\n*\tPlot: In The Matrix, does Neo take the blue pill or the red pill?\n\n\nMusic: \n\n*\tSinger: What singer has had a Billboard No. 1 hit in each of the last four decades?\n\n*\tBand: Before Bleachers and fun., Jack Antonoff fronted what band?\n\n*\tTitle: What was Madonna's first top 10 hit?\n\n*\tHistory: Which classical composer was deaf?\n\n\nLifestyle: \n\n*\tClothes: What clothing company, founded by a tennis player, has an alligator logo?\n\n*\tDecoration: What was the first perfume sold by Coco Chanel?\n\n\nSport: \n\n*\tKnown facts: What sport is best known as the \u2018king of sports\u2019?\n\n*\tNationality: What\u2019s the national sport of Canada?\n\n*\tSport player: The classic 1980 movie called Raging Bull is about which real-life boxer?\n\n*\tCountry: What country has competed the most times in the Summer Olympics yet hasn\u2019t won any kind of medal?\n\n\n**Diversity** \n\nOther than category diversity, if you find an ingenious way of writing questions about underrepresented countries, you will get bonus points (indicate which questions you included the diversity component in your writeup). You may decide which are underrepresented countries with your own reasonable reason (etc., less population may indicate underrepresented), but make sure to articulate this in your writeup. \n\n* Run state of the art QA systems on the questions to show they struggle, give individual results for each question and a summary over all questions\n\nFor an example of what the writeup for a single question should look like, see the adversarial HW:\nhttps://github.com/Pinafore/nlp-hw/blob/master/adversarial/question.tex\n\nProposal\n==================\n\nThe project proposal is a one page PDF document that describes:\n\n* Who is on your team (team sizes can be between three and six\n  students, but six is really too big to be effective; my suggestion\n  is that most groups should be between four or five).\n\n* What techniques you will explore \n\n* Your timeline for completing the project (be realistic; you should\n  have your first submission in a week or two)\n\nSubmit the proposal on Gradescope, but make sure to include all group\nmembers.  If all group members are not included, you will lose points.  Late days cannot be used on this\nassignment.\n\nMilestone 1\n====================== \n\nYou'll have to update how things are going: what's\nworking, what isn't, and how does it change your timeline?  How does it change your division of labor?\n\n*Question Writing*: You'll need to have answers selected for all of\nyour questions and first drafts of at least 15 questions.  This must\nbe submitted as a JSON file so that we run computer QA systems on it.\n\n*Project*: You'll need to have made a submission to the leaderboard with something that satisfies the API.\n\nSubmit a PDF updating on your progress to Gradescope.  If all team\nmembers are not on the submission, you will lose points.\n\nMilestone 2\n===================\n\nAs before, provide an updated timeline / division of labor, provide your intermediary results.  \n\n*Question Writing*: You'll need to have reflected the feedback from the first questions and completed a first draft of at least 30 questions.  You'll also need machine results to your questions and an overall evaluation of your human/computer accuracy.\n\n*Project*: You'll need to have a made a submission to the leaderboard with a working system (e.g., not just obey the API, but actually get reasonable answers).\n\nSubmit a PDF updating on your progress.\n\nFinal Presentation\n======================\n\nThe final presentation will be virtual (uploading a video).  In\nthe final presentation you will:\n\n* Explain what you did\n\n* Who did what.  For example, for the question writing project a team of five people might write: A wrote the first draft of questions.  B and C verified they were initially answerable by a human.  B ran computer systems to verify they were challenging to a computer.  C edited the questions and increased the computer difficulty.  D and E verified that the edited questions were still answerable by a human.  D and E checked all of the questions for factual accuracy and created citations and the writeup.\n\n* What challenges you had\n\n* Review how well you did (based on the competition or your own metrics).  If you do not use the course infrastructure to evaluate your project's work, you should talk about what alternative evaluations you used, why they're appropriate/fair, and how well you did on them.\n\n* Provide an error analysis.  An error analysis must contain examples from the\n  development set that you get wrong.  You should show those sentences\n  and explain why (in terms of features or the model) they have the\n  wrong answer.  You should have been doing this all along as you\n  derive new features, but this is your final inspection of\n  your errors. The feature or model problems you discover should not\n  be trivial features you could add easily.  Instead, these should be\n  features or models that are difficult to correct.  An error analysis\n  is not the same thing as simply presenting the error matrix, as it\n  does not inspect any individual examples.  If you're writing questions, talk about examples of questions that didn't work out as intended.\n\n* The linguistic motivation for your features / how your wrote the questions.  This is a\n  computational linguistics class, so you should give precedence to\n  features / techniques that we use in this class (e.g., syntax,\n  morphology, part of speech, word sense, etc.).  Given two features\n  that work equally well and one that is linguistically motivated,\n  we'll prefer the linguistically motivated one.\n\n* Presumably you did many different things; how did they each\n  individually contribute to your final result?\n\nEach group has 10 minutes to deliver their presentation. Please record the video, and upload it to Google Drive, and include the link in your writeup submission.\n\nFinal Question Submission\n======================\n\nBecause we need to get the questions ready for the systems, upload your raw questions on May 10.  This doesn't include the citations or other parts of the writeup.\n\nSystem Submission\n======================\n\nYou must submit a version of your system by May 12. It may not be perfect, but this what the question writing teams will use to test their results.\n\nYour system should be sent directly to the professor and TAs in zip files, including the correct dependencies and a working inference code. Your inference code should run successfully in the root folder (extracted from zip folder) directory with the command:\n\n```\n> python3 inference.py --data=evaluation_set.json \n\n```\n\nThe input will be in the form of a .json file () in the same format as the file the adversarial question writing team submits. The output format should also be in string.\n\nIf you have any notes or comments that we should be aware of while running your code, please include them in the folder as a .txt file. Also, dependency information should be included as a .txt file.\u00a0\n\nPlease prepend your email title with [2024-CMSC 470 System Submission].\n\nProject Writeup and JSON file\n======================\n\nBy May 17, submit your project writeup explaining what\nyou did and what results you achieved.  This document should\nmake it clear:\n\n* Why this is a good idea\n* What you did\n* Who did what\n* Whether your technique worked or not\n\nFor systems, please do not go over 2500 words unless you have a really good reason.\nImages are a much better use of space than words, usually (there's no\nlimit on including images, but use judgement and be selective).\n\nFor question writing, you have one page (single spaced, two column) per question plus a two page summary of results. Talk about how you organized the question writing, how you evaluated the questions, and a summary of the results.  Along with your writeup, turn in a json including the raw text of the question and answer and category. The json file is included in this directory. Make sure your json file is in the correct format and is callable via below code. Your submission will not be graded if it does not follow the format of the example json file.\n\n```\nwith open('path to your json file', 'r') as f:\n    data = json.load(f)\n```\n\n\n\nGrade\n======================\n\nThe grade will be out of 25 points, broken into five areas:\n\n* _Presentation_: For your oral presentation, do you highlight what\n  you did and make people care?  Did you use time well during the\n  presentation?\n\n* _Writeup_: Does the writeup explain what you did in a way that is\n  clear and effective?\n\nThe final three areas are different between the system and the questions.\n\n|    |      System      |  Questions |\n|----------|:-------------:|------:|\n| _Technical Soundness_ |  Did you use the right tools for the job, and did you use them correctly?  Were they relevant to this class? | Were your questions correct and accurately cited. |\n| _Effort_ |  Did you do what you say you would, and was it the right ammount of effort.  | Are the questions well-written, interesting, and thoroughly edited? |\n| _Performance_ | How did your techniques perform in terms of accuracy, recall, etc.? | Is the human accuracy substantially higher than the computer accuracy? |\n\nAll members of the group will receive the same grade.  It's impossible for the course staff to adjudicate Rashomon-style accounts of who did what, and the goal of a group project is for all team members to work together to create a cohesive project that works well together.  While it makes sense to divide the work into distinct areas of responsibility, at grading time we have now way to know who really did what, so it's the groups responsibility to create a piece of output that reflects well on the whole group.\n",
    "library_name": "transformers"
  },
  {
    "model_id": "mrm8488/bert-tiny-finetuned-squadv2",
    "model_name": "mrm8488/bert-tiny-finetuned-squadv2",
    "author": "mrm8488",
    "downloads": 5251,
    "likes": 1,
    "tags": [
      "transformers",
      "pytorch",
      "jax",
      "safetensors",
      "bert",
      "question-answering",
      "QA",
      "en",
      "arxiv:1908.08962",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/mrm8488/bert-tiny-finetuned-squadv2",
    "dependencies": [
      [
        "transformers",
        "4.36.2"
      ],
      [
        "torch",
        "2.5.1"
      ]
    ],
    "analysis_date": "2025-03-26T00:50:22.770648",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "bert",
    "pipeline_tag": "question-answering",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": "en",
      "tags": [
        "QA"
      ]
    },
    "card_text": "\n# BERT-Tiny fine-tuned on SQuAD v2\n\n[BERT-Tiny](https://github.com/google-research/bert/) created by [Google Research](https://github.com/google-research) and fine-tuned on [SQuAD 2.0](https://rajpurkar.github.io/SQuAD-explorer/) for **Q&A** downstream task.\n\n**Mode size** (after training): **16.74 MB**\n\n## Details of BERT-Tiny and its 'family' (from their documentation)\n\nReleased on March 11th, 2020\n\nThis is model is a part of 24 smaller BERT models (English only, uncased, trained with WordPiece masking) referenced in [Well-Read Students Learn Better: On the Importance of Pre-training Compact Models](https://arxiv.org/abs/1908.08962).\n\nThe smaller BERT models are intended for environments with restricted computational resources. They can be fine-tuned in the same manner as the original BERT models. However, they are most effective in the context of knowledge distillation, where the fine-tuning labels are produced by a larger and more accurate teacher.\n\n## Details of the downstream task (Q&A) - Dataset\n\n[SQuAD2.0](https://rajpurkar.github.io/SQuAD-explorer/) combines the 100,000 questions in SQuAD1.1 with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuAD2.0, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering.\n\n| Dataset  | Split | # samples |\n| -------- | ----- | --------- |\n| SQuAD2.0 | train | 130k      |\n| SQuAD2.0 | eval  | 12.3k     |\n\n## Model training\n\nThe model was trained on a Tesla P100 GPU and 25GB of RAM.\nThe script for fine tuning can be found [here](https://github.com/huggingface/transformers/tree/main/examples/legacy/question-answering)\n\n## Results:\n\n| Metric | # Value   |\n| ------ | --------- |\n| **EM** | **48.60** |\n| **F1** | **49.73** |\n\n\n| Model                                                                                     | EM        | F1 score  | SIZE (MB) |\n| ----------------------------------------------------------------------------------------- | --------- | --------- | --------- |\n| [bert-tiny-finetuned-squadv2](https://huggingface.co/mrm8488/bert-tiny-finetuned-squadv2) | 48.60     | 49.73     | **16.74** |\n| [bert-tiny-5-finetuned-squadv2](https://huggingface.co/mrm8488/bert-tiny-5-finetuned-squadv2) | **57.12** | **60.86** | 24.34  \n\n## Model in action\n\nFast usage with **pipelines**:\n\n```python\nfrom transformers import pipeline\n\nqa_pipeline = pipeline(\n    \"question-answering\",\n    model=\"mrm8488/bert-tiny-finetuned-squadv2\",\n    tokenizer=\"mrm8488/bert-tiny-finetuned-squadv2\"\n)\n\nqa_pipeline({\n    'context': \"Manuel Romero has been working hardly in the repository hugginface/transformers lately\",\n    'question': \"Who has been working hard for hugginface/transformers lately?\"\n\n})\n\n# Output:\n```\n\n```json\n{\n  \"answer\": \"Manuel Romero\",\n  \"end\": 13,\n  \"score\": 0.05684709993458714,\n  \"start\": 0\n}\n```\n\n### Yes! That was easy \ud83c\udf89 Let's try with another example\n\n```python\nqa_pipeline({\n    'context': \"Manuel Romero has been working hardly in the repository hugginface/transformers lately\",\n    'question': \"For which company has worked Manuel Romero?\"\n})\n\n# Output:\n```\n\n```json\n{\n  \"answer\": \"hugginface/transformers\",\n  \"end\": 79,\n  \"score\": 0.11613431826808274,\n  \"start\": 56\n}\n```\n\n### It works!! \ud83c\udf89 \ud83c\udf89 \ud83c\udf89\n\n> Created by [Manuel Romero/@mrm8488](https://twitter.com/mrm8488) | [LinkedIn](https://www.linkedin.com/in/manuel-romero-cs/)\n\n> Made with <span style=\"color: #e25555;\">&hearts;</span> in Spain\n",
    "card_content": "---\nlanguage: en\ntags:\n- QA\n---\n\n# BERT-Tiny fine-tuned on SQuAD v2\n\n[BERT-Tiny](https://github.com/google-research/bert/) created by [Google Research](https://github.com/google-research) and fine-tuned on [SQuAD 2.0](https://rajpurkar.github.io/SQuAD-explorer/) for **Q&A** downstream task.\n\n**Mode size** (after training): **16.74 MB**\n\n## Details of BERT-Tiny and its 'family' (from their documentation)\n\nReleased on March 11th, 2020\n\nThis is model is a part of 24 smaller BERT models (English only, uncased, trained with WordPiece masking) referenced in [Well-Read Students Learn Better: On the Importance of Pre-training Compact Models](https://arxiv.org/abs/1908.08962).\n\nThe smaller BERT models are intended for environments with restricted computational resources. They can be fine-tuned in the same manner as the original BERT models. However, they are most effective in the context of knowledge distillation, where the fine-tuning labels are produced by a larger and more accurate teacher.\n\n## Details of the downstream task (Q&A) - Dataset\n\n[SQuAD2.0](https://rajpurkar.github.io/SQuAD-explorer/) combines the 100,000 questions in SQuAD1.1 with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuAD2.0, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering.\n\n| Dataset  | Split | # samples |\n| -------- | ----- | --------- |\n| SQuAD2.0 | train | 130k      |\n| SQuAD2.0 | eval  | 12.3k     |\n\n## Model training\n\nThe model was trained on a Tesla P100 GPU and 25GB of RAM.\nThe script for fine tuning can be found [here](https://github.com/huggingface/transformers/tree/main/examples/legacy/question-answering)\n\n## Results:\n\n| Metric | # Value   |\n| ------ | --------- |\n| **EM** | **48.60** |\n| **F1** | **49.73** |\n\n\n| Model                                                                                     | EM        | F1 score  | SIZE (MB) |\n| ----------------------------------------------------------------------------------------- | --------- | --------- | --------- |\n| [bert-tiny-finetuned-squadv2](https://huggingface.co/mrm8488/bert-tiny-finetuned-squadv2) | 48.60     | 49.73     | **16.74** |\n| [bert-tiny-5-finetuned-squadv2](https://huggingface.co/mrm8488/bert-tiny-5-finetuned-squadv2) | **57.12** | **60.86** | 24.34  \n\n## Model in action\n\nFast usage with **pipelines**:\n\n```python\nfrom transformers import pipeline\n\nqa_pipeline = pipeline(\n    \"question-answering\",\n    model=\"mrm8488/bert-tiny-finetuned-squadv2\",\n    tokenizer=\"mrm8488/bert-tiny-finetuned-squadv2\"\n)\n\nqa_pipeline({\n    'context': \"Manuel Romero has been working hardly in the repository hugginface/transformers lately\",\n    'question': \"Who has been working hard for hugginface/transformers lately?\"\n\n})\n\n# Output:\n```\n\n```json\n{\n  \"answer\": \"Manuel Romero\",\n  \"end\": 13,\n  \"score\": 0.05684709993458714,\n  \"start\": 0\n}\n```\n\n### Yes! That was easy \ud83c\udf89 Let's try with another example\n\n```python\nqa_pipeline({\n    'context': \"Manuel Romero has been working hardly in the repository hugginface/transformers lately\",\n    'question': \"For which company has worked Manuel Romero?\"\n})\n\n# Output:\n```\n\n```json\n{\n  \"answer\": \"hugginface/transformers\",\n  \"end\": 79,\n  \"score\": 0.11613431826808274,\n  \"start\": 56\n}\n```\n\n### It works!! \ud83c\udf89 \ud83c\udf89 \ud83c\udf89\n\n> Created by [Manuel Romero/@mrm8488](https://twitter.com/mrm8488) | [LinkedIn](https://www.linkedin.com/in/manuel-romero-cs/)\n\n> Made with <span style=\"color: #e25555;\">&hearts;</span> in Spain\n",
    "library_name": "transformers"
  },
  {
    "model_id": "Xenova/distilbert-base-cased-distilled-squad",
    "model_name": "Xenova/distilbert-base-cased-distilled-squad",
    "author": "Xenova",
    "downloads": 5242,
    "likes": 4,
    "tags": [
      "transformers.js",
      "onnx",
      "distilbert",
      "question-answering",
      "base_model:distilbert/distilbert-base-cased-distilled-squad",
      "base_model:quantized:distilbert/distilbert-base-cased-distilled-squad",
      "region:us"
    ],
    "card_url": "https://huggingface.co/Xenova/distilbert-base-cased-distilled-squad",
    "dependencies": [
      [
        "transformers",
        "4.36.2"
      ],
      [
        "torch",
        "2.5.1"
      ]
    ],
    "analysis_date": "2025-03-26T00:50:24.023740",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "distilbert",
    "pipeline_tag": "question-answering",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "library_name": "transformers.js",
      "base_model": "distilbert-base-cased-distilled-squad"
    },
    "card_text": "\nhttps://huggingface.co/distilbert-base-cased-distilled-squad with ONNX weights to be compatible with Transformers.js.\n\nNote: Having a separate repo for ONNX weights is intended to be a temporary solution until WebML gains more traction. If you would like to make your models web-ready, we recommend converting to ONNX using [\ud83e\udd17 Optimum](https://huggingface.co/docs/optimum/index) and structuring your repo like this one (with ONNX weights located in a subfolder named `onnx`).",
    "card_content": "---\nlibrary_name: transformers.js\nbase_model: distilbert-base-cased-distilled-squad\n---\n\nhttps://huggingface.co/distilbert-base-cased-distilled-squad with ONNX weights to be compatible with Transformers.js.\n\nNote: Having a separate repo for ONNX weights is intended to be a temporary solution until WebML gains more traction. If you would like to make your models web-ready, we recommend converting to ONNX using [\ud83e\udd17 Optimum](https://huggingface.co/docs/optimum/index) and structuring your repo like this one (with ONNX weights located in a subfolder named `onnx`).",
    "library_name": "transformers.js"
  },
  {
    "model_id": "twmkn9/albert-base-v2-squad2",
    "model_name": "twmkn9/albert-base-v2-squad2",
    "author": "twmkn9",
    "downloads": 5173,
    "likes": 4,
    "tags": [
      "transformers",
      "pytorch",
      "albert",
      "question-answering",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/twmkn9/albert-base-v2-squad2",
    "dependencies": null,
    "analysis_date": "2025-03-26T00:50:24.677063",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "albert",
    "pipeline_tag": "question-answering",
    "task_categories": null,
    "license": null,
    "card_metadata": {},
    "card_text": "This model is [ALBERT base v2](https://huggingface.co/albert-base-v2) trained on SQuAD v2 as:\n\n```\nexport SQUAD_DIR=../../squad2\npython3 run_squad.py \n    --model_type albert \n    --model_name_or_path albert-base-v2 \n    --do_train \n    --do_eval \n    --overwrite_cache \n    --do_lower_case \n    --version_2_with_negative \n    --save_steps 100000 \n    --train_file $SQUAD_DIR/train-v2.0.json \n    --predict_file $SQUAD_DIR/dev-v2.0.json \n    --per_gpu_train_batch_size 8 \n    --num_train_epochs 3 \n    --learning_rate 3e-5 \n    --max_seq_length 384 \n    --doc_stride 128 \n    --output_dir ./tmp/albert_fine/\n```\n\nPerformance on a dev subset is close to the original paper:\n\n```\nResults: \n{\n    'exact': 78.71010200723923, \n    'f1': 81.89228117126069, \n    'total': 6078, \n    'HasAns_exact': 75.39518900343643, \n    'HasAns_f1': 82.04167868004215, \n    'HasAns_total': 2910, \n    'NoAns_exact': 81.7550505050505, \n    'NoAns_f1': 81.7550505050505, \n    'NoAns_total': 3168, \n    'best_exact': 78.72655478775913, \n    'best_exact_thresh': 0.0, \n    'best_f1': 81.90873395178066, \n    'best_f1_thresh': 0.0\n}\n```\n\nWe are hopeful this might save you time, energy, and compute. Cheers!",
    "card_content": "---\n{}\n---\nThis model is [ALBERT base v2](https://huggingface.co/albert-base-v2) trained on SQuAD v2 as:\n\n```\nexport SQUAD_DIR=../../squad2\npython3 run_squad.py \n    --model_type albert \n    --model_name_or_path albert-base-v2 \n    --do_train \n    --do_eval \n    --overwrite_cache \n    --do_lower_case \n    --version_2_with_negative \n    --save_steps 100000 \n    --train_file $SQUAD_DIR/train-v2.0.json \n    --predict_file $SQUAD_DIR/dev-v2.0.json \n    --per_gpu_train_batch_size 8 \n    --num_train_epochs 3 \n    --learning_rate 3e-5 \n    --max_seq_length 384 \n    --doc_stride 128 \n    --output_dir ./tmp/albert_fine/\n```\n\nPerformance on a dev subset is close to the original paper:\n\n```\nResults: \n{\n    'exact': 78.71010200723923, \n    'f1': 81.89228117126069, \n    'total': 6078, \n    'HasAns_exact': 75.39518900343643, \n    'HasAns_f1': 82.04167868004215, \n    'HasAns_total': 2910, \n    'NoAns_exact': 81.7550505050505, \n    'NoAns_f1': 81.7550505050505, \n    'NoAns_total': 3168, \n    'best_exact': 78.72655478775913, \n    'best_exact_thresh': 0.0, \n    'best_f1': 81.90873395178066, \n    'best_f1_thresh': 0.0\n}\n```\n\nWe are hopeful this might save you time, energy, and compute. Cheers!",
    "library_name": "transformers"
  },
  {
    "model_id": "houyu0930/test-demo-qa",
    "model_name": "houyu0930/test-demo-qa",
    "author": "houyu0930",
    "downloads": 5163,
    "likes": 0,
    "tags": [
      "transformers",
      "safetensors",
      "distilbert",
      "question-answering",
      "arxiv:1910.09700",
      "license:mit",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/houyu0930/test-demo-qa",
    "dependencies": [
      [
        "transformers",
        "4.36.2"
      ],
      [
        "torch",
        "2.6.0)"
      ]
    ],
    "analysis_date": "2025-03-26T00:50:25.749371",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "distilbert",
    "pipeline_tag": "question-answering",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "license": "mit",
      "library_name": "transformers"
    },
    "card_text": "\n# Model Card for Model ID\n\n<!-- Provide a quick summary of what the model is/does. -->\n\n\n\n## Model Details\n\n### Model Description\n\n<!-- Provide a longer summary of what this model is. -->\n\nThis is the model card of a \ud83e\udd17 transformers model that has been pushed on the Hub. This model card has been automatically generated.\n\n- **Developed by:** [More Information Needed]\n- **Funded by [optional]:** [More Information Needed]\n- **Shared by [optional]:** [More Information Needed]\n- **Model type:** [More Information Needed]\n- **Language(s) (NLP):** [More Information Needed]\n- **License:** [More Information Needed]\n- **Finetuned from model [optional]:** [More Information Needed]\n\n### Model Sources [optional]\n\n<!-- Provide the basic links for the model. -->\n\n- **Repository:** [More Information Needed]\n- **Paper [optional]:** [More Information Needed]\n- **Demo [optional]:** [More Information Needed]\n\n## Uses\n\n<!-- Address questions around how the model is intended to be used, including the foreseeable users of the model and those affected by the model. -->\n\n### Direct Use\n\n<!-- This section is for the model use without fine-tuning or plugging into a larger ecosystem/app. -->\n\n[More Information Needed]\n\n### Downstream Use [optional]\n\n<!-- This section is for the model use when fine-tuned for a task, or when plugged into a larger ecosystem/app -->\n\n[More Information Needed]\n\n### Out-of-Scope Use\n\n<!-- This section addresses misuse, malicious use, and uses that the model will not work well for. -->\n\n[More Information Needed]\n\n## Bias, Risks, and Limitations\n\n<!-- This section is meant to convey both technical and sociotechnical limitations. -->\n\n[More Information Needed]\n\n### Recommendations\n\n<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->\n\nUsers (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.\n\n## How to Get Started with the Model\n\nUse the code below to get started with the model.\n\n[More Information Needed]\n\n## Training Details\n\n### Training Data\n\n<!-- This should link to a Dataset Card, perhaps with a short stub of information on what the training data is all about as well as documentation related to data pre-processing or additional filtering. -->\n\n[More Information Needed]\n\n### Training Procedure \n\n<!-- This relates heavily to the Technical Specifications. Content here should link to that section when it is relevant to the training procedure. -->\n\n#### Preprocessing [optional]\n\n[More Information Needed]\n\n\n#### Training Hyperparameters\n\n- **Training regime:** [More Information Needed] <!--fp32, fp16 mixed precision, bf16 mixed precision, bf16 non-mixed precision, fp16 non-mixed precision, fp8 mixed precision -->\n\n#### Speeds, Sizes, Times [optional]\n\n<!-- This section provides information about throughput, start/end time, checkpoint size if relevant, etc. -->\n\n[More Information Needed]\n\n## Evaluation\n\n<!-- This section describes the evaluation protocols and provides the results. -->\n\n### Testing Data, Factors & Metrics\n\n#### Testing Data\n\n<!-- This should link to a Dataset Card if possible. -->\n\n[More Information Needed]\n\n#### Factors\n\n<!-- These are the things the evaluation is disaggregating by, e.g., subpopulations or domains. -->\n\n[More Information Needed]\n\n#### Metrics\n\n<!-- These are the evaluation metrics being used, ideally with a description of why. -->\n\n[More Information Needed]\n\n### Results\n\n[More Information Needed]\n\n#### Summary\n\n\n\n## Model Examination [optional]\n\n<!-- Relevant interpretability work for the model goes here -->\n\n[More Information Needed]\n\n## Environmental Impact\n\n<!-- Total emissions (in grams of CO2eq) and additional considerations, such as electricity usage, go here. Edit the suggested text below accordingly -->\n\nCarbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).\n\n- **Hardware Type:** [More Information Needed]\n- **Hours used:** [More Information Needed]\n- **Cloud Provider:** [More Information Needed]\n- **Compute Region:** [More Information Needed]\n- **Carbon Emitted:** [More Information Needed]\n\n## Technical Specifications [optional]\n\n### Model Architecture and Objective\n\n[More Information Needed]\n\n### Compute Infrastructure\n\n[More Information Needed]\n\n#### Hardware\n\n[More Information Needed]\n\n#### Software\n\n[More Information Needed]\n\n## Citation [optional]\n\n<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->\n\n**BibTeX:**\n\n[More Information Needed]\n\n**APA:**\n\n[More Information Needed]\n\n## Glossary [optional]\n\n<!-- If relevant, include terms and calculations in this section that can help readers understand the model or model card. -->\n\n[More Information Needed]\n\n## More Information [optional]\n\n[More Information Needed]\n\n## Model Card Authors [optional]\n\n[More Information Needed]\n\n## Model Card Contact\n\n[More Information Needed]",
    "card_content": "---\nlicense: mit\nlibrary_name: transformers\n---\n\n# Model Card for Model ID\n\n<!-- Provide a quick summary of what the model is/does. -->\n\n\n\n## Model Details\n\n### Model Description\n\n<!-- Provide a longer summary of what this model is. -->\n\nThis is the model card of a \ud83e\udd17 transformers model that has been pushed on the Hub. This model card has been automatically generated.\n\n- **Developed by:** [More Information Needed]\n- **Funded by [optional]:** [More Information Needed]\n- **Shared by [optional]:** [More Information Needed]\n- **Model type:** [More Information Needed]\n- **Language(s) (NLP):** [More Information Needed]\n- **License:** [More Information Needed]\n- **Finetuned from model [optional]:** [More Information Needed]\n\n### Model Sources [optional]\n\n<!-- Provide the basic links for the model. -->\n\n- **Repository:** [More Information Needed]\n- **Paper [optional]:** [More Information Needed]\n- **Demo [optional]:** [More Information Needed]\n\n## Uses\n\n<!-- Address questions around how the model is intended to be used, including the foreseeable users of the model and those affected by the model. -->\n\n### Direct Use\n\n<!-- This section is for the model use without fine-tuning or plugging into a larger ecosystem/app. -->\n\n[More Information Needed]\n\n### Downstream Use [optional]\n\n<!-- This section is for the model use when fine-tuned for a task, or when plugged into a larger ecosystem/app -->\n\n[More Information Needed]\n\n### Out-of-Scope Use\n\n<!-- This section addresses misuse, malicious use, and uses that the model will not work well for. -->\n\n[More Information Needed]\n\n## Bias, Risks, and Limitations\n\n<!-- This section is meant to convey both technical and sociotechnical limitations. -->\n\n[More Information Needed]\n\n### Recommendations\n\n<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->\n\nUsers (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.\n\n## How to Get Started with the Model\n\nUse the code below to get started with the model.\n\n[More Information Needed]\n\n## Training Details\n\n### Training Data\n\n<!-- This should link to a Dataset Card, perhaps with a short stub of information on what the training data is all about as well as documentation related to data pre-processing or additional filtering. -->\n\n[More Information Needed]\n\n### Training Procedure \n\n<!-- This relates heavily to the Technical Specifications. Content here should link to that section when it is relevant to the training procedure. -->\n\n#### Preprocessing [optional]\n\n[More Information Needed]\n\n\n#### Training Hyperparameters\n\n- **Training regime:** [More Information Needed] <!--fp32, fp16 mixed precision, bf16 mixed precision, bf16 non-mixed precision, fp16 non-mixed precision, fp8 mixed precision -->\n\n#### Speeds, Sizes, Times [optional]\n\n<!-- This section provides information about throughput, start/end time, checkpoint size if relevant, etc. -->\n\n[More Information Needed]\n\n## Evaluation\n\n<!-- This section describes the evaluation protocols and provides the results. -->\n\n### Testing Data, Factors & Metrics\n\n#### Testing Data\n\n<!-- This should link to a Dataset Card if possible. -->\n\n[More Information Needed]\n\n#### Factors\n\n<!-- These are the things the evaluation is disaggregating by, e.g., subpopulations or domains. -->\n\n[More Information Needed]\n\n#### Metrics\n\n<!-- These are the evaluation metrics being used, ideally with a description of why. -->\n\n[More Information Needed]\n\n### Results\n\n[More Information Needed]\n\n#### Summary\n\n\n\n## Model Examination [optional]\n\n<!-- Relevant interpretability work for the model goes here -->\n\n[More Information Needed]\n\n## Environmental Impact\n\n<!-- Total emissions (in grams of CO2eq) and additional considerations, such as electricity usage, go here. Edit the suggested text below accordingly -->\n\nCarbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).\n\n- **Hardware Type:** [More Information Needed]\n- **Hours used:** [More Information Needed]\n- **Cloud Provider:** [More Information Needed]\n- **Compute Region:** [More Information Needed]\n- **Carbon Emitted:** [More Information Needed]\n\n## Technical Specifications [optional]\n\n### Model Architecture and Objective\n\n[More Information Needed]\n\n### Compute Infrastructure\n\n[More Information Needed]\n\n#### Hardware\n\n[More Information Needed]\n\n#### Software\n\n[More Information Needed]\n\n## Citation [optional]\n\n<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->\n\n**BibTeX:**\n\n[More Information Needed]\n\n**APA:**\n\n[More Information Needed]\n\n## Glossary [optional]\n\n<!-- If relevant, include terms and calculations in this section that can help readers understand the model or model card. -->\n\n[More Information Needed]\n\n## More Information [optional]\n\n[More Information Needed]\n\n## Model Card Authors [optional]\n\n[More Information Needed]\n\n## Model Card Contact\n\n[More Information Needed]",
    "library_name": "transformers"
  },
  {
    "model_id": "nes470/pipeline-as-repo",
    "model_name": "nes470/pipeline-as-repo",
    "author": "nes470",
    "downloads": 5083,
    "likes": 0,
    "tags": [
      "transformers",
      "pytorch",
      "QA-umd-quizbowl",
      "question-answering",
      "custom_code",
      "license:mit",
      "region:us"
    ],
    "card_url": "https://huggingface.co/nes470/pipeline-as-repo",
    "dependencies": [
      [
        "transformers",
        "4.36.2"
      ],
      [
        "torch",
        "2.6.0)"
      ]
    ],
    "analysis_date": "2025-03-26T00:50:27.215564",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "QA-umd-quizbowl",
    "pipeline_tag": "question-answering",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "license": "mit",
      "library_name": "transformers"
    },
    "card_text": "\nNames: Nuran, Joshua, Robert \n\n\n\n\n\n\n\n\n\nThe evaluation of this project is to answer trivia questions.  You do\nnot need to do well at this task, but you should submit a system that\ncompletes the task or create adversarial questions in that setting.  This will help the whole class share data and\nresources.\n\nIf you focus on something other than predicting answers, *that's fine*!  \n\nAbout the Data\n==============\n\nQuiz bowl is an academic competition between schools in\nEnglish-speaking countries; hundreds of teams compete in dozens of\ntournaments each year. Quiz bowl is different from Jeopardy, a recent\napplication area.  While Jeopardy also uses signaling devices, these\nare only usable after a question is completed (interrupting Jeopardy's\nquestions would make for bad television).  Thus, Jeopardy is rapacious\nclassification followed by a race---among those who know the\nanswer---to punch a button first.\n\nHere's an example of a quiz bowl question:\n\nExpanding on a 1908 paper by Smoluchowski, he derived a formula for\nthe intensity of scattered light in media fluctuating densities that\nreduces to Rayleigh's law for ideal gases in The Theory of the\nOpalescence of Homogenous Fluids and Liquid Mixtures near the Critical\nState.  That research supported his theories of matter first developed\nwhen he calculated the diffusion constant in terms of fundamental\nparameters of the particles of a gas undergoing Brownian Motion.  In\nthat same year, 1905, he also published On a Heuristic Point of View\nConcerning the Production and Transformation of Light.  That\nexplication of the photoelectric effect won him 1921 Nobel in Physics.\nFor ten points, name this German physicist best known for his theory\nof Relativity.\n\n*ANSWER*: Albert _Einstein_\n\nTwo teams listen to the same question. Teams interrupt the question at\nany point by \"buzzing in\"; if the answer is correct, the team gets\npoints and the next question is read.  Otherwise, the team loses\npoints and the other team can answer.\n\nYou are welcome to use any *automatic* method to choose an answer.  It\nneed not be similar nor build on our provided systems.  In addition to\nthe data we provide, you are welcome to use any external data *except*\nour test quiz bowl questions (i.e., don't hack our server!).  You are\nwelcome (an encouraged) to use any publicly available software, but\nyou may want to check on Piazza for suggestions as many tools are\nbetter (or easier to use) than others.\n\nIf you don't like the interruptability of questions, you can also just answer entire questions.  However, you must also output a confidence.\n\nCompetition\n==================\nWe will use Dynabech website (https://dynabench.org/tasks/qa). If you remember the past workshop about Dynabench submission, this is the way to do it. The specific task name is \"Grounded QA\". Here, with the help of the video tutorial, you submit your QA model and assess how your QA model did compared to others. The assessment will take place by testing your QA model on several QA test datasets and the results of yours and your competitors will be visible on the leaderboard. Your goal is to rank the highest in terms of expected wins: you buzz in with probability proportional to your confidence, and if you're more right than the competition, you win. \n\nWriting Questions\n==================\n\nAlternatively, you can also *write* 50 adversarial questions that\nchallenge modern NLP systems. These questions must be diverse in the\nsubjects asked about, the skills computers need to answer the\nquestions, and the entities in those questions. Remember that your questions should be *factual* and\n*specific* enough for humans to answer, because your task is to stump\nthe computers relative to humans!\n\nIn addition to the raw questions, you will also need to create citations describing:\n* Why the question is difficult for computers: include citations from the NLP/AI/ML literature\n* Why the information in the question is correct: include citations from the sources you drew on the write the question\n* Why the question is interesting: include scholarly / popular culture artifacts to prove that people care about this\n* Why the question is pyramidal: discuss why your first clues are harder than your later clues\n\n**Category**\n\nWe want questions from many domains such as Art, Literature, Geography, History,\nScience, TV and Film, Music, Lifestyle, and Sport. The questions\nshould be written using all topics above (5 questions for each\ncategory and 5 more for the remaining categories). Indicate in your\nwriteup which category you chose to write on for each question.\n\n\nArt:\n\n* Questions about works: Mona Lisa, Raft of the Medussa\n\n* Questions about forms: color, contour, texture\n\n* Questions about artists: Picasso, Monet, Leonardo da Vinci\n\n* Questions about context: Renaissance, post-modernism, expressionism, surrealism\n\n\nLiterature: \n\n*\tQuestions about works: novels (1984), plays (The Lion and the Jewel), poems (Rubaiyat), criticism (Poetics)\n\n*\tQuestions about major characters or events in literature: The Death of Anna Karenina, Noboru Wataya, the Marriage of Hippolyta and Theseus\n\n*\tQuestions about literary movements (Sturm und Drang)\n\n*\tQuestions about translations\n\n*\tCross-cutting questions (appearances of Overcoats in novels)\n\n*\tCommon link questions (the literary output of a country/region)\n\n\nGeography: \n\n*\tQuestions about location: names of capital, state, river\n\n*\tQuestions about the place: temperature, wind flow, humidity\n\n\nHistory: \n\n*\tWhen: When did the First World war start? \n\n*\tWho: Who is called Napoleon of Iran? \n\n*\tWhere: Where was the first Summer Olympics held?\n\n*\tWhich: Which is the oldest civilization in the world?\n\n\nScience: \n\n*\tQuestions about terminology: The concept of gravity was discovered by which famous physicist?\n\n*\tQuestions about the experiment\n\n*\tQuestions about theory: The social action theory believes that individuals are influenced by this theory.\n\n\nTV and Film: \n\n*\tQuotes: What are the dying words of Charles Foster Kane in Citizen Kane?\n\n*\tTitle: What 1927 musical was the first \"talkie\"?\n\n*\tPlot: In The Matrix, does Neo take the blue pill or the red pill?\n\n\nMusic: \n\n*\tSinger: What singer has had a Billboard No. 1 hit in each of the last four decades?\n\n*\tBand: Before Bleachers and fun., Jack Antonoff fronted what band?\n\n*\tTitle: What was Madonna's first top 10 hit?\n\n*\tHistory: Which classical composer was deaf?\n\n\nLifestyle: \n\n*\tClothes: What clothing company, founded by a tennis player, has an alligator logo?\n\n*\tDecoration: What was the first perfume sold by Coco Chanel?\n\n\nSport: \n\n*\tKnown facts: What sport is best known as the \u2018king of sports\u2019?\n\n*\tNationality: What\u2019s the national sport of Canada?\n\n*\tSport player: The classic 1980 movie called Raging Bull is about which real-life boxer?\n\n*\tCountry: What country has competed the most times in the Summer Olympics yet hasn\u2019t won any kind of medal?\n\n\n**Diversity** \n\nOther than category diversity, if you find an ingenious way of writing questions about underrepresented countries, you will get bonus points (indicate which questions you included the diversity component in your writeup). You may decide which are underrepresented countries with your own reasonable reason (etc., less population may indicate underrepresented), but make sure to articulate this in your writeup. \n\n* Run state of the art QA systems on the questions to show they struggle, give individual results for each question and a summary over all questions\n\nFor an example of what the writeup for a single question should look like, see the adversarial HW:\nhttps://github.com/Pinafore/nlp-hw/blob/master/adversarial/question.tex\n\nProposal\n==================\n\nThe project proposal is a one page PDF document that describes:\n\n* Who is on your team (team sizes can be between three and six\n  students, but six is really too big to be effective; my suggestion\n  is that most groups should be between four or five).\n\n* What techniques you will explore \n\n* Your timeline for completing the project (be realistic; you should\n  have your first submission in a week or two)\n\nSubmit the proposal on Gradescope, but make sure to include all group\nmembers.  If all group members are not included, you will lose points.  Late days cannot be used on this\nassignment.\n\nMilestone 1\n====================== \n\nYou'll have to update how things are going: what's\nworking, what isn't, and how does it change your timeline?  How does it change your division of labor?\n\n*Question Writing*: You'll need to have answers selected for all of\nyour questions and first drafts of at least 15 questions.  This must\nbe submitted as a JSON file so that we run computer QA systems on it.\n\n*Project*: You'll need to have made a submission to the leaderboard with something that satisfies the API.\n\nSubmit a PDF updating on your progress to Gradescope.  If all team\nmembers are not on the submission, you will lose points.\n\nMilestone 2\n===================\n\nAs before, provide an updated timeline / division of labor, provide your intermediary results.  \n\n*Question Writing*: You'll need to have reflected the feedback from the first questions and completed a first draft of at least 30 questions.  You'll also need machine results to your questions and an overall evaluation of your human/computer accuracy.\n\n*Project*: You'll need to have a made a submission to the leaderboard with a working system (e.g., not just obey the API, but actually get reasonable answers).\n\nSubmit a PDF updating on your progress.\n\nFinal Presentation\n======================\n\nThe final presentation will be virtual (uploading a video).  In\nthe final presentation you will:\n\n* Explain what you did\n\n* Who did what.  For example, for the question writing project a team of five people might write: A wrote the first draft of questions.  B and C verified they were initially answerable by a human.  B ran computer systems to verify they were challenging to a computer.  C edited the questions and increased the computer difficulty.  D and E verified that the edited questions were still answerable by a human.  D and E checked all of the questions for factual accuracy and created citations and the writeup.\n\n* What challenges you had\n\n* Review how well you did (based on the competition or your own metrics).  If you do not use the course infrastructure to evaluate your project's work, you should talk about what alternative evaluations you used, why they're appropriate/fair, and how well you did on them.\n\n* Provide an error analysis.  An error analysis must contain examples from the\n  development set that you get wrong.  You should show those sentences\n  and explain why (in terms of features or the model) they have the\n  wrong answer.  You should have been doing this all along as you\n  derive new features, but this is your final inspection of\n  your errors. The feature or model problems you discover should not\n  be trivial features you could add easily.  Instead, these should be\n  features or models that are difficult to correct.  An error analysis\n  is not the same thing as simply presenting the error matrix, as it\n  does not inspect any individual examples.  If you're writing questions, talk about examples of questions that didn't work out as intended.\n\n* The linguistic motivation for your features / how your wrote the questions.  This is a\n  computational linguistics class, so you should give precedence to\n  features / techniques that we use in this class (e.g., syntax,\n  morphology, part of speech, word sense, etc.).  Given two features\n  that work equally well and one that is linguistically motivated,\n  we'll prefer the linguistically motivated one.\n\n* Presumably you did many different things; how did they each\n  individually contribute to your final result?\n\nEach group has 10 minutes to deliver their presentation. Please record the video, and upload it to Google Drive, and include the link in your writeup submission.\n\nFinal Question Submission\n======================\n\nBecause we need to get the questions ready for the systems, upload your raw questions on May 10.  This doesn't include the citations or other parts of the writeup.\n\nSystem Submission\n======================\n\nYou must submit a version of your system by May 12. It may not be perfect, but this what the question writing teams will use to test their results.\n\nYour system should be sent directly to the professor and TAs in zip files, including the correct dependencies and a working inference code. Your inference code should run successfully in the root folder (extracted from zip folder) directory with the command:\n\n```\n> python3 inference.py --data=evaluation_set.json \n\n```\n\nThe input will be in the form of a .json file () in the same format as the file the adversarial question writing team submits. The output format should also be in string.\n\nIf you have any notes or comments that we should be aware of while running your code, please include them in the folder as a .txt file. Also, dependency information should be included as a .txt file.\u00a0\n\nPlease prepend your email title with [2024-CMSC 470 System Submission].\n\nProject Writeup and JSON file\n======================\n\nBy May 17, submit your project writeup explaining what\nyou did and what results you achieved.  This document should\nmake it clear:\n\n* Why this is a good idea\n* What you did\n* Who did what\n* Whether your technique worked or not\n\nFor systems, please do not go over 2500 words unless you have a really good reason.\nImages are a much better use of space than words, usually (there's no\nlimit on including images, but use judgement and be selective).\n\nFor question writing, you have one page (single spaced, two column) per question plus a two page summary of results. Talk about how you organized the question writing, how you evaluated the questions, and a summary of the results.  Along with your writeup, turn in a json including the raw text of the question and answer and category. The json file is included in this directory. Make sure your json file is in the correct format and is callable via below code. Your submission will not be graded if it does not follow the format of the example json file.\n\n```\nwith open('path to your json file', 'r') as f:\n    data = json.load(f)\n```\n\n\n\nGrade\n======================\n\nThe grade will be out of 25 points, broken into five areas:\n\n* _Presentation_: For your oral presentation, do you highlight what\n  you did and make people care?  Did you use time well during the\n  presentation?\n\n* _Writeup_: Does the writeup explain what you did in a way that is\n  clear and effective?\n\nThe final three areas are different between the system and the questions.\n\n|    |      System      |  Questions |\n|----------|:-------------:|------:|\n| _Technical Soundness_ |  Did you use the right tools for the job, and did you use them correctly?  Were they relevant to this class? | Were your questions correct and accurately cited. |\n| _Effort_ |  Did you do what you say you would, and was it the right ammount of effort.  | Are the questions well-written, interesting, and thoroughly edited? |\n| _Performance_ | How did your techniques perform in terms of accuracy, recall, etc.? | Is the human accuracy substantially higher than the computer accuracy? |\n\nAll members of the group will receive the same grade.  It's impossible for the course staff to adjudicate Rashomon-style accounts of who did what, and the goal of a group project is for all team members to work together to create a cohesive project that works well together.  While it makes sense to divide the work into distinct areas of responsibility, at grading time we have now way to know who really did what, so it's the groups responsibility to create a piece of output that reflects well on the whole group.",
    "card_content": "---\nlicense: mit\nlibrary_name: transformers\n---\n\nNames: Nuran, Joshua, Robert \n\n\n\n\n\n\n\n\n\nThe evaluation of this project is to answer trivia questions.  You do\nnot need to do well at this task, but you should submit a system that\ncompletes the task or create adversarial questions in that setting.  This will help the whole class share data and\nresources.\n\nIf you focus on something other than predicting answers, *that's fine*!  \n\nAbout the Data\n==============\n\nQuiz bowl is an academic competition between schools in\nEnglish-speaking countries; hundreds of teams compete in dozens of\ntournaments each year. Quiz bowl is different from Jeopardy, a recent\napplication area.  While Jeopardy also uses signaling devices, these\nare only usable after a question is completed (interrupting Jeopardy's\nquestions would make for bad television).  Thus, Jeopardy is rapacious\nclassification followed by a race---among those who know the\nanswer---to punch a button first.\n\nHere's an example of a quiz bowl question:\n\nExpanding on a 1908 paper by Smoluchowski, he derived a formula for\nthe intensity of scattered light in media fluctuating densities that\nreduces to Rayleigh's law for ideal gases in The Theory of the\nOpalescence of Homogenous Fluids and Liquid Mixtures near the Critical\nState.  That research supported his theories of matter first developed\nwhen he calculated the diffusion constant in terms of fundamental\nparameters of the particles of a gas undergoing Brownian Motion.  In\nthat same year, 1905, he also published On a Heuristic Point of View\nConcerning the Production and Transformation of Light.  That\nexplication of the photoelectric effect won him 1921 Nobel in Physics.\nFor ten points, name this German physicist best known for his theory\nof Relativity.\n\n*ANSWER*: Albert _Einstein_\n\nTwo teams listen to the same question. Teams interrupt the question at\nany point by \"buzzing in\"; if the answer is correct, the team gets\npoints and the next question is read.  Otherwise, the team loses\npoints and the other team can answer.\n\nYou are welcome to use any *automatic* method to choose an answer.  It\nneed not be similar nor build on our provided systems.  In addition to\nthe data we provide, you are welcome to use any external data *except*\nour test quiz bowl questions (i.e., don't hack our server!).  You are\nwelcome (an encouraged) to use any publicly available software, but\nyou may want to check on Piazza for suggestions as many tools are\nbetter (or easier to use) than others.\n\nIf you don't like the interruptability of questions, you can also just answer entire questions.  However, you must also output a confidence.\n\nCompetition\n==================\nWe will use Dynabech website (https://dynabench.org/tasks/qa). If you remember the past workshop about Dynabench submission, this is the way to do it. The specific task name is \"Grounded QA\". Here, with the help of the video tutorial, you submit your QA model and assess how your QA model did compared to others. The assessment will take place by testing your QA model on several QA test datasets and the results of yours and your competitors will be visible on the leaderboard. Your goal is to rank the highest in terms of expected wins: you buzz in with probability proportional to your confidence, and if you're more right than the competition, you win. \n\nWriting Questions\n==================\n\nAlternatively, you can also *write* 50 adversarial questions that\nchallenge modern NLP systems. These questions must be diverse in the\nsubjects asked about, the skills computers need to answer the\nquestions, and the entities in those questions. Remember that your questions should be *factual* and\n*specific* enough for humans to answer, because your task is to stump\nthe computers relative to humans!\n\nIn addition to the raw questions, you will also need to create citations describing:\n* Why the question is difficult for computers: include citations from the NLP/AI/ML literature\n* Why the information in the question is correct: include citations from the sources you drew on the write the question\n* Why the question is interesting: include scholarly / popular culture artifacts to prove that people care about this\n* Why the question is pyramidal: discuss why your first clues are harder than your later clues\n\n**Category**\n\nWe want questions from many domains such as Art, Literature, Geography, History,\nScience, TV and Film, Music, Lifestyle, and Sport. The questions\nshould be written using all topics above (5 questions for each\ncategory and 5 more for the remaining categories). Indicate in your\nwriteup which category you chose to write on for each question.\n\n\nArt:\n\n* Questions about works: Mona Lisa, Raft of the Medussa\n\n* Questions about forms: color, contour, texture\n\n* Questions about artists: Picasso, Monet, Leonardo da Vinci\n\n* Questions about context: Renaissance, post-modernism, expressionism, surrealism\n\n\nLiterature: \n\n*\tQuestions about works: novels (1984), plays (The Lion and the Jewel), poems (Rubaiyat), criticism (Poetics)\n\n*\tQuestions about major characters or events in literature: The Death of Anna Karenina, Noboru Wataya, the Marriage of Hippolyta and Theseus\n\n*\tQuestions about literary movements (Sturm und Drang)\n\n*\tQuestions about translations\n\n*\tCross-cutting questions (appearances of Overcoats in novels)\n\n*\tCommon link questions (the literary output of a country/region)\n\n\nGeography: \n\n*\tQuestions about location: names of capital, state, river\n\n*\tQuestions about the place: temperature, wind flow, humidity\n\n\nHistory: \n\n*\tWhen: When did the First World war start? \n\n*\tWho: Who is called Napoleon of Iran? \n\n*\tWhere: Where was the first Summer Olympics held?\n\n*\tWhich: Which is the oldest civilization in the world?\n\n\nScience: \n\n*\tQuestions about terminology: The concept of gravity was discovered by which famous physicist?\n\n*\tQuestions about the experiment\n\n*\tQuestions about theory: The social action theory believes that individuals are influenced by this theory.\n\n\nTV and Film: \n\n*\tQuotes: What are the dying words of Charles Foster Kane in Citizen Kane?\n\n*\tTitle: What 1927 musical was the first \"talkie\"?\n\n*\tPlot: In The Matrix, does Neo take the blue pill or the red pill?\n\n\nMusic: \n\n*\tSinger: What singer has had a Billboard No. 1 hit in each of the last four decades?\n\n*\tBand: Before Bleachers and fun., Jack Antonoff fronted what band?\n\n*\tTitle: What was Madonna's first top 10 hit?\n\n*\tHistory: Which classical composer was deaf?\n\n\nLifestyle: \n\n*\tClothes: What clothing company, founded by a tennis player, has an alligator logo?\n\n*\tDecoration: What was the first perfume sold by Coco Chanel?\n\n\nSport: \n\n*\tKnown facts: What sport is best known as the \u2018king of sports\u2019?\n\n*\tNationality: What\u2019s the national sport of Canada?\n\n*\tSport player: The classic 1980 movie called Raging Bull is about which real-life boxer?\n\n*\tCountry: What country has competed the most times in the Summer Olympics yet hasn\u2019t won any kind of medal?\n\n\n**Diversity** \n\nOther than category diversity, if you find an ingenious way of writing questions about underrepresented countries, you will get bonus points (indicate which questions you included the diversity component in your writeup). You may decide which are underrepresented countries with your own reasonable reason (etc., less population may indicate underrepresented), but make sure to articulate this in your writeup. \n\n* Run state of the art QA systems on the questions to show they struggle, give individual results for each question and a summary over all questions\n\nFor an example of what the writeup for a single question should look like, see the adversarial HW:\nhttps://github.com/Pinafore/nlp-hw/blob/master/adversarial/question.tex\n\nProposal\n==================\n\nThe project proposal is a one page PDF document that describes:\n\n* Who is on your team (team sizes can be between three and six\n  students, but six is really too big to be effective; my suggestion\n  is that most groups should be between four or five).\n\n* What techniques you will explore \n\n* Your timeline for completing the project (be realistic; you should\n  have your first submission in a week or two)\n\nSubmit the proposal on Gradescope, but make sure to include all group\nmembers.  If all group members are not included, you will lose points.  Late days cannot be used on this\nassignment.\n\nMilestone 1\n====================== \n\nYou'll have to update how things are going: what's\nworking, what isn't, and how does it change your timeline?  How does it change your division of labor?\n\n*Question Writing*: You'll need to have answers selected for all of\nyour questions and first drafts of at least 15 questions.  This must\nbe submitted as a JSON file so that we run computer QA systems on it.\n\n*Project*: You'll need to have made a submission to the leaderboard with something that satisfies the API.\n\nSubmit a PDF updating on your progress to Gradescope.  If all team\nmembers are not on the submission, you will lose points.\n\nMilestone 2\n===================\n\nAs before, provide an updated timeline / division of labor, provide your intermediary results.  \n\n*Question Writing*: You'll need to have reflected the feedback from the first questions and completed a first draft of at least 30 questions.  You'll also need machine results to your questions and an overall evaluation of your human/computer accuracy.\n\n*Project*: You'll need to have a made a submission to the leaderboard with a working system (e.g., not just obey the API, but actually get reasonable answers).\n\nSubmit a PDF updating on your progress.\n\nFinal Presentation\n======================\n\nThe final presentation will be virtual (uploading a video).  In\nthe final presentation you will:\n\n* Explain what you did\n\n* Who did what.  For example, for the question writing project a team of five people might write: A wrote the first draft of questions.  B and C verified they were initially answerable by a human.  B ran computer systems to verify they were challenging to a computer.  C edited the questions and increased the computer difficulty.  D and E verified that the edited questions were still answerable by a human.  D and E checked all of the questions for factual accuracy and created citations and the writeup.\n\n* What challenges you had\n\n* Review how well you did (based on the competition or your own metrics).  If you do not use the course infrastructure to evaluate your project's work, you should talk about what alternative evaluations you used, why they're appropriate/fair, and how well you did on them.\n\n* Provide an error analysis.  An error analysis must contain examples from the\n  development set that you get wrong.  You should show those sentences\n  and explain why (in terms of features or the model) they have the\n  wrong answer.  You should have been doing this all along as you\n  derive new features, but this is your final inspection of\n  your errors. The feature or model problems you discover should not\n  be trivial features you could add easily.  Instead, these should be\n  features or models that are difficult to correct.  An error analysis\n  is not the same thing as simply presenting the error matrix, as it\n  does not inspect any individual examples.  If you're writing questions, talk about examples of questions that didn't work out as intended.\n\n* The linguistic motivation for your features / how your wrote the questions.  This is a\n  computational linguistics class, so you should give precedence to\n  features / techniques that we use in this class (e.g., syntax,\n  morphology, part of speech, word sense, etc.).  Given two features\n  that work equally well and one that is linguistically motivated,\n  we'll prefer the linguistically motivated one.\n\n* Presumably you did many different things; how did they each\n  individually contribute to your final result?\n\nEach group has 10 minutes to deliver their presentation. Please record the video, and upload it to Google Drive, and include the link in your writeup submission.\n\nFinal Question Submission\n======================\n\nBecause we need to get the questions ready for the systems, upload your raw questions on May 10.  This doesn't include the citations or other parts of the writeup.\n\nSystem Submission\n======================\n\nYou must submit a version of your system by May 12. It may not be perfect, but this what the question writing teams will use to test their results.\n\nYour system should be sent directly to the professor and TAs in zip files, including the correct dependencies and a working inference code. Your inference code should run successfully in the root folder (extracted from zip folder) directory with the command:\n\n```\n> python3 inference.py --data=evaluation_set.json \n\n```\n\nThe input will be in the form of a .json file () in the same format as the file the adversarial question writing team submits. The output format should also be in string.\n\nIf you have any notes or comments that we should be aware of while running your code, please include them in the folder as a .txt file. Also, dependency information should be included as a .txt file.\u00a0\n\nPlease prepend your email title with [2024-CMSC 470 System Submission].\n\nProject Writeup and JSON file\n======================\n\nBy May 17, submit your project writeup explaining what\nyou did and what results you achieved.  This document should\nmake it clear:\n\n* Why this is a good idea\n* What you did\n* Who did what\n* Whether your technique worked or not\n\nFor systems, please do not go over 2500 words unless you have a really good reason.\nImages are a much better use of space than words, usually (there's no\nlimit on including images, but use judgement and be selective).\n\nFor question writing, you have one page (single spaced, two column) per question plus a two page summary of results. Talk about how you organized the question writing, how you evaluated the questions, and a summary of the results.  Along with your writeup, turn in a json including the raw text of the question and answer and category. The json file is included in this directory. Make sure your json file is in the correct format and is callable via below code. Your submission will not be graded if it does not follow the format of the example json file.\n\n```\nwith open('path to your json file', 'r') as f:\n    data = json.load(f)\n```\n\n\n\nGrade\n======================\n\nThe grade will be out of 25 points, broken into five areas:\n\n* _Presentation_: For your oral presentation, do you highlight what\n  you did and make people care?  Did you use time well during the\n  presentation?\n\n* _Writeup_: Does the writeup explain what you did in a way that is\n  clear and effective?\n\nThe final three areas are different between the system and the questions.\n\n|    |      System      |  Questions |\n|----------|:-------------:|------:|\n| _Technical Soundness_ |  Did you use the right tools for the job, and did you use them correctly?  Were they relevant to this class? | Were your questions correct and accurately cited. |\n| _Effort_ |  Did you do what you say you would, and was it the right ammount of effort.  | Are the questions well-written, interesting, and thoroughly edited? |\n| _Performance_ | How did your techniques perform in terms of accuracy, recall, etc.? | Is the human accuracy substantially higher than the computer accuracy? |\n\nAll members of the group will receive the same grade.  It's impossible for the course staff to adjudicate Rashomon-style accounts of who did what, and the goal of a group project is for all team members to work together to create a cohesive project that works well together.  While it makes sense to divide the work into distinct areas of responsibility, at grading time we have now way to know who really did what, so it's the groups responsibility to create a piece of output that reflects well on the whole group.",
    "library_name": "transformers"
  },
  {
    "model_id": "sahilnishad/Florence-2-FT-DocVQA",
    "model_name": "sahilnishad/Florence-2-FT-DocVQA",
    "author": "sahilnishad",
    "downloads": 5042,
    "likes": 0,
    "tags": [
      "transformers",
      "safetensors",
      "florence2",
      "text-generation",
      "document-vqa",
      "vqa",
      "image-to-text",
      "multimodal",
      "question-answering",
      "custom_code",
      "en",
      "dataset:HuggingFaceM4/DocumentVQA",
      "base_model:microsoft/Florence-2-base",
      "base_model:finetune:microsoft/Florence-2-base",
      "doi:10.57967/hf/3473",
      "license:mit",
      "autotrain_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/sahilnishad/Florence-2-FT-DocVQA",
    "dependencies": [
      [
        "torch",
        "2.2.0"
      ],
      [
        "transformers",
        "4.36.2"
      ],
      [
        "datasets",
        "2.16.1"
      ],
      [
        "flash_attn",
        "2.5.6"
      ]
    ],
    "analysis_date": "2025-03-26T00:50:28.590577",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "florence2",
    "pipeline_tag": "question-answering",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": [
        "en"
      ],
      "license": "mit",
      "library_name": "transformers",
      "tags": [
        "transformers",
        "florence2",
        "document-vqa",
        "vqa",
        "image-to-text",
        "multimodal",
        "question-answering"
      ],
      "datasets": [
        "HuggingFaceM4/DocumentVQA"
      ],
      "base_model": [
        "microsoft/Florence-2-base"
      ]
    },
    "card_text": "\n\n# Model Description\nFine-tuned Florence-2 model on DocumentVQA dataset to perform question answering on document images\n- **[Github](https://github.com/sahilnishad/Fine-Tuning-Florence-2-DocumentVQA)**\n\n# Get Started with the Model\n#### 1. Installation\n```python\n!pip install torch transformers datasets flash_attn\n```\n#### 2. Loading model and processor\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoProcessor\n\nmodel = AutoModelForCausalLM.from_pretrained(\"sahilnishad/Florence-2-FT-DocVQA\", trust_remote_code=True)\nprocessor = AutoProcessor.from_pretrained(\"sahilnishad/Florence-2-FT-DocVQA\", trust_remote_code=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n```\n#### 3. Running inference\n```python\ndef run_inference(task_prompt, question, image):\n    prompt = task_prompt + question\n\n    if image.mode != \"RGB\":\n        image = image.convert(\"RGB\")\n\n    inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(device)\n    \n    with torch.no_grad():\n        generated_ids = model.generate(\n            input_ids=inputs[\"input_ids\"],\n            pixel_values=inputs[\"pixel_values\"],\n            max_new_tokens=1024,\n            num_beams=3\n        )\n    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n    return generated_text\n```\n#### 4. Example\n```python\nfrom PIL import Image\nfrom datasets import load_dataset\n\ndata = load_dataset(\"HuggingFaceM4/DocumentVQA\")\n\nquestion = \"What do you see in this image?\"\nimage = data['train'][0]['image']\nprint(run_inference(\"<DocVQA>\", question, image))\n```\n---\n\n# BibTeX:\n```bibtex\n@misc{sahilnishad_florence_2_ft_docvqa,\n  author       = {Sahil Nishad},\n  title        = {Fine-Tuning Florence-2 For Document Visual Question-Answering},\n  year         = {2024},\n  url          = {https://huggingface.co/sahilnishad/Florence-2-FT-DocVQA},\n  note         = {Model available on HuggingFace Hub},\n  howpublished = {\\url{https://huggingface.co/sahilnishad/Florence-2-FT-DocVQA}},\n}",
    "card_content": "---\nlanguage:\n- en\nlicense: mit\nlibrary_name: transformers\ntags:\n- transformers\n- florence2\n- document-vqa\n- vqa\n- image-to-text\n- multimodal\n- question-answering\ndatasets:\n- HuggingFaceM4/DocumentVQA\nbase_model:\n- microsoft/Florence-2-base\n---\n\n\n# Model Description\nFine-tuned Florence-2 model on DocumentVQA dataset to perform question answering on document images\n- **[Github](https://github.com/sahilnishad/Fine-Tuning-Florence-2-DocumentVQA)**\n\n# Get Started with the Model\n#### 1. Installation\n```python\n!pip install torch transformers datasets flash_attn\n```\n#### 2. Loading model and processor\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoProcessor\n\nmodel = AutoModelForCausalLM.from_pretrained(\"sahilnishad/Florence-2-FT-DocVQA\", trust_remote_code=True)\nprocessor = AutoProcessor.from_pretrained(\"sahilnishad/Florence-2-FT-DocVQA\", trust_remote_code=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n```\n#### 3. Running inference\n```python\ndef run_inference(task_prompt, question, image):\n    prompt = task_prompt + question\n\n    if image.mode != \"RGB\":\n        image = image.convert(\"RGB\")\n\n    inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(device)\n    \n    with torch.no_grad():\n        generated_ids = model.generate(\n            input_ids=inputs[\"input_ids\"],\n            pixel_values=inputs[\"pixel_values\"],\n            max_new_tokens=1024,\n            num_beams=3\n        )\n    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n    return generated_text\n```\n#### 4. Example\n```python\nfrom PIL import Image\nfrom datasets import load_dataset\n\ndata = load_dataset(\"HuggingFaceM4/DocumentVQA\")\n\nquestion = \"What do you see in this image?\"\nimage = data['train'][0]['image']\nprint(run_inference(\"<DocVQA>\", question, image))\n```\n---\n\n# BibTeX:\n```bibtex\n@misc{sahilnishad_florence_2_ft_docvqa,\n  author       = {Sahil Nishad},\n  title        = {Fine-Tuning Florence-2 For Document Visual Question-Answering},\n  year         = {2024},\n  url          = {https://huggingface.co/sahilnishad/Florence-2-FT-DocVQA},\n  note         = {Model available on HuggingFace Hub},\n  howpublished = {\\url{https://huggingface.co/sahilnishad/Florence-2-FT-DocVQA}},\n}",
    "library_name": "transformers"
  },
  {
    "model_id": "qiyuw/WSPAlign-mbert-base",
    "model_name": "qiyuw/WSPAlign-mbert-base",
    "author": "qiyuw",
    "downloads": 4987,
    "likes": 0,
    "tags": [
      "transformers",
      "pytorch",
      "bert",
      "question-answering",
      "word alignment",
      "multilingual",
      "translation",
      "en",
      "de",
      "fr",
      "zh",
      "ja",
      "ro",
      "license:cc-by-nc-sa-4.0",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/qiyuw/WSPAlign-mbert-base",
    "dependencies": [
      [
        "transformers",
        "4.36.2"
      ],
      [
        "SpaCy",
        null
      ],
      [
        "torch",
        null
      ],
      [
        "spacy",
        "3.7.2"
      ]
    ],
    "analysis_date": "2025-03-26T00:50:29.592688",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "bert",
    "pipeline_tag": "translation",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": [
        "en",
        "de",
        "fr",
        "zh",
        "ja",
        "ro"
      ],
      "license": "cc-by-nc-sa-4.0",
      "tags": [
        "word alignment",
        "multilingual",
        "translation"
      ]
    },
    "card_text": "# Model Description \nRefer to [https://github.com/qiyuw/WSPAlign](https://github.com/qiyuw/WSPAlign) and [https://github.com/qiyuw/WSPAlign.InferEval](https://github.com/qiyuw/WSPAlign.InferEval) for details.\n\n# Qucik Usage\n\nFirst clone inference repository:\n```\ngit clone https://github.com/qiyuw/WSPAlign.InferEval.git\n```\nThen install the requirements following [https://github.com/qiyuw/WSPAlign.InferEval](https://github.com/qiyuw/WSPAlign.InferEval). For inference only `transformers`, `SpaCy` and `torch` are required.\n\nFinally, run the following example:\n```\npython inference.py --model_name_or_path qiyuw/WSPAlign-ft-kftt --src_lang ja --src_text=\"\u79c1\u306f\u732b\u304c\u597d\u304d\u3067\u3059\u3002\" --tgt_lang en --tgt_text=\"I like cats.\"\n```\nCheck `inference.py` for details usage.\n\n# Citation\nCite our paper if WSPAlign helps your work:\n\n```bibtex\n@inproceedings{wu-etal-2023-wspalign,\n    title = \"{WSPA}lign: Word Alignment Pre-training via Large-Scale Weakly Supervised Span Prediction\",\n    author = \"Wu, Qiyu  and Nagata, Masaaki  and Tsuruoka, Yoshimasa\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.621\",\n    pages = \"11084--11099\",\n}\n```",
    "card_content": "---\nlanguage:\n- en\n- de\n- fr\n- zh\n- ja\n- ro\nlicense: cc-by-nc-sa-4.0\ntags:\n- word alignment\n- multilingual\n- translation\n---\n# Model Description \nRefer to [https://github.com/qiyuw/WSPAlign](https://github.com/qiyuw/WSPAlign) and [https://github.com/qiyuw/WSPAlign.InferEval](https://github.com/qiyuw/WSPAlign.InferEval) for details.\n\n# Qucik Usage\n\nFirst clone inference repository:\n```\ngit clone https://github.com/qiyuw/WSPAlign.InferEval.git\n```\nThen install the requirements following [https://github.com/qiyuw/WSPAlign.InferEval](https://github.com/qiyuw/WSPAlign.InferEval). For inference only `transformers`, `SpaCy` and `torch` are required.\n\nFinally, run the following example:\n```\npython inference.py --model_name_or_path qiyuw/WSPAlign-ft-kftt --src_lang ja --src_text=\"\u79c1\u306f\u732b\u304c\u597d\u304d\u3067\u3059\u3002\" --tgt_lang en --tgt_text=\"I like cats.\"\n```\nCheck `inference.py` for details usage.\n\n# Citation\nCite our paper if WSPAlign helps your work:\n\n```bibtex\n@inproceedings{wu-etal-2023-wspalign,\n    title = \"{WSPA}lign: Word Alignment Pre-training via Large-Scale Weakly Supervised Span Prediction\",\n    author = \"Wu, Qiyu  and Nagata, Masaaki  and Tsuruoka, Yoshimasa\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-long.621\",\n    pages = \"11084--11099\",\n}\n```",
    "library_name": "transformers"
  },
  {
    "model_id": "deepset/bert-base-uncased-squad2",
    "model_name": "deepset/bert-base-uncased-squad2",
    "author": "deepset",
    "downloads": 4859,
    "likes": 6,
    "tags": [
      "transformers",
      "pytorch",
      "safetensors",
      "bert",
      "question-answering",
      "en",
      "dataset:squad_v2",
      "license:cc-by-4.0",
      "model-index",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/deepset/bert-base-uncased-squad2",
    "dependencies": [
      [
        "haystack-ai",
        "2.11.2"
      ],
      [
        "transformers",
        "4.36.2"
      ],
      [
        "torch",
        "2.5.1"
      ]
    ],
    "analysis_date": "2025-03-26T00:50:30.925939",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "bert",
    "pipeline_tag": "question-answering",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": "en",
      "license": "cc-by-4.0",
      "datasets": [
        "squad_v2"
      ],
      "model-index": [
        {
          "name": "deepset/bert-base-uncased-squad2",
          "results": [
            {
              "task": {
                "type": "question-answering",
                "name": "Question Answering"
              },
              "dataset": {
                "name": "squad_v2",
                "type": "squad_v2",
                "config": "squad_v2",
                "split": "validation"
              },
              "metrics": [
                {
                  "type": "exact_match",
                  "value": 75.6529,
                  "name": "Exact Match",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiNTY2YmQ0ZDFjMjRlZWRiZWQ2YWQ4MTM0ODkyYTQ0NmYwMzBlNWViZWQ0ODFhMGJmMmY4ZGYwOTQyMDAyZGNjYyIsInZlcnNpb24iOjF9.UyqonQTsCB0BW86LfPy17kLt3a4r3wMeh04MDam5t_UhElp6N02YpiKOqcb1ethNHjAR0WGyxrcV3TI4d-wFAQ"
                },
                {
                  "type": "f1",
                  "value": 78.6191,
                  "name": "F1",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZWRkZWVjMDU2YTcxYWVkZTU1YmUzY2FkNWI5NDJkM2YwMjFmMmE0Njc3MjI5N2Q0NDdhZDNkZWNjMWE5YTRmZiIsInZlcnNpb24iOjF9.ol0Zacd9ZryXazXjgVssGFYG4s5FzbhGGaj1ZEDLVN2ziyzx23bo4GH9PSuGTFxRK2BO5_dxvDupLRqJOF59Bg"
                }
              ]
            }
          ]
        }
      ]
    },
    "card_text": "\n# bert-base-uncased for Extractive QA \n\n## Overview\n**Language model:** bert-base-uncased  \n**Language:** English  \n**Downstream-task:** Extractive QA  \n**Training data:** SQuAD 2.0  \n**Eval data:** SQuAD 2.0  \n**Code:**  See [an example extractive QA pipeline built with Haystack](https://haystack.deepset.ai/tutorials/34_extractive_qa_pipeline)  \n**Infrastructure**: 1x Tesla v100\n\n## Hyperparameters\n\n```\nbatch_size = 32\nn_epochs = 3\nbase_LM_model = \"bert-base-uncased\"\nmax_seq_len = 384\nlearning_rate = 3e-5\nlr_schedule = LinearWarmup\nwarmup_proportion = 0.2\ndoc_stride=128\nmax_query_length=64\n``` \n\n## Usage\n\n### In Haystack\nHaystack is an AI orchestration framework to build customizable, production-ready LLM applications. You can use this model in Haystack to do extractive question answering on documents. \nTo load and run the model with [Haystack](https://github.com/deepset-ai/haystack/):\n```python\n# After running pip install haystack-ai \"transformers[torch,sentencepiece]\"\n\nfrom haystack import Document\nfrom haystack.components.readers import ExtractiveReader\n\ndocs = [\n    Document(content=\"Python is a popular programming language\"),\n    Document(content=\"python ist eine beliebte Programmiersprache\"),\n]\n\nreader = ExtractiveReader(model=\"deepset/bert-base-uncased-squad2\")\nreader.warm_up()\n\nquestion = \"What is a popular programming language?\"\nresult = reader.run(query=question, documents=docs)\n# {'answers': [ExtractedAnswer(query='What is a popular programming language?', score=0.5740374326705933, data='python', document=Document(id=..., content: '...'), context=None, document_offset=ExtractedAnswer.Span(start=0, end=6),...)]}\n```\nFor a complete example with an extractive question answering pipeline that scales over many documents, check out the [corresponding Haystack tutorial](https://haystack.deepset.ai/tutorials/34_extractive_qa_pipeline).\n\n### In Transformers\n```python\nfrom transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n\nmodel_name = \"deepset/bert-base-uncased-squad2\"\n\n# a) Get predictions\nnlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\nQA_input = {\n    'question': 'Why is model conversion important?',\n    'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\n}\nres = nlp(QA_input)\n\n# b) Load model & tokenizer\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n```\n\n## Performance\n```\n\"exact\": 73.67977764676156\n\"f1\": 77.87647139308865\n```\n\n## Authors\n- Timo M\u00f6ller: `timo.moeller [at] deepset.ai`\n- Julian Risch: `julian.risch [at] deepset.ai`\n- Malte Pietsch: `malte.pietsch [at] deepset.ai`\n- Michel Bartels: `michel.bartels [at] deepset.ai`\n\n## About us\n\n<div class=\"grid lg:grid-cols-2 gap-x-4 gap-y-3\">\n    <div class=\"w-full h-40 object-cover mb-2 rounded-lg flex items-center justify-center\">\n         <img alt=\"\" src=\"https://raw.githubusercontent.com/deepset-ai/.github/main/deepset-logo-colored.png\" class=\"w-40\"/>\n     </div>\n     <div class=\"w-full h-40 object-cover mb-2 rounded-lg flex items-center justify-center\">\n         <img alt=\"\" src=\"https://raw.githubusercontent.com/deepset-ai/.github/main/haystack-logo-colored.png\" class=\"w-40\"/>\n     </div>\n</div>\n\n[deepset](http://deepset.ai/) is the company behind the production-ready open-source AI framework [Haystack](https://haystack.deepset.ai/).\n\n  \nSome of our other work: \n- [Distilled roberta-base-squad2 (aka \"tinyroberta-squad2\")](https://huggingface.co/deepset/tinyroberta-squad2)\n- [German BERT](https://deepset.ai/german-bert), [GermanQuAD and GermanDPR](https://deepset.ai/germanquad), [German embedding model](https://huggingface.co/mixedbread-ai/deepset-mxbai-embed-de-large-v1)\n- [deepset Cloud](https://www.deepset.ai/deepset-cloud-product)\n- [deepset Studio](https://www.deepset.ai/deepset-studio)\n\n## Get in touch and join the Haystack community\n\n<p>For more info on Haystack, visit our <strong><a href=\"https://github.com/deepset-ai/haystack\">GitHub</a></strong> repo and <strong><a href=\"https://docs.haystack.deepset.ai\">Documentation</a></strong>. \n\nWe also have a <strong><a class=\"h-7\" href=\"https://haystack.deepset.ai/community\">Discord community open to everyone!</a></strong></p>\n\n[Twitter](https://twitter.com/Haystack_AI) | [LinkedIn](https://www.linkedin.com/company/deepset-ai/) | [Discord](https://haystack.deepset.ai/community) | [GitHub Discussions](https://github.com/deepset-ai/haystack/discussions) | [Website](https://haystack.deepset.ai/) | [YouTube](https://www.youtube.com/@deepset_ai)\n\nBy the way: [we're hiring!](http://www.deepset.ai/jobs)",
    "card_content": "---\nlanguage: en\nlicense: cc-by-4.0\ndatasets:\n- squad_v2\nmodel-index:\n- name: deepset/bert-base-uncased-squad2\n  results:\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squad_v2\n      type: squad_v2\n      config: squad_v2\n      split: validation\n    metrics:\n    - type: exact_match\n      value: 75.6529\n      name: Exact Match\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiNTY2YmQ0ZDFjMjRlZWRiZWQ2YWQ4MTM0ODkyYTQ0NmYwMzBlNWViZWQ0ODFhMGJmMmY4ZGYwOTQyMDAyZGNjYyIsInZlcnNpb24iOjF9.UyqonQTsCB0BW86LfPy17kLt3a4r3wMeh04MDam5t_UhElp6N02YpiKOqcb1ethNHjAR0WGyxrcV3TI4d-wFAQ\n    - type: f1\n      value: 78.6191\n      name: F1\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZWRkZWVjMDU2YTcxYWVkZTU1YmUzY2FkNWI5NDJkM2YwMjFmMmE0Njc3MjI5N2Q0NDdhZDNkZWNjMWE5YTRmZiIsInZlcnNpb24iOjF9.ol0Zacd9ZryXazXjgVssGFYG4s5FzbhGGaj1ZEDLVN2ziyzx23bo4GH9PSuGTFxRK2BO5_dxvDupLRqJOF59Bg\n---\n\n# bert-base-uncased for Extractive QA \n\n## Overview\n**Language model:** bert-base-uncased  \n**Language:** English  \n**Downstream-task:** Extractive QA  \n**Training data:** SQuAD 2.0  \n**Eval data:** SQuAD 2.0  \n**Code:**  See [an example extractive QA pipeline built with Haystack](https://haystack.deepset.ai/tutorials/34_extractive_qa_pipeline)  \n**Infrastructure**: 1x Tesla v100\n\n## Hyperparameters\n\n```\nbatch_size = 32\nn_epochs = 3\nbase_LM_model = \"bert-base-uncased\"\nmax_seq_len = 384\nlearning_rate = 3e-5\nlr_schedule = LinearWarmup\nwarmup_proportion = 0.2\ndoc_stride=128\nmax_query_length=64\n``` \n\n## Usage\n\n### In Haystack\nHaystack is an AI orchestration framework to build customizable, production-ready LLM applications. You can use this model in Haystack to do extractive question answering on documents. \nTo load and run the model with [Haystack](https://github.com/deepset-ai/haystack/):\n```python\n# After running pip install haystack-ai \"transformers[torch,sentencepiece]\"\n\nfrom haystack import Document\nfrom haystack.components.readers import ExtractiveReader\n\ndocs = [\n    Document(content=\"Python is a popular programming language\"),\n    Document(content=\"python ist eine beliebte Programmiersprache\"),\n]\n\nreader = ExtractiveReader(model=\"deepset/bert-base-uncased-squad2\")\nreader.warm_up()\n\nquestion = \"What is a popular programming language?\"\nresult = reader.run(query=question, documents=docs)\n# {'answers': [ExtractedAnswer(query='What is a popular programming language?', score=0.5740374326705933, data='python', document=Document(id=..., content: '...'), context=None, document_offset=ExtractedAnswer.Span(start=0, end=6),...)]}\n```\nFor a complete example with an extractive question answering pipeline that scales over many documents, check out the [corresponding Haystack tutorial](https://haystack.deepset.ai/tutorials/34_extractive_qa_pipeline).\n\n### In Transformers\n```python\nfrom transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n\nmodel_name = \"deepset/bert-base-uncased-squad2\"\n\n# a) Get predictions\nnlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\nQA_input = {\n    'question': 'Why is model conversion important?',\n    'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\n}\nres = nlp(QA_input)\n\n# b) Load model & tokenizer\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n```\n\n## Performance\n```\n\"exact\": 73.67977764676156\n\"f1\": 77.87647139308865\n```\n\n## Authors\n- Timo M\u00f6ller: `timo.moeller [at] deepset.ai`\n- Julian Risch: `julian.risch [at] deepset.ai`\n- Malte Pietsch: `malte.pietsch [at] deepset.ai`\n- Michel Bartels: `michel.bartels [at] deepset.ai`\n\n## About us\n\n<div class=\"grid lg:grid-cols-2 gap-x-4 gap-y-3\">\n    <div class=\"w-full h-40 object-cover mb-2 rounded-lg flex items-center justify-center\">\n         <img alt=\"\" src=\"https://raw.githubusercontent.com/deepset-ai/.github/main/deepset-logo-colored.png\" class=\"w-40\"/>\n     </div>\n     <div class=\"w-full h-40 object-cover mb-2 rounded-lg flex items-center justify-center\">\n         <img alt=\"\" src=\"https://raw.githubusercontent.com/deepset-ai/.github/main/haystack-logo-colored.png\" class=\"w-40\"/>\n     </div>\n</div>\n\n[deepset](http://deepset.ai/) is the company behind the production-ready open-source AI framework [Haystack](https://haystack.deepset.ai/).\n\n  \nSome of our other work: \n- [Distilled roberta-base-squad2 (aka \"tinyroberta-squad2\")](https://huggingface.co/deepset/tinyroberta-squad2)\n- [German BERT](https://deepset.ai/german-bert), [GermanQuAD and GermanDPR](https://deepset.ai/germanquad), [German embedding model](https://huggingface.co/mixedbread-ai/deepset-mxbai-embed-de-large-v1)\n- [deepset Cloud](https://www.deepset.ai/deepset-cloud-product)\n- [deepset Studio](https://www.deepset.ai/deepset-studio)\n\n## Get in touch and join the Haystack community\n\n<p>For more info on Haystack, visit our <strong><a href=\"https://github.com/deepset-ai/haystack\">GitHub</a></strong> repo and <strong><a href=\"https://docs.haystack.deepset.ai\">Documentation</a></strong>. \n\nWe also have a <strong><a class=\"h-7\" href=\"https://haystack.deepset.ai/community\">Discord community open to everyone!</a></strong></p>\n\n[Twitter](https://twitter.com/Haystack_AI) | [LinkedIn](https://www.linkedin.com/company/deepset-ai/) | [Discord](https://haystack.deepset.ai/community) | [GitHub Discussions](https://github.com/deepset-ai/haystack/discussions) | [Website](https://haystack.deepset.ai/) | [YouTube](https://www.youtube.com/@deepset_ai)\n\nBy the way: [we're hiring!](http://www.deepset.ai/jobs)",
    "library_name": "transformers"
  },
  {
    "model_id": "deepset/deberta-v3-large-squad2",
    "model_name": "deepset/deberta-v3-large-squad2",
    "author": "deepset",
    "downloads": 4798,
    "likes": 51,
    "tags": [
      "transformers",
      "pytorch",
      "safetensors",
      "deberta-v2",
      "question-answering",
      "deberta",
      "deberta-v3",
      "deberta-v3-large",
      "en",
      "dataset:squad_v2",
      "license:cc-by-4.0",
      "model-index",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/deepset/deberta-v3-large-squad2",
    "dependencies": [
      [
        "haystack-ai",
        "2.11.2"
      ],
      [
        "transformers",
        "4.36.2"
      ],
      [
        "torch",
        "2.5.1"
      ]
    ],
    "analysis_date": "2025-03-26T00:50:32.506592",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "deberta-v2",
    "pipeline_tag": "question-answering",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": "en",
      "license": "cc-by-4.0",
      "tags": [
        "deberta",
        "deberta-v3",
        "deberta-v3-large"
      ],
      "datasets": [
        "squad_v2"
      ],
      "model-index": [
        {
          "name": "deepset/deberta-v3-large-squad2",
          "results": [
            {
              "task": {
                "type": "question-answering",
                "name": "Question Answering"
              },
              "dataset": {
                "name": "squad_v2",
                "type": "squad_v2",
                "config": "squad_v2",
                "split": "validation"
              },
              "metrics": [
                {
                  "type": "exact_match",
                  "value": 88.0876,
                  "name": "Exact Match",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZmE0MWEwNjBkNTA1MmU0ZDkyYTA1OGEwNzY3NGE4NWU4NGI0NTQzNjRlNjY1NGRmNDU2MjA0NjU1N2JlZmNhYiIsInZlcnNpb24iOjF9.PnBF_vD0HujNBSShGJzsJnjmiBP_qT8xb2E7ORmpKfNspKXEuN_pBk9iV0IHRzdqOSyllcxlCv93XMPblNjWDw"
                },
                {
                  "type": "f1",
                  "value": 91.1623,
                  "name": "F1",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZDBkNDUzZmNkNDQwOGRkMmVlZjkxZWVlMzk3NzFmMGIxMTFmMjZlZDcyOWFiMjljNjM5MThlZDM4OWRmNzMwOCIsInZlcnNpb24iOjF9.bacyetziNI2DxO67GWpTyeRPXqF1POkyv00wEHXlyZu71pZngsNpZyrnuj2aJlCqQwHGnF_lT2ysaXKHprQRBg"
                }
              ]
            },
            {
              "task": {
                "type": "question-answering",
                "name": "Question Answering"
              },
              "dataset": {
                "name": "squad",
                "type": "squad",
                "config": "plain_text",
                "split": "validation"
              },
              "metrics": [
                {
                  "type": "exact_match",
                  "value": 89.2366,
                  "name": "Exact Match",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMjQ1Yjk3YTdiYTY1NmYxMTI1ZGZlMjRkNTlhZTkyNjRkNjgxYWJiNDk2NzE3NjAyYmY3YmRjNjg4YmEyNDkyYyIsInZlcnNpb24iOjF9.SEWyqX_FPQJOJt2KjOCNgQ2giyVeLj5bmLI5LT_Pfo33tbWPWD09TySYdsthaVTjUGT5DvDzQLASSwBH05FyBw"
                },
                {
                  "type": "f1",
                  "value": 95.0569,
                  "name": "F1",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiY2QyODQ1NWVlYjQxMjA0YTgyNmQ2NmIxOWY3MDRmZjE3ZWI5Yjc4ZDE4NzA2YjE2YTE1YTBlNzNiYmNmNzI3NCIsInZlcnNpb24iOjF9.NcXEc9xoggV76w1bQKxuJDYbOTxFzdny2k-85_b6AIMtfpYV3rGR1Z5YF6tVY2jyp7mgm5Jd5YSgGI3NvNE-CQ"
                }
              ]
            },
            {
              "task": {
                "type": "question-answering",
                "name": "Question Answering"
              },
              "dataset": {
                "name": "adversarial_qa",
                "type": "adversarial_qa",
                "config": "adversarialQA",
                "split": "validation"
              },
              "metrics": [
                {
                  "type": "exact_match",
                  "value": 42.1,
                  "name": "Exact Match"
                },
                {
                  "type": "f1",
                  "value": 56.587,
                  "name": "F1"
                }
              ]
            },
            {
              "task": {
                "type": "question-answering",
                "name": "Question Answering"
              },
              "dataset": {
                "name": "squad_adversarial",
                "type": "squad_adversarial",
                "config": "AddOneSent",
                "split": "validation"
              },
              "metrics": [
                {
                  "type": "exact_match",
                  "value": 83.548,
                  "name": "Exact Match"
                },
                {
                  "type": "f1",
                  "value": 89.385,
                  "name": "F1"
                }
              ]
            },
            {
              "task": {
                "type": "question-answering",
                "name": "Question Answering"
              },
              "dataset": {
                "name": "squadshifts amazon",
                "type": "squadshifts",
                "config": "amazon",
                "split": "test"
              },
              "metrics": [
                {
                  "type": "exact_match",
                  "value": 72.979,
                  "name": "Exact Match"
                },
                {
                  "type": "f1",
                  "value": 87.254,
                  "name": "F1"
                }
              ]
            },
            {
              "task": {
                "type": "question-answering",
                "name": "Question Answering"
              },
              "dataset": {
                "name": "squadshifts new_wiki",
                "type": "squadshifts",
                "config": "new_wiki",
                "split": "test"
              },
              "metrics": [
                {
                  "type": "exact_match",
                  "value": 83.938,
                  "name": "Exact Match"
                },
                {
                  "type": "f1",
                  "value": 92.695,
                  "name": "F1"
                }
              ]
            },
            {
              "task": {
                "type": "question-answering",
                "name": "Question Answering"
              },
              "dataset": {
                "name": "squadshifts nyt",
                "type": "squadshifts",
                "config": "nyt",
                "split": "test"
              },
              "metrics": [
                {
                  "type": "exact_match",
                  "value": 85.534,
                  "name": "Exact Match"
                },
                {
                  "type": "f1",
                  "value": 93.153,
                  "name": "F1"
                }
              ]
            },
            {
              "task": {
                "type": "question-answering",
                "name": "Question Answering"
              },
              "dataset": {
                "name": "squadshifts reddit",
                "type": "squadshifts",
                "config": "reddit",
                "split": "test"
              },
              "metrics": [
                {
                  "type": "exact_match",
                  "value": 73.284,
                  "name": "Exact Match"
                },
                {
                  "type": "f1",
                  "value": 85.307,
                  "name": "F1"
                }
              ]
            }
          ]
        }
      ]
    },
    "card_text": "# deberta-v3-large for Extractive QA \n\nThis is the [deberta-v3-large](https://huggingface.co/microsoft/deberta-v3-large) model, fine-tuned using the [SQuAD2.0](https://huggingface.co/datasets/squad_v2) dataset. It's been trained on question-answer pairs, including unanswerable questions, for the task of Extractive Question Answering. \n\n\n## Overview\n**Language model:** deberta-v3-large  \n**Language:** English  \n**Downstream-task:** Extractive QA  \n**Training data:** SQuAD 2.0  \n**Eval data:** SQuAD 2.0  \n**Code:**  See [an example extractive QA pipeline built with Haystack](https://haystack.deepset.ai/tutorials/34_extractive_qa_pipeline)  \n**Infrastructure**: 1x NVIDIA A10G\n\n## Hyperparameters\n\n```\nbatch_size = 2\ngrad_acc_steps = 32\nn_epochs = 6\nbase_LM_model = \"microsoft/deberta-v3-large\"\nmax_seq_len = 512\nlearning_rate = 7e-6\nlr_schedule = LinearWarmup\nwarmup_proportion = 0.2\ndoc_stride=128\nmax_query_length=64\n``` \n\n## Usage\n\n### In Haystack\nHaystack is an AI orchestration framework to build customizable, production-ready LLM applications. You can use this model in Haystack to do extractive question answering on documents. \nTo load and run the model with [Haystack](https://github.com/deepset-ai/haystack/):\n```python\n# After running pip install haystack-ai \"transformers[torch,sentencepiece]\"\n\nfrom haystack import Document\nfrom haystack.components.readers import ExtractiveReader\n\ndocs = [\n    Document(content=\"Python is a popular programming language\"),\n    Document(content=\"python ist eine beliebte Programmiersprache\"),\n]\n\nreader = ExtractiveReader(model=\"deepset/deberta-v3-large-squad2\")\nreader.warm_up()\n\nquestion = \"What is a popular programming language?\"\nresult = reader.run(query=question, documents=docs)\n# {'answers': [ExtractedAnswer(query='What is a popular programming language?', score=0.5740374326705933, data='python', document=Document(id=..., content: '...'), context=None, document_offset=ExtractedAnswer.Span(start=0, end=6),...)]}\n```\nFor a complete example with an extractive question answering pipeline that scales over many documents, check out the [corresponding Haystack tutorial](https://haystack.deepset.ai/tutorials/34_extractive_qa_pipeline).\n\n### In Transformers\n```python\nfrom transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n\nmodel_name = \"deepset/deberta-v3-large-squad2\"\n\n# a) Get predictions\nnlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\nQA_input = {\n    'question': 'Why is model conversion important?',\n    'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\n}\nres = nlp(QA_input)\n\n# b) Load model & tokenizer\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n```\n\n## Performance\nEvaluated on the SQuAD 2.0 dev set with the [official eval script](https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/contents/blob/).\n\n```\n\"exact\": 87.6105449338836,\n\"f1\": 90.75307008866517,\n\n\"total\": 11873,\n\"HasAns_exact\": 84.37921727395411,\n\"HasAns_f1\": 90.6732795483674,\n\"HasAns_total\": 5928,\n\"NoAns_exact\": 90.83263246425568,\n\"NoAns_f1\": 90.83263246425568,\n\"NoAns_total\": 5945\n```\n\n## About us\n\n<div class=\"grid lg:grid-cols-2 gap-x-4 gap-y-3\">\n    <div class=\"w-full h-40 object-cover mb-2 rounded-lg flex items-center justify-center\">\n         <img alt=\"\" src=\"https://raw.githubusercontent.com/deepset-ai/.github/main/deepset-logo-colored.png\" class=\"w-40\"/>\n     </div>\n     <div class=\"w-full h-40 object-cover mb-2 rounded-lg flex items-center justify-center\">\n         <img alt=\"\" src=\"https://raw.githubusercontent.com/deepset-ai/.github/main/haystack-logo-colored.png\" class=\"w-40\"/>\n     </div>\n</div>\n\n[deepset](http://deepset.ai/) is the company behind the production-ready open-source AI framework [Haystack](https://haystack.deepset.ai/).\n\nSome of our other work: \n- [Distilled roberta-base-squad2 (aka \"tinyroberta-squad2\")](https://huggingface.co/deepset/tinyroberta-squad2)\n- [German BERT](https://deepset.ai/german-bert), [GermanQuAD and GermanDPR](https://deepset.ai/germanquad), [German embedding model](https://huggingface.co/mixedbread-ai/deepset-mxbai-embed-de-large-v1)\n- [deepset Cloud](https://www.deepset.ai/deepset-cloud-product), [deepset Studio](https://www.deepset.ai/deepset-studio)\n\n## Get in touch and join the Haystack community\n\n<p>For more info on Haystack, visit our <strong><a href=\"https://github.com/deepset-ai/haystack\">GitHub</a></strong> repo and <strong><a href=\"https://docs.haystack.deepset.ai\">Documentation</a></strong>. \n\nWe also have a <strong><a class=\"h-7\" href=\"https://haystack.deepset.ai/community\">Discord community open to everyone!</a></strong></p>\n\n[Twitter](https://twitter.com/Haystack_AI) | [LinkedIn](https://www.linkedin.com/company/deepset-ai/) | [Discord](https://haystack.deepset.ai/community) | [GitHub Discussions](https://github.com/deepset-ai/haystack/discussions) | [Website](https://haystack.deepset.ai/) | [YouTube](https://www.youtube.com/@deepset_ai)\n\nBy the way: [we're hiring!](http://www.deepset.ai/jobs)",
    "card_content": "---\nlanguage: en\nlicense: cc-by-4.0\ntags:\n- deberta\n- deberta-v3\n- deberta-v3-large\ndatasets:\n- squad_v2\nmodel-index:\n- name: deepset/deberta-v3-large-squad2\n  results:\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squad_v2\n      type: squad_v2\n      config: squad_v2\n      split: validation\n    metrics:\n    - type: exact_match\n      value: 88.0876\n      name: Exact Match\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZmE0MWEwNjBkNTA1MmU0ZDkyYTA1OGEwNzY3NGE4NWU4NGI0NTQzNjRlNjY1NGRmNDU2MjA0NjU1N2JlZmNhYiIsInZlcnNpb24iOjF9.PnBF_vD0HujNBSShGJzsJnjmiBP_qT8xb2E7ORmpKfNspKXEuN_pBk9iV0IHRzdqOSyllcxlCv93XMPblNjWDw\n    - type: f1\n      value: 91.1623\n      name: F1\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZDBkNDUzZmNkNDQwOGRkMmVlZjkxZWVlMzk3NzFmMGIxMTFmMjZlZDcyOWFiMjljNjM5MThlZDM4OWRmNzMwOCIsInZlcnNpb24iOjF9.bacyetziNI2DxO67GWpTyeRPXqF1POkyv00wEHXlyZu71pZngsNpZyrnuj2aJlCqQwHGnF_lT2ysaXKHprQRBg\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squad\n      type: squad\n      config: plain_text\n      split: validation\n    metrics:\n    - type: exact_match\n      value: 89.2366\n      name: Exact Match\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMjQ1Yjk3YTdiYTY1NmYxMTI1ZGZlMjRkNTlhZTkyNjRkNjgxYWJiNDk2NzE3NjAyYmY3YmRjNjg4YmEyNDkyYyIsInZlcnNpb24iOjF9.SEWyqX_FPQJOJt2KjOCNgQ2giyVeLj5bmLI5LT_Pfo33tbWPWD09TySYdsthaVTjUGT5DvDzQLASSwBH05FyBw\n    - type: f1\n      value: 95.0569\n      name: F1\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiY2QyODQ1NWVlYjQxMjA0YTgyNmQ2NmIxOWY3MDRmZjE3ZWI5Yjc4ZDE4NzA2YjE2YTE1YTBlNzNiYmNmNzI3NCIsInZlcnNpb24iOjF9.NcXEc9xoggV76w1bQKxuJDYbOTxFzdny2k-85_b6AIMtfpYV3rGR1Z5YF6tVY2jyp7mgm5Jd5YSgGI3NvNE-CQ\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: adversarial_qa\n      type: adversarial_qa\n      config: adversarialQA\n      split: validation\n    metrics:\n    - type: exact_match\n      value: 42.1\n      name: Exact Match\n    - type: f1\n      value: 56.587\n      name: F1\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squad_adversarial\n      type: squad_adversarial\n      config: AddOneSent\n      split: validation\n    metrics:\n    - type: exact_match\n      value: 83.548\n      name: Exact Match\n    - type: f1\n      value: 89.385\n      name: F1\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squadshifts amazon\n      type: squadshifts\n      config: amazon\n      split: test\n    metrics:\n    - type: exact_match\n      value: 72.979\n      name: Exact Match\n    - type: f1\n      value: 87.254\n      name: F1\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squadshifts new_wiki\n      type: squadshifts\n      config: new_wiki\n      split: test\n    metrics:\n    - type: exact_match\n      value: 83.938\n      name: Exact Match\n    - type: f1\n      value: 92.695\n      name: F1\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squadshifts nyt\n      type: squadshifts\n      config: nyt\n      split: test\n    metrics:\n    - type: exact_match\n      value: 85.534\n      name: Exact Match\n    - type: f1\n      value: 93.153\n      name: F1\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squadshifts reddit\n      type: squadshifts\n      config: reddit\n      split: test\n    metrics:\n    - type: exact_match\n      value: 73.284\n      name: Exact Match\n    - type: f1\n      value: 85.307\n      name: F1\n---\n# deberta-v3-large for Extractive QA \n\nThis is the [deberta-v3-large](https://huggingface.co/microsoft/deberta-v3-large) model, fine-tuned using the [SQuAD2.0](https://huggingface.co/datasets/squad_v2) dataset. It's been trained on question-answer pairs, including unanswerable questions, for the task of Extractive Question Answering. \n\n\n## Overview\n**Language model:** deberta-v3-large  \n**Language:** English  \n**Downstream-task:** Extractive QA  \n**Training data:** SQuAD 2.0  \n**Eval data:** SQuAD 2.0  \n**Code:**  See [an example extractive QA pipeline built with Haystack](https://haystack.deepset.ai/tutorials/34_extractive_qa_pipeline)  \n**Infrastructure**: 1x NVIDIA A10G\n\n## Hyperparameters\n\n```\nbatch_size = 2\ngrad_acc_steps = 32\nn_epochs = 6\nbase_LM_model = \"microsoft/deberta-v3-large\"\nmax_seq_len = 512\nlearning_rate = 7e-6\nlr_schedule = LinearWarmup\nwarmup_proportion = 0.2\ndoc_stride=128\nmax_query_length=64\n``` \n\n## Usage\n\n### In Haystack\nHaystack is an AI orchestration framework to build customizable, production-ready LLM applications. You can use this model in Haystack to do extractive question answering on documents. \nTo load and run the model with [Haystack](https://github.com/deepset-ai/haystack/):\n```python\n# After running pip install haystack-ai \"transformers[torch,sentencepiece]\"\n\nfrom haystack import Document\nfrom haystack.components.readers import ExtractiveReader\n\ndocs = [\n    Document(content=\"Python is a popular programming language\"),\n    Document(content=\"python ist eine beliebte Programmiersprache\"),\n]\n\nreader = ExtractiveReader(model=\"deepset/deberta-v3-large-squad2\")\nreader.warm_up()\n\nquestion = \"What is a popular programming language?\"\nresult = reader.run(query=question, documents=docs)\n# {'answers': [ExtractedAnswer(query='What is a popular programming language?', score=0.5740374326705933, data='python', document=Document(id=..., content: '...'), context=None, document_offset=ExtractedAnswer.Span(start=0, end=6),...)]}\n```\nFor a complete example with an extractive question answering pipeline that scales over many documents, check out the [corresponding Haystack tutorial](https://haystack.deepset.ai/tutorials/34_extractive_qa_pipeline).\n\n### In Transformers\n```python\nfrom transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n\nmodel_name = \"deepset/deberta-v3-large-squad2\"\n\n# a) Get predictions\nnlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\nQA_input = {\n    'question': 'Why is model conversion important?',\n    'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\n}\nres = nlp(QA_input)\n\n# b) Load model & tokenizer\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n```\n\n## Performance\nEvaluated on the SQuAD 2.0 dev set with the [official eval script](https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/contents/blob/).\n\n```\n\"exact\": 87.6105449338836,\n\"f1\": 90.75307008866517,\n\n\"total\": 11873,\n\"HasAns_exact\": 84.37921727395411,\n\"HasAns_f1\": 90.6732795483674,\n\"HasAns_total\": 5928,\n\"NoAns_exact\": 90.83263246425568,\n\"NoAns_f1\": 90.83263246425568,\n\"NoAns_total\": 5945\n```\n\n## About us\n\n<div class=\"grid lg:grid-cols-2 gap-x-4 gap-y-3\">\n    <div class=\"w-full h-40 object-cover mb-2 rounded-lg flex items-center justify-center\">\n         <img alt=\"\" src=\"https://raw.githubusercontent.com/deepset-ai/.github/main/deepset-logo-colored.png\" class=\"w-40\"/>\n     </div>\n     <div class=\"w-full h-40 object-cover mb-2 rounded-lg flex items-center justify-center\">\n         <img alt=\"\" src=\"https://raw.githubusercontent.com/deepset-ai/.github/main/haystack-logo-colored.png\" class=\"w-40\"/>\n     </div>\n</div>\n\n[deepset](http://deepset.ai/) is the company behind the production-ready open-source AI framework [Haystack](https://haystack.deepset.ai/).\n\nSome of our other work: \n- [Distilled roberta-base-squad2 (aka \"tinyroberta-squad2\")](https://huggingface.co/deepset/tinyroberta-squad2)\n- [German BERT](https://deepset.ai/german-bert), [GermanQuAD and GermanDPR](https://deepset.ai/germanquad), [German embedding model](https://huggingface.co/mixedbread-ai/deepset-mxbai-embed-de-large-v1)\n- [deepset Cloud](https://www.deepset.ai/deepset-cloud-product), [deepset Studio](https://www.deepset.ai/deepset-studio)\n\n## Get in touch and join the Haystack community\n\n<p>For more info on Haystack, visit our <strong><a href=\"https://github.com/deepset-ai/haystack\">GitHub</a></strong> repo and <strong><a href=\"https://docs.haystack.deepset.ai\">Documentation</a></strong>. \n\nWe also have a <strong><a class=\"h-7\" href=\"https://haystack.deepset.ai/community\">Discord community open to everyone!</a></strong></p>\n\n[Twitter](https://twitter.com/Haystack_AI) | [LinkedIn](https://www.linkedin.com/company/deepset-ai/) | [Discord](https://haystack.deepset.ai/community) | [GitHub Discussions](https://github.com/deepset-ai/haystack/discussions) | [Website](https://haystack.deepset.ai/) | [YouTube](https://www.youtube.com/@deepset_ai)\n\nBy the way: [we're hiring!](http://www.deepset.ai/jobs)",
    "library_name": "transformers"
  },
  {
    "model_id": "michiyasunaga/BioLinkBERT-base",
    "model_name": "michiyasunaga/BioLinkBERT-base",
    "author": "michiyasunaga",
    "downloads": 4540,
    "likes": 36,
    "tags": [
      "transformers",
      "pytorch",
      "bert",
      "feature-extraction",
      "exbert",
      "linkbert",
      "biolinkbert",
      "fill-mask",
      "question-answering",
      "text-classification",
      "token-classification",
      "en",
      "dataset:pubmed",
      "arxiv:2203.15827",
      "license:apache-2.0",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/michiyasunaga/BioLinkBERT-base",
    "dependencies": [
      [
        "transformers",
        "4.36.2"
      ],
      [
        "torch",
        "2.2.0"
      ],
      [
        "numpy",
        "1.26.3"
      ]
    ],
    "analysis_date": "2025-03-26T00:50:33.716837",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "bert",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": "en",
      "license": "apache-2.0",
      "tags": [
        "bert",
        "exbert",
        "linkbert",
        "biolinkbert",
        "feature-extraction",
        "fill-mask",
        "question-answering",
        "text-classification",
        "token-classification"
      ],
      "datasets": [
        "pubmed"
      ],
      "widget": [
        {
          "text": "Sunitinib is a tyrosine kinase inhibitor"
        }
      ]
    },
    "card_text": "\r\n## BioLinkBERT-base\r\n\r\nBioLinkBERT-base model pretrained on [PubMed](https://pubmed.ncbi.nlm.nih.gov/) abstracts along with citation link information. It is introduced in the paper [LinkBERT: Pretraining Language Models with Document Links (ACL 2022)](https://arxiv.org/abs/2203.15827). The code and data are available in [this repository](https://github.com/michiyasunaga/LinkBERT).\r\n\r\nThis model achieves state-of-the-art performance on several biomedical NLP benchmarks such as [BLURB](https://microsoft.github.io/BLURB/) and [MedQA-USMLE](https://github.com/jind11/MedQA).\r\n\r\n\r\n## Model description\r\n\r\nLinkBERT is a transformer encoder (BERT-like) model pretrained on a large corpus of documents. It is an improvement of BERT that newly captures **document links** such as hyperlinks and citation links to include knowledge that spans across multiple documents. Specifically, it was pretrained by feeding linked documents into the same language model context, besides a single document.\r\n\r\nLinkBERT can be used as a drop-in replacement for BERT. It achieves better performance for general language understanding tasks (e.g. text classification), and is also particularly effective for **knowledge-intensive** tasks (e.g. question answering) and **cross-document** tasks (e.g. reading comprehension, document retrieval).\r\n\r\n\r\n## Intended uses & limitations\r\n\r\nThe model can be used by fine-tuning on a downstream task, such as question answering, sequence classification, and token classification.\r\nYou can also use the raw model for feature extraction (i.e. obtaining embeddings for input text).\r\n\r\n\r\n### How to use\r\n\r\nTo use the model to get the features of a given text in PyTorch:\r\n\r\n```python\r\nfrom transformers import AutoTokenizer, AutoModel\r\ntokenizer = AutoTokenizer.from_pretrained('michiyasunaga/BioLinkBERT-base')\r\nmodel = AutoModel.from_pretrained('michiyasunaga/BioLinkBERT-base')\r\ninputs = tokenizer(\"Sunitinib is a tyrosine kinase inhibitor\", return_tensors=\"pt\")\r\noutputs = model(**inputs)\r\nlast_hidden_states = outputs.last_hidden_state\r\n```\r\n\r\nFor fine-tuning, you can use [this repository](https://github.com/michiyasunaga/LinkBERT) or follow any other BERT fine-tuning codebases.\r\n\r\n\r\n## Evaluation results\r\n\r\nWhen fine-tuned on downstream tasks, LinkBERT achieves the following results.\r\n\r\n**Biomedical benchmarks ([BLURB](https://microsoft.github.io/BLURB/), [MedQA](https://github.com/jind11/MedQA), [MMLU](https://github.com/hendrycks/test), etc.):** BioLinkBERT attains new state-of-the-art.\r\n\r\n|                         | BLURB score | PubMedQA | BioASQ   | MedQA-USMLE |\r\n| ----------------------  | --------    | -------- | -------  | --------    |\r\n| PubmedBERT-base         | 81.10       | 55.8     | 87.5     | 38.1        |\r\n| **BioLinkBERT-base**    | **83.39**   | **70.2** | **91.4** | **40.0** |\r\n| **BioLinkBERT-large**   | **84.30**   | **72.2** | **94.8** | **44.6** |\r\n\r\n|                         | MMLU-professional medicine     |\r\n| ----------------------  | --------  |\r\n| GPT-3 (175 params)      | 38.7      |\r\n| UnifiedQA (11B params)  | 43.2      |\r\n| **BioLinkBERT-large (340M params)** | **50.7**  |\r\n\r\n\r\n## Citation\r\n\r\nIf you find LinkBERT useful in your project, please cite the following:\r\n\r\n```bibtex\r\n@InProceedings{yasunaga2022linkbert,\r\n  author =  {Michihiro Yasunaga and Jure Leskovec and Percy Liang},\r\n  title =   {LinkBERT: Pretraining Language Models with Document Links},\r\n  year =    {2022},  \r\n  booktitle = {Association for Computational Linguistics (ACL)},  \r\n}\r\n```\r\n",
    "card_content": "---\r\nlanguage: en\r\nlicense: apache-2.0\r\ntags:\r\n- bert\r\n- exbert\r\n- linkbert\r\n- biolinkbert\r\n- feature-extraction\r\n- fill-mask\r\n- question-answering\r\n- text-classification\r\n- token-classification\r\ndatasets:\r\n- pubmed\r\nwidget:\r\n- text: Sunitinib is a tyrosine kinase inhibitor\r\n---\r\n\r\n## BioLinkBERT-base\r\n\r\nBioLinkBERT-base model pretrained on [PubMed](https://pubmed.ncbi.nlm.nih.gov/) abstracts along with citation link information. It is introduced in the paper [LinkBERT: Pretraining Language Models with Document Links (ACL 2022)](https://arxiv.org/abs/2203.15827). The code and data are available in [this repository](https://github.com/michiyasunaga/LinkBERT).\r\n\r\nThis model achieves state-of-the-art performance on several biomedical NLP benchmarks such as [BLURB](https://microsoft.github.io/BLURB/) and [MedQA-USMLE](https://github.com/jind11/MedQA).\r\n\r\n\r\n## Model description\r\n\r\nLinkBERT is a transformer encoder (BERT-like) model pretrained on a large corpus of documents. It is an improvement of BERT that newly captures **document links** such as hyperlinks and citation links to include knowledge that spans across multiple documents. Specifically, it was pretrained by feeding linked documents into the same language model context, besides a single document.\r\n\r\nLinkBERT can be used as a drop-in replacement for BERT. It achieves better performance for general language understanding tasks (e.g. text classification), and is also particularly effective for **knowledge-intensive** tasks (e.g. question answering) and **cross-document** tasks (e.g. reading comprehension, document retrieval).\r\n\r\n\r\n## Intended uses & limitations\r\n\r\nThe model can be used by fine-tuning on a downstream task, such as question answering, sequence classification, and token classification.\r\nYou can also use the raw model for feature extraction (i.e. obtaining embeddings for input text).\r\n\r\n\r\n### How to use\r\n\r\nTo use the model to get the features of a given text in PyTorch:\r\n\r\n```python\r\nfrom transformers import AutoTokenizer, AutoModel\r\ntokenizer = AutoTokenizer.from_pretrained('michiyasunaga/BioLinkBERT-base')\r\nmodel = AutoModel.from_pretrained('michiyasunaga/BioLinkBERT-base')\r\ninputs = tokenizer(\"Sunitinib is a tyrosine kinase inhibitor\", return_tensors=\"pt\")\r\noutputs = model(**inputs)\r\nlast_hidden_states = outputs.last_hidden_state\r\n```\r\n\r\nFor fine-tuning, you can use [this repository](https://github.com/michiyasunaga/LinkBERT) or follow any other BERT fine-tuning codebases.\r\n\r\n\r\n## Evaluation results\r\n\r\nWhen fine-tuned on downstream tasks, LinkBERT achieves the following results.\r\n\r\n**Biomedical benchmarks ([BLURB](https://microsoft.github.io/BLURB/), [MedQA](https://github.com/jind11/MedQA), [MMLU](https://github.com/hendrycks/test), etc.):** BioLinkBERT attains new state-of-the-art.\r\n\r\n|                         | BLURB score | PubMedQA | BioASQ   | MedQA-USMLE |\r\n| ----------------------  | --------    | -------- | -------  | --------    |\r\n| PubmedBERT-base         | 81.10       | 55.8     | 87.5     | 38.1        |\r\n| **BioLinkBERT-base**    | **83.39**   | **70.2** | **91.4** | **40.0** |\r\n| **BioLinkBERT-large**   | **84.30**   | **72.2** | **94.8** | **44.6** |\r\n\r\n|                         | MMLU-professional medicine     |\r\n| ----------------------  | --------  |\r\n| GPT-3 (175 params)      | 38.7      |\r\n| UnifiedQA (11B params)  | 43.2      |\r\n| **BioLinkBERT-large (340M params)** | **50.7**  |\r\n\r\n\r\n## Citation\r\n\r\nIf you find LinkBERT useful in your project, please cite the following:\r\n\r\n```bibtex\r\n@InProceedings{yasunaga2022linkbert,\r\n  author =  {Michihiro Yasunaga and Jure Leskovec and Percy Liang},\r\n  title =   {LinkBERT: Pretraining Language Models with Document Links},\r\n  year =    {2022},  \r\n  booktitle = {Association for Computational Linguistics (ACL)},  \r\n}\r\n```\r\n",
    "library_name": "transformers"
  },
  {
    "model_id": "VietAI/vit5-base",
    "model_name": "VietAI/vit5-base",
    "author": "VietAI",
    "downloads": 4455,
    "likes": 11,
    "tags": [
      "transformers",
      "pytorch",
      "tf",
      "jax",
      "t5",
      "text2text-generation",
      "summarization",
      "translation",
      "question-answering",
      "vi",
      "dataset:cc100",
      "license:mit",
      "autotrain_compatible",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/VietAI/vit5-base",
    "dependencies": [
      [
        "transformers",
        "4.36.2"
      ],
      [
        "torch",
        "2.2.0"
      ]
    ],
    "analysis_date": "2025-03-26T00:50:34.889046",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "t5",
    "pipeline_tag": "question-answering",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": "vi",
      "license": "mit",
      "tags": [
        "summarization",
        "translation",
        "question-answering"
      ],
      "datasets": [
        "cc100"
      ]
    },
    "card_text": "\n# ViT5-base\n\nState-of-the-art pretrained Transformer-based encoder-decoder model for Vietnamese.\n\n## How to use\nFor more details, do check out [our Github repo](https://github.com/vietai/ViT5). \n\n[Finetunning Example can be found here](https://github.com/vietai/ViT5/tree/main/finetunning_huggingface).\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\u200b\ntokenizer = AutoTokenizer.from_pretrained(\"VietAI/vit5-base\")  \nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"VietAI/vit5-base\")\nmodel.cuda()\n```\n\n## Citation\n```\n@inproceedings{phan-etal-2022-vit5,\n    title = \"{V}i{T}5: Pretrained Text-to-Text Transformer for {V}ietnamese Language Generation\",\n    author = \"Phan, Long and Tran, Hieu and Nguyen, Hieu and Trinh, Trieu H.\",\n    booktitle = \"Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Student Research Workshop\",\n    year = \"2022\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.naacl-srw.18\",\n    pages = \"136--142\",\n}\n```",
    "card_content": "---\nlanguage: vi\nlicense: mit\ntags:\n- summarization\n- translation\n- question-answering\ndatasets:\n- cc100\n---\n\n# ViT5-base\n\nState-of-the-art pretrained Transformer-based encoder-decoder model for Vietnamese.\n\n## How to use\nFor more details, do check out [our Github repo](https://github.com/vietai/ViT5). \n\n[Finetunning Example can be found here](https://github.com/vietai/ViT5/tree/main/finetunning_huggingface).\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\u200b\ntokenizer = AutoTokenizer.from_pretrained(\"VietAI/vit5-base\")  \nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"VietAI/vit5-base\")\nmodel.cuda()\n```\n\n## Citation\n```\n@inproceedings{phan-etal-2022-vit5,\n    title = \"{V}i{T}5: Pretrained Text-to-Text Transformer for {V}ietnamese Language Generation\",\n    author = \"Phan, Long and Tran, Hieu and Nguyen, Hieu and Trinh, Trieu H.\",\n    booktitle = \"Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Student Research Workshop\",\n    year = \"2022\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.naacl-srw.18\",\n    pages = \"136--142\",\n}\n```",
    "library_name": "transformers"
  },
  {
    "model_id": "phiyodr/bert-large-finetuned-squad2",
    "model_name": "phiyodr/bert-large-finetuned-squad2",
    "author": "phiyodr",
    "downloads": 4424,
    "likes": 0,
    "tags": [
      "transformers",
      "pytorch",
      "jax",
      "bert",
      "question-answering",
      "en",
      "dataset:squad2",
      "arxiv:1810.04805",
      "arxiv:1806.03822",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/phiyodr/bert-large-finetuned-squad2",
    "dependencies": [
      [
        "transformers",
        "4.36.2"
      ],
      [
        "torch",
        "2.5.1"
      ]
    ],
    "analysis_date": "2025-03-26T00:50:35.880245",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "bert",
    "pipeline_tag": "question-answering",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": "en",
      "tags": [
        "pytorch",
        "question-answering"
      ],
      "datasets": [
        "squad2"
      ],
      "metrics": [
        "exact",
        "f1"
      ],
      "widget": [
        {
          "text": "What discipline did Winkelmann create?",
          "context": "Johann Joachim Winckelmann was a German art historian and archaeologist. He was a pioneering Hellenist who first articulated the difference between Greek, Greco-Roman and Roman art. The prophet and founding hero of modern archaeology, Winckelmann was one of the founders of scientific archaeology and first applied the categories of style on a large, systematic basis to the history of art."
        }
      ]
    },
    "card_text": "\n# bert-large-finetuned-squad2\n\n## Model description\n\nThis model is based on **[bert-large-uncased](https://huggingface.co/bert-large-uncased)** and was finetuned on **[SQuAD2.0](https://rajpurkar.github.io/SQuAD-explorer/)**. The corresponding papers you can found [here (model)](https://arxiv.org/abs/1810.04805) and [here (data)](https://arxiv.org/abs/1806.03822).\n\n\n## How to use\n\n```python\nfrom transformers.pipelines import pipeline\n\nmodel_name = \"phiyodr/bert-large-finetuned-squad2\"\nnlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\ninputs = {\n    'question': 'What discipline did Winkelmann create?',\n    'context': 'Johann Joachim Winckelmann was a German art historian and archaeologist. He was a pioneering Hellenist who first articulated the difference between Greek, Greco-Roman and Roman art. \"The prophet and founding hero of modern archaeology\", Winckelmann was one of the founders of scientific archaeology and first applied the categories of style on a large, systematic basis to the history of art. '\n}\nnlp(inputs)\n```\n\n\n\n## Training procedure\n\n```\n{\n\t\"base_model\": \"bert-large-uncased\",\n\t\"do_lower_case\": True,\n\t\"learning_rate\": 3e-5,\n\t\"num_train_epochs\": 4,\n\t\"max_seq_length\": 384,\n\t\"doc_stride\": 128,\n\t\"max_query_length\": 64,\n\t\"batch_size\": 96 \n}\n```\n\n## Eval results\n\n- Data: [dev-v2.0.json](https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json)\n- Script: [evaluate-v2.0.py](https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/contents/blob/) (original script from [here](https://github.com/huggingface/transformers/blob/master/examples/question-answering/README.md))\n\n```\n{\n  \"exact\": 76.22336393497852,\n  \"f1\": 79.72527570261339,\n  \"total\": 11873,\n  \"HasAns_exact\": 76.19770580296895,\n  \"HasAns_f1\": 83.21157193271408,\n  \"HasAns_total\": 5928,\n  \"NoAns_exact\": 76.24894869638352,\n  \"NoAns_f1\": 76.24894869638352,\n  \"NoAns_total\": 5945\n}\n```\n",
    "card_content": "---\nlanguage: en\ntags:\n- pytorch\n- question-answering\ndatasets:\n- squad2\nmetrics:\n- exact\n- f1\nwidget:\n- text: What discipline did Winkelmann create?\n  context: Johann Joachim Winckelmann was a German art historian and archaeologist.\n    He was a pioneering Hellenist who first articulated the difference between Greek,\n    Greco-Roman and Roman art. The prophet and founding hero of modern archaeology,\n    Winckelmann was one of the founders of scientific archaeology and first applied\n    the categories of style on a large, systematic basis to the history of art.\n---\n\n# bert-large-finetuned-squad2\n\n## Model description\n\nThis model is based on **[bert-large-uncased](https://huggingface.co/bert-large-uncased)** and was finetuned on **[SQuAD2.0](https://rajpurkar.github.io/SQuAD-explorer/)**. The corresponding papers you can found [here (model)](https://arxiv.org/abs/1810.04805) and [here (data)](https://arxiv.org/abs/1806.03822).\n\n\n## How to use\n\n```python\nfrom transformers.pipelines import pipeline\n\nmodel_name = \"phiyodr/bert-large-finetuned-squad2\"\nnlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\ninputs = {\n    'question': 'What discipline did Winkelmann create?',\n    'context': 'Johann Joachim Winckelmann was a German art historian and archaeologist. He was a pioneering Hellenist who first articulated the difference between Greek, Greco-Roman and Roman art. \"The prophet and founding hero of modern archaeology\", Winckelmann was one of the founders of scientific archaeology and first applied the categories of style on a large, systematic basis to the history of art. '\n}\nnlp(inputs)\n```\n\n\n\n## Training procedure\n\n```\n{\n\t\"base_model\": \"bert-large-uncased\",\n\t\"do_lower_case\": True,\n\t\"learning_rate\": 3e-5,\n\t\"num_train_epochs\": 4,\n\t\"max_seq_length\": 384,\n\t\"doc_stride\": 128,\n\t\"max_query_length\": 64,\n\t\"batch_size\": 96 \n}\n```\n\n## Eval results\n\n- Data: [dev-v2.0.json](https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json)\n- Script: [evaluate-v2.0.py](https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/contents/blob/) (original script from [here](https://github.com/huggingface/transformers/blob/master/examples/question-answering/README.md))\n\n```\n{\n  \"exact\": 76.22336393497852,\n  \"f1\": 79.72527570261339,\n  \"total\": 11873,\n  \"HasAns_exact\": 76.19770580296895,\n  \"HasAns_f1\": 83.21157193271408,\n  \"HasAns_total\": 5928,\n  \"NoAns_exact\": 76.24894869638352,\n  \"NoAns_f1\": 76.24894869638352,\n  \"NoAns_total\": 5945\n}\n```\n",
    "library_name": "transformers"
  },
  {
    "model_id": "pierreguillou/bert-base-cased-squad-v1.1-portuguese",
    "model_name": "pierreguillou/bert-base-cased-squad-v1.1-portuguese",
    "author": "pierreguillou",
    "downloads": 4296,
    "likes": 36,
    "tags": [
      "transformers",
      "pytorch",
      "tf",
      "jax",
      "bert",
      "question-answering",
      "bert-base",
      "pt",
      "dataset:brWaC",
      "dataset:squad",
      "dataset:squad_v1_pt",
      "license:mit",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/pierreguillou/bert-base-cased-squad-v1.1-portuguese",
    "dependencies": [
      [
        "transformers",
        "4.36.2"
      ],
      [
        "torch",
        "2.5.1"
      ]
    ],
    "analysis_date": "2025-03-26T00:50:37.027449",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "bert",
    "pipeline_tag": "question-answering",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": "pt",
      "license": "mit",
      "tags": [
        "question-answering",
        "bert",
        "bert-base",
        "pytorch"
      ],
      "datasets": [
        "brWaC",
        "squad",
        "squad_v1_pt"
      ],
      "metrics": [
        "squad"
      ],
      "widget": [
        {
          "text": "Quando come\u00e7ou a pandemia de Covid-19 no mundo?",
          "context": "A pandemia de COVID-19, tamb\u00e9m conhecida como pandemia de coronav\u00edrus, \u00e9 uma pandemia em curso de COVID-19, uma doen\u00e7a respirat\u00f3ria aguda causada pelo coronav\u00edrus da s\u00edndrome respirat\u00f3ria aguda grave 2 (SARS-CoV-2). A doen\u00e7a foi identificada pela primeira vez em Wuhan, na prov\u00edncia de Hubei, Rep\u00fablica Popular da China, em 1 de dezembro de 2019, mas o primeiro caso foi reportado em 31 de dezembro do mesmo ano."
        },
        {
          "text": "Onde foi descoberta a Covid-19?",
          "context": "A pandemia de COVID-19, tamb\u00e9m conhecida como pandemia de coronav\u00edrus, \u00e9 uma pandemia em curso de COVID-19, uma doen\u00e7a respirat\u00f3ria aguda causada pelo coronav\u00edrus da s\u00edndrome respirat\u00f3ria aguda grave 2 (SARS-CoV-2). A doen\u00e7a foi identificada pela primeira vez em Wuhan, na prov\u00edncia de Hubei, Rep\u00fablica Popular da China, em 1 de dezembro de 2019, mas o primeiro caso foi reportado em 31 de dezembro do mesmo ano."
        }
      ]
    },
    "card_text": "\n# Portuguese BERT base cased QA (Question Answering), finetuned on SQUAD v1.1\n\n![Exemple of what can do the Portuguese BERT base cased QA (Question Answering), finetuned on SQUAD v1.1](https://miro.medium.com/max/2000/1*te5MmdesAHCmg4KmK8zD3g.png)\n\n## Introduction\n\nThe model was trained on the dataset SQUAD v1.1 in portuguese from the [Deep Learning Brasil group](http://www.deeplearningbrasil.com.br/) on Google Colab. \n\nThe language model used is the [BERTimbau Base](https://huggingface.co/neuralmind/bert-base-portuguese-cased) (aka \"bert-base-portuguese-cased\") from [Neuralmind.ai](https://neuralmind.ai/): BERTimbau Base is a pretrained BERT model for Brazilian Portuguese that achieves state-of-the-art performances on three downstream NLP tasks: Named Entity Recognition, Sentence Textual Similarity and Recognizing Textual Entailment. It is available in two sizes: Base and Large.\n\n## Informations on the method used\n\nAll the informations are in the blog post : [NLP | Modelo de Question Answering em qualquer idioma baseado no BERT base (estudo de caso em portugu\u00eas)](https://medium.com/@pierre_guillou/nlp-modelo-de-question-answering-em-qualquer-idioma-baseado-no-bert-base-estudo-de-caso-em-12093d385e78)\n\n## Notebooks in Google Colab & GitHub\n\n- Google Colab: [colab_question_answering_BERT_base_cased_squad_v11_pt.ipynb](https://colab.research.google.com/drive/18ueLdi_V321Gz37x4gHq8mb4XZSGWfZx?usp=sharing)\n- GitHub: [colab_question_answering_BERT_base_cased_squad_v11_pt.ipynb](https://github.com/piegu/language-models/blob/master/colab_question_answering_BERT_base_cased_squad_v11_pt.ipynb)\n\n## Performance\n\nThe results obtained are the following:\n\n```\nf1 = 82.50\nexact match = 70.49\n```\n\n## How to use the model... with Pipeline\n\n```python\nimport transformers\nfrom transformers import pipeline\n\n# source: https://pt.wikipedia.org/wiki/Pandemia_de_COVID-19\ncontext = r\"\"\"\nA pandemia de COVID-19, tamb\u00e9m conhecida como pandemia de coronav\u00edrus, \u00e9 uma pandemia em curso de COVID-19, \numa doen\u00e7a respirat\u00f3ria aguda causada pelo coronav\u00edrus da s\u00edndrome respirat\u00f3ria aguda grave 2 (SARS-CoV-2). \nA doen\u00e7a foi identificada pela primeira vez em Wuhan, na prov\u00edncia de Hubei, Rep\u00fablica Popular da China, \nem 1 de dezembro de 2019, mas o primeiro caso foi reportado em 31 de dezembro do mesmo ano. \nAcredita-se que o v\u00edrus tenha uma origem zoon\u00f3tica, porque os primeiros casos confirmados \ntinham principalmente liga\u00e7\u00f5es ao Mercado Atacadista de Frutos do Mar de Huanan, que tamb\u00e9m vendia animais vivos. \nEm 11 de mar\u00e7o de 2020, a Organiza\u00e7\u00e3o Mundial da Sa\u00fade declarou o surto uma pandemia. At\u00e9 8 de fevereiro de 2021, \npelo menos 105 743 102 casos da doen\u00e7a foram confirmados em pelo menos 191 pa\u00edses e territ\u00f3rios, \ncom cerca de 2 308 943 mortes e 58 851 440 pessoas curadas.\n\"\"\"\n\nmodel_name = 'pierreguillou/bert-base-cased-squad-v1.1-portuguese'\nnlp = pipeline(\"question-answering\", model=model_name)\n\nquestion = \"Quando come\u00e7ou a pandemia de Covid-19 no mundo?\"\n\nresult = nlp(question=question, context=context)\n\nprint(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")\n\n# Answer: '1 de dezembro de 2019', score: 0.713, start: 328, end: 349\n```\n\n## How to use the model... with the Auto classes\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForQuestionAnswering\n  \ntokenizer = AutoTokenizer.from_pretrained(\"pierreguillou/bert-base-cased-squad-v1.1-portuguese\")\nmodel = AutoModelForQuestionAnswering.from_pretrained(\"pierreguillou/bert-base-cased-squad-v1.1-portuguese\")\n```             \n\nOr just clone the model repo:\n\n```python\ngit lfs install\ngit clone https://huggingface.co/pierreguillou/bert-base-cased-squad-v1.1-portuguese\n  \n# if you want to clone without large files \u2013 just their pointers\n# prepend your git clone with the following env var:\n  \nGIT_LFS_SKIP_SMUDGE=1\n```               \n\n## Limitations and bias\n\nThe training data used for this model come from Portuguese SQUAD. It could contain a lot of unfiltered content, which is far from neutral, and biases.\n\n## Author\n\nPortuguese BERT base cased QA (Question Answering), finetuned on SQUAD v1.1 was trained and evaluated by [Pierre GUILLOU](https://www.linkedin.com/in/pierreguillou/) thanks to the Open Source code, platforms and advices of many organizations ([link to the list](https://medium.com/@pierre_guillou/nlp-modelo-de-question-answering-em-qualquer-idioma-baseado-no-bert-base-estudo-de-caso-em-12093d385e78#c572)). In particular: [Hugging Face](https://huggingface.co/), [Neuralmind.ai](https://neuralmind.ai/), [Deep Learning Brasil group](http://www.deeplearningbrasil.com.br/), [Google Colab](https://colab.research.google.com/) and [AI Lab](https://ailab.unb.br/).\n\n## Citation\nIf you use our work, please cite:\n\n```bibtex\n@inproceedings{pierreguillou2021bertbasecasedsquadv11portuguese,\n  title={Portuguese BERT base cased QA (Question Answering), finetuned on SQUAD v1.1},\n  author={Pierre Guillou},\n  year={2021}\n}\n```",
    "card_content": "---\nlanguage: pt\nlicense: mit\ntags:\n- question-answering\n- bert\n- bert-base\n- pytorch\ndatasets:\n- brWaC\n- squad\n- squad_v1_pt\nmetrics:\n- squad\nwidget:\n- text: Quando come\u00e7ou a pandemia de Covid-19 no mundo?\n  context: A pandemia de COVID-19, tamb\u00e9m conhecida como pandemia de coronav\u00edrus,\n    \u00e9 uma pandemia em curso de COVID-19, uma doen\u00e7a respirat\u00f3ria aguda causada pelo\n    coronav\u00edrus da s\u00edndrome respirat\u00f3ria aguda grave 2 (SARS-CoV-2). A doen\u00e7a foi\n    identificada pela primeira vez em Wuhan, na prov\u00edncia de Hubei, Rep\u00fablica Popular\n    da China, em 1 de dezembro de 2019, mas o primeiro caso foi reportado em 31 de\n    dezembro do mesmo ano.\n- text: Onde foi descoberta a Covid-19?\n  context: A pandemia de COVID-19, tamb\u00e9m conhecida como pandemia de coronav\u00edrus,\n    \u00e9 uma pandemia em curso de COVID-19, uma doen\u00e7a respirat\u00f3ria aguda causada pelo\n    coronav\u00edrus da s\u00edndrome respirat\u00f3ria aguda grave 2 (SARS-CoV-2). A doen\u00e7a foi\n    identificada pela primeira vez em Wuhan, na prov\u00edncia de Hubei, Rep\u00fablica Popular\n    da China, em 1 de dezembro de 2019, mas o primeiro caso foi reportado em 31 de\n    dezembro do mesmo ano.\n---\n\n# Portuguese BERT base cased QA (Question Answering), finetuned on SQUAD v1.1\n\n![Exemple of what can do the Portuguese BERT base cased QA (Question Answering), finetuned on SQUAD v1.1](https://miro.medium.com/max/2000/1*te5MmdesAHCmg4KmK8zD3g.png)\n\n## Introduction\n\nThe model was trained on the dataset SQUAD v1.1 in portuguese from the [Deep Learning Brasil group](http://www.deeplearningbrasil.com.br/) on Google Colab. \n\nThe language model used is the [BERTimbau Base](https://huggingface.co/neuralmind/bert-base-portuguese-cased) (aka \"bert-base-portuguese-cased\") from [Neuralmind.ai](https://neuralmind.ai/): BERTimbau Base is a pretrained BERT model for Brazilian Portuguese that achieves state-of-the-art performances on three downstream NLP tasks: Named Entity Recognition, Sentence Textual Similarity and Recognizing Textual Entailment. It is available in two sizes: Base and Large.\n\n## Informations on the method used\n\nAll the informations are in the blog post : [NLP | Modelo de Question Answering em qualquer idioma baseado no BERT base (estudo de caso em portugu\u00eas)](https://medium.com/@pierre_guillou/nlp-modelo-de-question-answering-em-qualquer-idioma-baseado-no-bert-base-estudo-de-caso-em-12093d385e78)\n\n## Notebooks in Google Colab & GitHub\n\n- Google Colab: [colab_question_answering_BERT_base_cased_squad_v11_pt.ipynb](https://colab.research.google.com/drive/18ueLdi_V321Gz37x4gHq8mb4XZSGWfZx?usp=sharing)\n- GitHub: [colab_question_answering_BERT_base_cased_squad_v11_pt.ipynb](https://github.com/piegu/language-models/blob/master/colab_question_answering_BERT_base_cased_squad_v11_pt.ipynb)\n\n## Performance\n\nThe results obtained are the following:\n\n```\nf1 = 82.50\nexact match = 70.49\n```\n\n## How to use the model... with Pipeline\n\n```python\nimport transformers\nfrom transformers import pipeline\n\n# source: https://pt.wikipedia.org/wiki/Pandemia_de_COVID-19\ncontext = r\"\"\"\nA pandemia de COVID-19, tamb\u00e9m conhecida como pandemia de coronav\u00edrus, \u00e9 uma pandemia em curso de COVID-19, \numa doen\u00e7a respirat\u00f3ria aguda causada pelo coronav\u00edrus da s\u00edndrome respirat\u00f3ria aguda grave 2 (SARS-CoV-2). \nA doen\u00e7a foi identificada pela primeira vez em Wuhan, na prov\u00edncia de Hubei, Rep\u00fablica Popular da China, \nem 1 de dezembro de 2019, mas o primeiro caso foi reportado em 31 de dezembro do mesmo ano. \nAcredita-se que o v\u00edrus tenha uma origem zoon\u00f3tica, porque os primeiros casos confirmados \ntinham principalmente liga\u00e7\u00f5es ao Mercado Atacadista de Frutos do Mar de Huanan, que tamb\u00e9m vendia animais vivos. \nEm 11 de mar\u00e7o de 2020, a Organiza\u00e7\u00e3o Mundial da Sa\u00fade declarou o surto uma pandemia. At\u00e9 8 de fevereiro de 2021, \npelo menos 105 743 102 casos da doen\u00e7a foram confirmados em pelo menos 191 pa\u00edses e territ\u00f3rios, \ncom cerca de 2 308 943 mortes e 58 851 440 pessoas curadas.\n\"\"\"\n\nmodel_name = 'pierreguillou/bert-base-cased-squad-v1.1-portuguese'\nnlp = pipeline(\"question-answering\", model=model_name)\n\nquestion = \"Quando come\u00e7ou a pandemia de Covid-19 no mundo?\"\n\nresult = nlp(question=question, context=context)\n\nprint(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")\n\n# Answer: '1 de dezembro de 2019', score: 0.713, start: 328, end: 349\n```\n\n## How to use the model... with the Auto classes\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForQuestionAnswering\n  \ntokenizer = AutoTokenizer.from_pretrained(\"pierreguillou/bert-base-cased-squad-v1.1-portuguese\")\nmodel = AutoModelForQuestionAnswering.from_pretrained(\"pierreguillou/bert-base-cased-squad-v1.1-portuguese\")\n```             \n\nOr just clone the model repo:\n\n```python\ngit lfs install\ngit clone https://huggingface.co/pierreguillou/bert-base-cased-squad-v1.1-portuguese\n  \n# if you want to clone without large files \u2013 just their pointers\n# prepend your git clone with the following env var:\n  \nGIT_LFS_SKIP_SMUDGE=1\n```               \n\n## Limitations and bias\n\nThe training data used for this model come from Portuguese SQUAD. It could contain a lot of unfiltered content, which is far from neutral, and biases.\n\n## Author\n\nPortuguese BERT base cased QA (Question Answering), finetuned on SQUAD v1.1 was trained and evaluated by [Pierre GUILLOU](https://www.linkedin.com/in/pierreguillou/) thanks to the Open Source code, platforms and advices of many organizations ([link to the list](https://medium.com/@pierre_guillou/nlp-modelo-de-question-answering-em-qualquer-idioma-baseado-no-bert-base-estudo-de-caso-em-12093d385e78#c572)). In particular: [Hugging Face](https://huggingface.co/), [Neuralmind.ai](https://neuralmind.ai/), [Deep Learning Brasil group](http://www.deeplearningbrasil.com.br/), [Google Colab](https://colab.research.google.com/) and [AI Lab](https://ailab.unb.br/).\n\n## Citation\nIf you use our work, please cite:\n\n```bibtex\n@inproceedings{pierreguillou2021bertbasecasedsquadv11portuguese,\n  title={Portuguese BERT base cased QA (Question Answering), finetuned on SQUAD v1.1},\n  author={Pierre Guillou},\n  year={2021}\n}\n```",
    "library_name": "transformers"
  }
]