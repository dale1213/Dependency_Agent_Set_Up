{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All models and their dependencies across all files:\n",
      "\n",
      "Model: deepset/roberta-base-squad2\n",
      "  haystack-ai: null\n",
      "  transformers: null\n",
      "\n",
      "Model: distilbert/distilbert-base-cased-distilled-squad\n",
      "  transformers: null\n",
      "\n",
      "Model: sjrhuschlee/flan-t5-large-squad2\n",
      "  transformers: null\n",
      "  torch: null\n",
      "  peft: null\n",
      "\n",
      "Model: google-bert/bert-large-uncased-whole-word-masking-finetuned-squad\n",
      "  torch: null\n",
      "\n",
      "Model: deepset/bert-large-uncased-whole-word-masking-squad2\n",
      "  haystack-ai: null\n",
      "  transformers: null\n",
      "\n",
      "Model: deepset/minilm-uncased-squad2\n",
      "  haystack-ai: null\n",
      "  transformers: null\n",
      "\n",
      "Model: distilbert/distilbert-base-uncased-distilled-squad\n",
      "  transformers: null\n",
      "\n",
      "Model: google-bert/bert-large-cased-whole-word-masking-finetuned-squad\n",
      "  torch: null\n",
      "\n",
      "Model: csarron/mobilebert-uncased-squad-v2\n",
      "  transformers: null\n",
      "\n",
      "Model: SmallDoge/Doge-160M-Instruct\n",
      "  transformers: null\n",
      "  trl: null\n",
      "\n",
      "Model: deepset/bert-base-cased-squad2\n",
      "  haystack-ai: null\n",
      "  transformers: null\n",
      "\n",
      "Model: deepset/roberta-large-squad2\n",
      "  transformers: null\n",
      "\n",
      "Model: Rifky/Indobert-QA\n",
      "  transformers: null\n",
      "\n",
      "Model: philschmid/distilbert-onnx\n",
      "  transformers: null\n",
      "  torch: null\n",
      "  onnx: null\n",
      "  onnxruntime: null\n",
      "\n",
      "Model: AgentPublic/camembert-base-squadFR-fquad-piaf\n",
      "  transformers: null\n",
      "\n",
      "Model: furiosa-ai/mlperf-bert-large\n",
      "  transformers: null\n",
      "\n",
      "Model: Rakib/roberta-base-on-cuad\n",
      "  transformers: null\n",
      "\n",
      "Model: timpal0l/mdeberta-v3-base-squad2\n",
      "  transformers: null\n",
      "\n",
      "Model: deepset/tinyroberta-squad2\n",
      "  transformers: null\n",
      "  torch: null\n",
      "  sentencepiece: null\n",
      "\n",
      "Model: HPAI-BSC/Llama3-Aloe-8B-Alpha\n",
      "  transformers: null\n",
      "  torch: null\n",
      "\n",
      "Model: deepset/roberta-base-squad2-distilled\n",
      "  haystack-ai: null\n",
      "  transformers: null\n",
      "\n",
      "Model: Praise2112/ModernBERT-base-squad2-v0.2\n",
      "  transformers: 4.48.0.dev0\n",
      "  pytorch: 2.5.1+cu124\n",
      "  datasets: 2.20.0\n",
      "  tokenizers: 0.21.0\n",
      "\n",
      "Model: SmallDoge/Doge-60M-Instruct\n",
      "  transformers: null\n",
      "\n",
      "Model: ahotrod/electra_large_discriminator_squad2_512\n",
      "  transformers: 2.11.0\n",
      "  pytorch: 1.5.0\n",
      "  tensorflow: 2.2.0\n",
      "  python: 3.8.1\n",
      "\n",
      "Model: deepset/deberta-v3-base-squad2\n",
      "  transformers: null\n",
      "  haystack-ai: null\n",
      "\n",
      "Model: SmallDoge/Doge-160M-Reason-Distill\n",
      "  transformers: null\n",
      "\n",
      "Model: SmallDoge/Doge-20M-Instruct\n",
      "  transformers: null\n",
      "\n",
      "Model: deepset/xlm-roberta-large-squad2\n",
      "  haystack-ai: null\n",
      "  transformers: null\n",
      "\n",
      "Model: amara16/distilbert-extractive-qa-project\n",
      "  transformers: null\n",
      "\n",
      "Model: umarigan/llama-3.1-openhermes-tr\n",
      "  transformers: null\n",
      "\n",
      "Model: Backedman/TriviaAnsweringMachineREAL\n",
      "  protobuf: null\n",
      "\n",
      "Model: deepset/bert-base-uncased-squad2\n",
      "  haystack-ai: null\n",
      "  transformers: null\n",
      "\n",
      "Model: houyu0930/test-demo-qa\n",
      "  transformers: null\n",
      "\n",
      "Model: mrm8488/bert-tiny-finetuned-squadv2\n",
      "  transformers: null\n",
      "\n",
      "Model: nes470/pipeline-as-repo\n",
      "  transformers: null\n",
      "\n",
      "Model: VietAI/vit5-base\n",
      "  transformers: null\n",
      "\n",
      "Model: sahilnishad/Florence-2-FT-DocVQA\n",
      "  torch: null\n",
      "  transformers: null\n",
      "  datasets: null\n",
      "  flash_attn: null\n",
      "\n",
      "Model: deepset/deberta-v3-large-squad2\n",
      "  transformers: null\n",
      "\n",
      "Model: phiyodr/bert-large-finetuned-squad2\n",
      "  transformers: null\n",
      "\n",
      "Model: bigwiz83/sapbert-from-pubmedbert-squad2\n",
      "  transformers: 4.7.0\n",
      "  pytorch: 1.8.0\n",
      "  datasets: 1.4.1\n",
      "  tokenizers: 0.10.2\n",
      "\n",
      "Model: pierreguillou/bert-base-cased-squad-v1.1-portuguese\n",
      "  transformers: null\n",
      "\n",
      "Model: deepset/gelectra-large-germanquad\n",
      "  haystack-ai: null\n",
      "  transformers: null\n",
      "\n",
      "Model: nathantablang/question-answering-qa-may-12-tablang-LOCAL\n",
      "  transformers: null\n",
      "\n",
      "Model: cross-encoder/ms-marco-MiniLM-L6-v2\n",
      "  sentence_transformers: null\n",
      "  transformers: null\n",
      "  torch: null\n",
      "\n",
      "Model: distilbert/distilbert-base-uncased-finetuned-sst-2-english\n",
      "  torch: null\n",
      "  transformers: null\n",
      "\n",
      "Model: papluca/xlm-roberta-base-language-detection\n",
      "  transformers: 4.12.5\n",
      "  pytorch: 1.10.0+cu111\n",
      "  datasets: 1.15.1\n",
      "  tokenizers: 0.10.3\n",
      "\n",
      "Model: microsoft/deberta-xlarge-mnli\n",
      "  torch: null\n",
      "  transformers: null\n",
      "\n",
      "Model: cardiffnlp/twitter-roberta-base-sentiment-latest\n",
      "  transformers: null\n",
      "  numpy: null\n",
      "  scipy: null\n",
      "\n",
      "Model: cardiffnlp/twitter-roberta-base-sentiment\n",
      "  transformers: null\n",
      "  numpy: null\n",
      "  scipy: null\n",
      "  csv: null\n",
      "  urllib: null\n",
      "\n",
      "Model: cross-encoder/ms-marco-MiniLM-L4-v2\n",
      "  sentence_transformers: null\n",
      "  transformers: null\n",
      "  torch: null\n",
      "\n",
      "Model: cardiffnlp/twitter-xlm-roberta-base-sentiment\n",
      "  transformers: null\n",
      "  numpy: null\n",
      "  scipy: null\n",
      "\n",
      "Model: lucadiliello/BLEURT-20-D12\n",
      "  torch: null\n",
      "  bleurt_pytorch: null\n",
      "\n",
      "Model: BAAI/bge-reranker-v2-m3\n",
      "  FlagEmbedding: null\n",
      "  torch: null\n",
      "  transformers: null\n",
      "\n",
      "Model: cardiffnlp/twitter-roberta-base-emotion\n",
      "  transformers: null\n",
      "  numpy: null\n",
      "  scipy: null\n",
      "  csv: null\n",
      "  urllib: null\n",
      "\n",
      "Model: yiyanghkust/finbert-tone\n",
      "  transformers: null\n",
      "\n",
      "Model: cross-encoder/ms-marco-MiniLM-L12-v2\n",
      "  sentence_transformers: null\n",
      "  transformers: null\n",
      "  torch: null\n",
      "\n",
      "Model: michellejieli/emotion_text_classifier\n",
      "  transformers: null\n",
      "\n",
      "Model: sadickam/sdgBERT\n",
      "  transformers: null\n",
      "\n",
      "Model: TieIncred/distilbert-base-uncased-finetuned-emotional\n",
      "  transformers: 4.16.2\n",
      "  pytorch: 2.1.0+cu118\n",
      "  datasets: 2.9.0\n",
      "  tokenizers: 0.14.1\n",
      "\n",
      "Model: lxyuan/distilbert-base-multilingual-cased-sentiments-student\n",
      "  transformers: 4.28.1\n",
      "  pytorch: 2.0.0+cu118\n",
      "  datasets: 2.11.0\n",
      "  tokenizers: 0.13.3\n",
      "\n",
      "Model: microsoft/deberta-large-mnli\n",
      "  torch: null\n",
      "  transformers: null\n",
      "\n",
      "Model: unitary/toxic-bert\n",
      "  transformers: null\n",
      "  pytorch_lightning: null\n",
      "  torch: null\n",
      "  pandas: null\n",
      "  detoxify: null\n",
      "\n",
      "Model: j-hartmann/emotion-english-distilroberta-base\n",
      "  transformers: null\n",
      "\n",
      "Model: finiteautomata/beto-sentiment-analysis\n",
      "  pysentimiento: null\n",
      "\n",
      "Model: Elron/bleurt-tiny-512\n",
      "  transformers: null\n",
      "  torch: null\n",
      "\n",
      "Model: martin-ha/toxic-comment-model\n",
      "  transformers: null\n",
      "\n",
      "Model: SamLowe/roberta-base-go_emotions\n",
      "  transformers: null\n",
      "\n",
      "Model: tals/albert-xlarge-vitaminc-mnli\n",
      "  datasets: null\n",
      "\n",
      "Model: mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis\n",
      "  transformers: 4.10.2\n",
      "  pytorch: 1.9.0+cu102\n",
      "  datasets: 1.12.1\n",
      "  tokenizers: 0.10.3\n",
      "\n",
      "Model: cross-encoder/ms-marco-electra-base\n",
      "  sentence_transformers: null\n",
      "  transformers: null\n",
      "  torch: null\n",
      "\n",
      "Model: nbroad/ESG-BERT\n",
      "  torchserve: null\n",
      "  torch-model-archiver: null\n",
      "  torchvision: null\n",
      "  transformers: null\n",
      "\n",
      "Model: oliverguhr/german-sentiment-bert\n",
      "  germansentiment: null\n",
      "\n",
      "Model: madhurjindal/autonlp-Gibberish-Detector-492513457\n",
      "  torch: null\n",
      "  transformers: null\n",
      "\n",
      "Model: cross-encoder/ms-marco-MiniLM-L2-v2\n",
      "  sentence_transformers: null\n",
      "  transformers: null\n",
      "  torch: null\n",
      "\n",
      "Model: jinaai/jina-reranker-v2-base-multilingual\n",
      "  transformers: null\n",
      "  einops: null\n",
      "  sentence-transformers: null\n",
      "\n",
      "Model: WebOrganizer/TopicClassifier-NoURL\n",
      "  transformers: null\n",
      "  xformers: null\n",
      "  torch: null\n",
      "\n",
      "Model: cross-encoder/ms-marco-TinyBERT-L2-v2\n",
      "  transformers: null\n",
      "  torch: null\n",
      "  sentence_transformers: null\n",
      "\n",
      "Model: Alibaba-NLP/gte-multilingual-reranker-base\n",
      "  transformers: 4.36.0\n",
      "\n",
      "Model: finiteautomata/bertweet-base-sentiment-analysis\n",
      "  pysentimiento: null\n",
      "\n",
      "Model: amberoad/bert-multilingual-passage-reranking-msmarco\n",
      "  transformers: null\n",
      "\n",
      "Model: cross-encoder/mmarco-mMiniLMv2-L12-H384-v1\n",
      "  sentence_transformers: null\n",
      "  transformers: null\n",
      "  torch: null\n",
      "\n",
      "Model: tabularisai/multilingual-sentiment-analysis\n",
      "  transformers: null\n",
      "  torch: null\n",
      "\n",
      "Model: cross-encoder/stsb-roberta-base\n",
      "  sentence_transformers: null\n",
      "\n",
      "Model: jitesh/emotion-english\n",
      "  transformers: null\n",
      "\n",
      "Model: DiTy/cross-encoder-russian-msmarco\n",
      "  sentence-transformers: null\n",
      "  torch: null\n",
      "  transformers: null\n",
      "\n",
      "Model: cardiffnlp/twitter-xlm-roberta-base-sentiment-multilingual\n",
      "  tweetnlp: null\n",
      "\n",
      "Model: MoritzLaurer/ModernBERT-large-zeroshot-v2.0\n",
      "  transformers: 4.48.0.dev0\n",
      "  pytorch: 2.5.1+cu124\n",
      "  datasets: 3.2.0\n",
      "  tokenizers: 0.21.0\n",
      "\n",
      "Model: openai-community/gpt2\n",
      "  transformers: null\n",
      "\n",
      "Model: dslim/bert-base-NER\n",
      "  transformers: null\n",
      "\n",
      "Model: w11wo/indonesian-roberta-base-posp-tagger\n",
      "  transformers: 4.37.2\n",
      "  pytorch: 2.2.0+cu118\n",
      "  datasets: 2.16.1\n",
      "  tokenizers: 0.15.1\n",
      "\n",
      "Model: Babelscape/wikineural-multilingual-ner\n",
      "  transformers: null\n",
      "\n",
      "Model: gilf/french-camembert-postag-model\n",
      "  transformers: null\n",
      "\n",
      "Model: FacebookAI/xlm-roberta-large-finetuned-conll03-english\n",
      "  transformers: null\n",
      "\n",
      "Model: obi/deid_roberta_i2b2\n",
      "  spacy: null\n",
      "\n",
      "Model: MMG/xlm-roberta-large-ner-spanish\n",
      "  transformers: null\n",
      "\n",
      "Model: tsmatz/xlm-roberta-ner-japanese\n",
      "  transformers: 4.23.1\n",
      "  pytorch: 1.12.1+cu102\n",
      "  datasets: 2.6.1\n",
      "  tokenizers: 0.13.1\n",
      "\n",
      "Model: EvanD/xlm-roberta-base-romanian-ner-ronec\n",
      "  transformers: null\n",
      "\n",
      "Model: oliverguhr/fullstop-punctuation-multilang-large\n",
      "  deepmultilingualpunctuation: null\n",
      "\n",
      "Model: blaze999/Medical-NER\n",
      "  transformers: 4.37.0\n",
      "  pytorch: 2.1.2\n",
      "  datasets: 2.1.0\n",
      "  tokenizers: 0.15.1\n",
      "\n",
      "Model: microsoft/llmlingua-2-bert-base-multilingual-cased-meetingbank\n",
      "  llmlingua: null\n",
      "\n",
      "Model: Davlan/bert-base-multilingual-cased-ner-hrl\n",
      "  transformers: null\n",
      "\n",
      "Model: ckiplab/bert-base-chinese-ws\n",
      "  transformers: null\n",
      "\n",
      "Model: Davlan/distilbert-base-multilingual-cased-ner-hrl\n",
      "  transformers: null\n",
      "\n",
      "Model: Jean-Baptiste/camembert-ner\n",
      "  transformers: null\n",
      "\n",
      "Model: Jean-Baptiste/camembert-ner-with-dates\n",
      "  transformers: null\n",
      "\n",
      "Model: globis-university/deberta-v3-japanese-large\n",
      "  transformers: null\n",
      "\n",
      "Model: Jean-Baptiste/roberta-large-ner-english\n",
      "  transformers: null\n",
      "\n",
      "Model: dslim/bert-large-NER\n",
      "  transformers: null\n",
      "\n",
      "Model: Angelakeke/RaTE-NER-Deberta\n",
      "  transformers: null\n",
      "  torch: null\n",
      "\n",
      "Model: tner/roberta-large-ontonotes5\n",
      "  tner: null\n",
      "\n",
      "Model: ckiplab/bert-base-chinese-pos\n",
      "  transformers: null\n",
      "\n",
      "Model: 51la5/roberta-large-NER\n",
      "  transformers: null\n",
      "\n",
      "Model: yanekyuk/camembert-keyword-extractor\n",
      "  transformers: 4.19.2\n",
      "  pytorch: 1.11.0+cu113\n",
      "  datasets: 2.2.2\n",
      "  tokenizers: 0.12.1\n",
      "\n",
      "Model: sociocom/MedNERN-CR-JA\n",
      "  transformers: null\n",
      "  torch: null\n",
      "  numpy: null\n",
      "  pandas: null\n",
      "  tqdm: null\n",
      "  scikit-learn: null\n",
      "\n",
      "Model: ml6team/keyphrase-extraction-distilbert-inspec\n",
      "  transformers: null\n",
      "  numpy: null\n",
      "\n",
      "Model: syssec-utd/py312-pylingual-v1-segmenter\n",
      "  transformers: 4.48.2\n",
      "  pytorch: 2.2.1+cu121\n",
      "  datasets: 2.18.0\n",
      "  tokenizers: 0.21.0\n",
      "\n",
      "Model: ckiplab/bert-base-chinese-ner\n",
      "  transformers: null\n",
      "\n",
      "Model: savasy/bert-base-turkish-ner-cased\n",
      "  transformers: null\n",
      "\n",
      "Model: QCRI/bert-base-multilingual-cased-pos-english\n",
      "  transformers: null\n",
      "\n",
      "Model: pdelobelle/robbert-v2-dutch-ner\n",
      "  transformers: null\n",
      "\n",
      "Model: ckiplab/albert-tiny-chinese-ws\n",
      "  transformers: null\n",
      "\n",
      "Model: syafiqfaray/indobert-model-ner\n",
      "  transformers: 4.38.2\n",
      "  pytorch: 2.2.1+cu121\n",
      "  datasets: 2.18.0\n",
      "  tokenizers: 0.15.2\n",
      "\n",
      "Model: llm-book/bert-base-japanese-v3-ner-wikipedia-dataset\n",
      "  transformers: null\n",
      "\n",
      "Model: protectai/unbiased-toxic-roberta-onnx\n",
      "  optimum: null\n",
      "  transformers: null\n",
      "\n",
      "Model: microsoft/llmlingua-2-xlm-roberta-large-meetingbank\n",
      "  llmlingua: null\n",
      "\n",
      "Model: hatmimoha/arabic-ner\n",
      "  transformers: null\n",
      "  torch: null\n",
      "\n",
      "Model: NlpHUST/ner-vietnamese-electra-base\n",
      "  transformers: 4.20.1\n",
      "  pytorch: 1.8.0+cu111\n",
      "  datasets: 2.4.0\n",
      "  tokenizers: 0.12.1\n",
      "\n",
      "Model: Gherman/bert-base-NER-Russian\n",
      "  transformers: 4.28.1\n",
      "  pytorch: 1.13.0\n",
      "  datasets: 2.12.0\n",
      "  tokenizers: 0.13.3\n",
      "\n",
      "Models with non-null version dependencies across all files:\n",
      "\n",
      "Model: Praise2112/ModernBERT-base-squad2-v0.2\n",
      "  transformers: 4.48.0.dev0\n",
      "  pytorch: 2.5.1+cu124\n",
      "  datasets: 2.20.0\n",
      "  tokenizers: 0.21.0\n",
      "\n",
      "Model: ahotrod/electra_large_discriminator_squad2_512\n",
      "  transformers: 2.11.0\n",
      "  pytorch: 1.5.0\n",
      "  tensorflow: 2.2.0\n",
      "  python: 3.8.1\n",
      "\n",
      "Model: bigwiz83/sapbert-from-pubmedbert-squad2\n",
      "  transformers: 4.7.0\n",
      "  pytorch: 1.8.0\n",
      "  datasets: 1.4.1\n",
      "  tokenizers: 0.10.2\n",
      "\n",
      "Model: papluca/xlm-roberta-base-language-detection\n",
      "  transformers: 4.12.5\n",
      "  pytorch: 1.10.0+cu111\n",
      "  datasets: 1.15.1\n",
      "  tokenizers: 0.10.3\n",
      "\n",
      "Model: TieIncred/distilbert-base-uncased-finetuned-emotional\n",
      "  transformers: 4.16.2\n",
      "  pytorch: 2.1.0+cu118\n",
      "  datasets: 2.9.0\n",
      "  tokenizers: 0.14.1\n",
      "\n",
      "Model: lxyuan/distilbert-base-multilingual-cased-sentiments-student\n",
      "  transformers: 4.28.1\n",
      "  pytorch: 2.0.0+cu118\n",
      "  datasets: 2.11.0\n",
      "  tokenizers: 0.13.3\n",
      "\n",
      "Model: mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis\n",
      "  transformers: 4.10.2\n",
      "  pytorch: 1.9.0+cu102\n",
      "  datasets: 1.12.1\n",
      "  tokenizers: 0.10.3\n",
      "\n",
      "Model: Alibaba-NLP/gte-multilingual-reranker-base\n",
      "  transformers: 4.36.0\n",
      "\n",
      "Model: MoritzLaurer/ModernBERT-large-zeroshot-v2.0\n",
      "  transformers: 4.48.0.dev0\n",
      "  pytorch: 2.5.1+cu124\n",
      "  datasets: 3.2.0\n",
      "  tokenizers: 0.21.0\n",
      "\n",
      "Model: w11wo/indonesian-roberta-base-posp-tagger\n",
      "  transformers: 4.37.2\n",
      "  pytorch: 2.2.0+cu118\n",
      "  datasets: 2.16.1\n",
      "  tokenizers: 0.15.1\n",
      "\n",
      "Model: tsmatz/xlm-roberta-ner-japanese\n",
      "  transformers: 4.23.1\n",
      "  pytorch: 1.12.1+cu102\n",
      "  datasets: 2.6.1\n",
      "  tokenizers: 0.13.1\n",
      "\n",
      "Model: blaze999/Medical-NER\n",
      "  transformers: 4.37.0\n",
      "  pytorch: 2.1.2\n",
      "  datasets: 2.1.0\n",
      "  tokenizers: 0.15.1\n",
      "\n",
      "Model: yanekyuk/camembert-keyword-extractor\n",
      "  transformers: 4.19.2\n",
      "  pytorch: 1.11.0+cu113\n",
      "  datasets: 2.2.2\n",
      "  tokenizers: 0.12.1\n",
      "\n",
      "Model: syssec-utd/py312-pylingual-v1-segmenter\n",
      "  transformers: 4.48.2\n",
      "  pytorch: 2.2.1+cu121\n",
      "  datasets: 2.18.0\n",
      "  tokenizers: 0.21.0\n",
      "\n",
      "Model: syafiqfaray/indobert-model-ner\n",
      "  transformers: 4.38.2\n",
      "  pytorch: 2.2.1+cu121\n",
      "  datasets: 2.18.0\n",
      "  tokenizers: 0.15.2\n",
      "\n",
      "Model: NlpHUST/ner-vietnamese-electra-base\n",
      "  transformers: 4.20.1\n",
      "  pytorch: 1.8.0+cu111\n",
      "  datasets: 2.4.0\n",
      "  tokenizers: 0.12.1\n",
      "\n",
      "Model: Gherman/bert-base-NER-Russian\n",
      "  transformers: 4.28.1\n",
      "  pytorch: 1.13.0\n",
      "  datasets: 2.12.0\n",
      "  tokenizers: 0.13.3\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "# List of JSON files to process\n",
    "json_files = [\n",
    "    \"./model_analysis_type_question-answering_min_dl_1000_lib_transformers.json\",\n",
    "    \"./model_analysis_type_text-classification_min_dl_1000_lib_transformers.json\", \n",
    "    \"./model_analysis_type_text-generation_min_dl_1000_lib_transformers.json\",\n",
    "    \"./model_analysis_type_token-classification_min_dl_1000_lib_transformers.json\"\n",
    "]\n",
    "\n",
    "# Dictionary to store all models and their dependencies across files\n",
    "all_model_dependencies = {}\n",
    "models_with_version_info = {}\n",
    "\n",
    "# Process each JSON file\n",
    "for json_file in json_files:\n",
    "    try:\n",
    "        with open(json_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            \n",
    "        # Process models in current file\n",
    "        for model in data:\n",
    "            model_id = model['model_id']\n",
    "            deps = model.get('dependencies', [])\n",
    "            \n",
    "            if deps:\n",
    "                all_model_dependencies[model_id] = []\n",
    "                has_non_null_version = False\n",
    "                \n",
    "                for dep in deps:\n",
    "                    if len(dep) == 2:\n",
    "                        library, version = dep\n",
    "                        all_model_dependencies[model_id].append((library, version))\n",
    "                        if version is not None:\n",
    "                            has_non_null_version = True\n",
    "                \n",
    "                if has_non_null_version:\n",
    "                    models_with_version_info[model_id] = all_model_dependencies[model_id]\n",
    "                    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: Could not find file {json_file}\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Warning: Error parsing JSON from {json_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Unexpected error processing {json_file}: {str(e)}\")\n",
    "\n",
    "print(\"\\nAll models and their dependencies across all files:\")\n",
    "for model_id, deps in all_model_dependencies.items():\n",
    "    print(f\"\\nModel: {model_id}\")\n",
    "    for lib, version in deps:\n",
    "        version_str = version if version is not None else \"null\"\n",
    "        print(f\"  {lib}: {version_str}\")\n",
    "\n",
    "print(\"\\nModels with non-null version dependencies across all files:\")\n",
    "for model_id, deps in models_with_version_info.items():\n",
    "    print(f\"\\nModel: {model_id}\")\n",
    "    for lib, version in deps:\n",
    "        version_str = version if version is not None else \"null\"\n",
    "        print(f\"  {lib}: {version_str}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_dir = \"./model_analysis_type_question-answering_min_dl_1000_lib_transformers.json\"\n",
    "with open(json_file, 'r') as f:\n",
    "    data = json.load(f)\n",
    "# Process models in current file\n",
    "models_with_version_info = {}\n",
    "for model in data:\n",
    "    model_id = model['model_id']\n",
    "    deps = model.get('dependencies', [])\n",
    "    \n",
    "    if deps:\n",
    "        all_model_dependencies[model_id] = []\n",
    "        has_non_null_version = False\n",
    "        \n",
    "        for dep in deps:\n",
    "            if len(dep) == 2:\n",
    "                library, version = dep\n",
    "                all_model_dependencies[model_id].append((library, version))\n",
    "                if version is not None:\n",
    "                    has_non_null_version = True\n",
    "        \n",
    "        if has_non_null_version:\n",
    "            models_with_version_info[model_id] = all_model_dependencies[model_id]\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting transformers==4.48.0\n",
      "  Downloading transformers-4.48.0-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting torch==2.5.1\n",
      "  Using cached torch-2.5.1-cp39-cp39-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "Collecting datasets==2.20.0\n",
      "  Downloading datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting tokenizers==0.21.0\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/.local/lib/python3.9/site-packages (from transformers==4.48.0) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /home/ec2-user/.local/lib/python3.9/site-packages (from transformers==4.48.0) (0.28.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/.local/lib/python3.9/site-packages (from transformers==4.48.0) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/.local/lib/python3.9/site-packages (from transformers==4.48.0) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/.local/lib/python3.9/site-packages (from transformers==4.48.0) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/.local/lib/python3.9/site-packages (from transformers==4.48.0) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/ec2-user/.local/lib/python3.9/site-packages (from transformers==4.48.0) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/ec2-user/.local/lib/python3.9/site-packages (from transformers==4.48.0) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/.local/lib/python3.9/site-packages (from transformers==4.48.0) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/ec2-user/.local/lib/python3.9/site-packages (from torch==2.5.1) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/.local/lib/python3.9/site-packages (from torch==2.5.1) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/.local/lib/python3.9/site-packages (from torch==2.5.1) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /home/ec2-user/.local/lib/python3.9/site-packages (from torch==2.5.1) (2023.12.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/ec2-user/.local/lib/python3.9/site-packages (from torch==2.5.1) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/ec2-user/.local/lib/python3.9/site-packages (from torch==2.5.1) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/ec2-user/.local/lib/python3.9/site-packages (from torch==2.5.1) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/ec2-user/.local/lib/python3.9/site-packages (from torch==2.5.1) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/ec2-user/.local/lib/python3.9/site-packages (from torch==2.5.1) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/ec2-user/.local/lib/python3.9/site-packages (from torch==2.5.1) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/ec2-user/.local/lib/python3.9/site-packages (from torch==2.5.1) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/ec2-user/.local/lib/python3.9/site-packages (from torch==2.5.1) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/ec2-user/.local/lib/python3.9/site-packages (from torch==2.5.1) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/ec2-user/.local/lib/python3.9/site-packages (from torch==2.5.1) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/ec2-user/.local/lib/python3.9/site-packages (from torch==2.5.1) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/ec2-user/.local/lib/python3.9/site-packages (from torch==2.5.1) (12.4.127)\n",
      "Collecting triton==3.1.0 (from torch==2.5.1)\n",
      "  Using cached triton-3.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/ec2-user/.local/lib/python3.9/site-packages (from torch==2.5.1) (1.13.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/ec2-user/.local/lib/python3.9/site-packages (from datasets==2.20.0) (19.0.1)\n",
      "Collecting pyarrow-hotfix (from datasets==2.20.0)\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/ec2-user/.local/lib/python3.9/site-packages (from datasets==2.20.0) (0.3.7)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/.local/lib/python3.9/site-packages (from datasets==2.20.0) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /home/ec2-user/.local/lib/python3.9/site-packages (from datasets==2.20.0) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /home/ec2-user/.local/lib/python3.9/site-packages (from datasets==2.20.0) (0.70.15)\n",
      "Requirement already satisfied: aiohttp in /home/ec2-user/.local/lib/python3.9/site-packages (from datasets==2.20.0) (3.9.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ec2-user/.local/lib/python3.9/site-packages (from sympy==1.13.1->torch==2.5.1) (1.3.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ec2-user/.local/lib/python3.9/site-packages (from aiohttp->datasets==2.20.0) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/.local/lib/python3.9/site-packages (from aiohttp->datasets==2.20.0) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ec2-user/.local/lib/python3.9/site-packages (from aiohttp->datasets==2.20.0) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/.local/lib/python3.9/site-packages (from aiohttp->datasets==2.20.0) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ec2-user/.local/lib/python3.9/site-packages (from aiohttp->datasets==2.20.0) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/ec2-user/.local/lib/python3.9/site-packages (from aiohttp->datasets==2.20.0) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/.local/lib/python3.9/site-packages (from requests->transformers==4.48.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/.local/lib/python3.9/site-packages (from requests->transformers==4.48.0) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/.local/lib/python3.9/site-packages (from requests->transformers==4.48.0) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/.local/lib/python3.9/site-packages (from requests->transformers==4.48.0) (2023.11.17)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/.local/lib/python3.9/site-packages (from jinja2->torch==2.5.1) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ec2-user/.local/lib/python3.9/site-packages (from pandas->datasets==2.20.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/.local/lib/python3.9/site-packages (from pandas->datasets==2.20.0) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ec2-user/.local/lib/python3.9/site-packages (from pandas->datasets==2.20.0) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/.local/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.20.0) (1.16.0)\n",
      "Downloading transformers-4.48.0-py3-none-any.whl (9.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m125.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.5.1-cp39-cp39-manylinux1_x86_64.whl (906.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m906.5/906.5 MB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m167.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached triton-3.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
      "Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Installing collected packages: triton, pyarrow-hotfix, torch, tokenizers, transformers, datasets\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 2.0.0\n",
      "    Uninstalling triton-2.0.0:\n",
      "      Successfully uninstalled triton-2.0.0\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.6.0+cpu\n",
      "    Uninstalling torch-2.6.0+cpu:\n",
      "      Successfully uninstalled torch-2.6.0+cpu\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.13.3\n",
      "    Uninstalling tokenizers-0.13.3:\n",
      "      Successfully uninstalled tokenizers-0.13.3\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.27.1\n",
      "    Uninstalling transformers-4.27.1:\n",
      "      Successfully uninstalled transformers-4.27.1\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 2.14.3\n",
      "    Uninstalling datasets-2.14.3:\n",
      "      Successfully uninstalled datasets-2.14.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "facenet-pytorch 2.6.0 requires torch<2.3.0,>=2.2.0, but you have torch 2.5.1 which is incompatible.\n",
      "facenet-pytorch 2.6.0 requires torchvision<0.18.0,>=0.17.0, but you have torchvision 0.21.0+cpu which is incompatible.\n",
      "torchaudio 2.5.0 requires torch==2.5.0, but you have torch 2.5.1 which is incompatible.\n",
      "torchvision 0.21.0+cpu requires torch==2.6.0, but you have torch 2.5.1 which is incompatible.\n",
      "xformers 0.0.20 requires torch==2.0.1, but you have torch 2.5.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed datasets-2.20.0 pyarrow-hotfix-0.6 tokenizers-0.21.0 torch-2.5.1 transformers-4.48.0 triton-3.1.0\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'modernbert'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoModelForQuestionAnswering, AutoTokenizer\n\u001b[1;32m      6\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPraise2112/ModernBERT-base-squad2-v0.2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 7\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForQuestionAnswering\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[1;32m     10\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is dependency conflict?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:441\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m kwargs_copy\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    439\u001b[0m         _ \u001b[38;5;241m=\u001b[39m kwargs_copy\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 441\u001b[0m     config, kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mAutoConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_unused_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs_copy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mauto_map:\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m trust_remote_code:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/auto/configuration_auto.py:917\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    915\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict:\n\u001b[0;32m--> 917\u001b[0m     config_class \u001b[38;5;241m=\u001b[39m \u001b[43mCONFIG_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config_class\u001b[38;5;241m.\u001b[39mfrom_dict(config_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munused_kwargs)\n\u001b[1;32m    919\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    920\u001b[0m     \u001b[38;5;66;03m# Fallback: use pattern matching on the string.\u001b[39;00m\n\u001b[1;32m    921\u001b[0m     \u001b[38;5;66;03m# We go from longer names to shorter names to catch roberta before bert (for instance)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/auto/configuration_auto.py:623\u001b[0m, in \u001b[0;36m_LazyConfigMapping.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extra_content[key]\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mapping:\n\u001b[0;32m--> 623\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[1;32m    624\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mapping[key]\n\u001b[1;32m    625\u001b[0m module_name \u001b[38;5;241m=\u001b[39m model_type_to_module_name(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'modernbert'"
     ]
    }
   ],
   "source": [
    "# Code Block 1: Praise2112/ModernBERT-base-squad2-v0.2\n",
    "!pip install transformers==4.48.0 torch==2.5.1 datasets==2.20.0 tokenizers==0.21.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'modernbert'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoModelForQuestionAnswering, AutoTokenizer\n\u001b[1;32m      3\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPraise2112/ModernBERT-base-squad2-v0.2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForQuestionAnswering\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is dependency conflict?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:441\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m kwargs_copy\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    439\u001b[0m         _ \u001b[38;5;241m=\u001b[39m kwargs_copy\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 441\u001b[0m     config, kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mAutoConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_unused_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs_copy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mauto_map:\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m trust_remote_code:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/auto/configuration_auto.py:917\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    915\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict:\n\u001b[0;32m--> 917\u001b[0m     config_class \u001b[38;5;241m=\u001b[39m \u001b[43mCONFIG_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config_class\u001b[38;5;241m.\u001b[39mfrom_dict(config_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munused_kwargs)\n\u001b[1;32m    919\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    920\u001b[0m     \u001b[38;5;66;03m# Fallback: use pattern matching on the string.\u001b[39;00m\n\u001b[1;32m    921\u001b[0m     \u001b[38;5;66;03m# We go from longer names to shorter names to catch roberta before bert (for instance)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/auto/configuration_auto.py:623\u001b[0m, in \u001b[0;36m_LazyConfigMapping.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extra_content[key]\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mapping:\n\u001b[0;32m--> 623\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[1;32m    624\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mapping[key]\n\u001b[1;32m    625\u001b[0m module_name \u001b[38;5;241m=\u001b[39m model_type_to_module_name(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'modernbert'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n",
    "\n",
    "model_name = \"Praise2112/ModernBERT-base-squad2-v0.2\"\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "question = \"What is dependency conflict?\"\n",
    "context = \"Dependency conflicts occur when different packages require different versions of the same dependency.\"\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "print(outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting transformers==2.11.0\n",
      "  Downloading transformers-2.11.0-py3-none-any.whl.metadata (45 kB)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.5.0 (from versions: 1.7.1, 1.8.0, 1.8.1, 1.9.0, 1.9.1, 1.10.0, 1.10.1, 1.10.2, 1.11.0, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 2.0.0, 2.0.1, 2.1.0, 2.1.1, 2.1.2, 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.3.1, 2.4.0, 2.4.1, 2.5.0, 2.5.1, 2.6.0)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for torch==1.5.0\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65d6deb02e1042d788dfec3ea2703c6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/475 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/.local/lib/python3.9/site-packages/transformers/utils/import_utils.py\", line 1126, in _get_module\n",
      "    # Parse it and check the field \"sagemaker_distributed_dataparallel_enabled\".\n",
      "  File \"/usr/lib64/python3.9/importlib/__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1030, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 1007, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 984, in _find_and_load_unlocked\n",
      "ModuleNotFoundError: No module named 'transformers.models.ernie_m.configuration_ernie_m'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/.local/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3550, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_13607/713093529.py\", line 6, in <module>\n",
      "    model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
      "  File \"/home/ec2-user/.local/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py\", line 469, in from_pretrained\n",
      "    use_auth_token = hub_kwargs.pop(\"use_auth_token\", None)\n",
      "  File \"/home/ec2-user/.local/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py\", line 619, in keys\n",
      "    )\n",
      "  File \"/home/ec2-user/.local/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py\", line 620, in <listcomp>\n",
      "    return super().from_config(config, **kwargs)\n",
      "  File \"/home/ec2-user/.local/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py\", line 616, in _load_attr_from_module\n",
      "    features_only=features_only,\n",
      "  File \"/home/ec2-user/.local/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py\", line 561, in getattribute_from_module\n",
      "    )\n",
      "  File \"/home/ec2-user/.local/lib/python3.9/site-packages/transformers/utils/import_utils.py\", line 1116, in __getattr__\n",
      "  File \"/home/ec2-user/.local/lib/python3.9/site-packages/transformers/utils/import_utils.py\", line 1128, in _get_module\n",
      "    if not sagemaker_params.get(\"sagemaker_distributed_dataparallel_enabled\", False):\n",
      "RuntimeError: Failed to import transformers.models.ernie_m.configuration_ernie_m because of the following error (look up to see its traceback):\n",
      "No module named 'transformers.models.ernie_m.configuration_ernie_m'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/.local/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2144, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/home/ec2-user/.local/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/home/ec2-user/.local/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/home/ec2-user/.local/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1173, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/home/ec2-user/.local/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1088, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "  File \"/home/ec2-user/.local/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 970, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "  File \"/home/ec2-user/.local/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 792, in lines\n",
      "    return self._sd.lines\n",
      "  File \"/home/ec2-user/.local/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/ec2-user/.local/lib/python3.9/site-packages/stack_data/core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"/home/ec2-user/.local/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/ec2-user/.local/lib/python3.9/site-packages/stack_data/core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"/home/ec2-user/.local/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/ec2-user/.local/lib/python3.9/site-packages/stack_data/core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "  File \"/home/ec2-user/.local/lib/python3.9/site-packages/executing/executing.py\", line 116, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==2.11.0 torch==1.5.0 tensorflow==2.2.0\n",
    "\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n",
    "\n",
    "model_name = \"ahotrod/electra_large_discriminator_squad2_512\"\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "question = \"Explain version conflicts in dependency management.\"\n",
    "context = \"Version conflicts can cause errors during package installation or runtime execution.\"\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "print(outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting transformers==4.7.0\n",
      "  Downloading transformers-4.7.0-py3-none-any.whl.metadata (48 kB)\n",
      "Collecting torch==1.8.0\n",
      "  Using cached torch-1.8.0-cp39-cp39-manylinux1_x86_64.whl.metadata (23 kB)\n",
      "Collecting datasets==1.4.1\n",
      "  Downloading datasets-1.4.1-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting tokenizers==0.10.2\n",
      "  Downloading tokenizers-0.10.2-cp39-cp39-manylinux2010_x86_64.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/.local/lib/python3.9/site-packages (from transformers==4.7.0) (3.16.1)\n",
      "Collecting huggingface-hub==0.0.8 (from transformers==4.7.0)\n",
      "  Downloading huggingface_hub-0.0.8-py3-none-any.whl.metadata (8.7 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/.local/lib/python3.9/site-packages (from transformers==4.7.0) (1.26.3)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/.local/lib/python3.9/site-packages (from transformers==4.7.0) (24.1)\n",
      "Requirement already satisfied: pyyaml in /home/ec2-user/.local/lib/python3.9/site-packages (from transformers==4.7.0) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/.local/lib/python3.9/site-packages (from transformers==4.7.0) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/ec2-user/.local/lib/python3.9/site-packages (from transformers==4.7.0) (2.32.3)\n",
      "Collecting sacremoses (from transformers==4.7.0)\n",
      "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/.local/lib/python3.9/site-packages (from transformers==4.7.0) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions in /home/ec2-user/.local/lib/python3.9/site-packages (from torch==1.8.0) (4.12.2)\n",
      "Requirement already satisfied: pyarrow>=0.17.1 in /home/ec2-user/.local/lib/python3.9/site-packages (from datasets==1.4.1) (19.0.1)\n",
      "Requirement already satisfied: dill in /home/ec2-user/.local/lib/python3.9/site-packages (from datasets==1.4.1) (0.3.7)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/.local/lib/python3.9/site-packages (from datasets==1.4.1) (2.2.2)\n",
      "Collecting tqdm>=4.27 (from transformers==4.7.0)\n",
      "  Downloading tqdm-4.49.0-py2.py3-none-any.whl.metadata (55 kB)\n",
      "Requirement already satisfied: xxhash in /home/ec2-user/.local/lib/python3.9/site-packages (from datasets==1.4.1) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /home/ec2-user/.local/lib/python3.9/site-packages (from datasets==1.4.1) (0.70.15)\n",
      "Requirement already satisfied: fsspec in /home/ec2-user/.local/lib/python3.9/site-packages (from datasets==1.4.1) (2023.12.2)\n",
      "INFO: pip is looking at multiple versions of datasets to determine which version is compatible with other requirements. This could take a while.\n",
      "\u001b[31mERROR: Cannot install datasets==1.4.1 and transformers==4.7.0 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "The conflict is caused by:\n",
      "    transformers 4.7.0 depends on huggingface-hub==0.0.8\n",
      "    datasets 1.4.1 depends on huggingface-hub==0.0.2\n",
      "\n",
      "To fix this you could try to:\n",
      "1. loosen the range of package versions you've specified\n",
      "2. remove package versions to allow pip to attempt to solve the dependency conflict\n",
      "\n",
      "\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d4002016fca4717856fae8d00859058",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/642 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/.local/lib/python3.9/site-packages/transformers/utils/import_utils.py\", line 1126, in _get_module\n",
      "    # Parse it and check the field \"sagemaker_distributed_dataparallel_enabled\".\n",
      "  File \"/usr/lib64/python3.9/importlib/__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1030, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 1007, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 984, in _find_and_load_unlocked\n",
      "ModuleNotFoundError: No module named 'transformers.models.ernie_m.configuration_ernie_m'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/.local/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3550, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_13607/328999849.py\", line 7, in <module>\n",
      "    model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
      "  File \"/home/ec2-user/.local/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py\", line 469, in from_pretrained\n",
      "    use_auth_token = hub_kwargs.pop(\"use_auth_token\", None)\n",
      "  File \"/home/ec2-user/.local/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py\", line 619, in keys\n",
      "    )\n",
      "  File \"/home/ec2-user/.local/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py\", line 620, in <listcomp>\n",
      "    return super().from_config(config, **kwargs)\n",
      "  File \"/home/ec2-user/.local/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py\", line 616, in _load_attr_from_module\n",
      "    features_only=features_only,\n",
      "  File \"/home/ec2-user/.local/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py\", line 561, in getattribute_from_module\n",
      "    )\n",
      "  File \"/home/ec2-user/.local/lib/python3.9/site-packages/transformers/utils/import_utils.py\", line 1116, in __getattr__\n",
      "  File \"/home/ec2-user/.local/lib/python3.9/site-packages/transformers/utils/import_utils.py\", line 1128, in _get_module\n",
      "    if not sagemaker_params.get(\"sagemaker_distributed_dataparallel_enabled\", False):\n",
      "RuntimeError: Failed to import transformers.models.ernie_m.configuration_ernie_m because of the following error (look up to see its traceback):\n",
      "No module named 'transformers.models.ernie_m.configuration_ernie_m'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/.local/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2144, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/home/ec2-user/.local/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/home/ec2-user/.local/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/home/ec2-user/.local/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1173, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/home/ec2-user/.local/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1088, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "  File \"/home/ec2-user/.local/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 970, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "  File \"/home/ec2-user/.local/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 792, in lines\n",
      "    return self._sd.lines\n",
      "  File \"/home/ec2-user/.local/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/ec2-user/.local/lib/python3.9/site-packages/stack_data/core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"/home/ec2-user/.local/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/ec2-user/.local/lib/python3.9/site-packages/stack_data/core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"/home/ec2-user/.local/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/ec2-user/.local/lib/python3.9/site-packages/stack_data/core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "  File \"/home/ec2-user/.local/lib/python3.9/site-packages/executing/executing.py\", line 116, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "# Code Block 3: bigwiz83/sapbert-from-pubmedbert-squad2\n",
    "!pip install transformers==4.7.0 torch==1.8.0 datasets==1.4.1 tokenizers==0.10.2\n",
    "\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n",
    "\n",
    "model_name = \"bigwiz83/sapbert-from-pubmedbert-squad2\"\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "question = \"What issues arise from dependency conflicts?\"\n",
    "context = \"When multiple libraries require conflicting versions, errors are likely to happen.\"\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "print(outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting transformers==4.12.5\n",
      "  Downloading transformers-4.12.5-py3-none-any.whl.metadata (56 kB)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.10.0+cu111 (from versions: 1.7.1, 1.8.0, 1.8.1, 1.9.0, 1.9.1, 1.10.0, 1.10.1, 1.10.2, 1.11.0, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 2.0.0, 2.0.1, 2.1.0, 2.1.1, 2.1.2, 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.3.1, 2.4.0, 2.4.1, 2.5.0, 2.5.1, 2.6.0)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for torch==1.10.0+cu111\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a81183da60834401a4a9f2fb6097f45f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.42k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/.local/lib/python3.9/site-packages/transformers/utils/import_utils.py\", line 1126, in _get_module\n",
      "    # Parse it and check the field \"sagemaker_distributed_dataparallel_enabled\".\n",
      "  File \"/usr/lib64/python3.9/importlib/__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1030, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 1007, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 984, in _find_and_load_unlocked\n",
      "ModuleNotFoundError: No module named 'transformers.models.ernie_m.configuration_ernie_m'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/.local/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3550, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_13607/1746457046.py\", line 7, in <module>\n",
      "    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
      "  File \"/home/ec2-user/.local/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py\", line 469, in from_pretrained\n",
      "    use_auth_token = hub_kwargs.pop(\"use_auth_token\", None)\n",
      "  File \"/home/ec2-user/.local/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py\", line 619, in keys\n",
      "    )\n",
      "  File \"/home/ec2-user/.local/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py\", line 620, in <listcomp>\n",
      "    return super().from_config(config, **kwargs)\n",
      "  File \"/home/ec2-user/.local/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py\", line 616, in _load_attr_from_module\n",
      "    features_only=features_only,\n",
      "  File \"/home/ec2-user/.local/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py\", line 561, in getattribute_from_module\n",
      "    )\n",
      "  File \"/home/ec2-user/.local/lib/python3.9/site-packages/transformers/utils/import_utils.py\", line 1116, in __getattr__\n",
      "  File \"/home/ec2-user/.local/lib/python3.9/site-packages/transformers/utils/import_utils.py\", line 1128, in _get_module\n",
      "    if not sagemaker_params.get(\"sagemaker_distributed_dataparallel_enabled\", False):\n",
      "RuntimeError: Failed to import transformers.models.ernie_m.configuration_ernie_m because of the following error (look up to see its traceback):\n",
      "No module named 'transformers.models.ernie_m.configuration_ernie_m'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/.local/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2144, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/home/ec2-user/.local/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/home/ec2-user/.local/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/home/ec2-user/.local/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1173, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/home/ec2-user/.local/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1088, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "  File \"/home/ec2-user/.local/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 970, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "  File \"/home/ec2-user/.local/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 792, in lines\n",
      "    return self._sd.lines\n",
      "  File \"/home/ec2-user/.local/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/ec2-user/.local/lib/python3.9/site-packages/stack_data/core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"/home/ec2-user/.local/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/ec2-user/.local/lib/python3.9/site-packages/stack_data/core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"/home/ec2-user/.local/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/ec2-user/.local/lib/python3.9/site-packages/stack_data/core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "  File \"/home/ec2-user/.local/lib/python3.9/site-packages/executing/executing.py\", line 116, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "# Code Block 4: papluca/xlm-roberta-base-language-detection\n",
    "!pip install transformers==4.12.5 torch==1.10.0 datasets==1.15.1 tokenizers==0.10.3\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "model_name = \"papluca/xlm-roberta-base-language-detection\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "text = \"This is a test sentence.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "print(outputs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
