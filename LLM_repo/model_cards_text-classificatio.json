[
  {
    "model_id": "cross-encoder/ms-marco-MiniLM-L6-v2",
    "model_name": "cross-encoder/ms-marco-MiniLM-L6-v2",
    "author": "cross-encoder",
    "downloads": 10546456,
    "likes": 77,
    "tags": [
      "transformers",
      "pytorch",
      "jax",
      "safetensors",
      "bert",
      "text-classification",
      "license:apache-2.0",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/cross-encoder/ms-marco-MiniLM-L6-v2",
    "dependencies": [
      [
        "sentence_transformers",
        null
      ],
      [
        "transformers",
        null
      ],
      [
        "torch",
        null
      ],
      [
        "sentence-transformers",
        "2.2.2"
      ]
    ],
    "analysis_date": "2025-03-26T00:47:09.948770",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "bert",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "license": "apache-2.0"
    },
    "card_text": "# Cross-Encoder for MS Marco\n\nThis model was trained on the [MS Marco Passage Ranking](https://github.com/microsoft/MSMARCO-Passage-Ranking) task.\n\nThe model can be used for Information Retrieval: Given a query, encode the query will all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. See [SBERT.net Retrieve & Re-rank](https://www.sbert.net/examples/applications/retrieve_rerank/README.html) for more details. The training code is available here: [SBERT.net Training MS Marco](https://github.com/UKPLab/sentence-transformers/tree/master/examples/training/ms_marco)\n\n\n## Usage with SentenceTransformers\n\nThe usage is easy when you have [SentenceTransformers](https://www.sbert.net/) installed. Then you can use the pre-trained models like this:\n```python\nfrom sentence_transformers import CrossEncoder\n\nmodel = CrossEncoder('cross-encoder/ms-marco-MiniLM-L6-v2')\nscores = model.predict([\n    (\"How many people live in Berlin?\", \"Berlin had a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\"),\n    (\"How many people live in Berlin?\", \"Berlin is well known for its museums.\"),\n])\nprint(scores)\n# [ 8.607138 -4.320078]\n```\n\n\n## Usage with Transformers\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\n\nmodel = AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-MiniLM-L6-v2')\ntokenizer = AutoTokenizer.from_pretrained('cross-encoder/ms-marco-MiniLM-L6-v2')\n\nfeatures = tokenizer(['How many people live in Berlin?', 'How many people live in Berlin?'], ['Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.', 'New York City is famous for the Metropolitan Museum of Art.'],  padding=True, truncation=True, return_tensors=\"pt\")\n\nmodel.eval()\nwith torch.no_grad():\n    scores = model(**features).logits\n    print(scores)\n```\n\n\n## Performance\nIn the following table, we provide various pre-trained Cross-Encoders together with their performance on the [TREC Deep Learning 2019](https://microsoft.github.io/TREC-2019-Deep-Learning/) and the [MS Marco Passage Reranking](https://github.com/microsoft/MSMARCO-Passage-Ranking/) dataset. \n\n\n| Model-Name        | NDCG@10 (TREC DL 19) | MRR@10 (MS Marco Dev)  | Docs / Sec |\n| ------------- |:-------------| -----| --- | \n| **Version 2 models** | | | \n| cross-encoder/ms-marco-TinyBERT-L2-v2 | 69.84 | 32.56 | 9000\n| cross-encoder/ms-marco-MiniLM-L2-v2 | 71.01 | 34.85 | 4100\n| cross-encoder/ms-marco-MiniLM-L4-v2 | 73.04 | 37.70 | 2500\n| cross-encoder/ms-marco-MiniLM-L6-v2 | 74.30 | 39.01 | 1800\n| cross-encoder/ms-marco-MiniLM-L12-v2 | 74.31 | 39.02 | 960\n| **Version 1 models** | | | \n| cross-encoder/ms-marco-TinyBERT-L2  | 67.43 | 30.15  | 9000\n| cross-encoder/ms-marco-TinyBERT-L4  | 68.09 | 34.50  | 2900\n| cross-encoder/ms-marco-TinyBERT-L6 |  69.57 | 36.13  | 680\n| cross-encoder/ms-marco-electra-base | 71.99 | 36.41 | 340\n| **Other models** | | | \n| nboost/pt-tinybert-msmarco | 63.63 | 28.80 | 2900 \n| nboost/pt-bert-base-uncased-msmarco | 70.94 | 34.75 | 340 \n| nboost/pt-bert-large-msmarco | 73.36 | 36.48 | 100 \n| Capreolus/electra-base-msmarco | 71.23 | 36.89 | 340 \n| amberoad/bert-multilingual-passage-reranking-msmarco | 68.40 | 35.54 | 330 \n| sebastian-hofstaetter/distilbert-cat-margin_mse-T2-msmarco | 72.82 | 37.88 | 720\n \n Note: Runtime was computed on a V100 GPU.\n",
    "card_content": "---\nlicense: apache-2.0\n---\n# Cross-Encoder for MS Marco\n\nThis model was trained on the [MS Marco Passage Ranking](https://github.com/microsoft/MSMARCO-Passage-Ranking) task.\n\nThe model can be used for Information Retrieval: Given a query, encode the query will all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. See [SBERT.net Retrieve & Re-rank](https://www.sbert.net/examples/applications/retrieve_rerank/README.html) for more details. The training code is available here: [SBERT.net Training MS Marco](https://github.com/UKPLab/sentence-transformers/tree/master/examples/training/ms_marco)\n\n\n## Usage with SentenceTransformers\n\nThe usage is easy when you have [SentenceTransformers](https://www.sbert.net/) installed. Then you can use the pre-trained models like this:\n```python\nfrom sentence_transformers import CrossEncoder\n\nmodel = CrossEncoder('cross-encoder/ms-marco-MiniLM-L6-v2')\nscores = model.predict([\n    (\"How many people live in Berlin?\", \"Berlin had a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\"),\n    (\"How many people live in Berlin?\", \"Berlin is well known for its museums.\"),\n])\nprint(scores)\n# [ 8.607138 -4.320078]\n```\n\n\n## Usage with Transformers\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\n\nmodel = AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-MiniLM-L6-v2')\ntokenizer = AutoTokenizer.from_pretrained('cross-encoder/ms-marco-MiniLM-L6-v2')\n\nfeatures = tokenizer(['How many people live in Berlin?', 'How many people live in Berlin?'], ['Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.', 'New York City is famous for the Metropolitan Museum of Art.'],  padding=True, truncation=True, return_tensors=\"pt\")\n\nmodel.eval()\nwith torch.no_grad():\n    scores = model(**features).logits\n    print(scores)\n```\n\n\n## Performance\nIn the following table, we provide various pre-trained Cross-Encoders together with their performance on the [TREC Deep Learning 2019](https://microsoft.github.io/TREC-2019-Deep-Learning/) and the [MS Marco Passage Reranking](https://github.com/microsoft/MSMARCO-Passage-Ranking/) dataset. \n\n\n| Model-Name        | NDCG@10 (TREC DL 19) | MRR@10 (MS Marco Dev)  | Docs / Sec |\n| ------------- |:-------------| -----| --- | \n| **Version 2 models** | | | \n| cross-encoder/ms-marco-TinyBERT-L2-v2 | 69.84 | 32.56 | 9000\n| cross-encoder/ms-marco-MiniLM-L2-v2 | 71.01 | 34.85 | 4100\n| cross-encoder/ms-marco-MiniLM-L4-v2 | 73.04 | 37.70 | 2500\n| cross-encoder/ms-marco-MiniLM-L6-v2 | 74.30 | 39.01 | 1800\n| cross-encoder/ms-marco-MiniLM-L12-v2 | 74.31 | 39.02 | 960\n| **Version 1 models** | | | \n| cross-encoder/ms-marco-TinyBERT-L2  | 67.43 | 30.15  | 9000\n| cross-encoder/ms-marco-TinyBERT-L4  | 68.09 | 34.50  | 2900\n| cross-encoder/ms-marco-TinyBERT-L6 |  69.57 | 36.13  | 680\n| cross-encoder/ms-marco-electra-base | 71.99 | 36.41 | 340\n| **Other models** | | | \n| nboost/pt-tinybert-msmarco | 63.63 | 28.80 | 2900 \n| nboost/pt-bert-base-uncased-msmarco | 70.94 | 34.75 | 340 \n| nboost/pt-bert-large-msmarco | 73.36 | 36.48 | 100 \n| Capreolus/electra-base-msmarco | 71.23 | 36.89 | 340 \n| amberoad/bert-multilingual-passage-reranking-msmarco | 68.40 | 35.54 | 330 \n| sebastian-hofstaetter/distilbert-cat-margin_mse-T2-msmarco | 72.82 | 37.88 | 720\n \n Note: Runtime was computed on a V100 GPU.\n",
    "library_name": "transformers"
  },
  {
    "model_id": "distilbert/distilbert-base-uncased-finetuned-sst-2-english",
    "model_name": "distilbert/distilbert-base-uncased-finetuned-sst-2-english",
    "author": "distilbert",
    "downloads": 6808386,
    "likes": 718,
    "tags": [
      "transformers",
      "pytorch",
      "tf",
      "rust",
      "onnx",
      "safetensors",
      "distilbert",
      "text-classification",
      "en",
      "dataset:sst2",
      "dataset:glue",
      "arxiv:1910.01108",
      "doi:10.57967/hf/0181",
      "license:apache-2.0",
      "model-index",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english",
    "dependencies": [
      [
        "torch",
        "2.0.1"
      ],
      [
        "transformers",
        "4.30.2"
      ],
      [
        "numpy",
        "1.24.3"
      ],
      [
        "xformers",
        "0.0.22"
      ],
      [
        "hf_xet",
        "0.1.0"
      ]
    ],
    "analysis_date": "2025-03-26T00:47:11.590856",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "distilbert",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": "en",
      "license": "apache-2.0",
      "datasets": [
        "sst2",
        "glue"
      ],
      "model-index": [
        {
          "name": "distilbert-base-uncased-finetuned-sst-2-english",
          "results": [
            {
              "task": {
                "type": "text-classification",
                "name": "Text Classification"
              },
              "dataset": {
                "name": "glue",
                "type": "glue",
                "config": "sst2",
                "split": "validation"
              },
              "metrics": [
                {
                  "type": "accuracy",
                  "value": 0.9105504587155964,
                  "name": "Accuracy",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiN2YyOGMxYjY2Y2JhMjkxNjIzN2FmMjNiNmM2ZWViNGY3MTNmNWI2YzhiYjYxZTY0ZGUyN2M1NGIxZjRiMjQwZiIsInZlcnNpb24iOjF9.uui0srxV5ZHRhxbYN6082EZdwpnBgubPJ5R2-Wk8HTWqmxYE3QHidevR9LLAhidqGw6Ih93fK0goAXncld_gBg"
                },
                {
                  "type": "precision",
                  "value": 0.8978260869565218,
                  "name": "Precision",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMzgwYTYwYjA2MmM0ZTYwNDk0M2NmNTBkZmM2NGNhYzQ1OGEyN2NkNDQ3Mzc2NTQyMmZiNDJiNzBhNGVhZGUyOSIsInZlcnNpb24iOjF9.eHjLmw3K02OU69R2Au8eyuSqT3aBDHgZCn8jSzE3_urD6EUSSsLxUpiAYR4BGLD_U6-ZKcdxVo_A2rdXqvUJDA"
                },
                {
                  "type": "recall",
                  "value": 0.9301801801801802,
                  "name": "Recall",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMGIzM2E3MTI2Mzc2MDYwNmU3ZTVjYmZmZDBkNjY4ZTc5MGY0Y2FkNDU3NjY1MmVkNmE3Y2QzMzAwZDZhOWY1NiIsInZlcnNpb24iOjF9.PUZlqmct13-rJWBXdHm5tdkXgETL9F82GNbbSR4hI8MB-v39KrK59cqzFC2Ac7kJe_DtOeUyosj34O_mFt_1DQ"
                },
                {
                  "type": "auc",
                  "value": 0.9716626673402374,
                  "name": "AUC",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMDM0YWIwZmQ4YjUwOGZmMWU2MjI1YjIxZGQ2MzNjMzRmZmYxMzZkNGFjODhlMDcyZDM1Y2RkMWZlOWQ0MWYwNSIsInZlcnNpb24iOjF9.E7GRlAXmmpEkTHlXheVkuL1W4WNjv4JO3qY_WCVsTVKiO7bUu0UVjPIyQ6g-J1OxsfqZmW3Leli1wY8vPBNNCQ"
                },
                {
                  "type": "f1",
                  "value": 0.9137168141592922,
                  "name": "F1",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMGU4MjNmOGYwZjZjMDQ1ZTkyZTA4YTc1MWYwOTM0NDM4ZWY1ZGVkNDY5MzNhYTQyZGFlNzIyZmUwMDg3NDU0NyIsInZlcnNpb24iOjF9.mW5ftkq50Se58M-jm6a2Pu93QeKa3MfV7xcBwvG3PSB_KNJxZWTCpfMQp-Cmx_EMlmI2siKOyd8akYjJUrzJCA"
                },
                {
                  "type": "loss",
                  "value": 0.39013850688934326,
                  "name": "loss",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMTZiNzAyZDc0MzUzMmE1MGJiN2JlYzFiODE5ZTNlNGE4MmI4YzRiMTc2ODEzMTUwZmEzOTgxNzc4YjJjZTRmNiIsInZlcnNpb24iOjF9.VqIC7uYC-ZZ8ss9zQOlRV39YVOOLc5R36sIzCcVz8lolh61ux_5djm2XjpP6ARc6KqEnXC4ZtfNXsX2HZfrtCQ"
                }
              ]
            },
            {
              "task": {
                "type": "text-classification",
                "name": "Text Classification"
              },
              "dataset": {
                "name": "sst2",
                "type": "sst2",
                "config": "default",
                "split": "train"
              },
              "metrics": [
                {
                  "type": "accuracy",
                  "value": 0.9885521685548412,
                  "name": "Accuracy",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiY2I3NzU3YzhmMDkxZTViY2M3OTY1NmI0ZTdmMDQxNjNjYzJiZmQxNzczM2E4YmExYTY5ODY0NDBkY2I4ZjNkOCIsInZlcnNpb24iOjF9.4Gtk3FeVc9sPWSqZIaeUXJ9oVlPzm-NmujnWpK2y5s1Vhp1l6Y1pK5_78wW0-NxSvQqV6qd5KQf_OAEpVAkQDA"
                },
                {
                  "type": "precision",
                  "value": 0.9881965062029833,
                  "name": "Precision Macro",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZDdlZDMzY2I3MTAwYTljNmM4MGMyMzU2YjAzZDg1NDYwN2ZmM2Y5OWZhMjUyMGJiNjY1YmZiMzFhMDI2ODFhNyIsInZlcnNpb24iOjF9.cqmv6yBxu4St2mykRWrZ07tDsiSLdtLTz2hbqQ7Gm1rMzq9tdlkZ8MyJRxtME_Y8UaOG9rs68pV-gKVUs8wABw"
                },
                {
                  "type": "precision",
                  "value": 0.9885521685548412,
                  "name": "Precision Micro",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZjFlYzAzNmE1YjljNjUwNzBjZjEzZDY0ZDQyMmY5ZWM2OTBhNzNjYjYzYTk1YWE1NjU3YTMxZDQwOTE1Y2FkNyIsInZlcnNpb24iOjF9.jnCHOkUHuAOZZ_ZMVOnetx__OVJCS6LOno4caWECAmfrUaIPnPNV9iJ6izRO3sqkHRmxYpWBb-27GJ4N3LU-BQ"
                },
                {
                  "type": "precision",
                  "value": 0.9885639626373408,
                  "name": "Precision Weighted",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZGUyODFjNjBlNTE2MTY3ZDAxOGU1N2U0YjUyY2NiZjhkOGVmYThjYjBkNGU3NTRkYzkzNDQ2MmMwMjkwMWNiMyIsInZlcnNpb24iOjF9.zTNabMwApiZyXdr76QUn7WgGB7D7lP-iqS3bn35piqVTNsv3wnKjZOaKFVLIUvtBXq4gKw7N2oWxvWc4OcSNDg"
                },
                {
                  "type": "recall",
                  "value": 0.9886145346602994,
                  "name": "Recall Macro",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiNTU1YjlhODU3YTkyNTdiZDcwZGFlZDBiYjY0N2NjMGM2NTRiNjQ3MDNjNGMxOWY2ZGQ4NWU1YmMzY2UwZTI3YSIsInZlcnNpb24iOjF9.xaLPY7U-wHsJ3DDui1yyyM-xWjL0Jz5puRThy7fczal9x05eKEQ9s0a_WD-iLmapvJs0caXpV70hDe2NLcs-DA"
                },
                {
                  "type": "recall",
                  "value": 0.9885521685548412,
                  "name": "Recall Micro",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiODE0YTU0MDBlOGY4YzU0MjY5MzA3OTk2OGNhOGVkMmU5OGRjZmFiZWI2ZjY5ODEzZTQzMTI0N2NiOTVkNDliYiIsInZlcnNpb24iOjF9.SOt1baTBbuZRrsvGcak2sUwoTrQzmNCbyV2m1_yjGsU48SBH0NcKXicidNBSnJ6ihM5jf_Lv_B5_eOBkLfNWDQ"
                },
                {
                  "type": "recall",
                  "value": 0.9885521685548412,
                  "name": "Recall Weighted",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZWNkNmM0ZGRlNmYxYzIwNDk4OTI5MzIwZWU1NzZjZDVhMDcyNDFlMjBhNDQxODU5OWMwMWNhNGEzNjY3ZGUyOSIsInZlcnNpb24iOjF9.b15Fh70GwtlG3cSqPW-8VEZT2oy0CtgvgEOtWiYonOovjkIQ4RSLFVzVG-YfslaIyfg9RzMWzjhLnMY7Bpn2Aw"
                },
                {
                  "type": "f1",
                  "value": 0.9884019815052447,
                  "name": "F1 Macro",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiYmM4NjQ5Yjk5ODRhYTU1MTY3MmRhZDBmODM1NTg3OTFiNWM4NDRmYjI0MzZkNmQ1MzE3MzcxODZlYzBkYTMyYSIsInZlcnNpb24iOjF9.74RaDK8nBVuGRl2Se_-hwQvP6c4lvVxGHpcCWB4uZUCf2_HoC9NT9u7P3pMJfH_tK2cpV7U3VWGgSDhQDi-UBQ"
                },
                {
                  "type": "f1",
                  "value": 0.9885521685548412,
                  "name": "F1 Micro",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZDRmYWRmMmQ0YjViZmQxMzhhYTUyOTE1MTc0ZDU1ZjQyZjFhMDYzYzMzZDE0NzZlYzQyOTBhMTBhNmM5NTlkMiIsInZlcnNpb24iOjF9.VMn_psdAHIZTlW6GbjERZDe8MHhwzJ0rbjV_VJyuMrsdOh5QDmko-wEvaBWNEdT0cEKsbggm-6jd3Gh81PfHAQ"
                },
                {
                  "type": "f1",
                  "value": 0.9885546181087554,
                  "name": "F1 Weighted",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMjUyZWFhZDZhMGQ3MzBmYmRiNDVmN2FkZDBjMjk3ODk0OTAxNGZkMWE0NzU5ZjI0NzE0NGZiNzM0N2Y2NDYyOSIsInZlcnNpb24iOjF9.YsXBhnzEEFEW6jw3mQlFUuIrW7Gabad2Ils-iunYJr-myg0heF8NEnEWABKFE1SnvCWt-69jkLza6SupeyLVCA"
                },
                {
                  "type": "loss",
                  "value": 0.040652573108673096,
                  "name": "loss",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZTc3YjU3MjdjMzkxODA5MjU5NGUyY2NkMGVhZDg3ZWEzMmU1YWVjMmI0NmU2OWEyZTkzMTVjNDZiYTc0YjIyNCIsInZlcnNpb24iOjF9.lA90qXZVYiILHMFlr6t6H81Oe8a-4KmeX-vyCC1BDia2ofudegv6Vb46-4RzmbtuKeV6yy6YNNXxXxqVak1pAg"
                }
              ]
            }
          ]
        }
      ]
    },
    "card_text": "\n# DistilBERT base uncased finetuned SST-2\n\n## Table of Contents\n- [Model Details](#model-details)\n- [How to Get Started With the Model](#how-to-get-started-with-the-model)\n- [Uses](#uses)\n- [Risks, Limitations and Biases](#risks-limitations-and-biases)\n- [Training](#training)\n\n## Model Details\n**Model Description:** This model is a fine-tune checkpoint of [DistilBERT-base-uncased](https://huggingface.co/distilbert-base-uncased), fine-tuned on SST-2.\nThis model reaches an accuracy of 91.3 on the dev set (for comparison, Bert bert-base-uncased version reaches an accuracy of 92.7).\n- **Developed by:** Hugging Face\n- **Model Type:** Text Classification\n- **Language(s):** English\n- **License:** Apache-2.0\n- **Parent Model:** For more details about DistilBERT, we encourage users to check out [this model card](https://huggingface.co/distilbert-base-uncased).\n- **Resources for more information:**\n    - [Model Documentation](https://huggingface.co/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertForSequenceClassification)\n    - [DistilBERT paper](https://arxiv.org/abs/1910.01108)\n\n## How to Get Started With the Model\n\nExample of single-label classification:\n\u200b\u200b\n```python\nimport torch\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n\ntokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\nmodel = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n\ninputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\nwith torch.no_grad():\n    logits = model(**inputs).logits\n\npredicted_class_id = logits.argmax().item()\nmodel.config.id2label[predicted_class_id]\n\n```\n\n## Uses\n\n#### Direct Use\n\nThis model can be used for  topic classification. You can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to be fine-tuned on a downstream task. See the model hub to look for fine-tuned versions on a task that interests you.\n\n#### Misuse and Out-of-scope Use\nThe model should not be used to intentionally create hostile or alienating environments for people. In addition, the model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model.\n\n\n## Risks, Limitations and Biases\n\nBased on a few experimentations, we observed that this model could produce biased predictions that target underrepresented populations.\n\nFor instance, for sentences like `This film was filmed in COUNTRY`, this binary classification model will give radically different probabilities for the positive label depending on the country (0.89 if the country is France, but 0.08 if the country is Afghanistan) when nothing in the input indicates such a strong semantic shift. In this [colab](https://colab.research.google.com/gist/ageron/fb2f64fb145b4bc7c49efc97e5f114d3/biasmap.ipynb), [Aur\u00e9lien G\u00e9ron](https://twitter.com/aureliengeron) made an interesting map plotting these probabilities for each country.\n\n<img src=\"https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english/resolve/main/map.jpeg\" alt=\"Map of positive probabilities per country.\" width=\"500\"/>\n\nWe strongly advise users to thoroughly probe these aspects on their use-cases in order to evaluate the risks of this model. We recommend looking at the following bias evaluation datasets as a place to start: [WinoBias](https://huggingface.co/datasets/wino_bias), [WinoGender](https://huggingface.co/datasets/super_glue), [Stereoset](https://huggingface.co/datasets/stereoset).\n\n\n\n# Training\n\n\n#### Training Data\n\n\nThe authors use the following Stanford Sentiment Treebank([sst2](https://huggingface.co/datasets/sst2)) corpora for the model.\n\n#### Training Procedure\n\n###### Fine-tuning hyper-parameters\n\n\n- learning_rate = 1e-5\n- batch_size = 32\n- warmup = 600\n- max_seq_length = 128\n- num_train_epochs = 3.0\n\n\n",
    "card_content": "---\nlanguage: en\nlicense: apache-2.0\ndatasets:\n- sst2\n- glue\nmodel-index:\n- name: distilbert-base-uncased-finetuned-sst-2-english\n  results:\n  - task:\n      type: text-classification\n      name: Text Classification\n    dataset:\n      name: glue\n      type: glue\n      config: sst2\n      split: validation\n    metrics:\n    - type: accuracy\n      value: 0.9105504587155964\n      name: Accuracy\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiN2YyOGMxYjY2Y2JhMjkxNjIzN2FmMjNiNmM2ZWViNGY3MTNmNWI2YzhiYjYxZTY0ZGUyN2M1NGIxZjRiMjQwZiIsInZlcnNpb24iOjF9.uui0srxV5ZHRhxbYN6082EZdwpnBgubPJ5R2-Wk8HTWqmxYE3QHidevR9LLAhidqGw6Ih93fK0goAXncld_gBg\n    - type: precision\n      value: 0.8978260869565218\n      name: Precision\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMzgwYTYwYjA2MmM0ZTYwNDk0M2NmNTBkZmM2NGNhYzQ1OGEyN2NkNDQ3Mzc2NTQyMmZiNDJiNzBhNGVhZGUyOSIsInZlcnNpb24iOjF9.eHjLmw3K02OU69R2Au8eyuSqT3aBDHgZCn8jSzE3_urD6EUSSsLxUpiAYR4BGLD_U6-ZKcdxVo_A2rdXqvUJDA\n    - type: recall\n      value: 0.9301801801801802\n      name: Recall\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMGIzM2E3MTI2Mzc2MDYwNmU3ZTVjYmZmZDBkNjY4ZTc5MGY0Y2FkNDU3NjY1MmVkNmE3Y2QzMzAwZDZhOWY1NiIsInZlcnNpb24iOjF9.PUZlqmct13-rJWBXdHm5tdkXgETL9F82GNbbSR4hI8MB-v39KrK59cqzFC2Ac7kJe_DtOeUyosj34O_mFt_1DQ\n    - type: auc\n      value: 0.9716626673402374\n      name: AUC\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMDM0YWIwZmQ4YjUwOGZmMWU2MjI1YjIxZGQ2MzNjMzRmZmYxMzZkNGFjODhlMDcyZDM1Y2RkMWZlOWQ0MWYwNSIsInZlcnNpb24iOjF9.E7GRlAXmmpEkTHlXheVkuL1W4WNjv4JO3qY_WCVsTVKiO7bUu0UVjPIyQ6g-J1OxsfqZmW3Leli1wY8vPBNNCQ\n    - type: f1\n      value: 0.9137168141592922\n      name: F1\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMGU4MjNmOGYwZjZjMDQ1ZTkyZTA4YTc1MWYwOTM0NDM4ZWY1ZGVkNDY5MzNhYTQyZGFlNzIyZmUwMDg3NDU0NyIsInZlcnNpb24iOjF9.mW5ftkq50Se58M-jm6a2Pu93QeKa3MfV7xcBwvG3PSB_KNJxZWTCpfMQp-Cmx_EMlmI2siKOyd8akYjJUrzJCA\n    - type: loss\n      value: 0.39013850688934326\n      name: loss\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMTZiNzAyZDc0MzUzMmE1MGJiN2JlYzFiODE5ZTNlNGE4MmI4YzRiMTc2ODEzMTUwZmEzOTgxNzc4YjJjZTRmNiIsInZlcnNpb24iOjF9.VqIC7uYC-ZZ8ss9zQOlRV39YVOOLc5R36sIzCcVz8lolh61ux_5djm2XjpP6ARc6KqEnXC4ZtfNXsX2HZfrtCQ\n  - task:\n      type: text-classification\n      name: Text Classification\n    dataset:\n      name: sst2\n      type: sst2\n      config: default\n      split: train\n    metrics:\n    - type: accuracy\n      value: 0.9885521685548412\n      name: Accuracy\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiY2I3NzU3YzhmMDkxZTViY2M3OTY1NmI0ZTdmMDQxNjNjYzJiZmQxNzczM2E4YmExYTY5ODY0NDBkY2I4ZjNkOCIsInZlcnNpb24iOjF9.4Gtk3FeVc9sPWSqZIaeUXJ9oVlPzm-NmujnWpK2y5s1Vhp1l6Y1pK5_78wW0-NxSvQqV6qd5KQf_OAEpVAkQDA\n    - type: precision\n      value: 0.9881965062029833\n      name: Precision Macro\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZDdlZDMzY2I3MTAwYTljNmM4MGMyMzU2YjAzZDg1NDYwN2ZmM2Y5OWZhMjUyMGJiNjY1YmZiMzFhMDI2ODFhNyIsInZlcnNpb24iOjF9.cqmv6yBxu4St2mykRWrZ07tDsiSLdtLTz2hbqQ7Gm1rMzq9tdlkZ8MyJRxtME_Y8UaOG9rs68pV-gKVUs8wABw\n    - type: precision\n      value: 0.9885521685548412\n      name: Precision Micro\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZjFlYzAzNmE1YjljNjUwNzBjZjEzZDY0ZDQyMmY5ZWM2OTBhNzNjYjYzYTk1YWE1NjU3YTMxZDQwOTE1Y2FkNyIsInZlcnNpb24iOjF9.jnCHOkUHuAOZZ_ZMVOnetx__OVJCS6LOno4caWECAmfrUaIPnPNV9iJ6izRO3sqkHRmxYpWBb-27GJ4N3LU-BQ\n    - type: precision\n      value: 0.9885639626373408\n      name: Precision Weighted\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZGUyODFjNjBlNTE2MTY3ZDAxOGU1N2U0YjUyY2NiZjhkOGVmYThjYjBkNGU3NTRkYzkzNDQ2MmMwMjkwMWNiMyIsInZlcnNpb24iOjF9.zTNabMwApiZyXdr76QUn7WgGB7D7lP-iqS3bn35piqVTNsv3wnKjZOaKFVLIUvtBXq4gKw7N2oWxvWc4OcSNDg\n    - type: recall\n      value: 0.9886145346602994\n      name: Recall Macro\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiNTU1YjlhODU3YTkyNTdiZDcwZGFlZDBiYjY0N2NjMGM2NTRiNjQ3MDNjNGMxOWY2ZGQ4NWU1YmMzY2UwZTI3YSIsInZlcnNpb24iOjF9.xaLPY7U-wHsJ3DDui1yyyM-xWjL0Jz5puRThy7fczal9x05eKEQ9s0a_WD-iLmapvJs0caXpV70hDe2NLcs-DA\n    - type: recall\n      value: 0.9885521685548412\n      name: Recall Micro\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiODE0YTU0MDBlOGY4YzU0MjY5MzA3OTk2OGNhOGVkMmU5OGRjZmFiZWI2ZjY5ODEzZTQzMTI0N2NiOTVkNDliYiIsInZlcnNpb24iOjF9.SOt1baTBbuZRrsvGcak2sUwoTrQzmNCbyV2m1_yjGsU48SBH0NcKXicidNBSnJ6ihM5jf_Lv_B5_eOBkLfNWDQ\n    - type: recall\n      value: 0.9885521685548412\n      name: Recall Weighted\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZWNkNmM0ZGRlNmYxYzIwNDk4OTI5MzIwZWU1NzZjZDVhMDcyNDFlMjBhNDQxODU5OWMwMWNhNGEzNjY3ZGUyOSIsInZlcnNpb24iOjF9.b15Fh70GwtlG3cSqPW-8VEZT2oy0CtgvgEOtWiYonOovjkIQ4RSLFVzVG-YfslaIyfg9RzMWzjhLnMY7Bpn2Aw\n    - type: f1\n      value: 0.9884019815052447\n      name: F1 Macro\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiYmM4NjQ5Yjk5ODRhYTU1MTY3MmRhZDBmODM1NTg3OTFiNWM4NDRmYjI0MzZkNmQ1MzE3MzcxODZlYzBkYTMyYSIsInZlcnNpb24iOjF9.74RaDK8nBVuGRl2Se_-hwQvP6c4lvVxGHpcCWB4uZUCf2_HoC9NT9u7P3pMJfH_tK2cpV7U3VWGgSDhQDi-UBQ\n    - type: f1\n      value: 0.9885521685548412\n      name: F1 Micro\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZDRmYWRmMmQ0YjViZmQxMzhhYTUyOTE1MTc0ZDU1ZjQyZjFhMDYzYzMzZDE0NzZlYzQyOTBhMTBhNmM5NTlkMiIsInZlcnNpb24iOjF9.VMn_psdAHIZTlW6GbjERZDe8MHhwzJ0rbjV_VJyuMrsdOh5QDmko-wEvaBWNEdT0cEKsbggm-6jd3Gh81PfHAQ\n    - type: f1\n      value: 0.9885546181087554\n      name: F1 Weighted\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMjUyZWFhZDZhMGQ3MzBmYmRiNDVmN2FkZDBjMjk3ODk0OTAxNGZkMWE0NzU5ZjI0NzE0NGZiNzM0N2Y2NDYyOSIsInZlcnNpb24iOjF9.YsXBhnzEEFEW6jw3mQlFUuIrW7Gabad2Ils-iunYJr-myg0heF8NEnEWABKFE1SnvCWt-69jkLza6SupeyLVCA\n    - type: loss\n      value: 0.040652573108673096\n      name: loss\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZTc3YjU3MjdjMzkxODA5MjU5NGUyY2NkMGVhZDg3ZWEzMmU1YWVjMmI0NmU2OWEyZTkzMTVjNDZiYTc0YjIyNCIsInZlcnNpb24iOjF9.lA90qXZVYiILHMFlr6t6H81Oe8a-4KmeX-vyCC1BDia2ofudegv6Vb46-4RzmbtuKeV6yy6YNNXxXxqVak1pAg\n---\n\n# DistilBERT base uncased finetuned SST-2\n\n## Table of Contents\n- [Model Details](#model-details)\n- [How to Get Started With the Model](#how-to-get-started-with-the-model)\n- [Uses](#uses)\n- [Risks, Limitations and Biases](#risks-limitations-and-biases)\n- [Training](#training)\n\n## Model Details\n**Model Description:** This model is a fine-tune checkpoint of [DistilBERT-base-uncased](https://huggingface.co/distilbert-base-uncased), fine-tuned on SST-2.\nThis model reaches an accuracy of 91.3 on the dev set (for comparison, Bert bert-base-uncased version reaches an accuracy of 92.7).\n- **Developed by:** Hugging Face\n- **Model Type:** Text Classification\n- **Language(s):** English\n- **License:** Apache-2.0\n- **Parent Model:** For more details about DistilBERT, we encourage users to check out [this model card](https://huggingface.co/distilbert-base-uncased).\n- **Resources for more information:**\n    - [Model Documentation](https://huggingface.co/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertForSequenceClassification)\n    - [DistilBERT paper](https://arxiv.org/abs/1910.01108)\n\n## How to Get Started With the Model\n\nExample of single-label classification:\n\u200b\u200b\n```python\nimport torch\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n\ntokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\nmodel = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n\ninputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\nwith torch.no_grad():\n    logits = model(**inputs).logits\n\npredicted_class_id = logits.argmax().item()\nmodel.config.id2label[predicted_class_id]\n\n```\n\n## Uses\n\n#### Direct Use\n\nThis model can be used for  topic classification. You can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to be fine-tuned on a downstream task. See the model hub to look for fine-tuned versions on a task that interests you.\n\n#### Misuse and Out-of-scope Use\nThe model should not be used to intentionally create hostile or alienating environments for people. In addition, the model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model.\n\n\n## Risks, Limitations and Biases\n\nBased on a few experimentations, we observed that this model could produce biased predictions that target underrepresented populations.\n\nFor instance, for sentences like `This film was filmed in COUNTRY`, this binary classification model will give radically different probabilities for the positive label depending on the country (0.89 if the country is France, but 0.08 if the country is Afghanistan) when nothing in the input indicates such a strong semantic shift. In this [colab](https://colab.research.google.com/gist/ageron/fb2f64fb145b4bc7c49efc97e5f114d3/biasmap.ipynb), [Aur\u00e9lien G\u00e9ron](https://twitter.com/aureliengeron) made an interesting map plotting these probabilities for each country.\n\n<img src=\"https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english/resolve/main/map.jpeg\" alt=\"Map of positive probabilities per country.\" width=\"500\"/>\n\nWe strongly advise users to thoroughly probe these aspects on their use-cases in order to evaluate the risks of this model. We recommend looking at the following bias evaluation datasets as a place to start: [WinoBias](https://huggingface.co/datasets/wino_bias), [WinoGender](https://huggingface.co/datasets/super_glue), [Stereoset](https://huggingface.co/datasets/stereoset).\n\n\n\n# Training\n\n\n#### Training Data\n\n\nThe authors use the following Stanford Sentiment Treebank([sst2](https://huggingface.co/datasets/sst2)) corpora for the model.\n\n#### Training Procedure\n\n###### Fine-tuning hyper-parameters\n\n\n- learning_rate = 1e-5\n- batch_size = 32\n- warmup = 600\n- max_seq_length = 128\n- num_train_epochs = 3.0\n\n\n",
    "library_name": "transformers"
  },
  {
    "model_id": "papluca/xlm-roberta-base-language-detection",
    "model_name": "papluca/xlm-roberta-base-language-detection",
    "author": "papluca",
    "downloads": 4205869,
    "likes": 326,
    "tags": [
      "transformers",
      "pytorch",
      "tf",
      "safetensors",
      "xlm-roberta",
      "text-classification",
      "generated_from_trainer",
      "multilingual",
      "ar",
      "bg",
      "de",
      "el",
      "en",
      "es",
      "fr",
      "hi",
      "it",
      "ja",
      "nl",
      "pl",
      "pt",
      "ru",
      "sw",
      "th",
      "tr",
      "ur",
      "vi",
      "zh",
      "dataset:papluca/language-identification",
      "arxiv:1911.02116",
      "base_model:FacebookAI/xlm-roberta-base",
      "base_model:finetune:FacebookAI/xlm-roberta-base",
      "doi:10.57967/hf/2064",
      "license:mit",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/papluca/xlm-roberta-base-language-detection",
    "dependencies": [
      [
        "transformers",
        "4.12.5"
      ],
      [
        "torch",
        "1.10.0"
      ],
      [
        "datasets",
        "1.15.1"
      ],
      [
        "tokenizers",
        "0.10.3"
      ],
      [
        "numpy",
        "1.24.3"
      ]
    ],
    "analysis_date": "2025-03-26T00:47:13.179967",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "xlm-roberta",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": [
        "multilingual",
        "ar",
        "bg",
        "de",
        "el",
        "en",
        "es",
        "fr",
        "hi",
        "it",
        "ja",
        "nl",
        "pl",
        "pt",
        "ru",
        "sw",
        "th",
        "tr",
        "ur",
        "vi",
        "zh"
      ],
      "license": "mit",
      "tags": [
        "generated_from_trainer"
      ],
      "datasets": "papluca/language-identification",
      "metrics": [
        "accuracy",
        "f1"
      ],
      "base_model": "xlm-roberta-base",
      "model-index": [
        {
          "name": "xlm-roberta-base-language-detection",
          "results": []
        }
      ]
    },
    "card_text": "\n# xlm-roberta-base-language-detection\n\nThis model is a fine-tuned version of [xlm-roberta-base](https://huggingface.co/xlm-roberta-base) on the [Language Identification](https://huggingface.co/datasets/papluca/language-identification#additional-information) dataset.\n\n## Model description\n\nThis model is an XLM-RoBERTa transformer model with a classification head on top (i.e. a linear layer on top of the pooled output). \nFor additional information please refer to the [xlm-roberta-base](https://huggingface.co/xlm-roberta-base) model card or to the paper [Unsupervised Cross-lingual Representation Learning at Scale](https://arxiv.org/abs/1911.02116) by Conneau et al.\n\n## Intended uses & limitations\n\nYou can directly use this model as a language detector, i.e. for sequence classification tasks. Currently, it supports the following 20 languages: \n\n`arabic (ar), bulgarian (bg), german (de), modern greek (el), english (en), spanish (es), french (fr), hindi (hi), italian (it), japanese (ja), dutch (nl), polish (pl), portuguese (pt), russian (ru), swahili (sw), thai (th), turkish (tr), urdu (ur), vietnamese (vi), and chinese (zh)`\n\n## Training and evaluation data\n\nThe model was fine-tuned on the [Language Identification](https://huggingface.co/datasets/papluca/language-identification#additional-information) dataset, which consists of text sequences in 20 languages. The training set contains 70k samples, while the validation and test sets 10k each. The average accuracy on the test set is **99.6%** (this matches the average macro/weighted F1-score being the test set perfectly balanced). A more detailed evaluation is provided by the following table.\n\n| Language | Precision | Recall | F1-score | support |\n|:--------:|:---------:|:------:|:--------:|:-------:|\n|ar        |0.998      |0.996   |0.997     |500      |\n|bg        |0.998      |0.964   |0.981     |500      |\n|de        |0.998      |0.996   |0.997     |500      |\n|el        |0.996      |1.000   |0.998     |500      |\n|en        |1.000      |1.000   |1.000     |500      |\n|es        |0.967      |1.000   |0.983     |500      |\n|fr        |1.000      |1.000   |1.000     |500      |\n|hi        |0.994      |0.992   |0.993     |500      |\n|it        |1.000      |0.992   |0.996     |500      |\n|ja        |0.996      |0.996   |0.996     |500      |\n|nl        |1.000      |1.000   |1.000     |500      |\n|pl        |1.000      |1.000   |1.000     |500      |\n|pt        |0.988      |1.000   |0.994     |500      |\n|ru        |1.000      |0.994   |0.997     |500      |\n|sw        |1.000      |1.000   |1.000     |500      |\n|th        |1.000      |0.998   |0.999     |500      |\n|tr        |0.994      |0.992   |0.993     |500      |\n|ur        |1.000      |1.000   |1.000     |500      |\n|vi        |0.992      |1.000   |0.996     |500      |\n|zh        |1.000      |1.000   |1.000     |500      |\n\n### Benchmarks\n\nAs a baseline to compare `xlm-roberta-base-language-detection` against, we have used the Python [langid](https://github.com/saffsd/langid.py) library. Since it comes pre-trained on 97 languages, we have used its `.set_languages()` method to constrain the language set to our 20 languages. The average accuracy of langid on the test set is **98.5%**. More details are provided by the table below.\n\n| Language | Precision | Recall | F1-score | support |\n|:--------:|:---------:|:------:|:--------:|:-------:|\n|ar        |0.990      |0.970   |0.980     |500      |\n|bg        |0.998      |0.964   |0.981     |500      |\n|de        |0.992      |0.944   |0.967     |500      |\n|el        |1.000      |0.998   |0.999     |500      |\n|en        |1.000      |1.000   |1.000     |500      |\n|es        |1.000      |0.968   |0.984     |500      |\n|fr        |0.996      |1.000   |0.998     |500      |\n|hi        |0.949      |0.976   |0.963     |500      |\n|it        |0.990      |0.980   |0.985     |500      |\n|ja        |0.927      |0.988   |0.956     |500      |\n|nl        |0.980      |1.000   |0.990     |500      |\n|pl        |0.986      |0.996   |0.991     |500      |\n|pt        |0.950      |0.996   |0.973     |500      |\n|ru        |0.996      |0.974   |0.985     |500      |\n|sw        |1.000      |1.000   |1.000     |500      |\n|th        |1.000      |0.996   |0.998     |500      |\n|tr        |0.990      |0.968   |0.979     |500      |\n|ur        |0.998      |0.996   |0.997     |500      |\n|vi        |0.971      |0.990   |0.980     |500      |\n|zh        |1.000      |1.000   |1.000     |500      |\n\n## How to get started with the model\n\nThe easiest way to use the model is via the high-level `pipeline` API:\n\n```python\nfrom transformers import pipeline\n\ntext = [\n    \"Brevity is the soul of wit.\",\n    \"Amor, ch'a nullo amato amar perdona.\"\n]\n\nmodel_ckpt = \"papluca/xlm-roberta-base-language-detection\"\npipe = pipeline(\"text-classification\", model=model_ckpt)\npipe(text, top_k=1, truncation=True)\n```\n\nOr one can proceed with the tokenizer and model separately:\n\n```python\nimport torch\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\ntext = [\n    \"Brevity is the soul of wit.\",\n    \"Amor, ch'a nullo amato amar perdona.\"\n]\n\nmodel_ckpt = \"papluca/xlm-roberta-base-language-detection\"\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_ckpt)\n\ninputs = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n\nwith torch.no_grad():\n    logits = model(**inputs).logits\n\npreds = torch.softmax(logits, dim=-1)\n\n# Map raw predictions to languages\nid2lang = model.config.id2label\nvals, idxs = torch.max(preds, dim=1)\n{id2lang[k.item()]: v.item() for k, v in zip(idxs, vals)}\n```\n\n## Training procedure\n\nFine-tuning was done via the `Trainer` API. Here is the [Colab notebook](https://colab.research.google.com/drive/15LJTckS6gU3RQOmjLqxVNBmbsBdnUEvl?usp=sharing) with the training code.\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 64\n- eval_batch_size: 128\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 2\n- mixed_precision_training: Native AMP\n\n### Training results\n\nThe validation results on the `valid` split of the Language Identification dataset are summarised here below.\n\n| Training Loss | Epoch | Step | Validation Loss | Accuracy | F1     |\n|:-------------:|:-----:|:----:|:---------------:|:--------:|:------:|\n| 0.2492        | 1.0   | 1094 | 0.0149          | 0.9969   | 0.9969 |\n| 0.0101        | 2.0   | 2188 | 0.0103          | 0.9977   | 0.9977 |\n\nIn short, it achieves the following results on the validation set:\n- Loss: 0.0101\n- Accuracy: 0.9977\n- F1: 0.9977\n\n### Framework versions\n\n- Transformers 4.12.5\n- Pytorch 1.10.0+cu111\n- Datasets 1.15.1\n- Tokenizers 0.10.3\n",
    "card_content": "---\nlanguage:\n- multilingual\n- ar\n- bg\n- de\n- el\n- en\n- es\n- fr\n- hi\n- it\n- ja\n- nl\n- pl\n- pt\n- ru\n- sw\n- th\n- tr\n- ur\n- vi\n- zh\nlicense: mit\ntags:\n- generated_from_trainer\ndatasets: papluca/language-identification\nmetrics:\n- accuracy\n- f1\nbase_model: xlm-roberta-base\nmodel-index:\n- name: xlm-roberta-base-language-detection\n  results: []\n---\n\n# xlm-roberta-base-language-detection\n\nThis model is a fine-tuned version of [xlm-roberta-base](https://huggingface.co/xlm-roberta-base) on the [Language Identification](https://huggingface.co/datasets/papluca/language-identification#additional-information) dataset.\n\n## Model description\n\nThis model is an XLM-RoBERTa transformer model with a classification head on top (i.e. a linear layer on top of the pooled output). \nFor additional information please refer to the [xlm-roberta-base](https://huggingface.co/xlm-roberta-base) model card or to the paper [Unsupervised Cross-lingual Representation Learning at Scale](https://arxiv.org/abs/1911.02116) by Conneau et al.\n\n## Intended uses & limitations\n\nYou can directly use this model as a language detector, i.e. for sequence classification tasks. Currently, it supports the following 20 languages: \n\n`arabic (ar), bulgarian (bg), german (de), modern greek (el), english (en), spanish (es), french (fr), hindi (hi), italian (it), japanese (ja), dutch (nl), polish (pl), portuguese (pt), russian (ru), swahili (sw), thai (th), turkish (tr), urdu (ur), vietnamese (vi), and chinese (zh)`\n\n## Training and evaluation data\n\nThe model was fine-tuned on the [Language Identification](https://huggingface.co/datasets/papluca/language-identification#additional-information) dataset, which consists of text sequences in 20 languages. The training set contains 70k samples, while the validation and test sets 10k each. The average accuracy on the test set is **99.6%** (this matches the average macro/weighted F1-score being the test set perfectly balanced). A more detailed evaluation is provided by the following table.\n\n| Language | Precision | Recall | F1-score | support |\n|:--------:|:---------:|:------:|:--------:|:-------:|\n|ar        |0.998      |0.996   |0.997     |500      |\n|bg        |0.998      |0.964   |0.981     |500      |\n|de        |0.998      |0.996   |0.997     |500      |\n|el        |0.996      |1.000   |0.998     |500      |\n|en        |1.000      |1.000   |1.000     |500      |\n|es        |0.967      |1.000   |0.983     |500      |\n|fr        |1.000      |1.000   |1.000     |500      |\n|hi        |0.994      |0.992   |0.993     |500      |\n|it        |1.000      |0.992   |0.996     |500      |\n|ja        |0.996      |0.996   |0.996     |500      |\n|nl        |1.000      |1.000   |1.000     |500      |\n|pl        |1.000      |1.000   |1.000     |500      |\n|pt        |0.988      |1.000   |0.994     |500      |\n|ru        |1.000      |0.994   |0.997     |500      |\n|sw        |1.000      |1.000   |1.000     |500      |\n|th        |1.000      |0.998   |0.999     |500      |\n|tr        |0.994      |0.992   |0.993     |500      |\n|ur        |1.000      |1.000   |1.000     |500      |\n|vi        |0.992      |1.000   |0.996     |500      |\n|zh        |1.000      |1.000   |1.000     |500      |\n\n### Benchmarks\n\nAs a baseline to compare `xlm-roberta-base-language-detection` against, we have used the Python [langid](https://github.com/saffsd/langid.py) library. Since it comes pre-trained on 97 languages, we have used its `.set_languages()` method to constrain the language set to our 20 languages. The average accuracy of langid on the test set is **98.5%**. More details are provided by the table below.\n\n| Language | Precision | Recall | F1-score | support |\n|:--------:|:---------:|:------:|:--------:|:-------:|\n|ar        |0.990      |0.970   |0.980     |500      |\n|bg        |0.998      |0.964   |0.981     |500      |\n|de        |0.992      |0.944   |0.967     |500      |\n|el        |1.000      |0.998   |0.999     |500      |\n|en        |1.000      |1.000   |1.000     |500      |\n|es        |1.000      |0.968   |0.984     |500      |\n|fr        |0.996      |1.000   |0.998     |500      |\n|hi        |0.949      |0.976   |0.963     |500      |\n|it        |0.990      |0.980   |0.985     |500      |\n|ja        |0.927      |0.988   |0.956     |500      |\n|nl        |0.980      |1.000   |0.990     |500      |\n|pl        |0.986      |0.996   |0.991     |500      |\n|pt        |0.950      |0.996   |0.973     |500      |\n|ru        |0.996      |0.974   |0.985     |500      |\n|sw        |1.000      |1.000   |1.000     |500      |\n|th        |1.000      |0.996   |0.998     |500      |\n|tr        |0.990      |0.968   |0.979     |500      |\n|ur        |0.998      |0.996   |0.997     |500      |\n|vi        |0.971      |0.990   |0.980     |500      |\n|zh        |1.000      |1.000   |1.000     |500      |\n\n## How to get started with the model\n\nThe easiest way to use the model is via the high-level `pipeline` API:\n\n```python\nfrom transformers import pipeline\n\ntext = [\n    \"Brevity is the soul of wit.\",\n    \"Amor, ch'a nullo amato amar perdona.\"\n]\n\nmodel_ckpt = \"papluca/xlm-roberta-base-language-detection\"\npipe = pipeline(\"text-classification\", model=model_ckpt)\npipe(text, top_k=1, truncation=True)\n```\n\nOr one can proceed with the tokenizer and model separately:\n\n```python\nimport torch\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\ntext = [\n    \"Brevity is the soul of wit.\",\n    \"Amor, ch'a nullo amato amar perdona.\"\n]\n\nmodel_ckpt = \"papluca/xlm-roberta-base-language-detection\"\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_ckpt)\n\ninputs = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n\nwith torch.no_grad():\n    logits = model(**inputs).logits\n\npreds = torch.softmax(logits, dim=-1)\n\n# Map raw predictions to languages\nid2lang = model.config.id2label\nvals, idxs = torch.max(preds, dim=1)\n{id2lang[k.item()]: v.item() for k, v in zip(idxs, vals)}\n```\n\n## Training procedure\n\nFine-tuning was done via the `Trainer` API. Here is the [Colab notebook](https://colab.research.google.com/drive/15LJTckS6gU3RQOmjLqxVNBmbsBdnUEvl?usp=sharing) with the training code.\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 64\n- eval_batch_size: 128\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 2\n- mixed_precision_training: Native AMP\n\n### Training results\n\nThe validation results on the `valid` split of the Language Identification dataset are summarised here below.\n\n| Training Loss | Epoch | Step | Validation Loss | Accuracy | F1     |\n|:-------------:|:-----:|:----:|:---------------:|:--------:|:------:|\n| 0.2492        | 1.0   | 1094 | 0.0149          | 0.9969   | 0.9969 |\n| 0.0101        | 2.0   | 2188 | 0.0103          | 0.9977   | 0.9977 |\n\nIn short, it achieves the following results on the validation set:\n- Loss: 0.0101\n- Accuracy: 0.9977\n- F1: 0.9977\n\n### Framework versions\n\n- Transformers 4.12.5\n- Pytorch 1.10.0+cu111\n- Datasets 1.15.1\n- Tokenizers 0.10.3\n",
    "library_name": "transformers"
  },
  {
    "model_id": "MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli",
    "model_name": "MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli",
    "author": "MoritzLaurer",
    "downloads": 3238764,
    "likes": 202,
    "tags": [
      "transformers",
      "pytorch",
      "safetensors",
      "deberta-v2",
      "text-classification",
      "zero-shot-classification",
      "en",
      "dataset:multi_nli",
      "dataset:facebook/anli",
      "dataset:fever",
      "arxiv:2006.03654",
      "license:mit",
      "model-index",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli",
    "dependencies": null,
    "analysis_date": "2025-03-26T00:47:16.558188",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "deberta-v2",
    "pipeline_tag": "zero-shot-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": [
        "en"
      ],
      "license": "mit",
      "tags": [
        "text-classification",
        "zero-shot-classification"
      ],
      "datasets": [
        "multi_nli",
        "facebook/anli",
        "fever"
      ],
      "metrics": [
        "accuracy"
      ],
      "pipeline_tag": "zero-shot-classification",
      "model-index": [
        {
          "name": "MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli",
          "results": [
            {
              "task": {
                "type": "natural-language-inference",
                "name": "Natural Language Inference"
              },
              "dataset": {
                "name": "anli",
                "type": "anli",
                "config": "plain_text",
                "split": "test_r3"
              },
              "metrics": [
                {
                  "type": "accuracy",
                  "value": 0.495,
                  "name": "Accuracy",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiYWViYjQ5YTZlYjU4NjQyN2NhOTVhNjFjNGQyMmFiNmQyZjRkOTdhNzJmNjc3NGU4MmY0MjYyMzY5MjZhYzE0YiIsInZlcnNpb24iOjF9.S8pIQ7gEGokd_wKXMi6Bc3B2DThIP3cvVkTFErZ-2JxXTSCy1TBuulY3dzGfaiP7kTHbL52OuBhG_-wb7Ue9DQ"
                },
                {
                  "type": "precision",
                  "value": 0.4984740618243923,
                  "name": "Precision Macro",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiOTllZDU3NmVmYjk4ZmYzNjAwNzExMGZjNDMzOWRkZjRjMTRhNzhlZmI0ZmNlM2E0Mzk4OWE5NTM5MTYyYWU5NCIsInZlcnNpb24iOjF9.WHz_TUJgPVn-rU-9vBCDdmSMOuWzADwr09rJY6ktqRM46zytbyWs7Vcm7jqDrTkfU-rp0_7IyoNv_xEsKhJbBA"
                },
                {
                  "type": "precision",
                  "value": 0.495,
                  "name": "Precision Micro",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZjllODE3ZjUxZDhiMTI0MzZmYjY5OTUwYWI2OTc4ZjJhNTVjMjY2ODdkMmJlZjQ5YWQ1Mjk2ZThmYjJlM2RlYSIsInZlcnNpb24iOjF9.a9V06-O7l9S0Bv4vj0aard8128SAP61DZdXl_3XqdmNgt_C6KAoDBVueF2M2kF_kT6lRfEz6YW0ACIfJNXDYAA"
                },
                {
                  "type": "precision",
                  "value": 0.4984357572868885,
                  "name": "Precision Weighted",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiNjhiMzYzY2JiMmYwN2YxYzEwZTQ3NGI1NzFmMzliNjJkMDE2YzI5Njg1ZjEzMGIxODdiMDNmYmI4Y2Y2MmJkMiIsInZlcnNpb24iOjF9.xvZZaUMogw9MJjb3ls6h5liDlTqHMmNgqk6KbyDqQWfCcD255brCU3Xo6nECwaChS4te0dQu_iWGBqR_o2kYAA"
                },
                {
                  "type": "recall",
                  "value": 0.49461028192371476,
                  "name": "Recall Macro",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZDVjYTEzOTI0ZjVhOTk3ZTkzZmZhNTk5ODcxMWJhYWU4ZTRjYWVhNzcwOWY5YmI2NGFlYWE4NjM5MDY5NTExOSIsInZlcnNpb24iOjF9.xgHCB2rbCQBzHzUokw4u8JyOdhtF4yvPv1t8t7YiEkaAuM5MAPsVuCZ1VtlLapHS_IWetlocizsVl6akjh3cAQ"
                },
                {
                  "type": "recall",
                  "value": 0.495,
                  "name": "Recall Micro",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiYTEyYmM0ZDQ0M2RiMDNhNjIxNzQ4OWZiNTBiOTAwZDFkNjNmYjBhNjA4NmQ0NjFkNmNiZTljNDkxNDg3NzIyYSIsInZlcnNpb24iOjF9.3FJPwNtwgFNvMjVxVAayaVXXR1sWlr0sqAYmXzmMzMxl7IJh6RS77dGPwFaqD3jamLVBiqPn9wsfz5lFK5yTAA"
                },
                {
                  "type": "recall",
                  "value": 0.495,
                  "name": "Recall Weighted",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiNmY1MjZlZTQ4OTg5YzdlYmFhZDMzMmNlNjNkYmIyZGI4M2NjZjQ1ZDVkNmZkMTUxNjI3M2UwZmI1MDM1NDYwOSIsInZlcnNpb24iOjF9.cnbM6xjTLRa9z0wEDGd_Q4lTXVLRKIQ6_YLGLjf-t7Nto4lzxAeWF-RrwA0Mq9OPITlJq2Jk1Eg_0Utb13d9Dg"
                },
                {
                  "type": "f1",
                  "value": 0.4942810999491704,
                  "name": "F1 Macro",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiN2U3NGM1MDM4YTM4NzQxMGM4ZTIyZDM2YTQ1MGNlZWM1MzEzM2MxN2ZmZmRmYTM0OWJmZGJjYjM5OWEzMmZjNSIsInZlcnNpb24iOjF9.vMtge1F-tmMn9D3aVUuwcNEXjqpNgEyHAl9f5UDSoTYcOgTwi2vi5yRGRCl8y6Fx7BtgaCwMyoZVNbP5-GRtCA"
                },
                {
                  "type": "f1",
                  "value": 0.495,
                  "name": "F1 Micro",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiNjBjMTQ5MmQ5OGE5OWJjZGMyNzg4N2RmNDUzMzQ5Zjc4ZTc4N2JlMTk0MTc2M2RjZTgzOTNlYWQzODAwNDI0NCIsInZlcnNpb24iOjF9.yxXG0CNWW8__xJC14BjbTY9QkXD75x6uCIXR51oKDemkP0b_xGyd-A2wPIuwNJN1EYkQevPY0bhVpRWBKyO9Bg"
                },
                {
                  "type": "f1",
                  "value": 0.4944671868893595,
                  "name": "F1 Weighted",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMzczNjQzY2FmMmY4NTAwYjNkYjJlN2I2NjI2Yjc0ZmQ3NjZiN2U5YWEwYjk4OTUyOTMzZTYyZjYzOTMzZGU2YiIsInZlcnNpb24iOjF9.mLOnst2ScPX7ZQwaUF12W2nv7-w9lX9-BxHl3-0T0gkSWnmtBSwYcL5faTX0_I5q33Fjz5tfkjpCJuxP5JYIBQ"
                },
                {
                  "type": "loss",
                  "value": 1.8788293600082397,
                  "name": "loss",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMzRlOTYwYjU1Y2Y4ZGM0NDBjYTE2MmEzNWIwN2NiMWVkOWZlNzA2ZmQ3YjZjNzI4MjQwYWZhODIwMzU3ODAyZiIsInZlcnNpb24iOjF9._Xs9bl48MSavvp5eyamrP2iNlFWv35QZCrmWjJXLkUdIBx0ElCjEdxBb3dxPGnUxdpDzGMmOoKCPI44ZPXrtDw"
                }
              ]
            },
            {
              "task": {
                "type": "natural-language-inference",
                "name": "Natural Language Inference"
              },
              "dataset": {
                "name": "anli",
                "type": "anli",
                "config": "plain_text",
                "split": "test_r1"
              },
              "metrics": [
                {
                  "type": "accuracy",
                  "value": 0.712,
                  "name": "Accuracy",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiYWYxMGY0ZWU0YTEyY2I3NmQwZmQ3YmFmNzQxNGU5OGNjN2ViN2I0ZjdkYWUzM2RmYzkzMDg3ZjVmNGYwNGZkZCIsInZlcnNpb24iOjF9.snWBusAeo1rrQqWk--vTxb-CBcFqM298YCtwTQGBZiFegKGSTSKzj-SM6HMNsmoQWmMuv7UfYPqYlnzEthOSAg"
                },
                {
                  "type": "precision",
                  "value": 0.7134839439315348,
                  "name": "Precision Macro",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiNjMxMjg1Y2QwNzMwM2ZkNGM3ZTJhOGJmY2FkNGI1ZTFhOGQ3ODViNTJmZTYwMWJkZDYyYWRjMzFmZDI1NTM5YSIsInZlcnNpb24iOjF9.ZJnY6zYOBn-YEtN7uKzQ-VKXPwlIO1zq19Yuo37vBJNSs1dGDd8f1jgfdZuA19e_wA3Nc5nQKe9VXRwPHPgwAQ"
                },
                {
                  "type": "precision",
                  "value": 0.712,
                  "name": "Precision Micro",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZWM4YWQyODBlYTIwMWQxZDA1NmY1M2M2ODgwNDJiY2RhMDVhYTlkMDUzZTJkMThkYzRmNDg2YTdjMjczNGUwOCIsInZlcnNpb24iOjF9.SogsKHdbdlEs05IBYwXvlnaC_esg-DXAPc2KPRyHaVC5ItVHbxa63NpybSpao4baOoMlLG9aRe7TjG4gtB2dAQ"
                },
                {
                  "type": "precision",
                  "value": 0.7134676028447461,
                  "name": "Precision Weighted",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiODdjMzFkM2IwNWZiM2I4ZWViMmQ4NWM5MDY5ZWQxZjc1MGRmNjhmNzJhYWFmOWEwMjg3ZjhiZWM3YjlhOTIxNSIsInZlcnNpb24iOjF9._0JNIbiqLuDZrp_vrCljBe28xexZJPmigLyhkcO8AtH2VcNxWshwCpZuRF4bqvpMvnApJeuGMf3vXjCj0MC1Bw"
                },
                {
                  "type": "recall",
                  "value": 0.7119814425203647,
                  "name": "Recall Macro",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiYjU4MWEyMzkyYzg1ZTIxMTc0M2NhMTgzOGEyZmY5OTg3M2Q1ZmMwNmU3ZmU1ZjA1MDk0OGZkMzM5NDVlZjBlNSIsInZlcnNpb24iOjF9.sZ3GTcmGGthpTLL7_Zovq8aBmE3Dp_PZi5v8ZI9yG9N6B_GjWvBuPC8ENXK1NwmwiHLsSvtKTG5JmAum-su0Dg"
                },
                {
                  "type": "recall",
                  "value": 0.712,
                  "name": "Recall Micro",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZDg3NGViZTlmMWM2ZDNhMzIzZGZkYWZhODQxNzg2MjNiNjQ0Zjg0NjQ1OWZkY2I5ODdiY2Y3Y2JjNzRmYjJkMiIsInZlcnNpb24iOjF9.bCZUzJamsozKWehnNph6E5coww5zZTrJdbWevWrSyfT0PyXc_wkZ-NKdyBAoqprBz3_8L3i5hPM6Qsy56b4BDA"
                },
                {
                  "type": "recall",
                  "value": 0.712,
                  "name": "Recall Weighted",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMDk1MDJiOGUzZThlZjJjMzY4NjMzODFiZjUzZmIwMjIxY2UwNzBiN2IxMWEwMGJjZTkxODA0YzUxZDE3ODRhOCIsInZlcnNpb24iOjF9.z0dqvB3aBVYt3xRIb_M4svWebfQc0QaDFVFzHnlA5QGEHkHOW3OecGhHE4EzBqTDI3DASWZTGMjrMDDt0uOMBw"
                },
                {
                  "type": "f1",
                  "value": 0.7119226991285647,
                  "name": "F1 Macro",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiM2U0YjMwNzhmOTEyNDZhODU3MTU0YTM4MmQ0NzEzNWI1YjY0ZWQ3MWRiMTdiNTUzNWRkZThjMWE4M2NkZmI0MiIsInZlcnNpb24iOjF9.hhj1BXkuWi9wXrCjT9NwqaPETtOoYNiyqYsJEw-ufA8A4hVThKA6ZBtma1Q_M65-DZFfPEBDBNASLZ7EPSbmDw"
                },
                {
                  "type": "f1",
                  "value": 0.712,
                  "name": "F1 Micro",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiODk0Y2EyMzc5M2ZlNWFlNDg2Zjc1OTQxNGY3YjA5YjUxYTYzZjRlZmU4ODYxNjA3ZjkxNGUzYjBmNmMxMzY5YiIsInZlcnNpb24iOjF9.DvKk-3hNh2LhN2ug5e0FgUntL3Ozdfl06Kz7jvmB-deOJH6INi2a2ZySXoEePoo8t2nR6ENFYu9QjMA2ojnpCA"
                },
                {
                  "type": "f1",
                  "value": 0.7119242267218338,
                  "name": "F1 Weighted",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiN2MxOWFlMmI2NGRiMjkwN2Q5MWZhNDFlYzQxNWNmNzQ3OWYxZThmNDU2OWU1MTE5OGY2MWRlYWUyNDM3OTkzZCIsInZlcnNpb24iOjF9.QrTD1gE8_wRok9u59W-Mx0cX89K-h2Ad6qa8J5rmP8lc_rkG0ft2n5_GqH1CBZBJwMFYv91Pn6TuE3eGxJuUDA"
                },
                {
                  "type": "loss",
                  "value": 1.0105403661727905,
                  "name": "loss",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMmUwMTg4NjM3ZTBiZTIyODcyNDNmNTE5ZDZhMzNkMDMyNjcwOGQ5NmY0NTlhMjgyNmIzZjRiNDFiNjA3M2RkZSIsInZlcnNpb24iOjF9.sjBDVJV-jnygwcppmByAXpoo-Wzz178bBzozJEuYEiJaHSbk_xEevfJS1PmLUuplYslKb1iyEctnjI-5bl-XDw"
                }
              ]
            },
            {
              "task": {
                "type": "natural-language-inference",
                "name": "Natural Language Inference"
              },
              "dataset": {
                "name": "multi_nli",
                "type": "multi_nli",
                "config": "default",
                "split": "validation_mismatched"
              },
              "metrics": [
                {
                  "type": "accuracy",
                  "value": 0.902766476810415,
                  "name": "Accuracy",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMjExZWM3YzA3ZDNlNjEwMmViNWEwZTE3MjJjNjEyNDhjOTQxNGFmMzBjZTk0ODUwYTc2OGNiZjYyMTBmNWZjZSIsInZlcnNpb24iOjF9.zbFAGrv2flpmweqS7Poxib7qHFLdW8eUTzshdOm2B9H-KWpIZCWC-P4p8TLMdNJnUcZJZ03Okil4qjIMqqIRCA"
                },
                {
                  "type": "precision",
                  "value": 0.9023816542652491,
                  "name": "Precision Macro",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiN2U2MGViNmJjNWQxNzRjOTkxNDIxZjZjNmM5YzE4ZjU5NTE5NjFlNmEzZWRlOGYxN2E3NTAwMTEwYjNhNzE0YSIsInZlcnNpb24iOjF9.WJjDJf56FROvf7Y5ShWnnxMvK_ZpQ2PibAOtSFhSiYJ7bt4TGOzMwaZ5RSTf_mcfXgRfWbXmy1jCwNhDb-5EAw"
                },
                {
                  "type": "precision",
                  "value": 0.902766476810415,
                  "name": "Precision Micro",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiYzRhZTExOTc5NDczZjI1YmMzOGYyOTU2MDU1OGE5ZTczMDE0MmU0NzZhY2YzMDI1ZGQ3MGM5MmJiODFkNzUzZiIsInZlcnNpb24iOjF9.aRYcGEI1Y8-a0d8XOoXhBgsFyj9LWNwEjoIPc594y7kJn91wXIsXoR0-_0iy3uz41mWaTTlwJx7lI-kipFDvDQ"
                },
                {
                  "type": "precision",
                  "value": 0.9034597464719761,
                  "name": "Precision Weighted",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMWQyMTZiZDA2OTUwZjRmNTFiMWRlZTNmOTliZmI2MWFmMjdjYzEyYTgwNzkyOTQzOTBmNTUyYjMwNTUxMTFkNiIsInZlcnNpb24iOjF9.hUtAMTl0THHUkaLcgk1Vy9IhjqJAXCJ_5STJ5A7k7s_SO9DHp3b6qusgwPmcGLYyPy1-j1dB2AIstxK4tHfmDA"
                },
                {
                  "type": "recall",
                  "value": 0.9024304801555488,
                  "name": "Recall Macro",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMzAxZGJhNGI3ZDNlMjg2ZDIxNTgwMDY5MTFjM2ExZmIxMDBmZjUyNTliNWNkOGI0OTY3NTYyNWU3OWFlYTA3YiIsInZlcnNpb24iOjF9.1o_GNq8zmXa_50MUF_K63IDc2aUKNeUkNQ5fT592-SAo8WgiaP9Dh6bOEu2OqrpRQ57P4qm7OdJt7UKsrosMDA"
                },
                {
                  "type": "recall",
                  "value": 0.902766476810415,
                  "name": "Recall Micro",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZjhiMWE4Yjk0ODFkZjlkYjRlMjU1OTJmMjA2Njg1N2M4MzQ0OWE3N2FlYjY4NDgxZThjMmExYWQ5OGNmYmI1NSIsInZlcnNpb24iOjF9.Gmm5lf_qpxjXWWrycDze7LHR-6WGQc62WZTmcoc5uxWd0tivEUqCAFzFdbEU1jVKxQBIyDX77CPuBm7mUA4sCg"
                },
                {
                  "type": "recall",
                  "value": 0.902766476810415,
                  "name": "Recall Weighted",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiY2EzZWYwNjNkYWE1YTcyZGZjNTNhMmNlNzgzYjk5MGJjOWJmZmE5NmYwM2U2NTA5ZDY3ZjFiMmRmZmQwY2QwYiIsInZlcnNpb24iOjF9.yA68rslg3e9kUR3rFTNJJTAad6Usr4uFmJvE_a7G2IvSKqLxG_pqsHszsWfg5mFBQLjWEAyCtdQYMdVayuYMBA"
                },
                {
                  "type": "f1",
                  "value": 0.9023086094638595,
                  "name": "F1 Macro",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMzMyMzZhNjI5MWRmZWJhMjkzN2E0MjM4ZTM5YzZmNTk5YTZmYzU4NDRiYjczZGQ4MDdhNjJiMGU0MjE3NDEwNyIsInZlcnNpb24iOjF9.RCMqH_xUMN97Vos54pTFfAMbLstXUMdFTs-eNaypbDb_Fc-MW8NLmJ6dzJsp9sSvhXyYjugjRMUpMpnQseKXDA"
                },
                {
                  "type": "f1",
                  "value": 0.902766476810415,
                  "name": "F1 Micro",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZTYxZTZhZGM0NThlNTAzNmYwMTA4NDNkN2FiNzhhN2RlYThlYjcxMjE5MjBkMzhiOGYxZGRmMjE0NGM2ZWQ5ZSIsInZlcnNpb24iOjF9.wRfllNw2Gibmi1keU7d_GjkyO0F9HESCgJlJ9PHGZQRRT414nnB-DyRvulHjCNnaNjXqMi0LJimC3iBrNawwAw"
                },
                {
                  "type": "f1",
                  "value": 0.9030161011457231,
                  "name": "F1 Weighted",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiNDA0YjAxMWU5MjI4MWEzNTNjMzJlNjM3ZDMxOTE0ZTZhYmZlNmUyNDViNTU2NmMyMmM3MjAxZWVjNWJmZjI4MCIsInZlcnNpb24iOjF9.vJ8aUjfTbFMc1BgNUVpoVDuYwQJYQjwZQxblkUdvSoGtkW_AzQJ_KJ8Njc7IBA3ADgj8iZHjRQNIZkFCf-xICw"
                },
                {
                  "type": "loss",
                  "value": 0.3283354640007019,
                  "name": "loss",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiODdmYzYzNTUzZDNmOWIxM2E0ZmUyOWUzM2Y2NGRmZDNiYjg3ZTMzYTUyNzg3OWEzNzYyN2IyNmExOGRlMWUxYSIsInZlcnNpb24iOjF9.Qv0FzFZPkcBs9aHGf4TEREX4jdkc40NazdMlP2M_-w2wHwyjoAjvhk611RLXHcbicozNelZJLnsOMdEMnPLEDg"
                }
              ]
            },
            {
              "task": {
                "type": "natural-language-inference",
                "name": "Natural Language Inference"
              },
              "dataset": {
                "name": "anli",
                "type": "anli",
                "config": "plain_text",
                "split": "dev_r1"
              },
              "metrics": [
                {
                  "type": "accuracy",
                  "value": 0.737,
                  "name": "Accuracy",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMTQ1ZGVkOTVmNTlhYjhkMjVlNTNhMjNmZWFjZWZjZjcxZmRhMDVlOWI0YTdkOTMwYjVjNWFlOGY4OTc1MmRhNiIsInZlcnNpb24iOjF9.wGLgKA1E46ljbLokdPeip_UCr1gqK8iSSbsJKX2vgKuuhDdUWWiECrUFN-bv_78JWKoKW5T0GF_hb-RVDzA0AQ"
                },
                {
                  "type": "precision",
                  "value": 0.737681071614645,
                  "name": "Precision Macro",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiYmFkMGUwMjNhN2E3NzMxNTc5NDM0MjY1MGU5ODllM2Q2YzA1MDI3OGI1ZmI4YTcxN2E4ZDk5OWY2OGNiN2I0MCIsInZlcnNpb24iOjF9.6G5qhccjheaNfasgRyrkKBTaQPRzuPMZZ0hrLxTNzAydMDgx09FkFP3hni7WLRMWp0IpwzkEeBlxV-mPyQBtBw"
                },
                {
                  "type": "precision",
                  "value": 0.737,
                  "name": "Precision Micro",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiN2QzYjQ4ZDZjOGU5YzI3YmFlMThlYTRkYTUyYWIyNzc4NDkwNzM1OWFiMTgyMzA0NDZmMGI3YTQxODBjM2EwMCIsInZlcnNpb24iOjF9.bvNWyzfct1CLJFx_EuD2GeKieVtyGJy0cwUBP2qJE1ey2i9SVn6n1Dr0AALTGBkxQ6n5-fJ61QFNufpdr2KvCA"
                },
                {
                  "type": "precision",
                  "value": 0.7376755842752241,
                  "name": "Precision Weighted",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiN2VmYWYzZWQwZmMzMDk0NTdlY2Y3NDkzYWY5ZTdmOGU0ZTUzZWE4YWFhZjVmODhkZmE1Njg4NjA5YjJmYWVhOSIsInZlcnNpb24iOjF9.50FQR2aoBpORLgYa7482ZTrRhT-KfIgv5ltBEHndUBMmqGF9Ru0LHENSGwyD_tO89sGPfiW32TxpbrNWiBdIBA"
                },
                {
                  "type": "recall",
                  "value": 0.7369675064285843,
                  "name": "Recall Macro",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZTM4OTAyNDYwNjY4Zjc5NDljNjBmNTg2Mzk4YjYxM2MyYTA0MDllYTMyNzEwOGI1ZTEwYWE3ZmU0NDZmZDg2NiIsInZlcnNpb24iOjF9.UvWBxuApNV3vd4hpgwqd6XPHCbkA_bB_Cw24ooquiOf0dstvjP3JvpGoDp5SniOzIOg3i2aYbcvFCLJqEXMZCQ"
                },
                {
                  "type": "recall",
                  "value": 0.737,
                  "name": "Recall Micro",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiYmQ4MjMzNzRmNTI5NjIzNGQ0ZDFmZTA1MDU3OTk0MzYyMGI0NTMzZTZlMTQ1MDc1MzBkMGMzYjcxZjU1NDNjOSIsInZlcnNpb24iOjF9.kpbdXOpDG3CUB-kUEXsgFT3HWWIbu70wwzs2TNf0rhIuRrzdZz3dXXvwqu1BcLJTsOxl8G6NTiYXgnv-ul8lDg"
                },
                {
                  "type": "recall",
                  "value": 0.737,
                  "name": "Recall Weighted",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiNmU1ZWJkNWE0NjczY2NiZWYyNzYyMzllNzZmZTIxNWRkYTEyZDgxN2E0NTNmM2ExMTc1ZWVjMzBiYjg0ZmM1MiIsInZlcnNpb24iOjF9.S6HHWCWnut_LJqXbEA_Z8ZOTtyq6V51ZeiA0qbwzr0hapDYZOZHrN4prvSLvoNv-GiYDYKatwIsAZxCZc5fmCA"
                },
                {
                  "type": "f1",
                  "value": 0.7366853496239583,
                  "name": "F1 Macro",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiNzkxYmY2NTcyOTE0ZDdjNGY2ZmE4MzQwMGIxZTA2MDg1NzI5YTQ0MTdkZjdkNzNkMDM2NTk2MTNiNjU4ODMwZCIsInZlcnNpb24iOjF9.ECVaCBqGd0pnQT3xJF7yWrgecIb-5TMiVWpEO0MQGhYy43snkI6Qs-2FOXzvfwIWqG-Q6XIIhGbWZh5TFEGKCA"
                },
                {
                  "type": "f1",
                  "value": 0.737,
                  "name": "F1 Micro",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiNDMwMWZiNzQyNWEzNmMzMDJjOTAxYzAxNzc0MTNlYzRkZjllYmNjZmU0OTgzZDFkNWM1ZWI5OTA2NzE5Y2YxOSIsInZlcnNpb24iOjF9.8yZFol_Gcj9n3w9Yk5wx48yql7p3wriDecv-6VSTAB6Q_MWLQAWsCEGRRhgGJ3zvhoRehJZdb35ozk36VOinDQ"
                },
                {
                  "type": "f1",
                  "value": 0.7366990292378379,
                  "name": "F1 Weighted",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMjhhN2ZkMjc5ZGQ3ZGM1Nzk3ZTgwY2E1N2NjYjdhNjZlOTdhYmRlNGVjN2EwNTIzN2UyYTY2ODVlODhmY2Q4ZCIsInZlcnNpb24iOjF9.Cz7ClDAfCGpqdRTYd5v3dPjXFq8lZLXx8AX_rqmF-Jb8KocqVDsHWeZScW5I2oy951UrdMpiUOLieBuJLOmCCQ"
                },
                {
                  "type": "loss",
                  "value": 0.9349392056465149,
                  "name": "loss",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiNmI4MTI5MDM1NjBmMzgzMzc2NjM5MzZhOGUyNTgyY2RlZTEyYTIzYzY2ZGJmODcxY2Q5OTVjOWU3OTQ2MzM1NSIsInZlcnNpb24iOjF9.bSOFnYC4Y2y2pW1AR-bgPUHKafR-0OHf8PvexK8eQLsS323Xy9-rYkKUaP09KY6_fk9GqAawv5eqj72B_uyeCA"
                }
              ]
            }
          ]
        }
      ]
    },
    "card_text": "# DeBERTa-v3-base-mnli-fever-anli\n## Model description\nThis model was trained on the MultiNLI, Fever-NLI and Adversarial-NLI (ANLI) datasets, which comprise 763 913 NLI hypothesis-premise pairs. This base model outperforms almost all large models on the [ANLI benchmark](https://github.com/facebookresearch/anli). \nThe base model is [DeBERTa-v3-base from Microsoft](https://huggingface.co/microsoft/deberta-v3-base). The v3 variant of DeBERTa substantially outperforms previous versions of the model by including a different pre-training objective, see annex 11 of the original [DeBERTa paper](https://arxiv.org/pdf/2006.03654.pdf). \n\nFor highest performance (but less speed), I recommend using https://huggingface.co/MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli. \n\n\n### How to use the model\n#### Simple zero-shot classification pipeline\n```python\n#!pip install transformers[sentencepiece]\nfrom transformers import pipeline\nclassifier = pipeline(\"zero-shot-classification\", model=\"MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli\")\nsequence_to_classify = \"Angela Merkel is a politician in Germany and leader of the CDU\"\ncandidate_labels = [\"politics\", \"economy\", \"entertainment\", \"environment\"]\noutput = classifier(sequence_to_classify, candidate_labels, multi_label=False)\nprint(output)\n```\n#### NLI use-case\n```python\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n\nmodel_name = \"MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name)\n\npremise = \"I first thought that I liked the movie, but upon second thought it was actually disappointing.\"\nhypothesis = \"The movie was good.\"\n\ninput = tokenizer(premise, hypothesis, truncation=True, return_tensors=\"pt\")\noutput = model(input[\"input_ids\"].to(device))  # device = \"cuda:0\" or \"cpu\"\nprediction = torch.softmax(output[\"logits\"][0], -1).tolist()\nlabel_names = [\"entailment\", \"neutral\", \"contradiction\"]\nprediction = {name: round(float(pred) * 100, 1) for pred, name in zip(prediction, label_names)}\nprint(prediction)\n```\n### Training data\nDeBERTa-v3-base-mnli-fever-anli was trained on the MultiNLI, Fever-NLI and Adversarial-NLI (ANLI) datasets, which comprise 763 913 NLI hypothesis-premise pairs.\n\n### Training procedure\nDeBERTa-v3-base-mnli-fever-anli was trained using the Hugging Face trainer with the following hyperparameters.\n```\ntraining_args = TrainingArguments(\n    num_train_epochs=3,              # total number of training epochs\n    learning_rate=2e-05,\n    per_device_train_batch_size=32,   # batch size per device during training\n    per_device_eval_batch_size=32,    # batch size for evaluation\n    warmup_ratio=0.1,                # number of warmup steps for learning rate scheduler\n    weight_decay=0.06,               # strength of weight decay\n    fp16=True                        # mixed precision training\n)\n```\n### Eval results\nThe model was evaluated using the test sets for MultiNLI and ANLI and the dev set for Fever-NLI. The metric used is accuracy.\n\nmnli-m | mnli-mm | fever-nli | anli-all | anli-r3\n---------|----------|---------|----------|----------\n0.903 | 0.903 | 0.777 | 0.579 | 0.495\n\n## Limitations and bias\nPlease consult the original DeBERTa paper and literature on different NLI datasets for potential biases. \n\n## Citation\nIf you use this model, please cite: Laurer, Moritz, Wouter van Atteveldt, Andreu Salleras Casas, and Kasper Welbers. 2022. \u2018Less Annotating, More Classifying \u2013 Addressing the Data Scarcity Issue of Supervised Machine Learning with Deep Transfer Learning and BERT - NLI\u2019. Preprint, June. Open Science Framework. https://osf.io/74b8k.\n\n### Ideas for cooperation or questions?\nIf you have questions or ideas for cooperation, contact me at m{dot}laurer{at}vu{dot}nl or [LinkedIn](https://www.linkedin.com/in/moritz-laurer/)\n\n### Debugging and issues\nNote that DeBERTa-v3 was released on 06.12.21 and older versions of HF Transformers seem to have issues running the model (e.g. resulting in an issue with the tokenizer). Using Transformers>=4.13 might solve some issues. \nAlso make sure to install sentencepiece to avoid tokenizer errors. Run:  `pip install transformers[sentencepiece]` or `pip install sentencepiece`\n\n\n## Model Recycling\n\n[Evaluation on 36 datasets](https://ibm.github.io/model-recycling/model_gain_chart?avg=0.65&mnli_lp=nan&20_newsgroup=-0.61&ag_news=-0.01&amazon_reviews_multi=0.46&anli=0.84&boolq=2.12&cb=16.07&cola=-0.76&copa=8.60&dbpedia=-0.40&esnli=-0.29&financial_phrasebank=-1.98&imdb=-0.47&isear=-0.22&mnli=-0.21&mrpc=0.50&multirc=1.91&poem_sentiment=1.73&qnli=0.07&qqp=-0.37&rotten_tomatoes=-0.74&rte=3.94&sst2=-0.45&sst_5bins=0.07&stsb=1.27&trec_coarse=-0.16&trec_fine=0.18&tweet_ev_emoji=-0.93&tweet_ev_emotion=-1.33&tweet_ev_hate=-1.67&tweet_ev_irony=-5.46&tweet_ev_offensive=-0.17&tweet_ev_sentiment=-0.11&wic=-0.21&wnli=-1.20&wsc=4.18&yahoo_answers=-0.70&model_name=MoritzLaurer%2FDeBERTa-v3-base-mnli-fever-anli&base_name=microsoft%2Fdeberta-v3-base) using MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli as a base model yields average score of 79.69 in comparison to 79.04 by microsoft/deberta-v3-base.\n\nThe model is ranked 2nd among all tested models for the microsoft/deberta-v3-base architecture as of 09/01/2023.\n\nResults:\n\n|   20_newsgroup |   ag_news |   amazon_reviews_multi |   anli |   boolq |      cb |    cola |   copa |   dbpedia |   esnli |   financial_phrasebank |   imdb |   isear |    mnli |    mrpc |   multirc |   poem_sentiment |   qnli |     qqp |   rotten_tomatoes |     rte |    sst2 |   sst_5bins |    stsb |   trec_coarse |   trec_fine |   tweet_ev_emoji |   tweet_ev_emotion |   tweet_ev_hate |   tweet_ev_irony |   tweet_ev_offensive |   tweet_ev_sentiment |     wic |    wnli |     wsc |   yahoo_answers |\n|---------------:|----------:|-----------------------:|-------:|--------:|--------:|--------:|-------:|----------:|--------:|-----------------------:|-------:|--------:|--------:|--------:|----------:|-----------------:|-------:|--------:|------------------:|--------:|--------:|------------:|--------:|--------------:|------------:|-----------------:|-------------------:|----------------:|-----------------:|---------------------:|---------------------:|--------:|--------:|--------:|----------------:|\n|        85.8072 |   90.4333 |                  67.32 | 59.625 |  85.107 | 91.0714 | 85.8102 |     67 |   79.0333 | 91.6327 |                   82.5 |  94.02 | 71.6428 | 89.5749 | 89.7059 |   64.1708 |          88.4615 | 93.575 | 91.4148 |           89.6811 | 86.2816 | 94.6101 |     57.0588 | 91.5508 |          97.6 |        91.2 |           45.264 |            82.6179 |         54.5455 |          74.3622 |              84.8837 |              71.6949 | 71.0031 | 69.0141 | 68.2692 |         71.3333 |\n\n\nFor more information, see: [Model Recycling](https://ibm.github.io/model-recycling/)\n",
    "card_content": "---\nlanguage:\n- en\nlicense: mit\ntags:\n- text-classification\n- zero-shot-classification\ndatasets:\n- multi_nli\n- facebook/anli\n- fever\nmetrics:\n- accuracy\npipeline_tag: zero-shot-classification\nmodel-index:\n- name: MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli\n  results:\n  - task:\n      type: natural-language-inference\n      name: Natural Language Inference\n    dataset:\n      name: anli\n      type: anli\n      config: plain_text\n      split: test_r3\n    metrics:\n    - type: accuracy\n      value: 0.495\n      name: Accuracy\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiYWViYjQ5YTZlYjU4NjQyN2NhOTVhNjFjNGQyMmFiNmQyZjRkOTdhNzJmNjc3NGU4MmY0MjYyMzY5MjZhYzE0YiIsInZlcnNpb24iOjF9.S8pIQ7gEGokd_wKXMi6Bc3B2DThIP3cvVkTFErZ-2JxXTSCy1TBuulY3dzGfaiP7kTHbL52OuBhG_-wb7Ue9DQ\n    - type: precision\n      value: 0.4984740618243923\n      name: Precision Macro\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiOTllZDU3NmVmYjk4ZmYzNjAwNzExMGZjNDMzOWRkZjRjMTRhNzhlZmI0ZmNlM2E0Mzk4OWE5NTM5MTYyYWU5NCIsInZlcnNpb24iOjF9.WHz_TUJgPVn-rU-9vBCDdmSMOuWzADwr09rJY6ktqRM46zytbyWs7Vcm7jqDrTkfU-rp0_7IyoNv_xEsKhJbBA\n    - type: precision\n      value: 0.495\n      name: Precision Micro\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZjllODE3ZjUxZDhiMTI0MzZmYjY5OTUwYWI2OTc4ZjJhNTVjMjY2ODdkMmJlZjQ5YWQ1Mjk2ZThmYjJlM2RlYSIsInZlcnNpb24iOjF9.a9V06-O7l9S0Bv4vj0aard8128SAP61DZdXl_3XqdmNgt_C6KAoDBVueF2M2kF_kT6lRfEz6YW0ACIfJNXDYAA\n    - type: precision\n      value: 0.4984357572868885\n      name: Precision Weighted\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiNjhiMzYzY2JiMmYwN2YxYzEwZTQ3NGI1NzFmMzliNjJkMDE2YzI5Njg1ZjEzMGIxODdiMDNmYmI4Y2Y2MmJkMiIsInZlcnNpb24iOjF9.xvZZaUMogw9MJjb3ls6h5liDlTqHMmNgqk6KbyDqQWfCcD255brCU3Xo6nECwaChS4te0dQu_iWGBqR_o2kYAA\n    - type: recall\n      value: 0.49461028192371476\n      name: Recall Macro\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZDVjYTEzOTI0ZjVhOTk3ZTkzZmZhNTk5ODcxMWJhYWU4ZTRjYWVhNzcwOWY5YmI2NGFlYWE4NjM5MDY5NTExOSIsInZlcnNpb24iOjF9.xgHCB2rbCQBzHzUokw4u8JyOdhtF4yvPv1t8t7YiEkaAuM5MAPsVuCZ1VtlLapHS_IWetlocizsVl6akjh3cAQ\n    - type: recall\n      value: 0.495\n      name: Recall Micro\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiYTEyYmM0ZDQ0M2RiMDNhNjIxNzQ4OWZiNTBiOTAwZDFkNjNmYjBhNjA4NmQ0NjFkNmNiZTljNDkxNDg3NzIyYSIsInZlcnNpb24iOjF9.3FJPwNtwgFNvMjVxVAayaVXXR1sWlr0sqAYmXzmMzMxl7IJh6RS77dGPwFaqD3jamLVBiqPn9wsfz5lFK5yTAA\n    - type: recall\n      value: 0.495\n      name: Recall Weighted\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiNmY1MjZlZTQ4OTg5YzdlYmFhZDMzMmNlNjNkYmIyZGI4M2NjZjQ1ZDVkNmZkMTUxNjI3M2UwZmI1MDM1NDYwOSIsInZlcnNpb24iOjF9.cnbM6xjTLRa9z0wEDGd_Q4lTXVLRKIQ6_YLGLjf-t7Nto4lzxAeWF-RrwA0Mq9OPITlJq2Jk1Eg_0Utb13d9Dg\n    - type: f1\n      value: 0.4942810999491704\n      name: F1 Macro\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiN2U3NGM1MDM4YTM4NzQxMGM4ZTIyZDM2YTQ1MGNlZWM1MzEzM2MxN2ZmZmRmYTM0OWJmZGJjYjM5OWEzMmZjNSIsInZlcnNpb24iOjF9.vMtge1F-tmMn9D3aVUuwcNEXjqpNgEyHAl9f5UDSoTYcOgTwi2vi5yRGRCl8y6Fx7BtgaCwMyoZVNbP5-GRtCA\n    - type: f1\n      value: 0.495\n      name: F1 Micro\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiNjBjMTQ5MmQ5OGE5OWJjZGMyNzg4N2RmNDUzMzQ5Zjc4ZTc4N2JlMTk0MTc2M2RjZTgzOTNlYWQzODAwNDI0NCIsInZlcnNpb24iOjF9.yxXG0CNWW8__xJC14BjbTY9QkXD75x6uCIXR51oKDemkP0b_xGyd-A2wPIuwNJN1EYkQevPY0bhVpRWBKyO9Bg\n    - type: f1\n      value: 0.4944671868893595\n      name: F1 Weighted\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMzczNjQzY2FmMmY4NTAwYjNkYjJlN2I2NjI2Yjc0ZmQ3NjZiN2U5YWEwYjk4OTUyOTMzZTYyZjYzOTMzZGU2YiIsInZlcnNpb24iOjF9.mLOnst2ScPX7ZQwaUF12W2nv7-w9lX9-BxHl3-0T0gkSWnmtBSwYcL5faTX0_I5q33Fjz5tfkjpCJuxP5JYIBQ\n    - type: loss\n      value: 1.8788293600082397\n      name: loss\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMzRlOTYwYjU1Y2Y4ZGM0NDBjYTE2MmEzNWIwN2NiMWVkOWZlNzA2ZmQ3YjZjNzI4MjQwYWZhODIwMzU3ODAyZiIsInZlcnNpb24iOjF9._Xs9bl48MSavvp5eyamrP2iNlFWv35QZCrmWjJXLkUdIBx0ElCjEdxBb3dxPGnUxdpDzGMmOoKCPI44ZPXrtDw\n  - task:\n      type: natural-language-inference\n      name: Natural Language Inference\n    dataset:\n      name: anli\n      type: anli\n      config: plain_text\n      split: test_r1\n    metrics:\n    - type: accuracy\n      value: 0.712\n      name: Accuracy\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiYWYxMGY0ZWU0YTEyY2I3NmQwZmQ3YmFmNzQxNGU5OGNjN2ViN2I0ZjdkYWUzM2RmYzkzMDg3ZjVmNGYwNGZkZCIsInZlcnNpb24iOjF9.snWBusAeo1rrQqWk--vTxb-CBcFqM298YCtwTQGBZiFegKGSTSKzj-SM6HMNsmoQWmMuv7UfYPqYlnzEthOSAg\n    - type: precision\n      value: 0.7134839439315348\n      name: Precision Macro\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiNjMxMjg1Y2QwNzMwM2ZkNGM3ZTJhOGJmY2FkNGI1ZTFhOGQ3ODViNTJmZTYwMWJkZDYyYWRjMzFmZDI1NTM5YSIsInZlcnNpb24iOjF9.ZJnY6zYOBn-YEtN7uKzQ-VKXPwlIO1zq19Yuo37vBJNSs1dGDd8f1jgfdZuA19e_wA3Nc5nQKe9VXRwPHPgwAQ\n    - type: precision\n      value: 0.712\n      name: Precision Micro\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZWM4YWQyODBlYTIwMWQxZDA1NmY1M2M2ODgwNDJiY2RhMDVhYTlkMDUzZTJkMThkYzRmNDg2YTdjMjczNGUwOCIsInZlcnNpb24iOjF9.SogsKHdbdlEs05IBYwXvlnaC_esg-DXAPc2KPRyHaVC5ItVHbxa63NpybSpao4baOoMlLG9aRe7TjG4gtB2dAQ\n    - type: precision\n      value: 0.7134676028447461\n      name: Precision Weighted\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiODdjMzFkM2IwNWZiM2I4ZWViMmQ4NWM5MDY5ZWQxZjc1MGRmNjhmNzJhYWFmOWEwMjg3ZjhiZWM3YjlhOTIxNSIsInZlcnNpb24iOjF9._0JNIbiqLuDZrp_vrCljBe28xexZJPmigLyhkcO8AtH2VcNxWshwCpZuRF4bqvpMvnApJeuGMf3vXjCj0MC1Bw\n    - type: recall\n      value: 0.7119814425203647\n      name: Recall Macro\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiYjU4MWEyMzkyYzg1ZTIxMTc0M2NhMTgzOGEyZmY5OTg3M2Q1ZmMwNmU3ZmU1ZjA1MDk0OGZkMzM5NDVlZjBlNSIsInZlcnNpb24iOjF9.sZ3GTcmGGthpTLL7_Zovq8aBmE3Dp_PZi5v8ZI9yG9N6B_GjWvBuPC8ENXK1NwmwiHLsSvtKTG5JmAum-su0Dg\n    - type: recall\n      value: 0.712\n      name: Recall Micro\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZDg3NGViZTlmMWM2ZDNhMzIzZGZkYWZhODQxNzg2MjNiNjQ0Zjg0NjQ1OWZkY2I5ODdiY2Y3Y2JjNzRmYjJkMiIsInZlcnNpb24iOjF9.bCZUzJamsozKWehnNph6E5coww5zZTrJdbWevWrSyfT0PyXc_wkZ-NKdyBAoqprBz3_8L3i5hPM6Qsy56b4BDA\n    - type: recall\n      value: 0.712\n      name: Recall Weighted\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMDk1MDJiOGUzZThlZjJjMzY4NjMzODFiZjUzZmIwMjIxY2UwNzBiN2IxMWEwMGJjZTkxODA0YzUxZDE3ODRhOCIsInZlcnNpb24iOjF9.z0dqvB3aBVYt3xRIb_M4svWebfQc0QaDFVFzHnlA5QGEHkHOW3OecGhHE4EzBqTDI3DASWZTGMjrMDDt0uOMBw\n    - type: f1\n      value: 0.7119226991285647\n      name: F1 Macro\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiM2U0YjMwNzhmOTEyNDZhODU3MTU0YTM4MmQ0NzEzNWI1YjY0ZWQ3MWRiMTdiNTUzNWRkZThjMWE4M2NkZmI0MiIsInZlcnNpb24iOjF9.hhj1BXkuWi9wXrCjT9NwqaPETtOoYNiyqYsJEw-ufA8A4hVThKA6ZBtma1Q_M65-DZFfPEBDBNASLZ7EPSbmDw\n    - type: f1\n      value: 0.712\n      name: F1 Micro\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiODk0Y2EyMzc5M2ZlNWFlNDg2Zjc1OTQxNGY3YjA5YjUxYTYzZjRlZmU4ODYxNjA3ZjkxNGUzYjBmNmMxMzY5YiIsInZlcnNpb24iOjF9.DvKk-3hNh2LhN2ug5e0FgUntL3Ozdfl06Kz7jvmB-deOJH6INi2a2ZySXoEePoo8t2nR6ENFYu9QjMA2ojnpCA\n    - type: f1\n      value: 0.7119242267218338\n      name: F1 Weighted\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiN2MxOWFlMmI2NGRiMjkwN2Q5MWZhNDFlYzQxNWNmNzQ3OWYxZThmNDU2OWU1MTE5OGY2MWRlYWUyNDM3OTkzZCIsInZlcnNpb24iOjF9.QrTD1gE8_wRok9u59W-Mx0cX89K-h2Ad6qa8J5rmP8lc_rkG0ft2n5_GqH1CBZBJwMFYv91Pn6TuE3eGxJuUDA\n    - type: loss\n      value: 1.0105403661727905\n      name: loss\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMmUwMTg4NjM3ZTBiZTIyODcyNDNmNTE5ZDZhMzNkMDMyNjcwOGQ5NmY0NTlhMjgyNmIzZjRiNDFiNjA3M2RkZSIsInZlcnNpb24iOjF9.sjBDVJV-jnygwcppmByAXpoo-Wzz178bBzozJEuYEiJaHSbk_xEevfJS1PmLUuplYslKb1iyEctnjI-5bl-XDw\n  - task:\n      type: natural-language-inference\n      name: Natural Language Inference\n    dataset:\n      name: multi_nli\n      type: multi_nli\n      config: default\n      split: validation_mismatched\n    metrics:\n    - type: accuracy\n      value: 0.902766476810415\n      name: Accuracy\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMjExZWM3YzA3ZDNlNjEwMmViNWEwZTE3MjJjNjEyNDhjOTQxNGFmMzBjZTk0ODUwYTc2OGNiZjYyMTBmNWZjZSIsInZlcnNpb24iOjF9.zbFAGrv2flpmweqS7Poxib7qHFLdW8eUTzshdOm2B9H-KWpIZCWC-P4p8TLMdNJnUcZJZ03Okil4qjIMqqIRCA\n    - type: precision\n      value: 0.9023816542652491\n      name: Precision Macro\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiN2U2MGViNmJjNWQxNzRjOTkxNDIxZjZjNmM5YzE4ZjU5NTE5NjFlNmEzZWRlOGYxN2E3NTAwMTEwYjNhNzE0YSIsInZlcnNpb24iOjF9.WJjDJf56FROvf7Y5ShWnnxMvK_ZpQ2PibAOtSFhSiYJ7bt4TGOzMwaZ5RSTf_mcfXgRfWbXmy1jCwNhDb-5EAw\n    - type: precision\n      value: 0.902766476810415\n      name: Precision Micro\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiYzRhZTExOTc5NDczZjI1YmMzOGYyOTU2MDU1OGE5ZTczMDE0MmU0NzZhY2YzMDI1ZGQ3MGM5MmJiODFkNzUzZiIsInZlcnNpb24iOjF9.aRYcGEI1Y8-a0d8XOoXhBgsFyj9LWNwEjoIPc594y7kJn91wXIsXoR0-_0iy3uz41mWaTTlwJx7lI-kipFDvDQ\n    - type: precision\n      value: 0.9034597464719761\n      name: Precision Weighted\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMWQyMTZiZDA2OTUwZjRmNTFiMWRlZTNmOTliZmI2MWFmMjdjYzEyYTgwNzkyOTQzOTBmNTUyYjMwNTUxMTFkNiIsInZlcnNpb24iOjF9.hUtAMTl0THHUkaLcgk1Vy9IhjqJAXCJ_5STJ5A7k7s_SO9DHp3b6qusgwPmcGLYyPy1-j1dB2AIstxK4tHfmDA\n    - type: recall\n      value: 0.9024304801555488\n      name: Recall Macro\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMzAxZGJhNGI3ZDNlMjg2ZDIxNTgwMDY5MTFjM2ExZmIxMDBmZjUyNTliNWNkOGI0OTY3NTYyNWU3OWFlYTA3YiIsInZlcnNpb24iOjF9.1o_GNq8zmXa_50MUF_K63IDc2aUKNeUkNQ5fT592-SAo8WgiaP9Dh6bOEu2OqrpRQ57P4qm7OdJt7UKsrosMDA\n    - type: recall\n      value: 0.902766476810415\n      name: Recall Micro\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZjhiMWE4Yjk0ODFkZjlkYjRlMjU1OTJmMjA2Njg1N2M4MzQ0OWE3N2FlYjY4NDgxZThjMmExYWQ5OGNmYmI1NSIsInZlcnNpb24iOjF9.Gmm5lf_qpxjXWWrycDze7LHR-6WGQc62WZTmcoc5uxWd0tivEUqCAFzFdbEU1jVKxQBIyDX77CPuBm7mUA4sCg\n    - type: recall\n      value: 0.902766476810415\n      name: Recall Weighted\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiY2EzZWYwNjNkYWE1YTcyZGZjNTNhMmNlNzgzYjk5MGJjOWJmZmE5NmYwM2U2NTA5ZDY3ZjFiMmRmZmQwY2QwYiIsInZlcnNpb24iOjF9.yA68rslg3e9kUR3rFTNJJTAad6Usr4uFmJvE_a7G2IvSKqLxG_pqsHszsWfg5mFBQLjWEAyCtdQYMdVayuYMBA\n    - type: f1\n      value: 0.9023086094638595\n      name: F1 Macro\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMzMyMzZhNjI5MWRmZWJhMjkzN2E0MjM4ZTM5YzZmNTk5YTZmYzU4NDRiYjczZGQ4MDdhNjJiMGU0MjE3NDEwNyIsInZlcnNpb24iOjF9.RCMqH_xUMN97Vos54pTFfAMbLstXUMdFTs-eNaypbDb_Fc-MW8NLmJ6dzJsp9sSvhXyYjugjRMUpMpnQseKXDA\n    - type: f1\n      value: 0.902766476810415\n      name: F1 Micro\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZTYxZTZhZGM0NThlNTAzNmYwMTA4NDNkN2FiNzhhN2RlYThlYjcxMjE5MjBkMzhiOGYxZGRmMjE0NGM2ZWQ5ZSIsInZlcnNpb24iOjF9.wRfllNw2Gibmi1keU7d_GjkyO0F9HESCgJlJ9PHGZQRRT414nnB-DyRvulHjCNnaNjXqMi0LJimC3iBrNawwAw\n    - type: f1\n      value: 0.9030161011457231\n      name: F1 Weighted\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiNDA0YjAxMWU5MjI4MWEzNTNjMzJlNjM3ZDMxOTE0ZTZhYmZlNmUyNDViNTU2NmMyMmM3MjAxZWVjNWJmZjI4MCIsInZlcnNpb24iOjF9.vJ8aUjfTbFMc1BgNUVpoVDuYwQJYQjwZQxblkUdvSoGtkW_AzQJ_KJ8Njc7IBA3ADgj8iZHjRQNIZkFCf-xICw\n    - type: loss\n      value: 0.3283354640007019\n      name: loss\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiODdmYzYzNTUzZDNmOWIxM2E0ZmUyOWUzM2Y2NGRmZDNiYjg3ZTMzYTUyNzg3OWEzNzYyN2IyNmExOGRlMWUxYSIsInZlcnNpb24iOjF9.Qv0FzFZPkcBs9aHGf4TEREX4jdkc40NazdMlP2M_-w2wHwyjoAjvhk611RLXHcbicozNelZJLnsOMdEMnPLEDg\n  - task:\n      type: natural-language-inference\n      name: Natural Language Inference\n    dataset:\n      name: anli\n      type: anli\n      config: plain_text\n      split: dev_r1\n    metrics:\n    - type: accuracy\n      value: 0.737\n      name: Accuracy\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMTQ1ZGVkOTVmNTlhYjhkMjVlNTNhMjNmZWFjZWZjZjcxZmRhMDVlOWI0YTdkOTMwYjVjNWFlOGY4OTc1MmRhNiIsInZlcnNpb24iOjF9.wGLgKA1E46ljbLokdPeip_UCr1gqK8iSSbsJKX2vgKuuhDdUWWiECrUFN-bv_78JWKoKW5T0GF_hb-RVDzA0AQ\n    - type: precision\n      value: 0.737681071614645\n      name: Precision Macro\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiYmFkMGUwMjNhN2E3NzMxNTc5NDM0MjY1MGU5ODllM2Q2YzA1MDI3OGI1ZmI4YTcxN2E4ZDk5OWY2OGNiN2I0MCIsInZlcnNpb24iOjF9.6G5qhccjheaNfasgRyrkKBTaQPRzuPMZZ0hrLxTNzAydMDgx09FkFP3hni7WLRMWp0IpwzkEeBlxV-mPyQBtBw\n    - type: precision\n      value: 0.737\n      name: Precision Micro\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiN2QzYjQ4ZDZjOGU5YzI3YmFlMThlYTRkYTUyYWIyNzc4NDkwNzM1OWFiMTgyMzA0NDZmMGI3YTQxODBjM2EwMCIsInZlcnNpb24iOjF9.bvNWyzfct1CLJFx_EuD2GeKieVtyGJy0cwUBP2qJE1ey2i9SVn6n1Dr0AALTGBkxQ6n5-fJ61QFNufpdr2KvCA\n    - type: precision\n      value: 0.7376755842752241\n      name: Precision Weighted\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiN2VmYWYzZWQwZmMzMDk0NTdlY2Y3NDkzYWY5ZTdmOGU0ZTUzZWE4YWFhZjVmODhkZmE1Njg4NjA5YjJmYWVhOSIsInZlcnNpb24iOjF9.50FQR2aoBpORLgYa7482ZTrRhT-KfIgv5ltBEHndUBMmqGF9Ru0LHENSGwyD_tO89sGPfiW32TxpbrNWiBdIBA\n    - type: recall\n      value: 0.7369675064285843\n      name: Recall Macro\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZTM4OTAyNDYwNjY4Zjc5NDljNjBmNTg2Mzk4YjYxM2MyYTA0MDllYTMyNzEwOGI1ZTEwYWE3ZmU0NDZmZDg2NiIsInZlcnNpb24iOjF9.UvWBxuApNV3vd4hpgwqd6XPHCbkA_bB_Cw24ooquiOf0dstvjP3JvpGoDp5SniOzIOg3i2aYbcvFCLJqEXMZCQ\n    - type: recall\n      value: 0.737\n      name: Recall Micro\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiYmQ4MjMzNzRmNTI5NjIzNGQ0ZDFmZTA1MDU3OTk0MzYyMGI0NTMzZTZlMTQ1MDc1MzBkMGMzYjcxZjU1NDNjOSIsInZlcnNpb24iOjF9.kpbdXOpDG3CUB-kUEXsgFT3HWWIbu70wwzs2TNf0rhIuRrzdZz3dXXvwqu1BcLJTsOxl8G6NTiYXgnv-ul8lDg\n    - type: recall\n      value: 0.737\n      name: Recall Weighted\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiNmU1ZWJkNWE0NjczY2NiZWYyNzYyMzllNzZmZTIxNWRkYTEyZDgxN2E0NTNmM2ExMTc1ZWVjMzBiYjg0ZmM1MiIsInZlcnNpb24iOjF9.S6HHWCWnut_LJqXbEA_Z8ZOTtyq6V51ZeiA0qbwzr0hapDYZOZHrN4prvSLvoNv-GiYDYKatwIsAZxCZc5fmCA\n    - type: f1\n      value: 0.7366853496239583\n      name: F1 Macro\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiNzkxYmY2NTcyOTE0ZDdjNGY2ZmE4MzQwMGIxZTA2MDg1NzI5YTQ0MTdkZjdkNzNkMDM2NTk2MTNiNjU4ODMwZCIsInZlcnNpb24iOjF9.ECVaCBqGd0pnQT3xJF7yWrgecIb-5TMiVWpEO0MQGhYy43snkI6Qs-2FOXzvfwIWqG-Q6XIIhGbWZh5TFEGKCA\n    - type: f1\n      value: 0.737\n      name: F1 Micro\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiNDMwMWZiNzQyNWEzNmMzMDJjOTAxYzAxNzc0MTNlYzRkZjllYmNjZmU0OTgzZDFkNWM1ZWI5OTA2NzE5Y2YxOSIsInZlcnNpb24iOjF9.8yZFol_Gcj9n3w9Yk5wx48yql7p3wriDecv-6VSTAB6Q_MWLQAWsCEGRRhgGJ3zvhoRehJZdb35ozk36VOinDQ\n    - type: f1\n      value: 0.7366990292378379\n      name: F1 Weighted\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMjhhN2ZkMjc5ZGQ3ZGM1Nzk3ZTgwY2E1N2NjYjdhNjZlOTdhYmRlNGVjN2EwNTIzN2UyYTY2ODVlODhmY2Q4ZCIsInZlcnNpb24iOjF9.Cz7ClDAfCGpqdRTYd5v3dPjXFq8lZLXx8AX_rqmF-Jb8KocqVDsHWeZScW5I2oy951UrdMpiUOLieBuJLOmCCQ\n    - type: loss\n      value: 0.9349392056465149\n      name: loss\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiNmI4MTI5MDM1NjBmMzgzMzc2NjM5MzZhOGUyNTgyY2RlZTEyYTIzYzY2ZGJmODcxY2Q5OTVjOWU3OTQ2MzM1NSIsInZlcnNpb24iOjF9.bSOFnYC4Y2y2pW1AR-bgPUHKafR-0OHf8PvexK8eQLsS323Xy9-rYkKUaP09KY6_fk9GqAawv5eqj72B_uyeCA\n---\n# DeBERTa-v3-base-mnli-fever-anli\n## Model description\nThis model was trained on the MultiNLI, Fever-NLI and Adversarial-NLI (ANLI) datasets, which comprise 763 913 NLI hypothesis-premise pairs. This base model outperforms almost all large models on the [ANLI benchmark](https://github.com/facebookresearch/anli). \nThe base model is [DeBERTa-v3-base from Microsoft](https://huggingface.co/microsoft/deberta-v3-base). The v3 variant of DeBERTa substantially outperforms previous versions of the model by including a different pre-training objective, see annex 11 of the original [DeBERTa paper](https://arxiv.org/pdf/2006.03654.pdf). \n\nFor highest performance (but less speed), I recommend using https://huggingface.co/MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli. \n\n\n### How to use the model\n#### Simple zero-shot classification pipeline\n```python\n#!pip install transformers[sentencepiece]\nfrom transformers import pipeline\nclassifier = pipeline(\"zero-shot-classification\", model=\"MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli\")\nsequence_to_classify = \"Angela Merkel is a politician in Germany and leader of the CDU\"\ncandidate_labels = [\"politics\", \"economy\", \"entertainment\", \"environment\"]\noutput = classifier(sequence_to_classify, candidate_labels, multi_label=False)\nprint(output)\n```\n#### NLI use-case\n```python\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n\nmodel_name = \"MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name)\n\npremise = \"I first thought that I liked the movie, but upon second thought it was actually disappointing.\"\nhypothesis = \"The movie was good.\"\n\ninput = tokenizer(premise, hypothesis, truncation=True, return_tensors=\"pt\")\noutput = model(input[\"input_ids\"].to(device))  # device = \"cuda:0\" or \"cpu\"\nprediction = torch.softmax(output[\"logits\"][0], -1).tolist()\nlabel_names = [\"entailment\", \"neutral\", \"contradiction\"]\nprediction = {name: round(float(pred) * 100, 1) for pred, name in zip(prediction, label_names)}\nprint(prediction)\n```\n### Training data\nDeBERTa-v3-base-mnli-fever-anli was trained on the MultiNLI, Fever-NLI and Adversarial-NLI (ANLI) datasets, which comprise 763 913 NLI hypothesis-premise pairs.\n\n### Training procedure\nDeBERTa-v3-base-mnli-fever-anli was trained using the Hugging Face trainer with the following hyperparameters.\n```\ntraining_args = TrainingArguments(\n    num_train_epochs=3,              # total number of training epochs\n    learning_rate=2e-05,\n    per_device_train_batch_size=32,   # batch size per device during training\n    per_device_eval_batch_size=32,    # batch size for evaluation\n    warmup_ratio=0.1,                # number of warmup steps for learning rate scheduler\n    weight_decay=0.06,               # strength of weight decay\n    fp16=True                        # mixed precision training\n)\n```\n### Eval results\nThe model was evaluated using the test sets for MultiNLI and ANLI and the dev set for Fever-NLI. The metric used is accuracy.\n\nmnli-m | mnli-mm | fever-nli | anli-all | anli-r3\n---------|----------|---------|----------|----------\n0.903 | 0.903 | 0.777 | 0.579 | 0.495\n\n## Limitations and bias\nPlease consult the original DeBERTa paper and literature on different NLI datasets for potential biases. \n\n## Citation\nIf you use this model, please cite: Laurer, Moritz, Wouter van Atteveldt, Andreu Salleras Casas, and Kasper Welbers. 2022. \u2018Less Annotating, More Classifying \u2013 Addressing the Data Scarcity Issue of Supervised Machine Learning with Deep Transfer Learning and BERT - NLI\u2019. Preprint, June. Open Science Framework. https://osf.io/74b8k.\n\n### Ideas for cooperation or questions?\nIf you have questions or ideas for cooperation, contact me at m{dot}laurer{at}vu{dot}nl or [LinkedIn](https://www.linkedin.com/in/moritz-laurer/)\n\n### Debugging and issues\nNote that DeBERTa-v3 was released on 06.12.21 and older versions of HF Transformers seem to have issues running the model (e.g. resulting in an issue with the tokenizer). Using Transformers>=4.13 might solve some issues. \nAlso make sure to install sentencepiece to avoid tokenizer errors. Run:  `pip install transformers[sentencepiece]` or `pip install sentencepiece`\n\n\n## Model Recycling\n\n[Evaluation on 36 datasets](https://ibm.github.io/model-recycling/model_gain_chart?avg=0.65&mnli_lp=nan&20_newsgroup=-0.61&ag_news=-0.01&amazon_reviews_multi=0.46&anli=0.84&boolq=2.12&cb=16.07&cola=-0.76&copa=8.60&dbpedia=-0.40&esnli=-0.29&financial_phrasebank=-1.98&imdb=-0.47&isear=-0.22&mnli=-0.21&mrpc=0.50&multirc=1.91&poem_sentiment=1.73&qnli=0.07&qqp=-0.37&rotten_tomatoes=-0.74&rte=3.94&sst2=-0.45&sst_5bins=0.07&stsb=1.27&trec_coarse=-0.16&trec_fine=0.18&tweet_ev_emoji=-0.93&tweet_ev_emotion=-1.33&tweet_ev_hate=-1.67&tweet_ev_irony=-5.46&tweet_ev_offensive=-0.17&tweet_ev_sentiment=-0.11&wic=-0.21&wnli=-1.20&wsc=4.18&yahoo_answers=-0.70&model_name=MoritzLaurer%2FDeBERTa-v3-base-mnli-fever-anli&base_name=microsoft%2Fdeberta-v3-base) using MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli as a base model yields average score of 79.69 in comparison to 79.04 by microsoft/deberta-v3-base.\n\nThe model is ranked 2nd among all tested models for the microsoft/deberta-v3-base architecture as of 09/01/2023.\n\nResults:\n\n|   20_newsgroup |   ag_news |   amazon_reviews_multi |   anli |   boolq |      cb |    cola |   copa |   dbpedia |   esnli |   financial_phrasebank |   imdb |   isear |    mnli |    mrpc |   multirc |   poem_sentiment |   qnli |     qqp |   rotten_tomatoes |     rte |    sst2 |   sst_5bins |    stsb |   trec_coarse |   trec_fine |   tweet_ev_emoji |   tweet_ev_emotion |   tweet_ev_hate |   tweet_ev_irony |   tweet_ev_offensive |   tweet_ev_sentiment |     wic |    wnli |     wsc |   yahoo_answers |\n|---------------:|----------:|-----------------------:|-------:|--------:|--------:|--------:|-------:|----------:|--------:|-----------------------:|-------:|--------:|--------:|--------:|----------:|-----------------:|-------:|--------:|------------------:|--------:|--------:|------------:|--------:|--------------:|------------:|-----------------:|-------------------:|----------------:|-----------------:|---------------------:|---------------------:|--------:|--------:|--------:|----------------:|\n|        85.8072 |   90.4333 |                  67.32 | 59.625 |  85.107 | 91.0714 | 85.8102 |     67 |   79.0333 | 91.6327 |                   82.5 |  94.02 | 71.6428 | 89.5749 | 89.7059 |   64.1708 |          88.4615 | 93.575 | 91.4148 |           89.6811 | 86.2816 | 94.6101 |     57.0588 | 91.5508 |          97.6 |        91.2 |           45.264 |            82.6179 |         54.5455 |          74.3622 |              84.8837 |              71.6949 | 71.0031 | 69.0141 | 68.2692 |         71.3333 |\n\n\nFor more information, see: [Model Recycling](https://ibm.github.io/model-recycling/)\n",
    "library_name": "transformers"
  },
  {
    "model_id": "facebook/bart-large-mnli",
    "model_name": "facebook/bart-large-mnli",
    "author": "facebook",
    "downloads": 2711103,
    "likes": 1337,
    "tags": [
      "transformers",
      "pytorch",
      "jax",
      "rust",
      "safetensors",
      "bart",
      "text-classification",
      "zero-shot-classification",
      "dataset:multi_nli",
      "arxiv:1910.13461",
      "arxiv:1909.00161",
      "license:mit",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/facebook/bart-large-mnli",
    "dependencies": [
      [
        "transformers",
        "4.30.2"
      ],
      [
        "torch",
        "2.0.1"
      ],
      [
        "numpy",
        "1.24.3"
      ],
      [
        "hf_xet",
        "0.1.0"
      ]
    ],
    "analysis_date": "2025-03-26T00:47:17.893293",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "bart",
    "pipeline_tag": "zero-shot-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "license": "mit",
      "datasets": [
        "multi_nli"
      ],
      "thumbnail": "https://huggingface.co/front/thumbnails/facebook.png",
      "pipeline_tag": "zero-shot-classification"
    },
    "card_text": "\n# bart-large-mnli\n\nThis is the checkpoint for [bart-large](https://huggingface.co/facebook/bart-large) after being trained on the [MultiNLI (MNLI)](https://huggingface.co/datasets/multi_nli) dataset.\n\nAdditional information about this model:\n- The [bart-large](https://huggingface.co/facebook/bart-large) model page\n- [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension\n](https://arxiv.org/abs/1910.13461)\n- [BART fairseq implementation](https://github.com/pytorch/fairseq/tree/master/fairseq/models/bart)\n\n## NLI-based Zero Shot Text Classification\n\n[Yin et al.](https://arxiv.org/abs/1909.00161) proposed a method for using pre-trained NLI models as a ready-made zero-shot sequence classifiers. The method works by posing the sequence to be classified as the NLI premise and to construct a hypothesis from each candidate label. For example, if we want to evaluate whether a sequence belongs to the class \"politics\", we could construct a hypothesis of `This text is about politics.`. The probabilities for entailment and contradiction are then converted to label probabilities.\n\nThis method is surprisingly effective in many cases, particularly when used with larger pre-trained models like BART and Roberta. See [this blog post](https://joeddav.github.io/blog/2020/05/29/ZSL.html) for a more expansive introduction to this and other zero shot methods, and see the code snippets below for examples of using this model for zero-shot classification both with Hugging Face's built-in pipeline and with native Transformers/PyTorch code.\n\n#### With the zero-shot classification pipeline\n\nThe model can be loaded with the `zero-shot-classification` pipeline like so:\n\n```python\nfrom transformers import pipeline\nclassifier = pipeline(\"zero-shot-classification\",\n                      model=\"facebook/bart-large-mnli\")\n```\n\nYou can then use this pipeline to classify sequences into any of the class names you specify.\n\n```python\nsequence_to_classify = \"one day I will see the world\"\ncandidate_labels = ['travel', 'cooking', 'dancing']\nclassifier(sequence_to_classify, candidate_labels)\n#{'labels': ['travel', 'dancing', 'cooking'],\n# 'scores': [0.9938651323318481, 0.0032737774308770895, 0.002861034357920289],\n# 'sequence': 'one day I will see the world'}\n```\n\nIf more than one candidate label can be correct, pass `multi_label=True` to calculate each class independently:\n\n```python\ncandidate_labels = ['travel', 'cooking', 'dancing', 'exploration']\nclassifier(sequence_to_classify, candidate_labels, multi_label=True)\n#{'labels': ['travel', 'exploration', 'dancing', 'cooking'],\n# 'scores': [0.9945111274719238,\n#  0.9383890628814697,\n#  0.0057061901316046715,\n#  0.0018193122232332826],\n# 'sequence': 'one day I will see the world'}\n```\n\n\n#### With manual PyTorch\n\n```python\n# pose sequence as a NLI premise and label as a hypothesis\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nnli_model = AutoModelForSequenceClassification.from_pretrained('facebook/bart-large-mnli')\ntokenizer = AutoTokenizer.from_pretrained('facebook/bart-large-mnli')\n\npremise = sequence\nhypothesis = f'This example is {label}.'\n\n# run through model pre-trained on MNLI\nx = tokenizer.encode(premise, hypothesis, return_tensors='pt',\n                     truncation_strategy='only_first')\nlogits = nli_model(x.to(device))[0]\n\n# we throw away \"neutral\" (dim 1) and take the probability of\n# \"entailment\" (2) as the probability of the label being true \nentail_contradiction_logits = logits[:,[0,2]]\nprobs = entail_contradiction_logits.softmax(dim=1)\nprob_label_is_true = probs[:,1]\n```\n",
    "card_content": "---\nlicense: mit\ndatasets:\n- multi_nli\nthumbnail: https://huggingface.co/front/thumbnails/facebook.png\npipeline_tag: zero-shot-classification\n---\n\n# bart-large-mnli\n\nThis is the checkpoint for [bart-large](https://huggingface.co/facebook/bart-large) after being trained on the [MultiNLI (MNLI)](https://huggingface.co/datasets/multi_nli) dataset.\n\nAdditional information about this model:\n- The [bart-large](https://huggingface.co/facebook/bart-large) model page\n- [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension\n](https://arxiv.org/abs/1910.13461)\n- [BART fairseq implementation](https://github.com/pytorch/fairseq/tree/master/fairseq/models/bart)\n\n## NLI-based Zero Shot Text Classification\n\n[Yin et al.](https://arxiv.org/abs/1909.00161) proposed a method for using pre-trained NLI models as a ready-made zero-shot sequence classifiers. The method works by posing the sequence to be classified as the NLI premise and to construct a hypothesis from each candidate label. For example, if we want to evaluate whether a sequence belongs to the class \"politics\", we could construct a hypothesis of `This text is about politics.`. The probabilities for entailment and contradiction are then converted to label probabilities.\n\nThis method is surprisingly effective in many cases, particularly when used with larger pre-trained models like BART and Roberta. See [this blog post](https://joeddav.github.io/blog/2020/05/29/ZSL.html) for a more expansive introduction to this and other zero shot methods, and see the code snippets below for examples of using this model for zero-shot classification both with Hugging Face's built-in pipeline and with native Transformers/PyTorch code.\n\n#### With the zero-shot classification pipeline\n\nThe model can be loaded with the `zero-shot-classification` pipeline like so:\n\n```python\nfrom transformers import pipeline\nclassifier = pipeline(\"zero-shot-classification\",\n                      model=\"facebook/bart-large-mnli\")\n```\n\nYou can then use this pipeline to classify sequences into any of the class names you specify.\n\n```python\nsequence_to_classify = \"one day I will see the world\"\ncandidate_labels = ['travel', 'cooking', 'dancing']\nclassifier(sequence_to_classify, candidate_labels)\n#{'labels': ['travel', 'dancing', 'cooking'],\n# 'scores': [0.9938651323318481, 0.0032737774308770895, 0.002861034357920289],\n# 'sequence': 'one day I will see the world'}\n```\n\nIf more than one candidate label can be correct, pass `multi_label=True` to calculate each class independently:\n\n```python\ncandidate_labels = ['travel', 'cooking', 'dancing', 'exploration']\nclassifier(sequence_to_classify, candidate_labels, multi_label=True)\n#{'labels': ['travel', 'exploration', 'dancing', 'cooking'],\n# 'scores': [0.9945111274719238,\n#  0.9383890628814697,\n#  0.0057061901316046715,\n#  0.0018193122232332826],\n# 'sequence': 'one day I will see the world'}\n```\n\n\n#### With manual PyTorch\n\n```python\n# pose sequence as a NLI premise and label as a hypothesis\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nnli_model = AutoModelForSequenceClassification.from_pretrained('facebook/bart-large-mnli')\ntokenizer = AutoTokenizer.from_pretrained('facebook/bart-large-mnli')\n\npremise = sequence\nhypothesis = f'This example is {label}.'\n\n# run through model pre-trained on MNLI\nx = tokenizer.encode(premise, hypothesis, return_tensors='pt',\n                     truncation_strategy='only_first')\nlogits = nli_model(x.to(device))[0]\n\n# we throw away \"neutral\" (dim 1) and take the probability of\n# \"entailment\" (2) as the probability of the label being true \nentail_contradiction_logits = logits[:,[0,2]]\nprobs = entail_contradiction_logits.softmax(dim=1)\nprob_label_is_true = probs[:,1]\n```\n",
    "library_name": "transformers"
  },
  {
    "model_id": "ProsusAI/finbert",
    "model_name": "ProsusAI/finbert",
    "author": "ProsusAI",
    "downloads": 2708284,
    "likes": 831,
    "tags": [
      "transformers",
      "pytorch",
      "tf",
      "jax",
      "bert",
      "text-classification",
      "financial-sentiment-analysis",
      "sentiment-analysis",
      "en",
      "arxiv:1908.10063",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/ProsusAI/finbert",
    "dependencies": [
      [
        "transformers",
        "4.30.2"
      ],
      [
        "torch",
        "2.0.1"
      ],
      [
        "numpy",
        "1.24.3"
      ],
      [
        "xformers",
        "0.0.20"
      ],
      [
        "hf_xet",
        "0.1.0"
      ]
    ],
    "analysis_date": "2025-03-26T00:47:18.919330",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "bert",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": "en",
      "tags": [
        "financial-sentiment-analysis",
        "sentiment-analysis"
      ],
      "widget": [
        {
          "text": "Stocks rallied and the British pound gained."
        }
      ]
    },
    "card_text": "\nFinBERT is a pre-trained NLP model to analyze sentiment of financial text. It is built by further training the BERT language model in the finance domain, using a large financial corpus and thereby fine-tuning it for financial sentiment classification. [Financial PhraseBank](https://www.researchgate.net/publication/251231107_Good_Debt_or_Bad_Debt_Detecting_Semantic_Orientations_in_Economic_Texts) by Malo et al. (2014) is used for fine-tuning. For more details, please see the paper [FinBERT: Financial Sentiment Analysis with Pre-trained Language Models](https://arxiv.org/abs/1908.10063) and our related [blog post](https://medium.com/prosus-ai-tech-blog/finbert-financial-sentiment-analysis-with-bert-b277a3607101) on Medium.\n\nThe model will give softmax outputs for three labels: positive, negative or neutral.\n\n---\n\nAbout Prosus\n\nProsus is a global consumer internet group and one of the largest technology investors in the world. Operating and investing globally in markets with long-term growth potential, Prosus builds leading consumer internet companies that empower people and enrich communities. For more information, please visit www.prosus.com.\n\nContact information\n\nPlease contact Dogu Araci dogu.araci[at]prosus[dot]com and Zulkuf Genc zulkuf.genc[at]prosus[dot]com about any FinBERT related issues and questions.\n",
    "card_content": "---\nlanguage: en\ntags:\n- financial-sentiment-analysis\n- sentiment-analysis\nwidget:\n- text: Stocks rallied and the British pound gained.\n---\n\nFinBERT is a pre-trained NLP model to analyze sentiment of financial text. It is built by further training the BERT language model in the finance domain, using a large financial corpus and thereby fine-tuning it for financial sentiment classification. [Financial PhraseBank](https://www.researchgate.net/publication/251231107_Good_Debt_or_Bad_Debt_Detecting_Semantic_Orientations_in_Economic_Texts) by Malo et al. (2014) is used for fine-tuning. For more details, please see the paper [FinBERT: Financial Sentiment Analysis with Pre-trained Language Models](https://arxiv.org/abs/1908.10063) and our related [blog post](https://medium.com/prosus-ai-tech-blog/finbert-financial-sentiment-analysis-with-bert-b277a3607101) on Medium.\n\nThe model will give softmax outputs for three labels: positive, negative or neutral.\n\n---\n\nAbout Prosus\n\nProsus is a global consumer internet group and one of the largest technology investors in the world. Operating and investing globally in markets with long-term growth potential, Prosus builds leading consumer internet companies that empower people and enrich communities. For more information, please visit www.prosus.com.\n\nContact information\n\nPlease contact Dogu Araci dogu.araci[at]prosus[dot]com and Zulkuf Genc zulkuf.genc[at]prosus[dot]com about any FinBERT related issues and questions.\n",
    "library_name": "transformers"
  },
  {
    "model_id": "microsoft/deberta-xlarge-mnli",
    "model_name": "microsoft/deberta-xlarge-mnli",
    "author": "microsoft",
    "downloads": 2457798,
    "likes": 17,
    "tags": [
      "transformers",
      "pytorch",
      "tf",
      "deberta",
      "text-classification",
      "deberta-v1",
      "deberta-mnli",
      "en",
      "arxiv:2006.03654",
      "license:mit",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/microsoft/deberta-xlarge-mnli",
    "dependencies": [
      [
        "transformers",
        "4.30.2"
      ],
      [
        "torch",
        "2.0.1"
      ],
      [
        "numpy",
        "1.24.3"
      ]
    ],
    "analysis_date": "2025-03-26T00:47:20.264705",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "deberta",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": "en",
      "license": "mit",
      "tags": [
        "deberta-v1",
        "deberta-mnli"
      ],
      "tasks": "mnli",
      "thumbnail": "https://huggingface.co/front/thumbnails/microsoft.png",
      "widget": [
        {
          "text": "[CLS] I love you. [SEP] I like you. [SEP]"
        }
      ]
    },
    "card_text": "\n## DeBERTa: Decoding-enhanced BERT with Disentangled Attention\n\n[DeBERTa](https://arxiv.org/abs/2006.03654) improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder. It outperforms BERT and RoBERTa on  majority of NLU tasks with 80GB training data. \n\nPlease check the [official repository](https://github.com/microsoft/DeBERTa) for more details and updates.\n\nThis the DeBERTa xlarge model(750M) fine-tuned with mnli task.\n\n### Fine-tuning on NLU tasks\n\nWe present the dev results on SQuAD 1.1/2.0 and several GLUE benchmark tasks.\n\n| Model                     | SQuAD 1.1 | SQuAD 2.0 | MNLI-m/mm   | SST-2 | QNLI | CoLA | RTE    | MRPC  | QQP   |STS-B |\n|---------------------------|-----------|-----------|-------------|-------|------|------|--------|-------|-------|------|\n|                           | F1/EM     | F1/EM     | Acc         | Acc   | Acc  | MCC  | Acc    |Acc/F1 |Acc/F1 |P/S   |\n| BERT-Large                | 90.9/84.1 | 81.8/79.0 | 86.6/-      | 93.2  | 92.3 | 60.6 | 70.4   | 88.0/-       | 91.3/- |90.0/- |\n| RoBERTa-Large             | 94.6/88.9 | 89.4/86.5 | 90.2/-      | 96.4  | 93.9 | 68.0 | 86.6   | 90.9/-       | 92.2/- |92.4/- |\n| XLNet-Large               | 95.1/89.7 | 90.6/87.9 | 90.8/-      | 97.0  | 94.9 | 69.0 | 85.9   | 90.8/-       | 92.3/- |92.5/- |\n| [DeBERTa-Large](https://huggingface.co/microsoft/deberta-large)<sup>1</sup> | 95.5/90.1 | 90.7/88.0 | 91.3/91.1| 96.5|95.3| 69.5| 91.0| 92.6/94.6| 92.3/- |92.8/92.5 |\n| [DeBERTa-XLarge](https://huggingface.co/microsoft/deberta-xlarge)<sup>1</sup> | -/-  | -/-  | 91.5/91.2| 97.0 | - | -    | 93.1   | 92.1/94.3    | -    |92.9/92.7|\n| [DeBERTa-V2-XLarge](https://huggingface.co/microsoft/deberta-v2-xlarge)<sup>1</sup>|95.8/90.8| 91.4/88.9|91.7/91.6| **97.5**| 95.8|71.1|**93.9**|92.0/94.2|92.3/89.8|92.9/92.9|\n|**[DeBERTa-V2-XXLarge](https://huggingface.co/microsoft/deberta-v2-xxlarge)<sup>1,2</sup>**|**96.1/91.4**|**92.2/89.7**|**91.7/91.9**|97.2|**96.0**|**72.0**| 93.5| **93.1/94.9**|**92.7/90.3** |**93.2/93.1** |\n--------\n#### Notes.\n - <sup>1</sup> Following RoBERTa, for RTE, MRPC, STS-B, we fine-tune the tasks based on [DeBERTa-Large-MNLI](https://huggingface.co/microsoft/deberta-large-mnli), [DeBERTa-XLarge-MNLI](https://huggingface.co/microsoft/deberta-xlarge-mnli), [DeBERTa-V2-XLarge-MNLI](https://huggingface.co/microsoft/deberta-v2-xlarge-mnli), [DeBERTa-V2-XXLarge-MNLI](https://huggingface.co/microsoft/deberta-v2-xxlarge-mnli). The results of SST-2/QQP/QNLI/SQuADv2 will also be slightly improved when start from MNLI fine-tuned models, however, we only report the numbers fine-tuned from pretrained base models for those 4 tasks.\n - <sup>2</sup> To try the **XXLarge** model with **[HF transformers](https://huggingface.co/transformers/main_classes/trainer.html)**, you need to specify **--sharded_ddp**\n \n```bash  \ncd transformers/examples/text-classification/\nexport TASK_NAME=mrpc\npython -m torch.distributed.launch --nproc_per_node=8 run_glue.py   --model_name_or_path microsoft/deberta-v2-xxlarge   \\\\\n--task_name $TASK_NAME   --do_train   --do_eval   --max_seq_length 128   --per_device_train_batch_size 4   \\\\\n--learning_rate 3e-6   --num_train_epochs 3   --output_dir /tmp/$TASK_NAME/ --overwrite_output_dir --sharded_ddp --fp16\n```\n\n### Citation\n\nIf you find DeBERTa useful for your work, please cite the following paper:\n\n``` latex\n@inproceedings{\nhe2021deberta,\ntitle={DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION},\nauthor={Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=XPZIaotutsD}\n}\n```\n",
    "card_content": "---\nlanguage: en\nlicense: mit\ntags:\n- deberta-v1\n- deberta-mnli\ntasks: mnli\nthumbnail: https://huggingface.co/front/thumbnails/microsoft.png\nwidget:\n- text: '[CLS] I love you. [SEP] I like you. [SEP]'\n---\n\n## DeBERTa: Decoding-enhanced BERT with Disentangled Attention\n\n[DeBERTa](https://arxiv.org/abs/2006.03654) improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder. It outperforms BERT and RoBERTa on  majority of NLU tasks with 80GB training data. \n\nPlease check the [official repository](https://github.com/microsoft/DeBERTa) for more details and updates.\n\nThis the DeBERTa xlarge model(750M) fine-tuned with mnli task.\n\n### Fine-tuning on NLU tasks\n\nWe present the dev results on SQuAD 1.1/2.0 and several GLUE benchmark tasks.\n\n| Model                     | SQuAD 1.1 | SQuAD 2.0 | MNLI-m/mm   | SST-2 | QNLI | CoLA | RTE    | MRPC  | QQP   |STS-B |\n|---------------------------|-----------|-----------|-------------|-------|------|------|--------|-------|-------|------|\n|                           | F1/EM     | F1/EM     | Acc         | Acc   | Acc  | MCC  | Acc    |Acc/F1 |Acc/F1 |P/S   |\n| BERT-Large                | 90.9/84.1 | 81.8/79.0 | 86.6/-      | 93.2  | 92.3 | 60.6 | 70.4   | 88.0/-       | 91.3/- |90.0/- |\n| RoBERTa-Large             | 94.6/88.9 | 89.4/86.5 | 90.2/-      | 96.4  | 93.9 | 68.0 | 86.6   | 90.9/-       | 92.2/- |92.4/- |\n| XLNet-Large               | 95.1/89.7 | 90.6/87.9 | 90.8/-      | 97.0  | 94.9 | 69.0 | 85.9   | 90.8/-       | 92.3/- |92.5/- |\n| [DeBERTa-Large](https://huggingface.co/microsoft/deberta-large)<sup>1</sup> | 95.5/90.1 | 90.7/88.0 | 91.3/91.1| 96.5|95.3| 69.5| 91.0| 92.6/94.6| 92.3/- |92.8/92.5 |\n| [DeBERTa-XLarge](https://huggingface.co/microsoft/deberta-xlarge)<sup>1</sup> | -/-  | -/-  | 91.5/91.2| 97.0 | - | -    | 93.1   | 92.1/94.3    | -    |92.9/92.7|\n| [DeBERTa-V2-XLarge](https://huggingface.co/microsoft/deberta-v2-xlarge)<sup>1</sup>|95.8/90.8| 91.4/88.9|91.7/91.6| **97.5**| 95.8|71.1|**93.9**|92.0/94.2|92.3/89.8|92.9/92.9|\n|**[DeBERTa-V2-XXLarge](https://huggingface.co/microsoft/deberta-v2-xxlarge)<sup>1,2</sup>**|**96.1/91.4**|**92.2/89.7**|**91.7/91.9**|97.2|**96.0**|**72.0**| 93.5| **93.1/94.9**|**92.7/90.3** |**93.2/93.1** |\n--------\n#### Notes.\n - <sup>1</sup> Following RoBERTa, for RTE, MRPC, STS-B, we fine-tune the tasks based on [DeBERTa-Large-MNLI](https://huggingface.co/microsoft/deberta-large-mnli), [DeBERTa-XLarge-MNLI](https://huggingface.co/microsoft/deberta-xlarge-mnli), [DeBERTa-V2-XLarge-MNLI](https://huggingface.co/microsoft/deberta-v2-xlarge-mnli), [DeBERTa-V2-XXLarge-MNLI](https://huggingface.co/microsoft/deberta-v2-xxlarge-mnli). The results of SST-2/QQP/QNLI/SQuADv2 will also be slightly improved when start from MNLI fine-tuned models, however, we only report the numbers fine-tuned from pretrained base models for those 4 tasks.\n - <sup>2</sup> To try the **XXLarge** model with **[HF transformers](https://huggingface.co/transformers/main_classes/trainer.html)**, you need to specify **--sharded_ddp**\n \n```bash  \ncd transformers/examples/text-classification/\nexport TASK_NAME=mrpc\npython -m torch.distributed.launch --nproc_per_node=8 run_glue.py   --model_name_or_path microsoft/deberta-v2-xxlarge   \\\\\n--task_name $TASK_NAME   --do_train   --do_eval   --max_seq_length 128   --per_device_train_batch_size 4   \\\\\n--learning_rate 3e-6   --num_train_epochs 3   --output_dir /tmp/$TASK_NAME/ --overwrite_output_dir --sharded_ddp --fp16\n```\n\n### Citation\n\nIf you find DeBERTa useful for your work, please cite the following paper:\n\n``` latex\n@inproceedings{\nhe2021deberta,\ntitle={DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION},\nauthor={Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=XPZIaotutsD}\n}\n```\n",
    "library_name": "transformers"
  },
  {
    "model_id": "cardiffnlp/twitter-roberta-base-sentiment",
    "model_name": "cardiffnlp/twitter-roberta-base-sentiment",
    "author": "cardiffnlp",
    "downloads": 2281959,
    "likes": 293,
    "tags": [
      "transformers",
      "pytorch",
      "tf",
      "jax",
      "roberta",
      "text-classification",
      "en",
      "dataset:tweet_eval",
      "arxiv:2010.12421",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment",
    "dependencies": [
      [
        "transformers",
        "4.30.2"
      ],
      [
        "numpy",
        "1.24.3"
      ],
      [
        "scipy",
        "1.10.1"
      ],
      [
        "urllib",
        null
      ],
      [
        "csv",
        null
      ],
      [
        "urllib3",
        "1.26.15"
      ]
    ],
    "analysis_date": "2025-03-26T00:47:21.841840",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "roberta",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": [
        "en"
      ],
      "datasets": [
        "tweet_eval"
      ]
    },
    "card_text": "# Twitter-roBERTa-base for Sentiment Analysis\n\nThis is a roBERTa-base model trained on ~58M tweets and finetuned for sentiment analysis with the TweetEval benchmark. This model is suitable for English (for a similar multilingual model, see [XLM-T](https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment)).\n\n- Reference Paper: [_TweetEval_ (Findings of EMNLP 2020)](https://arxiv.org/pdf/2010.12421.pdf). \n- Git Repo: [Tweeteval official repository](https://github.com/cardiffnlp/tweeteval).\n\n<b>Labels</b>: \n0 -> Negative;\n1 -> Neutral;\n2 -> Positive\n\n<b>New!</b> We just released a new sentiment analysis model trained on more recent and a larger quantity of tweets. \nSee [twitter-roberta-base-sentiment-latest](https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest) and [TweetNLP](https://tweetnlp.org) for more details.\n\n## Example of classification\n\n```python\nfrom transformers import AutoModelForSequenceClassification\nfrom transformers import TFAutoModelForSequenceClassification\nfrom transformers import AutoTokenizer\nimport numpy as np\nfrom scipy.special import softmax\nimport csv\nimport urllib.request\n\n# Preprocess text (username and link placeholders)\ndef preprocess(text):\n    new_text = []\n \n \n    for t in text.split(\" \"):\n        t = '@user' if t.startswith('@') and len(t) > 1 else t\n        t = 'http' if t.startswith('http') else t\n        new_text.append(t)\n    return \" \".join(new_text)\n\n# Tasks:\n# emoji, emotion, hate, irony, offensive, sentiment\n# stance/abortion, stance/atheism, stance/climate, stance/feminist, stance/hillary\n\ntask='sentiment'\nMODEL = f\"cardiffnlp/twitter-roberta-base-{task}\"\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL)\n\n# download label mapping\nlabels=[]\nmapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt\"\nwith urllib.request.urlopen(mapping_link) as f:\n    html = f.read().decode('utf-8').split(\"\\n\")\n    csvreader = csv.reader(html, delimiter='\\t')\nlabels = [row[1] for row in csvreader if len(row) > 1]\n\n# PT\nmodel = AutoModelForSequenceClassification.from_pretrained(MODEL)\nmodel.save_pretrained(MODEL)\n\ntext = \"Good night \ud83d\ude0a\"\ntext = preprocess(text)\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\nscores = output[0][0].detach().numpy()\nscores = softmax(scores)\n\n# # TF\n# model = TFAutoModelForSequenceClassification.from_pretrained(MODEL)\n# model.save_pretrained(MODEL)\n\n# text = \"Good night \ud83d\ude0a\"\n# encoded_input = tokenizer(text, return_tensors='tf')\n# output = model(encoded_input)\n# scores = output[0][0].numpy()\n# scores = softmax(scores)\n\nranking = np.argsort(scores)\nranking = ranking[::-1]\nfor i in range(scores.shape[0]):\n    l = labels[ranking[i]]\n    s = scores[ranking[i]]\n    print(f\"{i+1}) {l} {np.round(float(s), 4)}\")\n\n```\n\nOutput: \n\n```\n1) positive 0.8466\n2) neutral 0.1458\n3) negative 0.0076\n```\n\n### BibTeX entry and citation info\n\nPlease cite the [reference paper](https://aclanthology.org/2020.findings-emnlp.148/) if you use this model.\n\n```bibtex\n@inproceedings{barbieri-etal-2020-tweeteval,\n    title = \"{T}weet{E}val: Unified Benchmark and Comparative Evaluation for Tweet Classification\",\n    author = \"Barbieri, Francesco  and\n      Camacho-Collados, Jose  and\n      Espinosa Anke, Luis  and\n      Neves, Leonardo\",\n    booktitle = \"Findings of the Association for Computational Linguistics: EMNLP 2020\",\n    month = nov,\n    year = \"2020\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2020.findings-emnlp.148\",\n    doi = \"10.18653/v1/2020.findings-emnlp.148\",\n    pages = \"1644--1650\"\n}\n```",
    "card_content": "---\nlanguage:\n- en\ndatasets:\n- tweet_eval\n---\n# Twitter-roBERTa-base for Sentiment Analysis\n\nThis is a roBERTa-base model trained on ~58M tweets and finetuned for sentiment analysis with the TweetEval benchmark. This model is suitable for English (for a similar multilingual model, see [XLM-T](https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment)).\n\n- Reference Paper: [_TweetEval_ (Findings of EMNLP 2020)](https://arxiv.org/pdf/2010.12421.pdf). \n- Git Repo: [Tweeteval official repository](https://github.com/cardiffnlp/tweeteval).\n\n<b>Labels</b>: \n0 -> Negative;\n1 -> Neutral;\n2 -> Positive\n\n<b>New!</b> We just released a new sentiment analysis model trained on more recent and a larger quantity of tweets. \nSee [twitter-roberta-base-sentiment-latest](https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest) and [TweetNLP](https://tweetnlp.org) for more details.\n\n## Example of classification\n\n```python\nfrom transformers import AutoModelForSequenceClassification\nfrom transformers import TFAutoModelForSequenceClassification\nfrom transformers import AutoTokenizer\nimport numpy as np\nfrom scipy.special import softmax\nimport csv\nimport urllib.request\n\n# Preprocess text (username and link placeholders)\ndef preprocess(text):\n    new_text = []\n \n \n    for t in text.split(\" \"):\n        t = '@user' if t.startswith('@') and len(t) > 1 else t\n        t = 'http' if t.startswith('http') else t\n        new_text.append(t)\n    return \" \".join(new_text)\n\n# Tasks:\n# emoji, emotion, hate, irony, offensive, sentiment\n# stance/abortion, stance/atheism, stance/climate, stance/feminist, stance/hillary\n\ntask='sentiment'\nMODEL = f\"cardiffnlp/twitter-roberta-base-{task}\"\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL)\n\n# download label mapping\nlabels=[]\nmapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt\"\nwith urllib.request.urlopen(mapping_link) as f:\n    html = f.read().decode('utf-8').split(\"\\n\")\n    csvreader = csv.reader(html, delimiter='\\t')\nlabels = [row[1] for row in csvreader if len(row) > 1]\n\n# PT\nmodel = AutoModelForSequenceClassification.from_pretrained(MODEL)\nmodel.save_pretrained(MODEL)\n\ntext = \"Good night \ud83d\ude0a\"\ntext = preprocess(text)\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\nscores = output[0][0].detach().numpy()\nscores = softmax(scores)\n\n# # TF\n# model = TFAutoModelForSequenceClassification.from_pretrained(MODEL)\n# model.save_pretrained(MODEL)\n\n# text = \"Good night \ud83d\ude0a\"\n# encoded_input = tokenizer(text, return_tensors='tf')\n# output = model(encoded_input)\n# scores = output[0][0].numpy()\n# scores = softmax(scores)\n\nranking = np.argsort(scores)\nranking = ranking[::-1]\nfor i in range(scores.shape[0]):\n    l = labels[ranking[i]]\n    s = scores[ranking[i]]\n    print(f\"{i+1}) {l} {np.round(float(s), 4)}\")\n\n```\n\nOutput: \n\n```\n1) positive 0.8466\n2) neutral 0.1458\n3) negative 0.0076\n```\n\n### BibTeX entry and citation info\n\nPlease cite the [reference paper](https://aclanthology.org/2020.findings-emnlp.148/) if you use this model.\n\n```bibtex\n@inproceedings{barbieri-etal-2020-tweeteval,\n    title = \"{T}weet{E}val: Unified Benchmark and Comparative Evaluation for Tweet Classification\",\n    author = \"Barbieri, Francesco  and\n      Camacho-Collados, Jose  and\n      Espinosa Anke, Luis  and\n      Neves, Leonardo\",\n    booktitle = \"Findings of the Association for Computational Linguistics: EMNLP 2020\",\n    month = nov,\n    year = \"2020\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2020.findings-emnlp.148\",\n    doi = \"10.18653/v1/2020.findings-emnlp.148\",\n    pages = \"1644--1650\"\n}\n```",
    "library_name": "transformers"
  },
  {
    "model_id": "cardiffnlp/twitter-roberta-base-sentiment-latest",
    "model_name": "cardiffnlp/twitter-roberta-base-sentiment-latest",
    "author": "cardiffnlp",
    "downloads": 2165430,
    "likes": 653,
    "tags": [
      "transformers",
      "pytorch",
      "tf",
      "roberta",
      "text-classification",
      "en",
      "dataset:tweet_eval",
      "arxiv:2202.03829",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest",
    "dependencies": [
      [
        "transformers",
        "4.30.2"
      ],
      [
        "numpy",
        "1.24.3"
      ],
      [
        "scipy",
        "1.10.1"
      ],
      [
        "torch",
        "2.0.1"
      ]
    ],
    "analysis_date": "2025-03-26T00:47:23.284890",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "roberta",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": "en",
      "datasets": [
        "tweet_eval"
      ],
      "widget": [
        {
          "text": "Covid cases are increasing fast!"
        }
      ]
    },
    "card_text": "\n\n# Twitter-roBERTa-base for Sentiment Analysis - UPDATED (2022)\n\nThis is a RoBERTa-base model trained on ~124M tweets from January 2018 to December 2021, and finetuned for sentiment analysis with the TweetEval benchmark. \nThe original Twitter-based RoBERTa model can be found [here](https://huggingface.co/cardiffnlp/twitter-roberta-base-2021-124m) and the original reference paper is [TweetEval](https://github.com/cardiffnlp/tweeteval). This model is suitable for English. \n\n- Reference Paper: [TimeLMs paper](https://arxiv.org/abs/2202.03829). \n- Git Repo: [TimeLMs official repository](https://github.com/cardiffnlp/timelms).\n\n<b>Labels</b>: \n0 -> Negative;\n1 -> Neutral;\n2 -> Positive\n\nThis sentiment analysis model has been integrated into [TweetNLP](https://github.com/cardiffnlp/tweetnlp). You can access the demo [here](https://tweetnlp.org).\n\n## Example Pipeline\n```python\nfrom transformers import pipeline\nsentiment_task = pipeline(\"sentiment-analysis\", model=model_path, tokenizer=model_path)\nsentiment_task(\"Covid cases are increasing fast!\")\n```\n```\n[{'label': 'Negative', 'score': 0.7236}]\n```\n\n## Full classification example\n\n```python\nfrom transformers import AutoModelForSequenceClassification\nfrom transformers import TFAutoModelForSequenceClassification\nfrom transformers import AutoTokenizer, AutoConfig\nimport numpy as np\nfrom scipy.special import softmax\n# Preprocess text (username and link placeholders)\ndef preprocess(text):\n    new_text = []\n    for t in text.split(\" \"):\n        t = '@user' if t.startswith('@') and len(t) > 1 else t\n        t = 'http' if t.startswith('http') else t\n        new_text.append(t)\n    return \" \".join(new_text)\nMODEL = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\ntokenizer = AutoTokenizer.from_pretrained(MODEL)\nconfig = AutoConfig.from_pretrained(MODEL)\n# PT\nmodel = AutoModelForSequenceClassification.from_pretrained(MODEL)\n#model.save_pretrained(MODEL)\ntext = \"Covid cases are increasing fast!\"\ntext = preprocess(text)\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\nscores = output[0][0].detach().numpy()\nscores = softmax(scores)\n# # TF\n# model = TFAutoModelForSequenceClassification.from_pretrained(MODEL)\n# model.save_pretrained(MODEL)\n# text = \"Covid cases are increasing fast!\"\n# encoded_input = tokenizer(text, return_tensors='tf')\n# output = model(encoded_input)\n# scores = output[0][0].numpy()\n# scores = softmax(scores)\n# Print labels and scores\nranking = np.argsort(scores)\nranking = ranking[::-1]\nfor i in range(scores.shape[0]):\n    l = config.id2label[ranking[i]]\n    s = scores[ranking[i]]\n    print(f\"{i+1}) {l} {np.round(float(s), 4)}\")\n```\n\nOutput: \n\n```\n1) Negative 0.7236\n2) Neutral 0.2287\n3) Positive 0.0477\n```\n\n\n### References \n```\n@inproceedings{camacho-collados-etal-2022-tweetnlp,\n    title = \"{T}weet{NLP}: Cutting-Edge Natural Language Processing for Social Media\",\n    author = \"Camacho-collados, Jose  and\n      Rezaee, Kiamehr  and\n      Riahi, Talayeh  and\n      Ushio, Asahi  and\n      Loureiro, Daniel  and\n      Antypas, Dimosthenis  and\n      Boisson, Joanne  and\n      Espinosa Anke, Luis  and\n      Liu, Fangyu  and\n      Mart{\\'\\i}nez C{\\'a}mara, Eugenio\" and others,\n    booktitle = \"Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: System Demonstrations\",\n    month = dec,\n    year = \"2022\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.emnlp-demos.5\",\n    pages = \"38--49\"\n}\n\n```\n\n```\n@inproceedings{loureiro-etal-2022-timelms,\n    title = \"{T}ime{LM}s: Diachronic Language Models from {T}witter\",\n    author = \"Loureiro, Daniel  and\n      Barbieri, Francesco  and\n      Neves, Leonardo  and\n      Espinosa Anke, Luis  and\n      Camacho-collados, Jose\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-demo.25\",\n    doi = \"10.18653/v1/2022.acl-demo.25\",\n    pages = \"251--260\"\n}\n\n```\n",
    "card_content": "---\nlanguage: en\ndatasets:\n- tweet_eval\nwidget:\n- text: Covid cases are increasing fast!\n---\n\n\n# Twitter-roBERTa-base for Sentiment Analysis - UPDATED (2022)\n\nThis is a RoBERTa-base model trained on ~124M tweets from January 2018 to December 2021, and finetuned for sentiment analysis with the TweetEval benchmark. \nThe original Twitter-based RoBERTa model can be found [here](https://huggingface.co/cardiffnlp/twitter-roberta-base-2021-124m) and the original reference paper is [TweetEval](https://github.com/cardiffnlp/tweeteval). This model is suitable for English. \n\n- Reference Paper: [TimeLMs paper](https://arxiv.org/abs/2202.03829). \n- Git Repo: [TimeLMs official repository](https://github.com/cardiffnlp/timelms).\n\n<b>Labels</b>: \n0 -> Negative;\n1 -> Neutral;\n2 -> Positive\n\nThis sentiment analysis model has been integrated into [TweetNLP](https://github.com/cardiffnlp/tweetnlp). You can access the demo [here](https://tweetnlp.org).\n\n## Example Pipeline\n```python\nfrom transformers import pipeline\nsentiment_task = pipeline(\"sentiment-analysis\", model=model_path, tokenizer=model_path)\nsentiment_task(\"Covid cases are increasing fast!\")\n```\n```\n[{'label': 'Negative', 'score': 0.7236}]\n```\n\n## Full classification example\n\n```python\nfrom transformers import AutoModelForSequenceClassification\nfrom transformers import TFAutoModelForSequenceClassification\nfrom transformers import AutoTokenizer, AutoConfig\nimport numpy as np\nfrom scipy.special import softmax\n# Preprocess text (username and link placeholders)\ndef preprocess(text):\n    new_text = []\n    for t in text.split(\" \"):\n        t = '@user' if t.startswith('@') and len(t) > 1 else t\n        t = 'http' if t.startswith('http') else t\n        new_text.append(t)\n    return \" \".join(new_text)\nMODEL = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\ntokenizer = AutoTokenizer.from_pretrained(MODEL)\nconfig = AutoConfig.from_pretrained(MODEL)\n# PT\nmodel = AutoModelForSequenceClassification.from_pretrained(MODEL)\n#model.save_pretrained(MODEL)\ntext = \"Covid cases are increasing fast!\"\ntext = preprocess(text)\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\nscores = output[0][0].detach().numpy()\nscores = softmax(scores)\n# # TF\n# model = TFAutoModelForSequenceClassification.from_pretrained(MODEL)\n# model.save_pretrained(MODEL)\n# text = \"Covid cases are increasing fast!\"\n# encoded_input = tokenizer(text, return_tensors='tf')\n# output = model(encoded_input)\n# scores = output[0][0].numpy()\n# scores = softmax(scores)\n# Print labels and scores\nranking = np.argsort(scores)\nranking = ranking[::-1]\nfor i in range(scores.shape[0]):\n    l = config.id2label[ranking[i]]\n    s = scores[ranking[i]]\n    print(f\"{i+1}) {l} {np.round(float(s), 4)}\")\n```\n\nOutput: \n\n```\n1) Negative 0.7236\n2) Neutral 0.2287\n3) Positive 0.0477\n```\n\n\n### References \n```\n@inproceedings{camacho-collados-etal-2022-tweetnlp,\n    title = \"{T}weet{NLP}: Cutting-Edge Natural Language Processing for Social Media\",\n    author = \"Camacho-collados, Jose  and\n      Rezaee, Kiamehr  and\n      Riahi, Talayeh  and\n      Ushio, Asahi  and\n      Loureiro, Daniel  and\n      Antypas, Dimosthenis  and\n      Boisson, Joanne  and\n      Espinosa Anke, Luis  and\n      Liu, Fangyu  and\n      Mart{\\'\\i}nez C{\\'a}mara, Eugenio\" and others,\n    booktitle = \"Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: System Demonstrations\",\n    month = dec,\n    year = \"2022\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.emnlp-demos.5\",\n    pages = \"38--49\"\n}\n\n```\n\n```\n@inproceedings{loureiro-etal-2022-timelms,\n    title = \"{T}ime{LM}s: Diachronic Language Models from {T}witter\",\n    author = \"Loureiro, Daniel  and\n      Barbieri, Francesco  and\n      Neves, Leonardo  and\n      Espinosa Anke, Luis  and\n      Camacho-collados, Jose\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-demo.25\",\n    doi = \"10.18653/v1/2022.acl-demo.25\",\n    pages = \"251--260\"\n}\n\n```\n",
    "library_name": "transformers"
  },
  {
    "model_id": "cross-encoder/ms-marco-MiniLM-L4-v2",
    "model_name": "cross-encoder/ms-marco-MiniLM-L4-v2",
    "author": "cross-encoder",
    "downloads": 1599873,
    "likes": 9,
    "tags": [
      "transformers",
      "pytorch",
      "jax",
      "safetensors",
      "bert",
      "text-classification",
      "license:apache-2.0",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/cross-encoder/ms-marco-MiniLM-L4-v2",
    "dependencies": [
      [
        "sentence_transformers",
        null
      ],
      [
        "transformers",
        null
      ],
      [
        "torch",
        null
      ],
      [
        "sentence-transformers",
        "2.2.2"
      ]
    ],
    "analysis_date": "2025-03-26T00:47:24.745135",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "bert",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "license": "apache-2.0"
    },
    "card_text": "# Cross-Encoder for MS Marco\n\nThis model was trained on the [MS Marco Passage Ranking](https://github.com/microsoft/MSMARCO-Passage-Ranking) task.\n\nThe model can be used for Information Retrieval: Given a query, encode the query will all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. See [SBERT.net Retrieve & Re-rank](https://www.sbert.net/examples/applications/retrieve_rerank/README.html) for more details. The training code is available here: [SBERT.net Training MS Marco](https://github.com/UKPLab/sentence-transformers/tree/master/examples/training/ms_marco)\n\n\n## Usage with SentenceTransformers\n\nThe usage is easy when you have [SentenceTransformers](https://www.sbert.net/) installed. Then you can use the pre-trained models like this:\n```python\nfrom sentence_transformers import CrossEncoder\n\nmodel = CrossEncoder('cross-encoder/ms-marco-MiniLM-L4-v2')\nscores = model.predict([\n    (\"How many people live in Berlin?\", \"Berlin had a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\"),\n    (\"How many people live in Berlin?\", \"Berlin is well known for its museums.\"),\n])\nprint(scores)\n# [ 9.1273365 -4.569759 ]\n```\n\n\n## Usage with Transformers\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\n\nmodel = AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-MiniLM-L4-v2')\ntokenizer = AutoTokenizer.from_pretrained('cross-encoder/ms-marco-MiniLM-L4-v2')\n\nfeatures = tokenizer(['How many people live in Berlin?', 'How many people live in Berlin?'], ['Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.', 'New York City is famous for the Metropolitan Museum of Art.'],  padding=True, truncation=True, return_tensors=\"pt\")\n\nmodel.eval()\nwith torch.no_grad():\n    scores = model(**features).logits\n    print(scores)\n```\n\n\n## Performance\nIn the following table, we provide various pre-trained Cross-Encoders together with their performance on the [TREC Deep Learning 2019](https://microsoft.github.io/TREC-2019-Deep-Learning/) and the [MS Marco Passage Reranking](https://github.com/microsoft/MSMARCO-Passage-Ranking/) dataset. \n\n\n| Model-Name        | NDCG@10 (TREC DL 19) | MRR@10 (MS Marco Dev)  | Docs / Sec |\n| ------------- |:-------------| -----| --- | \n| **Version 2 models** | | | \n| cross-encoder/ms-marco-TinyBERT-L2-v2 | 69.84 | 32.56 | 9000\n| cross-encoder/ms-marco-MiniLM-L2-v2 | 71.01 | 34.85 | 4100\n| cross-encoder/ms-marco-MiniLM-L4-v2 | 73.04 | 37.70 | 2500\n| cross-encoder/ms-marco-MiniLM-L6-v2 | 74.30 | 39.01 | 1800\n| cross-encoder/ms-marco-MiniLM-L12-v2 | 74.31 | 39.02 | 960\n| **Version 1 models** | | | \n| cross-encoder/ms-marco-TinyBERT-L2  | 67.43 | 30.15  | 9000\n| cross-encoder/ms-marco-TinyBERT-L4  | 68.09 | 34.50  | 2900\n| cross-encoder/ms-marco-TinyBERT-L6 |  69.57 | 36.13  | 680\n| cross-encoder/ms-marco-electra-base | 71.99 | 36.41 | 340\n| **Other models** | | | \n| nboost/pt-tinybert-msmarco | 63.63 | 28.80 | 2900 \n| nboost/pt-bert-base-uncased-msmarco | 70.94 | 34.75 | 340 \n| nboost/pt-bert-large-msmarco | 73.36 | 36.48 | 100 \n| Capreolus/electra-base-msmarco | 71.23 | 36.89 | 340 \n| amberoad/bert-multilingual-passage-reranking-msmarco | 68.40 | 35.54 | 330 \n| sebastian-hofstaetter/distilbert-cat-margin_mse-T2-msmarco | 72.82 | 37.88 | 720\n \n Note: Runtime was computed on a V100 GPU.\n",
    "card_content": "---\nlicense: apache-2.0\n---\n# Cross-Encoder for MS Marco\n\nThis model was trained on the [MS Marco Passage Ranking](https://github.com/microsoft/MSMARCO-Passage-Ranking) task.\n\nThe model can be used for Information Retrieval: Given a query, encode the query will all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. See [SBERT.net Retrieve & Re-rank](https://www.sbert.net/examples/applications/retrieve_rerank/README.html) for more details. The training code is available here: [SBERT.net Training MS Marco](https://github.com/UKPLab/sentence-transformers/tree/master/examples/training/ms_marco)\n\n\n## Usage with SentenceTransformers\n\nThe usage is easy when you have [SentenceTransformers](https://www.sbert.net/) installed. Then you can use the pre-trained models like this:\n```python\nfrom sentence_transformers import CrossEncoder\n\nmodel = CrossEncoder('cross-encoder/ms-marco-MiniLM-L4-v2')\nscores = model.predict([\n    (\"How many people live in Berlin?\", \"Berlin had a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\"),\n    (\"How many people live in Berlin?\", \"Berlin is well known for its museums.\"),\n])\nprint(scores)\n# [ 9.1273365 -4.569759 ]\n```\n\n\n## Usage with Transformers\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\n\nmodel = AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-MiniLM-L4-v2')\ntokenizer = AutoTokenizer.from_pretrained('cross-encoder/ms-marco-MiniLM-L4-v2')\n\nfeatures = tokenizer(['How many people live in Berlin?', 'How many people live in Berlin?'], ['Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.', 'New York City is famous for the Metropolitan Museum of Art.'],  padding=True, truncation=True, return_tensors=\"pt\")\n\nmodel.eval()\nwith torch.no_grad():\n    scores = model(**features).logits\n    print(scores)\n```\n\n\n## Performance\nIn the following table, we provide various pre-trained Cross-Encoders together with their performance on the [TREC Deep Learning 2019](https://microsoft.github.io/TREC-2019-Deep-Learning/) and the [MS Marco Passage Reranking](https://github.com/microsoft/MSMARCO-Passage-Ranking/) dataset. \n\n\n| Model-Name        | NDCG@10 (TREC DL 19) | MRR@10 (MS Marco Dev)  | Docs / Sec |\n| ------------- |:-------------| -----| --- | \n| **Version 2 models** | | | \n| cross-encoder/ms-marco-TinyBERT-L2-v2 | 69.84 | 32.56 | 9000\n| cross-encoder/ms-marco-MiniLM-L2-v2 | 71.01 | 34.85 | 4100\n| cross-encoder/ms-marco-MiniLM-L4-v2 | 73.04 | 37.70 | 2500\n| cross-encoder/ms-marco-MiniLM-L6-v2 | 74.30 | 39.01 | 1800\n| cross-encoder/ms-marco-MiniLM-L12-v2 | 74.31 | 39.02 | 960\n| **Version 1 models** | | | \n| cross-encoder/ms-marco-TinyBERT-L2  | 67.43 | 30.15  | 9000\n| cross-encoder/ms-marco-TinyBERT-L4  | 68.09 | 34.50  | 2900\n| cross-encoder/ms-marco-TinyBERT-L6 |  69.57 | 36.13  | 680\n| cross-encoder/ms-marco-electra-base | 71.99 | 36.41 | 340\n| **Other models** | | | \n| nboost/pt-tinybert-msmarco | 63.63 | 28.80 | 2900 \n| nboost/pt-bert-base-uncased-msmarco | 70.94 | 34.75 | 340 \n| nboost/pt-bert-large-msmarco | 73.36 | 36.48 | 100 \n| Capreolus/electra-base-msmarco | 71.23 | 36.89 | 340 \n| amberoad/bert-multilingual-passage-reranking-msmarco | 68.40 | 35.54 | 330 \n| sebastian-hofstaetter/distilbert-cat-margin_mse-T2-msmarco | 72.82 | 37.88 | 720\n \n Note: Runtime was computed on a V100 GPU.\n",
    "library_name": "transformers"
  },
  {
    "model_id": "cardiffnlp/twitter-xlm-roberta-base-sentiment",
    "model_name": "cardiffnlp/twitter-xlm-roberta-base-sentiment",
    "author": "cardiffnlp",
    "downloads": 1396298,
    "likes": 210,
    "tags": [
      "transformers",
      "pytorch",
      "tf",
      "xlm-roberta",
      "text-classification",
      "multilingual",
      "arxiv:2104.12250",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment",
    "dependencies": [
      [
        "transformers",
        "4.30.2"
      ],
      [
        "numpy",
        "1.23.5"
      ],
      [
        "scipy",
        "1.10.1"
      ],
      [
        "torch",
        "2.0.1"
      ],
      [
        "sentencepiece",
        "0.1.99"
      ],
      [
        "protobuf",
        "3.20.3"
      ]
    ],
    "analysis_date": "2025-03-26T00:47:25.877559",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "xlm-roberta",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": "multilingual",
      "widget": [
        {
          "text": "\ud83e\udd17"
        },
        {
          "text": "T'estimo! \u2764\ufe0f"
        },
        {
          "text": "I love you!"
        },
        {
          "text": "I hate you \ud83e\udd2e"
        },
        {
          "text": "Mahal kita!"
        },
        {
          "text": "\uc0ac\ub791\ud574!"
        },
        {
          "text": "\ub09c \ub108\uac00 \uc2eb\uc5b4"
        },
        {
          "text": "\ud83d\ude0d\ud83d\ude0d\ud83d\ude0d"
        }
      ]
    },
    "card_text": "\n\n# twitter-XLM-roBERTa-base for Sentiment Analysis\n\nThis is a multilingual XLM-roBERTa-base model trained on ~198M tweets and finetuned for sentiment analysis. The sentiment fine-tuning was done on 8 languages (Ar, En, Fr, De, Hi, It, Sp, Pt) but it can be used for more languages (see paper for details).\n\n- Paper: [XLM-T: A Multilingual Language Model Toolkit for Twitter](https://arxiv.org/abs/2104.12250). \n- Git Repo: [XLM-T official repository](https://github.com/cardiffnlp/xlm-t).\n\nThis model has been integrated into the [TweetNLP library](https://github.com/cardiffnlp/tweetnlp).\n\n## Example Pipeline\n```python\nfrom transformers import pipeline\nmodel_path = \"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\nsentiment_task = pipeline(\"sentiment-analysis\", model=model_path, tokenizer=model_path)\nsentiment_task(\"T'estimo!\")\n```\n```\n[{'label': 'Positive', 'score': 0.6600581407546997}]\n```\n\n## Full classification example\n\n```python\nfrom transformers import AutoModelForSequenceClassification\nfrom transformers import TFAutoModelForSequenceClassification\nfrom transformers import AutoTokenizer, AutoConfig\nimport numpy as np\nfrom scipy.special import softmax\n\n# Preprocess text (username and link placeholders)\ndef preprocess(text):\n    new_text = []\n    for t in text.split(\" \"):\n        t = '@user' if t.startswith('@') and len(t) > 1 else t\n        t = 'http' if t.startswith('http') else t\n        new_text.append(t)\n    return \" \".join(new_text)\n\nMODEL = f\"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL)\nconfig = AutoConfig.from_pretrained(MODEL)\n\n# PT\nmodel = AutoModelForSequenceClassification.from_pretrained(MODEL)\nmodel.save_pretrained(MODEL)\n\ntext = \"Good night \ud83d\ude0a\"\ntext = preprocess(text)\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\nscores = output[0][0].detach().numpy()\nscores = softmax(scores)\n\n# # TF\n# model = TFAutoModelForSequenceClassification.from_pretrained(MODEL)\n# model.save_pretrained(MODEL)\n\n# text = \"Good night \ud83d\ude0a\"\n# encoded_input = tokenizer(text, return_tensors='tf')\n# output = model(encoded_input)\n# scores = output[0][0].numpy()\n# scores = softmax(scores)\n\n# Print labels and scores\nranking = np.argsort(scores)\nranking = ranking[::-1]\nfor i in range(scores.shape[0]):\n    l = config.id2label[ranking[i]]\n    s = scores[ranking[i]]\n    print(f\"{i+1}) {l} {np.round(float(s), 4)}\")\n\n```\n\nOutput: \n\n```\n1) Positive 0.7673\n2) Neutral 0.2015\n3) Negative 0.0313\n```\n\n### Reference\n```\n@inproceedings{barbieri-etal-2022-xlm,\n    title = \"{XLM}-{T}: Multilingual Language Models in {T}witter for Sentiment Analysis and Beyond\",\n    author = \"Barbieri, Francesco  and\n      Espinosa Anke, Luis  and\n      Camacho-Collados, Jose\",\n    booktitle = \"Proceedings of the Thirteenth Language Resources and Evaluation Conference\",\n    month = jun,\n    year = \"2022\",\n    address = \"Marseille, France\",\n    publisher = \"European Language Resources Association\",\n    url = \"https://aclanthology.org/2022.lrec-1.27\",\n    pages = \"258--266\"\n}\n\n```\n\n",
    "card_content": "---\nlanguage: multilingual\nwidget:\n- text: \ud83e\udd17\n- text: T'estimo! \u2764\ufe0f\n- text: I love you!\n- text: I hate you \ud83e\udd2e\n- text: Mahal kita!\n- text: \uc0ac\ub791\ud574!\n- text: \ub09c \ub108\uac00 \uc2eb\uc5b4\n- text: \ud83d\ude0d\ud83d\ude0d\ud83d\ude0d\n---\n\n\n# twitter-XLM-roBERTa-base for Sentiment Analysis\n\nThis is a multilingual XLM-roBERTa-base model trained on ~198M tweets and finetuned for sentiment analysis. The sentiment fine-tuning was done on 8 languages (Ar, En, Fr, De, Hi, It, Sp, Pt) but it can be used for more languages (see paper for details).\n\n- Paper: [XLM-T: A Multilingual Language Model Toolkit for Twitter](https://arxiv.org/abs/2104.12250). \n- Git Repo: [XLM-T official repository](https://github.com/cardiffnlp/xlm-t).\n\nThis model has been integrated into the [TweetNLP library](https://github.com/cardiffnlp/tweetnlp).\n\n## Example Pipeline\n```python\nfrom transformers import pipeline\nmodel_path = \"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\nsentiment_task = pipeline(\"sentiment-analysis\", model=model_path, tokenizer=model_path)\nsentiment_task(\"T'estimo!\")\n```\n```\n[{'label': 'Positive', 'score': 0.6600581407546997}]\n```\n\n## Full classification example\n\n```python\nfrom transformers import AutoModelForSequenceClassification\nfrom transformers import TFAutoModelForSequenceClassification\nfrom transformers import AutoTokenizer, AutoConfig\nimport numpy as np\nfrom scipy.special import softmax\n\n# Preprocess text (username and link placeholders)\ndef preprocess(text):\n    new_text = []\n    for t in text.split(\" \"):\n        t = '@user' if t.startswith('@') and len(t) > 1 else t\n        t = 'http' if t.startswith('http') else t\n        new_text.append(t)\n    return \" \".join(new_text)\n\nMODEL = f\"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL)\nconfig = AutoConfig.from_pretrained(MODEL)\n\n# PT\nmodel = AutoModelForSequenceClassification.from_pretrained(MODEL)\nmodel.save_pretrained(MODEL)\n\ntext = \"Good night \ud83d\ude0a\"\ntext = preprocess(text)\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\nscores = output[0][0].detach().numpy()\nscores = softmax(scores)\n\n# # TF\n# model = TFAutoModelForSequenceClassification.from_pretrained(MODEL)\n# model.save_pretrained(MODEL)\n\n# text = \"Good night \ud83d\ude0a\"\n# encoded_input = tokenizer(text, return_tensors='tf')\n# output = model(encoded_input)\n# scores = output[0][0].numpy()\n# scores = softmax(scores)\n\n# Print labels and scores\nranking = np.argsort(scores)\nranking = ranking[::-1]\nfor i in range(scores.shape[0]):\n    l = config.id2label[ranking[i]]\n    s = scores[ranking[i]]\n    print(f\"{i+1}) {l} {np.round(float(s), 4)}\")\n\n```\n\nOutput: \n\n```\n1) Positive 0.7673\n2) Neutral 0.2015\n3) Negative 0.0313\n```\n\n### Reference\n```\n@inproceedings{barbieri-etal-2022-xlm,\n    title = \"{XLM}-{T}: Multilingual Language Models in {T}witter for Sentiment Analysis and Beyond\",\n    author = \"Barbieri, Francesco  and\n      Espinosa Anke, Luis  and\n      Camacho-Collados, Jose\",\n    booktitle = \"Proceedings of the Thirteenth Language Resources and Evaluation Conference\",\n    month = jun,\n    year = \"2022\",\n    address = \"Marseille, France\",\n    publisher = \"European Language Resources Association\",\n    url = \"https://aclanthology.org/2022.lrec-1.27\",\n    pages = \"258--266\"\n}\n\n```\n\n",
    "library_name": "transformers"
  },
  {
    "model_id": "facebook/roberta-hate-speech-dynabench-r4-target",
    "model_name": "facebook/roberta-hate-speech-dynabench-r4-target",
    "author": "facebook",
    "downloads": 1225478,
    "likes": 77,
    "tags": [
      "transformers",
      "pytorch",
      "safetensors",
      "roberta",
      "text-classification",
      "en",
      "arxiv:2012.15761",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/facebook/roberta-hate-speech-dynabench-r4-target",
    "dependencies": null,
    "analysis_date": "2025-03-26T00:47:26.601514",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "roberta",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": "en"
    },
    "card_text": "\n# LFTW R4 Target\n\nThe R4 Target model from [Learning from the Worst: Dynamically Generated Datasets to Improve Online Hate Detection](https://arxiv.org/abs/2012.15761)\n\n## Citation Information\n\n```bibtex\n@inproceedings{vidgen2021lftw,\n  title={Learning from the Worst: Dynamically Generated Datasets to Improve Online Hate Detection},\n  author={Bertie Vidgen and Tristan Thrush and Zeerak Waseem and Douwe Kiela},\n  booktitle={ACL},\n  year={2021}\n}\n```\n\nThanks to Kushal Tirumala and Adina Williams for helping the authors put the model on the hub!",
    "card_content": "---\nlanguage: en\n---\n\n# LFTW R4 Target\n\nThe R4 Target model from [Learning from the Worst: Dynamically Generated Datasets to Improve Online Hate Detection](https://arxiv.org/abs/2012.15761)\n\n## Citation Information\n\n```bibtex\n@inproceedings{vidgen2021lftw,\n  title={Learning from the Worst: Dynamically Generated Datasets to Improve Online Hate Detection},\n  author={Bertie Vidgen and Tristan Thrush and Zeerak Waseem and Douwe Kiela},\n  booktitle={ACL},\n  year={2021}\n}\n```\n\nThanks to Kushal Tirumala and Adina Williams for helping the authors put the model on the hub!",
    "library_name": "transformers"
  },
  {
    "model_id": "lucadiliello/BLEURT-20-D12",
    "model_name": "lucadiliello/BLEURT-20-D12",
    "author": "lucadiliello",
    "downloads": 1193292,
    "likes": 0,
    "tags": [
      "transformers",
      "pytorch",
      "bleurt",
      "text-classification",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/lucadiliello/BLEURT-20-D12",
    "dependencies": [
      [
        "torch",
        "2.0.1"
      ],
      [
        "bleurt_pytorch",
        "1.1.0"
      ]
    ],
    "analysis_date": "2025-03-26T00:47:27.577597",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "bleurt",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {},
    "card_text": "This model is based on a custom Transformer model that can be installed with:\n\n```bash\npip install git+https://github.com/lucadiliello/bleurt-pytorch.git\n```\n\nNow load the model and make predictions with:\n\n```python\nimport torch\nfrom bleurt_pytorch import BleurtConfig, BleurtForSequenceClassification, BleurtTokenizer\n\nconfig = BleurtConfig.from_pretrained('lucadiliello/BLEURT-20-D12')\nmodel = BleurtForSequenceClassification.from_pretrained('lucadiliello/BLEURT-20-D12')\ntokenizer = BleurtTokenizer.from_pretrained('lucadiliello/BLEURT-20-D12')\n\nreferences = [\"a bird chirps by the window\", \"this is a random sentence\"]\ncandidates = [\"a bird chirps by the window\", \"this looks like a random sentence\"]\n\nmodel.eval()\nwith torch.no_grad():\n    inputs = tokenizer(references, candidates, padding='longest', return_tensors='pt')\n    res = model(**inputs).logits.flatten().tolist()\nprint(res)\n# [0.9604414105415344, 0.8080050349235535]\n```\n\nTake a look at this [repository](https://github.com/lucadiliello/bleurt-pytorch) for the definition of `BleurtConfig`, `BleurtForSequenceClassification` and `BleurtTokenizer` in PyTorch.",
    "card_content": "---\n{}\n---\nThis model is based on a custom Transformer model that can be installed with:\n\n```bash\npip install git+https://github.com/lucadiliello/bleurt-pytorch.git\n```\n\nNow load the model and make predictions with:\n\n```python\nimport torch\nfrom bleurt_pytorch import BleurtConfig, BleurtForSequenceClassification, BleurtTokenizer\n\nconfig = BleurtConfig.from_pretrained('lucadiliello/BLEURT-20-D12')\nmodel = BleurtForSequenceClassification.from_pretrained('lucadiliello/BLEURT-20-D12')\ntokenizer = BleurtTokenizer.from_pretrained('lucadiliello/BLEURT-20-D12')\n\nreferences = [\"a bird chirps by the window\", \"this is a random sentence\"]\ncandidates = [\"a bird chirps by the window\", \"this looks like a random sentence\"]\n\nmodel.eval()\nwith torch.no_grad():\n    inputs = tokenizer(references, candidates, padding='longest', return_tensors='pt')\n    res = model(**inputs).logits.flatten().tolist()\nprint(res)\n# [0.9604414105415344, 0.8080050349235535]\n```\n\nTake a look at this [repository](https://github.com/lucadiliello/bleurt-pytorch) for the definition of `BleurtConfig`, `BleurtForSequenceClassification` and `BleurtTokenizer` in PyTorch.",
    "library_name": "transformers"
  },
  {
    "model_id": "nlptown/bert-base-multilingual-uncased-sentiment",
    "model_name": "nlptown/bert-base-multilingual-uncased-sentiment",
    "author": "nlptown",
    "downloads": 1143764,
    "likes": 358,
    "tags": [
      "transformers",
      "pytorch",
      "tf",
      "jax",
      "safetensors",
      "bert",
      "text-classification",
      "en",
      "nl",
      "de",
      "fr",
      "it",
      "es",
      "doi:10.57967/hf/1515",
      "license:mit",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment",
    "dependencies": [
      [
        "transformers",
        "4.30.2"
      ],
      [
        "torch",
        "2.0.1"
      ],
      [
        "numpy",
        "1.24.3"
      ],
      [
        "xformers",
        "0.0.20"
      ]
    ],
    "analysis_date": "2025-03-26T00:47:28.510294",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "bert",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": [
        "en",
        "nl",
        "de",
        "fr",
        "it",
        "es"
      ],
      "license": "mit"
    },
    "card_text": "\n# bert-base-multilingual-uncased-sentiment\n\nVisit the [NLP Town website](https://www.nlp.town) for an updated version of this model, with a 40% error reduction on product reviews.\n\nThis is a bert-base-multilingual-uncased model finetuned for sentiment analysis on product reviews in six languages: English, Dutch, German, French, Spanish, and Italian. It predicts the sentiment of the review as a number of stars (between 1 and 5).\n\nThis model is intended for direct use as a sentiment analysis model for product reviews in any of the six languages above or for further finetuning on related sentiment analysis tasks.\n\n## Training data\n\nHere is the number of product reviews we used for finetuning the model: \n\n| Language | Number of reviews |\n| -------- | ----------------- |\n| English  | 150k           |\n| Dutch    | 80k            |\n| German   | 137k           |\n| French   | 140k           |\n| Italian  | 72k            |\n| Spanish  | 50k            |\n\n## Accuracy\n\nThe fine-tuned model obtained the following accuracy on 5,000 held-out product reviews in each of the languages:\n\n- Accuracy (exact) is the exact match for the number of stars.\n- Accuracy (off-by-1) is the percentage of reviews where the number of stars the model predicts differs by a maximum of 1 from the number given by the human reviewer. \n\n\n| Language | Accuracy (exact) | Accuracy (off-by-1) |\n| -------- | ---------------------- | ------------------- |\n| English  | 67%                 | 95%\n| Dutch    | 57%                 | 93%\n| German   | 61%                 | 94%\n| French   | 59%                 | 94%\n| Italian  | 59%                 | 95%\n| Spanish  | 58%                 | 95%\n\n## Contact \n\nIn addition to this model, [NLP Town](http://nlp.town) offers custom models for many languages and NLP tasks.\n\nIf you found this model useful, you can [buy us a coffee](https://www.buymeacoffee.com/yvespeirsman).\n\nFeel free to contact us for questions, feedback and/or requests for similar models.",
    "card_content": "---\nlanguage:\n- en\n- nl\n- de\n- fr\n- it\n- es\nlicense: mit\n---\n\n# bert-base-multilingual-uncased-sentiment\n\nVisit the [NLP Town website](https://www.nlp.town) for an updated version of this model, with a 40% error reduction on product reviews.\n\nThis is a bert-base-multilingual-uncased model finetuned for sentiment analysis on product reviews in six languages: English, Dutch, German, French, Spanish, and Italian. It predicts the sentiment of the review as a number of stars (between 1 and 5).\n\nThis model is intended for direct use as a sentiment analysis model for product reviews in any of the six languages above or for further finetuning on related sentiment analysis tasks.\n\n## Training data\n\nHere is the number of product reviews we used for finetuning the model: \n\n| Language | Number of reviews |\n| -------- | ----------------- |\n| English  | 150k           |\n| Dutch    | 80k            |\n| German   | 137k           |\n| French   | 140k           |\n| Italian  | 72k            |\n| Spanish  | 50k            |\n\n## Accuracy\n\nThe fine-tuned model obtained the following accuracy on 5,000 held-out product reviews in each of the languages:\n\n- Accuracy (exact) is the exact match for the number of stars.\n- Accuracy (off-by-1) is the percentage of reviews where the number of stars the model predicts differs by a maximum of 1 from the number given by the human reviewer. \n\n\n| Language | Accuracy (exact) | Accuracy (off-by-1) |\n| -------- | ---------------------- | ------------------- |\n| English  | 67%                 | 95%\n| Dutch    | 57%                 | 93%\n| German   | 61%                 | 94%\n| French   | 59%                 | 94%\n| Italian  | 59%                 | 95%\n| Spanish  | 58%                 | 95%\n\n## Contact \n\nIn addition to this model, [NLP Town](http://nlp.town) offers custom models for many languages and NLP tasks.\n\nIf you found this model useful, you can [buy us a coffee](https://www.buymeacoffee.com/yvespeirsman).\n\nFeel free to contact us for questions, feedback and/or requests for similar models.",
    "library_name": "transformers"
  },
  {
    "model_id": "BAAI/bge-reranker-v2-m3",
    "model_name": "BAAI/bge-reranker-v2-m3",
    "author": "BAAI",
    "downloads": 990506,
    "likes": 573,
    "tags": [
      "sentence-transformers",
      "safetensors",
      "xlm-roberta",
      "text-classification",
      "transformers",
      "text-embeddings-inference",
      "multilingual",
      "arxiv:2312.15503",
      "arxiv:2402.03216",
      "license:apache-2.0",
      "region:us"
    ],
    "card_url": "https://huggingface.co/BAAI/bge-reranker-v2-m3",
    "dependencies": [
      [
        "FlagEmbedding",
        "1.1.9"
      ],
      [
        "huggingface_hub",
        "0.19.4"
      ],
      [
        "setuptools",
        "68.2.2"
      ]
    ],
    "analysis_date": "2025-03-26T00:47:30.319972",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "xlm-roberta",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": [
        "multilingual"
      ],
      "license": "apache-2.0",
      "tags": [
        "transformers",
        "sentence-transformers",
        "text-embeddings-inference"
      ],
      "pipeline_tag": "text-classification"
    },
    "card_text": "\n# Reranker\n\n**More details please refer to our Github: [FlagEmbedding](https://github.com/FlagOpen/FlagEmbedding/tree/master).**\n\n- [Model List](#model-list)\n- [Usage](#usage)\n- [Fine-tuning](#fine-tune)\n- [Evaluation](#evaluation)\n- [Citation](#citation)\n\nDifferent from embedding model, reranker uses question and document as input and directly output similarity instead of embedding. \nYou can get a relevance score by inputting query and passage to the reranker. \nAnd the score can be mapped to a float value in [0,1] by sigmoid function.\n\n\n## Model List\n\n| Model                                                                     | Base model                                                           | Language | layerwise |                           feature                            |\n|:--------------------------------------------------------------------------|:--------:|:-----------------------------------------------------------------------------------------------------------------------------------:|:----------------------------------------------------------------------------------------------:|:----------------------------------------------------------------------------------------------:|\n| [BAAI/bge-reranker-base](https://huggingface.co/BAAI/bge-reranker-base) | [xlm-roberta-base](https://huggingface.co/xlm-roberta-base) | Chinese and English |     -     | Lightweight reranker model, easy to deploy, with fast inference. |\n| [BAAI/bge-reranker-large](https://huggingface.co/BAAI/bge-reranker-large) | [xlm-roberta-large](https://huggingface.co/FacebookAI/xlm-roberta-large) | Chinese and English |     -     | Lightweight reranker model, easy to deploy, with fast inference. |\n| [BAAI/bge-reranker-v2-m3](https://huggingface.co/BAAI/bge-reranker-v2-m3) | [bge-m3](https://huggingface.co/BAAI/bge-m3) |    Multilingual     |     -     | Lightweight reranker model, possesses strong multilingual capabilities, easy to deploy, with fast inference. |\n| [BAAI/bge-reranker-v2-gemma](https://huggingface.co/BAAI/bge-reranker-v2-gemma) |      [gemma-2b](https://huggingface.co/google/gemma-2b)      |    Multilingual     |     -     | Suitable for multilingual contexts, performs well in both English proficiency and multilingual capabilities. |\n| [BAAI/bge-reranker-v2-minicpm-layerwise](https://huggingface.co/BAAI/bge-reranker-v2-minicpm-layerwise) | [MiniCPM-2B-dpo-bf16](https://huggingface.co/openbmb/MiniCPM-2B-dpo-bf16) |    Multilingual     |   8-40    | Suitable for multilingual contexts, performs well in both English and Chinese proficiency, allows freedom to select layers for output, facilitating accelerated inference. |\n\n\nYou can select the model according your senario and resource. \n- For **multilingual**, utilize [BAAI/bge-reranker-v2-m3](https://huggingface.co/BAAI/bge-reranker-v2-m3) and [BAAI/bge-reranker-v2-gemma](https://huggingface.co/BAAI/bge-reranker-v2-gemma)\n\n- For **Chinese or English**, utilize [BAAI/bge-reranker-v2-m3](https://huggingface.co/BAAI/bge-reranker-v2-m3) and [BAAI/bge-reranker-v2-minicpm-layerwise](https://huggingface.co/BAAI/bge-reranker-v2-minicpm-layerwise). \n\n- For **efficiency**, utilize [BAAI/bge-reranker-v2-m3](https://huggingface.co/BAAI/bge-reranker-v2-m3) and the low layer of [BAAI/bge-reranker-v2-minicpm-layerwise](https://huggingface.co/BAAI/bge-reranker-v2-minicpm-layerwise). \n\n- For better performance, recommand [BAAI/bge-reranker-v2-minicpm-layerwise](https://huggingface.co/BAAI/bge-reranker-v2-minicpm-layerwise) and [BAAI/bge-reranker-v2-gemma](https://huggingface.co/BAAI/bge-reranker-v2-gemma)\n\n## Usage \n### Using FlagEmbedding\n\n```\npip install -U FlagEmbedding\n```\n\n#### For normal reranker (bge-reranker-base / bge-reranker-large / bge-reranker-v2-m3 )\n\nGet relevance scores (higher scores indicate more relevance):\n\n```python\nfrom FlagEmbedding import FlagReranker\nreranker = FlagReranker('BAAI/bge-reranker-v2-m3', use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\n\nscore = reranker.compute_score(['query', 'passage'])\nprint(score) # -5.65234375\n\n# You can map the scores into 0-1 by set \"normalize=True\", which will apply sigmoid function to the score\nscore = reranker.compute_score(['query', 'passage'], normalize=True)\nprint(score) # 0.003497010252573502\n\nscores = reranker.compute_score([['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']])\nprint(scores) # [-8.1875, 5.26171875]\n\n# You can map the scores into 0-1 by set \"normalize=True\", which will apply sigmoid function to the score\nscores = reranker.compute_score([['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']], normalize=True)\nprint(scores) # [0.00027803096387751553, 0.9948403768236574]\n```\n\n#### For LLM-based reranker\n\n```python\nfrom FlagEmbedding import FlagLLMReranker\nreranker = FlagLLMReranker('BAAI/bge-reranker-v2-gemma', use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\n# reranker = FlagLLMReranker('BAAI/bge-reranker-v2-gemma', use_bf16=True) # You can also set use_bf16=True to speed up computation with a slight performance degradation\n\nscore = reranker.compute_score(['query', 'passage'])\nprint(score)\n\nscores = reranker.compute_score([['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']])\nprint(scores)\n```\n\n#### For LLM-based layerwise reranker\n\n```python\nfrom FlagEmbedding import LayerWiseFlagLLMReranker\nreranker = LayerWiseFlagLLMReranker('BAAI/bge-reranker-v2-minicpm-layerwise', use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\n# reranker = LayerWiseFlagLLMReranker('BAAI/bge-reranker-v2-minicpm-layerwise', use_bf16=True) # You can also set use_bf16=True to speed up computation with a slight performance degradation\n\nscore = reranker.compute_score(['query', 'passage'], cutoff_layers=[28]) # Adjusting 'cutoff_layers' to pick which layers are used for computing the score.\nprint(score)\n\nscores = reranker.compute_score([['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']], cutoff_layers=[28])\nprint(scores)\n```\n\n### Using Huggingface transformers\n\n#### For normal reranker (bge-reranker-base / bge-reranker-large / bge-reranker-v2-m3 )\n\nGet relevance scores (higher scores indicate more relevance):\n\n```python\nimport torch\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained('BAAI/bge-reranker-v2-m3')\nmodel = AutoModelForSequenceClassification.from_pretrained('BAAI/bge-reranker-v2-m3')\nmodel.eval()\n\npairs = [['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']]\nwith torch.no_grad():\n    inputs = tokenizer(pairs, padding=True, truncation=True, return_tensors='pt', max_length=512)\n    scores = model(**inputs, return_dict=True).logits.view(-1, ).float()\n    print(scores)\n```\n\n#### For LLM-based reranker\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ndef get_inputs(pairs, tokenizer, prompt=None, max_length=1024):\n    if prompt is None:\n        prompt = \"Given a query A and a passage B, determine whether the passage contains an answer to the query by providing a prediction of either 'Yes' or 'No'.\"\n    sep = \"\\n\"\n    prompt_inputs = tokenizer(prompt,\n                              return_tensors=None,\n                              add_special_tokens=False)['input_ids']\n    sep_inputs = tokenizer(sep,\n                           return_tensors=None,\n                           add_special_tokens=False)['input_ids']\n    inputs = []\n    for query, passage in pairs:\n        query_inputs = tokenizer(f'A: {query}',\n                                 return_tensors=None,\n                                 add_special_tokens=False,\n                                 max_length=max_length * 3 // 4,\n                                 truncation=True)\n        passage_inputs = tokenizer(f'B: {passage}',\n                                   return_tensors=None,\n                                   add_special_tokens=False,\n                                   max_length=max_length,\n                                   truncation=True)\n        item = tokenizer.prepare_for_model(\n            [tokenizer.bos_token_id] + query_inputs['input_ids'],\n            sep_inputs + passage_inputs['input_ids'],\n            truncation='only_second',\n            max_length=max_length,\n            padding=False,\n            return_attention_mask=False,\n            return_token_type_ids=False,\n            add_special_tokens=False\n        )\n        item['input_ids'] = item['input_ids'] + sep_inputs + prompt_inputs\n        item['attention_mask'] = [1] * len(item['input_ids'])\n        inputs.append(item)\n    return tokenizer.pad(\n            inputs,\n            padding=True,\n            max_length=max_length + len(sep_inputs) + len(prompt_inputs),\n            pad_to_multiple_of=8,\n            return_tensors='pt',\n    )\n\ntokenizer = AutoTokenizer.from_pretrained('BAAI/bge-reranker-v2-gemma')\nmodel = AutoModelForCausalLM.from_pretrained('BAAI/bge-reranker-v2-gemma')\nyes_loc = tokenizer('Yes', add_special_tokens=False)['input_ids'][0]\nmodel.eval()\n\npairs = [['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']]\nwith torch.no_grad():\n    inputs = get_inputs(pairs, tokenizer)\n    scores = model(**inputs, return_dict=True).logits[:, -1, yes_loc].view(-1, ).float()\n    print(scores)\n```\n\n#### For LLM-based layerwise reranker\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ndef get_inputs(pairs, tokenizer, prompt=None, max_length=1024):\n    if prompt is None:\n        prompt = \"Given a query A and a passage B, determine whether the passage contains an answer to the query by providing a prediction of either 'Yes' or 'No'.\"\n    sep = \"\\n\"\n    prompt_inputs = tokenizer(prompt,\n                              return_tensors=None,\n                              add_special_tokens=False)['input_ids']\n    sep_inputs = tokenizer(sep,\n                           return_tensors=None,\n                           add_special_tokens=False)['input_ids']\n    inputs = []\n    for query, passage in pairs:\n        query_inputs = tokenizer(f'A: {query}',\n                                 return_tensors=None,\n                                 add_special_tokens=False,\n                                 max_length=max_length * 3 // 4,\n                                 truncation=True)\n        passage_inputs = tokenizer(f'B: {passage}',\n                                   return_tensors=None,\n                                   add_special_tokens=False,\n                                   max_length=max_length,\n                                   truncation=True)\n        item = tokenizer.prepare_for_model(\n            [tokenizer.bos_token_id] + query_inputs['input_ids'],\n            sep_inputs + passage_inputs['input_ids'],\n            truncation='only_second',\n            max_length=max_length,\n            padding=False,\n            return_attention_mask=False,\n            return_token_type_ids=False,\n            add_special_tokens=False\n        )\n        item['input_ids'] = item['input_ids'] + sep_inputs + prompt_inputs\n        item['attention_mask'] = [1] * len(item['input_ids'])\n        inputs.append(item)\n    return tokenizer.pad(\n            inputs,\n            padding=True,\n            max_length=max_length + len(sep_inputs) + len(prompt_inputs),\n            pad_to_multiple_of=8,\n            return_tensors='pt',\n    )\n\ntokenizer = AutoTokenizer.from_pretrained('BAAI/bge-reranker-v2-minicpm-layerwise', trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained('BAAI/bge-reranker-v2-minicpm-layerwise', trust_remote_code=True, torch_dtype=torch.bfloat16)\nmodel = model.to('cuda')\nmodel.eval()\n\npairs = [['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']]\nwith torch.no_grad():\n    inputs = get_inputs(pairs, tokenizer).to(model.device)\n    all_scores = model(**inputs, return_dict=True, cutoff_layers=[28])\n    all_scores = [scores[:, -1].view(-1, ).float() for scores in all_scores[0]]\n    print(all_scores)\n```\n\n## Fine-tune\n\n### Data Format\n\nTrain data should be a json file, where each line is a dict like this:\n\n```\n{\"query\": str, \"pos\": List[str], \"neg\":List[str], \"prompt\": str}\n```\n\n`query` is the query, and `pos` is a list of positive texts, `neg` is a list of negative texts, `prompt` indicates the relationship between query and texts. If you have no negative texts for a query, you can random sample some from the entire corpus as the negatives.\n\nSee [toy_finetune_data.jsonl](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/llm_reranker/toy_finetune_data.jsonl) for a toy data file.\n\n### Train\n\nYou can fine-tune the reranker with the following code:\n\n**For llm-based reranker**\n\n```shell\ntorchrun --nproc_per_node {number of gpus} \\\n-m FlagEmbedding.llm_reranker.finetune_for_instruction.run \\\n--output_dir {path to save model} \\\n--model_name_or_path google/gemma-2b \\\n--train_data ./toy_finetune_data.jsonl \\\n--learning_rate 2e-4 \\\n--num_train_epochs 1 \\\n--per_device_train_batch_size 1 \\\n--gradient_accumulation_steps 16 \\\n--dataloader_drop_last True \\\n--query_max_len 512 \\\n--passage_max_len 512 \\\n--train_group_size 16 \\\n--logging_steps 1 \\\n--save_steps 2000 \\\n--save_total_limit 50 \\\n--ddp_find_unused_parameters False \\\n--gradient_checkpointing \\\n--deepspeed stage1.json \\\n--warmup_ratio 0.1 \\\n--bf16 \\\n--use_lora True \\\n--lora_rank 32 \\\n--lora_alpha 64 \\\n--use_flash_attn True \\\n--target_modules q_proj k_proj v_proj o_proj\n```\n\n**For llm-based layerwise reranker**\n\n```shell\ntorchrun --nproc_per_node {number of gpus} \\\n-m FlagEmbedding.llm_reranker.finetune_for_layerwise.run \\\n--output_dir {path to save model} \\\n--model_name_or_path openbmb/MiniCPM-2B-dpo-bf16 \\\n--train_data ./toy_finetune_data.jsonl \\\n--learning_rate 2e-4 \\\n--num_train_epochs 1 \\\n--per_device_train_batch_size 1 \\\n--gradient_accumulation_steps 16 \\\n--dataloader_drop_last True \\\n--query_max_len 512 \\\n--passage_max_len 512 \\\n--train_group_size 16 \\\n--logging_steps 1 \\\n--save_steps 2000 \\\n--save_total_limit 50 \\\n--ddp_find_unused_parameters False \\\n--gradient_checkpointing \\\n--deepspeed stage1.json \\\n--warmup_ratio 0.1 \\\n--bf16 \\\n--use_lora True \\\n--lora_rank 32 \\\n--lora_alpha 64 \\\n--use_flash_attn True \\\n--target_modules q_proj k_proj v_proj o_proj \\\n--start_layer 8 \\\n--head_multi True \\\n--head_type simple \\\n--lora_extra_parameters linear_head\n```\n\nOur rerankers are initialized from [google/gemma-2b](https://huggingface.co/google/gemma-2b) (for llm-based reranker) and [openbmb/MiniCPM-2B-dpo-bf16](https://huggingface.co/openbmb/MiniCPM-2B-dpo-bf16) (for llm-based layerwise reranker), and we train it on a mixture of multilingual datasets:\n\n- [bge-m3-data](https://huggingface.co/datasets/Shitao/bge-m3-data)\n- [quora train data](https://huggingface.co/datasets/quora)\n- [fever train data](https://fever.ai/dataset/fever.html)\n\n## Evaluation\n\n- llama-index.\n\n![image-20240317193909373](./assets/llama-index.png)\n\n\n- BEIR.   \n\nrereank the top 100 results from bge-en-v1.5 large.\n\n![image-20240317174633333](./assets/BEIR-bge-en-v1.5.png)\n\nrereank the top 100 results from e5 mistral 7b instruct.\n\n![image-20240317172949713](./assets/BEIR-e5-mistral.png)\n\n- CMTEB-retrieval.   \nIt rereank the top 100 results from bge-zh-v1.5 large.\n\n![image-20240317173026235](./assets/CMTEB-retrieval-bge-zh-v1.5.png)\n\n- miracl (multi-language).   \nIt rereank the top 100 results from bge-m3.\n\n![image-20240317173117639](./assets/miracl-bge-m3.png)\n\n\n\n## Citation\n\nIf you find this repository useful, please consider giving a star and citation\n\n```bibtex\n@misc{li2023making,\n      title={Making Large Language Models A Better Foundation For Dense Retrieval}, \n      author={Chaofan Li and Zheng Liu and Shitao Xiao and Yingxia Shao},\n      year={2023},\n      eprint={2312.15503},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n@misc{chen2024bge,\n      title={BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation}, \n      author={Jianlv Chen and Shitao Xiao and Peitian Zhang and Kun Luo and Defu Lian and Zheng Liu},\n      year={2024},\n      eprint={2402.03216},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```",
    "card_content": "---\nlanguage:\n- multilingual\nlicense: apache-2.0\ntags:\n- transformers\n- sentence-transformers\n- text-embeddings-inference\npipeline_tag: text-classification\n---\n\n# Reranker\n\n**More details please refer to our Github: [FlagEmbedding](https://github.com/FlagOpen/FlagEmbedding/tree/master).**\n\n- [Model List](#model-list)\n- [Usage](#usage)\n- [Fine-tuning](#fine-tune)\n- [Evaluation](#evaluation)\n- [Citation](#citation)\n\nDifferent from embedding model, reranker uses question and document as input and directly output similarity instead of embedding. \nYou can get a relevance score by inputting query and passage to the reranker. \nAnd the score can be mapped to a float value in [0,1] by sigmoid function.\n\n\n## Model List\n\n| Model                                                                     | Base model                                                           | Language | layerwise |                           feature                            |\n|:--------------------------------------------------------------------------|:--------:|:-----------------------------------------------------------------------------------------------------------------------------------:|:----------------------------------------------------------------------------------------------:|:----------------------------------------------------------------------------------------------:|\n| [BAAI/bge-reranker-base](https://huggingface.co/BAAI/bge-reranker-base) | [xlm-roberta-base](https://huggingface.co/xlm-roberta-base) | Chinese and English |     -     | Lightweight reranker model, easy to deploy, with fast inference. |\n| [BAAI/bge-reranker-large](https://huggingface.co/BAAI/bge-reranker-large) | [xlm-roberta-large](https://huggingface.co/FacebookAI/xlm-roberta-large) | Chinese and English |     -     | Lightweight reranker model, easy to deploy, with fast inference. |\n| [BAAI/bge-reranker-v2-m3](https://huggingface.co/BAAI/bge-reranker-v2-m3) | [bge-m3](https://huggingface.co/BAAI/bge-m3) |    Multilingual     |     -     | Lightweight reranker model, possesses strong multilingual capabilities, easy to deploy, with fast inference. |\n| [BAAI/bge-reranker-v2-gemma](https://huggingface.co/BAAI/bge-reranker-v2-gemma) |      [gemma-2b](https://huggingface.co/google/gemma-2b)      |    Multilingual     |     -     | Suitable for multilingual contexts, performs well in both English proficiency and multilingual capabilities. |\n| [BAAI/bge-reranker-v2-minicpm-layerwise](https://huggingface.co/BAAI/bge-reranker-v2-minicpm-layerwise) | [MiniCPM-2B-dpo-bf16](https://huggingface.co/openbmb/MiniCPM-2B-dpo-bf16) |    Multilingual     |   8-40    | Suitable for multilingual contexts, performs well in both English and Chinese proficiency, allows freedom to select layers for output, facilitating accelerated inference. |\n\n\nYou can select the model according your senario and resource. \n- For **multilingual**, utilize [BAAI/bge-reranker-v2-m3](https://huggingface.co/BAAI/bge-reranker-v2-m3) and [BAAI/bge-reranker-v2-gemma](https://huggingface.co/BAAI/bge-reranker-v2-gemma)\n\n- For **Chinese or English**, utilize [BAAI/bge-reranker-v2-m3](https://huggingface.co/BAAI/bge-reranker-v2-m3) and [BAAI/bge-reranker-v2-minicpm-layerwise](https://huggingface.co/BAAI/bge-reranker-v2-minicpm-layerwise). \n\n- For **efficiency**, utilize [BAAI/bge-reranker-v2-m3](https://huggingface.co/BAAI/bge-reranker-v2-m3) and the low layer of [BAAI/bge-reranker-v2-minicpm-layerwise](https://huggingface.co/BAAI/bge-reranker-v2-minicpm-layerwise). \n\n- For better performance, recommand [BAAI/bge-reranker-v2-minicpm-layerwise](https://huggingface.co/BAAI/bge-reranker-v2-minicpm-layerwise) and [BAAI/bge-reranker-v2-gemma](https://huggingface.co/BAAI/bge-reranker-v2-gemma)\n\n## Usage \n### Using FlagEmbedding\n\n```\npip install -U FlagEmbedding\n```\n\n#### For normal reranker (bge-reranker-base / bge-reranker-large / bge-reranker-v2-m3 )\n\nGet relevance scores (higher scores indicate more relevance):\n\n```python\nfrom FlagEmbedding import FlagReranker\nreranker = FlagReranker('BAAI/bge-reranker-v2-m3', use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\n\nscore = reranker.compute_score(['query', 'passage'])\nprint(score) # -5.65234375\n\n# You can map the scores into 0-1 by set \"normalize=True\", which will apply sigmoid function to the score\nscore = reranker.compute_score(['query', 'passage'], normalize=True)\nprint(score) # 0.003497010252573502\n\nscores = reranker.compute_score([['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']])\nprint(scores) # [-8.1875, 5.26171875]\n\n# You can map the scores into 0-1 by set \"normalize=True\", which will apply sigmoid function to the score\nscores = reranker.compute_score([['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']], normalize=True)\nprint(scores) # [0.00027803096387751553, 0.9948403768236574]\n```\n\n#### For LLM-based reranker\n\n```python\nfrom FlagEmbedding import FlagLLMReranker\nreranker = FlagLLMReranker('BAAI/bge-reranker-v2-gemma', use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\n# reranker = FlagLLMReranker('BAAI/bge-reranker-v2-gemma', use_bf16=True) # You can also set use_bf16=True to speed up computation with a slight performance degradation\n\nscore = reranker.compute_score(['query', 'passage'])\nprint(score)\n\nscores = reranker.compute_score([['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']])\nprint(scores)\n```\n\n#### For LLM-based layerwise reranker\n\n```python\nfrom FlagEmbedding import LayerWiseFlagLLMReranker\nreranker = LayerWiseFlagLLMReranker('BAAI/bge-reranker-v2-minicpm-layerwise', use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\n# reranker = LayerWiseFlagLLMReranker('BAAI/bge-reranker-v2-minicpm-layerwise', use_bf16=True) # You can also set use_bf16=True to speed up computation with a slight performance degradation\n\nscore = reranker.compute_score(['query', 'passage'], cutoff_layers=[28]) # Adjusting 'cutoff_layers' to pick which layers are used for computing the score.\nprint(score)\n\nscores = reranker.compute_score([['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']], cutoff_layers=[28])\nprint(scores)\n```\n\n### Using Huggingface transformers\n\n#### For normal reranker (bge-reranker-base / bge-reranker-large / bge-reranker-v2-m3 )\n\nGet relevance scores (higher scores indicate more relevance):\n\n```python\nimport torch\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained('BAAI/bge-reranker-v2-m3')\nmodel = AutoModelForSequenceClassification.from_pretrained('BAAI/bge-reranker-v2-m3')\nmodel.eval()\n\npairs = [['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']]\nwith torch.no_grad():\n    inputs = tokenizer(pairs, padding=True, truncation=True, return_tensors='pt', max_length=512)\n    scores = model(**inputs, return_dict=True).logits.view(-1, ).float()\n    print(scores)\n```\n\n#### For LLM-based reranker\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ndef get_inputs(pairs, tokenizer, prompt=None, max_length=1024):\n    if prompt is None:\n        prompt = \"Given a query A and a passage B, determine whether the passage contains an answer to the query by providing a prediction of either 'Yes' or 'No'.\"\n    sep = \"\\n\"\n    prompt_inputs = tokenizer(prompt,\n                              return_tensors=None,\n                              add_special_tokens=False)['input_ids']\n    sep_inputs = tokenizer(sep,\n                           return_tensors=None,\n                           add_special_tokens=False)['input_ids']\n    inputs = []\n    for query, passage in pairs:\n        query_inputs = tokenizer(f'A: {query}',\n                                 return_tensors=None,\n                                 add_special_tokens=False,\n                                 max_length=max_length * 3 // 4,\n                                 truncation=True)\n        passage_inputs = tokenizer(f'B: {passage}',\n                                   return_tensors=None,\n                                   add_special_tokens=False,\n                                   max_length=max_length,\n                                   truncation=True)\n        item = tokenizer.prepare_for_model(\n            [tokenizer.bos_token_id] + query_inputs['input_ids'],\n            sep_inputs + passage_inputs['input_ids'],\n            truncation='only_second',\n            max_length=max_length,\n            padding=False,\n            return_attention_mask=False,\n            return_token_type_ids=False,\n            add_special_tokens=False\n        )\n        item['input_ids'] = item['input_ids'] + sep_inputs + prompt_inputs\n        item['attention_mask'] = [1] * len(item['input_ids'])\n        inputs.append(item)\n    return tokenizer.pad(\n            inputs,\n            padding=True,\n            max_length=max_length + len(sep_inputs) + len(prompt_inputs),\n            pad_to_multiple_of=8,\n            return_tensors='pt',\n    )\n\ntokenizer = AutoTokenizer.from_pretrained('BAAI/bge-reranker-v2-gemma')\nmodel = AutoModelForCausalLM.from_pretrained('BAAI/bge-reranker-v2-gemma')\nyes_loc = tokenizer('Yes', add_special_tokens=False)['input_ids'][0]\nmodel.eval()\n\npairs = [['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']]\nwith torch.no_grad():\n    inputs = get_inputs(pairs, tokenizer)\n    scores = model(**inputs, return_dict=True).logits[:, -1, yes_loc].view(-1, ).float()\n    print(scores)\n```\n\n#### For LLM-based layerwise reranker\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ndef get_inputs(pairs, tokenizer, prompt=None, max_length=1024):\n    if prompt is None:\n        prompt = \"Given a query A and a passage B, determine whether the passage contains an answer to the query by providing a prediction of either 'Yes' or 'No'.\"\n    sep = \"\\n\"\n    prompt_inputs = tokenizer(prompt,\n                              return_tensors=None,\n                              add_special_tokens=False)['input_ids']\n    sep_inputs = tokenizer(sep,\n                           return_tensors=None,\n                           add_special_tokens=False)['input_ids']\n    inputs = []\n    for query, passage in pairs:\n        query_inputs = tokenizer(f'A: {query}',\n                                 return_tensors=None,\n                                 add_special_tokens=False,\n                                 max_length=max_length * 3 // 4,\n                                 truncation=True)\n        passage_inputs = tokenizer(f'B: {passage}',\n                                   return_tensors=None,\n                                   add_special_tokens=False,\n                                   max_length=max_length,\n                                   truncation=True)\n        item = tokenizer.prepare_for_model(\n            [tokenizer.bos_token_id] + query_inputs['input_ids'],\n            sep_inputs + passage_inputs['input_ids'],\n            truncation='only_second',\n            max_length=max_length,\n            padding=False,\n            return_attention_mask=False,\n            return_token_type_ids=False,\n            add_special_tokens=False\n        )\n        item['input_ids'] = item['input_ids'] + sep_inputs + prompt_inputs\n        item['attention_mask'] = [1] * len(item['input_ids'])\n        inputs.append(item)\n    return tokenizer.pad(\n            inputs,\n            padding=True,\n            max_length=max_length + len(sep_inputs) + len(prompt_inputs),\n            pad_to_multiple_of=8,\n            return_tensors='pt',\n    )\n\ntokenizer = AutoTokenizer.from_pretrained('BAAI/bge-reranker-v2-minicpm-layerwise', trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained('BAAI/bge-reranker-v2-minicpm-layerwise', trust_remote_code=True, torch_dtype=torch.bfloat16)\nmodel = model.to('cuda')\nmodel.eval()\n\npairs = [['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']]\nwith torch.no_grad():\n    inputs = get_inputs(pairs, tokenizer).to(model.device)\n    all_scores = model(**inputs, return_dict=True, cutoff_layers=[28])\n    all_scores = [scores[:, -1].view(-1, ).float() for scores in all_scores[0]]\n    print(all_scores)\n```\n\n## Fine-tune\n\n### Data Format\n\nTrain data should be a json file, where each line is a dict like this:\n\n```\n{\"query\": str, \"pos\": List[str], \"neg\":List[str], \"prompt\": str}\n```\n\n`query` is the query, and `pos` is a list of positive texts, `neg` is a list of negative texts, `prompt` indicates the relationship between query and texts. If you have no negative texts for a query, you can random sample some from the entire corpus as the negatives.\n\nSee [toy_finetune_data.jsonl](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/llm_reranker/toy_finetune_data.jsonl) for a toy data file.\n\n### Train\n\nYou can fine-tune the reranker with the following code:\n\n**For llm-based reranker**\n\n```shell\ntorchrun --nproc_per_node {number of gpus} \\\n-m FlagEmbedding.llm_reranker.finetune_for_instruction.run \\\n--output_dir {path to save model} \\\n--model_name_or_path google/gemma-2b \\\n--train_data ./toy_finetune_data.jsonl \\\n--learning_rate 2e-4 \\\n--num_train_epochs 1 \\\n--per_device_train_batch_size 1 \\\n--gradient_accumulation_steps 16 \\\n--dataloader_drop_last True \\\n--query_max_len 512 \\\n--passage_max_len 512 \\\n--train_group_size 16 \\\n--logging_steps 1 \\\n--save_steps 2000 \\\n--save_total_limit 50 \\\n--ddp_find_unused_parameters False \\\n--gradient_checkpointing \\\n--deepspeed stage1.json \\\n--warmup_ratio 0.1 \\\n--bf16 \\\n--use_lora True \\\n--lora_rank 32 \\\n--lora_alpha 64 \\\n--use_flash_attn True \\\n--target_modules q_proj k_proj v_proj o_proj\n```\n\n**For llm-based layerwise reranker**\n\n```shell\ntorchrun --nproc_per_node {number of gpus} \\\n-m FlagEmbedding.llm_reranker.finetune_for_layerwise.run \\\n--output_dir {path to save model} \\\n--model_name_or_path openbmb/MiniCPM-2B-dpo-bf16 \\\n--train_data ./toy_finetune_data.jsonl \\\n--learning_rate 2e-4 \\\n--num_train_epochs 1 \\\n--per_device_train_batch_size 1 \\\n--gradient_accumulation_steps 16 \\\n--dataloader_drop_last True \\\n--query_max_len 512 \\\n--passage_max_len 512 \\\n--train_group_size 16 \\\n--logging_steps 1 \\\n--save_steps 2000 \\\n--save_total_limit 50 \\\n--ddp_find_unused_parameters False \\\n--gradient_checkpointing \\\n--deepspeed stage1.json \\\n--warmup_ratio 0.1 \\\n--bf16 \\\n--use_lora True \\\n--lora_rank 32 \\\n--lora_alpha 64 \\\n--use_flash_attn True \\\n--target_modules q_proj k_proj v_proj o_proj \\\n--start_layer 8 \\\n--head_multi True \\\n--head_type simple \\\n--lora_extra_parameters linear_head\n```\n\nOur rerankers are initialized from [google/gemma-2b](https://huggingface.co/google/gemma-2b) (for llm-based reranker) and [openbmb/MiniCPM-2B-dpo-bf16](https://huggingface.co/openbmb/MiniCPM-2B-dpo-bf16) (for llm-based layerwise reranker), and we train it on a mixture of multilingual datasets:\n\n- [bge-m3-data](https://huggingface.co/datasets/Shitao/bge-m3-data)\n- [quora train data](https://huggingface.co/datasets/quora)\n- [fever train data](https://fever.ai/dataset/fever.html)\n\n## Evaluation\n\n- llama-index.\n\n![image-20240317193909373](./assets/llama-index.png)\n\n\n- BEIR.   \n\nrereank the top 100 results from bge-en-v1.5 large.\n\n![image-20240317174633333](./assets/BEIR-bge-en-v1.5.png)\n\nrereank the top 100 results from e5 mistral 7b instruct.\n\n![image-20240317172949713](./assets/BEIR-e5-mistral.png)\n\n- CMTEB-retrieval.   \nIt rereank the top 100 results from bge-zh-v1.5 large.\n\n![image-20240317173026235](./assets/CMTEB-retrieval-bge-zh-v1.5.png)\n\n- miracl (multi-language).   \nIt rereank the top 100 results from bge-m3.\n\n![image-20240317173117639](./assets/miracl-bge-m3.png)\n\n\n\n## Citation\n\nIf you find this repository useful, please consider giving a star and citation\n\n```bibtex\n@misc{li2023making,\n      title={Making Large Language Models A Better Foundation For Dense Retrieval}, \n      author={Chaofan Li and Zheng Liu and Shitao Xiao and Yingxia Shao},\n      year={2023},\n      eprint={2312.15503},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n@misc{chen2024bge,\n      title={BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation}, \n      author={Jianlv Chen and Shitao Xiao and Peitian Zhang and Kun Luo and Defu Lian and Zheng Liu},\n      year={2024},\n      eprint={2402.03216},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```",
    "library_name": "sentence-transformers"
  },
  {
    "model_id": "cardiffnlp/twitter-roberta-base-emotion",
    "model_name": "cardiffnlp/twitter-roberta-base-emotion",
    "author": "cardiffnlp",
    "downloads": 955090,
    "likes": 43,
    "tags": [
      "transformers",
      "pytorch",
      "tf",
      "jax",
      "roberta",
      "text-classification",
      "arxiv:2010.12421",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/cardiffnlp/twitter-roberta-base-emotion",
    "dependencies": [
      [
        "transformers",
        "4.30.2"
      ],
      [
        "numpy",
        "1.23.5"
      ],
      [
        "scipy",
        "1.10.1"
      ],
      [
        "csv",
        "1.0"
      ],
      [
        "urllib",
        null
      ]
    ],
    "analysis_date": "2025-03-26T00:47:32.175783",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "roberta",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {},
    "card_text": "# Twitter-roBERTa-base for Emotion Recognition\n\nThis is a RoBERTa-base model trained on ~58M tweets and finetuned for emotion recognition with the TweetEval benchmark.\n\n- Paper: [_TweetEval_ benchmark (Findings of EMNLP 2020)](https://arxiv.org/pdf/2010.12421.pdf). \n- Git Repo: [Tweeteval official repository](https://github.com/cardiffnlp/tweeteval).\n\n<b>New!</b> We just released a new emotion recognition model trained with more emotion types and with a newer RoBERTa-based model. \nSee [twitter-roberta-base-emotion-multilabel-latest](https://huggingface.co/cardiffnlp/twitter-roberta-base-emotion-multilabel-latest) and [TweetNLP](https://github.com/cardiffnlp/tweetnlp) for more details.\n\n## Example of classification\n\n```python\nfrom transformers import AutoModelForSequenceClassification\nfrom transformers import TFAutoModelForSequenceClassification\nfrom transformers import AutoTokenizer\nimport numpy as np\nfrom scipy.special import softmax\nimport csv\nimport urllib.request\n\n# Preprocess text (username and link placeholders)\ndef preprocess(text):\n    new_text = []\n    for t in text.split(\" \"):\n        t = '@user' if t.startswith('@') and len(t) > 1 else t\n        t = 'http' if t.startswith('http') else t\n        new_text.append(t)\n    return \" \".join(new_text)\n\n# Tasks:\n# emoji, emotion, hate, irony, offensive, sentiment\n# stance/abortion, stance/atheism, stance/climate, stance/feminist, stance/hillary\n\ntask='emotion'\nMODEL = f\"cardiffnlp/twitter-roberta-base-{task}\"\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL)\n\n# download label mapping\nmapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt\"\nwith urllib.request.urlopen(mapping_link) as f:\n    html = f.read().decode('utf-8').split(\"\\n\")\n    csvreader = csv.reader(html, delimiter='\\t')\nlabels = [row[1] for row in csvreader if len(row) > 1]\n\n# PT\nmodel = AutoModelForSequenceClassification.from_pretrained(MODEL)\nmodel.save_pretrained(MODEL)\n\ntext = \"Celebrating my promotion \ud83d\ude0e\"\ntext = preprocess(text)\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\nscores = output[0][0].detach().numpy()\nscores = softmax(scores)\n\n# # TF\n# model = TFAutoModelForSequenceClassification.from_pretrained(MODEL)\n# model.save_pretrained(MODEL)\n\n# text = \"Celebrating my promotion \ud83d\ude0e\"\n# encoded_input = tokenizer(text, return_tensors='tf')\n# output = model(encoded_input)\n# scores = output[0][0].numpy()\n# scores = softmax(scores)\n\nranking = np.argsort(scores)\nranking = ranking[::-1]\nfor i in range(scores.shape[0]):\n    l = labels[ranking[i]]\n    s = scores[ranking[i]]\n    print(f\"{i+1}) {l} {np.round(float(s), 4)}\")\n\n```\n\nOutput: \n\n```\n1) joy 0.9382\n2) optimism 0.0362\n3) anger 0.0145\n4) sadness 0.0112\n```\n",
    "card_content": "---\n{}\n---\n# Twitter-roBERTa-base for Emotion Recognition\n\nThis is a RoBERTa-base model trained on ~58M tweets and finetuned for emotion recognition with the TweetEval benchmark.\n\n- Paper: [_TweetEval_ benchmark (Findings of EMNLP 2020)](https://arxiv.org/pdf/2010.12421.pdf). \n- Git Repo: [Tweeteval official repository](https://github.com/cardiffnlp/tweeteval).\n\n<b>New!</b> We just released a new emotion recognition model trained with more emotion types and with a newer RoBERTa-based model. \nSee [twitter-roberta-base-emotion-multilabel-latest](https://huggingface.co/cardiffnlp/twitter-roberta-base-emotion-multilabel-latest) and [TweetNLP](https://github.com/cardiffnlp/tweetnlp) for more details.\n\n## Example of classification\n\n```python\nfrom transformers import AutoModelForSequenceClassification\nfrom transformers import TFAutoModelForSequenceClassification\nfrom transformers import AutoTokenizer\nimport numpy as np\nfrom scipy.special import softmax\nimport csv\nimport urllib.request\n\n# Preprocess text (username and link placeholders)\ndef preprocess(text):\n    new_text = []\n    for t in text.split(\" \"):\n        t = '@user' if t.startswith('@') and len(t) > 1 else t\n        t = 'http' if t.startswith('http') else t\n        new_text.append(t)\n    return \" \".join(new_text)\n\n# Tasks:\n# emoji, emotion, hate, irony, offensive, sentiment\n# stance/abortion, stance/atheism, stance/climate, stance/feminist, stance/hillary\n\ntask='emotion'\nMODEL = f\"cardiffnlp/twitter-roberta-base-{task}\"\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL)\n\n# download label mapping\nmapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt\"\nwith urllib.request.urlopen(mapping_link) as f:\n    html = f.read().decode('utf-8').split(\"\\n\")\n    csvreader = csv.reader(html, delimiter='\\t')\nlabels = [row[1] for row in csvreader if len(row) > 1]\n\n# PT\nmodel = AutoModelForSequenceClassification.from_pretrained(MODEL)\nmodel.save_pretrained(MODEL)\n\ntext = \"Celebrating my promotion \ud83d\ude0e\"\ntext = preprocess(text)\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\nscores = output[0][0].detach().numpy()\nscores = softmax(scores)\n\n# # TF\n# model = TFAutoModelForSequenceClassification.from_pretrained(MODEL)\n# model.save_pretrained(MODEL)\n\n# text = \"Celebrating my promotion \ud83d\ude0e\"\n# encoded_input = tokenizer(text, return_tensors='tf')\n# output = model(encoded_input)\n# scores = output[0][0].numpy()\n# scores = softmax(scores)\n\nranking = np.argsort(scores)\nranking = ranking[::-1]\nfor i in range(scores.shape[0]):\n    l = labels[ranking[i]]\n    s = scores[ranking[i]]\n    print(f\"{i+1}) {l} {np.round(float(s), 4)}\")\n\n```\n\nOutput: \n\n```\n1) joy 0.9382\n2) optimism 0.0362\n3) anger 0.0145\n4) sadness 0.0112\n```\n",
    "library_name": "transformers"
  },
  {
    "model_id": "BAAI/bge-reranker-base",
    "model_name": "BAAI/bge-reranker-base",
    "author": "BAAI",
    "downloads": 944089,
    "likes": 179,
    "tags": [
      "sentence-transformers",
      "pytorch",
      "onnx",
      "safetensors",
      "xlm-roberta",
      "mteb",
      "text-embeddings-inference",
      "text-classification",
      "en",
      "zh",
      "arxiv:2401.03462",
      "arxiv:2312.15503",
      "arxiv:2311.13534",
      "arxiv:2310.07554",
      "arxiv:2309.07597",
      "license:mit",
      "model-index",
      "region:us"
    ],
    "card_url": "https://huggingface.co/BAAI/bge-reranker-base",
    "dependencies": null,
    "analysis_date": "2025-03-26T00:47:35.353716",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "xlm-roberta",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": [
        "en",
        "zh"
      ],
      "license": "mit",
      "library_name": "sentence-transformers",
      "tags": [
        "mteb",
        "text-embeddings-inference"
      ],
      "pipeline_tag": "text-classification",
      "model-index": [
        {
          "name": "bge-reranker-base",
          "results": [
            {
              "task": {
                "type": "Reranking"
              },
              "dataset": {
                "name": "MTEB CMedQAv1",
                "type": "C-MTEB/CMedQAv1-reranking",
                "config": "default",
                "split": "test",
                "revision": "None"
              },
              "metrics": [
                {
                  "type": "map",
                  "value": 81.27206722525007
                },
                {
                  "type": "mrr",
                  "value": 84.14238095238095
                }
              ]
            },
            {
              "task": {
                "type": "Reranking"
              },
              "dataset": {
                "name": "MTEB CMedQAv2",
                "type": "C-MTEB/CMedQAv2-reranking",
                "config": "default",
                "split": "test",
                "revision": "None"
              },
              "metrics": [
                {
                  "type": "map",
                  "value": 84.10369934291236
                },
                {
                  "type": "mrr",
                  "value": 86.79376984126984
                }
              ]
            },
            {
              "task": {
                "type": "Reranking"
              },
              "dataset": {
                "name": "MTEB MMarcoReranking",
                "type": "C-MTEB/Mmarco-reranking",
                "config": "default",
                "split": "dev",
                "revision": "None"
              },
              "metrics": [
                {
                  "type": "map",
                  "value": 35.4600511272538
                },
                {
                  "type": "mrr",
                  "value": 34.60238095238095
                }
              ]
            },
            {
              "task": {
                "type": "Reranking"
              },
              "dataset": {
                "name": "MTEB T2Reranking",
                "type": "C-MTEB/T2Reranking",
                "config": "default",
                "split": "dev",
                "revision": "None"
              },
              "metrics": [
                {
                  "type": "map",
                  "value": 67.27728847727172
                },
                {
                  "type": "mrr",
                  "value": 77.1315192743764
                }
              ]
            }
          ]
        }
      ]
    },
    "card_text": "\n**We have updated the [new reranker](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/llm_reranker), supporting larger lengths, more languages, and achieving better performance.**\n\n<h1 align=\"center\">FlagEmbedding</h1>\n\n\n<h4 align=\"center\">\n    <p>\n        <a href=#model-list>Model List</a> | \n        <a href=#frequently-asked-questions>FAQ</a> |\n        <a href=#usage>Usage</a>  |\n        <a href=\"#evaluation\">Evaluation</a> |\n        <a href=\"#train\">Train</a> |\n        <a href=\"#citation\">Citation</a> |\n        <a href=\"#license\">License</a> \n    <p>\n</h4>\n\n**More details please refer to our Github: [FlagEmbedding](https://github.com/FlagOpen/FlagEmbedding).**\n\n\n[English](README.md) | [\u4e2d\u6587](https://github.com/FlagOpen/FlagEmbedding/blob/master/README_zh.md)\n\n\nFlagEmbedding focuses on retrieval-augmented LLMs, consisting of the following projects currently:\n\n- **Long-Context LLM**: [Activation Beacon](https://github.com/FlagOpen/FlagEmbedding/tree/master/Long_LLM/activation_beacon)\n- **Fine-tuning of LM** : [LM-Cocktail](https://github.com/FlagOpen/FlagEmbedding/tree/master/LM_Cocktail)\n- **Embedding Model**: [Visualized-BGE](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/visual), [BGE-M3](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/BGE_M3), [LLM Embedder](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/llm_embedder), [BGE Embedding](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/baai_general_embedding)\n- **Reranker Model**: [llm rerankers](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/llm_reranker), [BGE Reranker](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/reranker)\n- **Benchmark**: [C-MTEB](https://github.com/FlagOpen/FlagEmbedding/tree/master/C_MTEB)\n\n## News \n- 3/18/2024: Release new [rerankers](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/llm_reranker), built upon powerful M3 and LLM (GEMMA and MiniCPM, not so large actually) backbones, supporitng multi-lingual processing and larger inputs, massive improvements of ranking performances on BEIR, C-MTEB/Retrieval, MIRACL, LlamaIndex Evaluation.\n- 3/18/2024: Release [Visualized-BGE](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/visual), equipping BGE with visual capabilities. Visualized-BGE can be utilized to generate embeddings for hybrid image-text data.\n- 1/30/2024: Release **BGE-M3**, a new member to BGE model series! M3 stands for **M**ulti-linguality (100+ languages), **M**ulti-granularities (input length up to 8192), **M**ulti-Functionality (unification of dense, lexical, multi-vec/colbert retrieval). \nIt is the first embedding model which supports all three retrieval methods, achieving new SOTA on multi-lingual (MIRACL) and cross-lingual (MKQA) benchmarks.\n[Technical Report](https://github.com/FlagOpen/FlagEmbedding/blob/master/FlagEmbedding/BGE_M3/BGE_M3.pdf) and [Code](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/BGE_M3). :fire:\n- 1/9/2024: Release [Activation-Beacon](https://github.com/FlagOpen/FlagEmbedding/tree/master/Long_LLM/activation_beacon), an effective, efficient, compatible, and low-cost (training) method to extend the context length of LLM. [Technical Report](https://arxiv.org/abs/2401.03462) :fire:\n- 12/24/2023: Release **LLaRA**, a LLaMA-7B based dense retriever, leading to state-of-the-art performances on MS MARCO and BEIR. Model and code will be open-sourced. Please stay tuned. [Technical Report](https://arxiv.org/abs/2312.15503)\n- 11/23/2023: Release [LM-Cocktail](https://github.com/FlagOpen/FlagEmbedding/tree/master/LM_Cocktail), a method to maintain general capabilities during fine-tuning by merging multiple language models. [Technical Report](https://arxiv.org/abs/2311.13534) :fire:  \n- 10/12/2023: Release [LLM-Embedder](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/llm_embedder), a unified embedding model to support diverse retrieval augmentation needs for LLMs. [Technical Report](https://arxiv.org/pdf/2310.07554.pdf)\n- 09/15/2023: The [technical report](https://arxiv.org/pdf/2309.07597.pdf) of BGE has been released \n- 09/15/2023: The [massive training data](https://data.baai.ac.cn/details/BAAI-MTP) of BGE has been released \n- 09/12/2023: New models: \n    - **New reranker model**: release cross-encoder models `BAAI/bge-reranker-base` and `BAAI/bge-reranker-large`, which are more powerful than embedding model. We recommend to use/fine-tune them to re-rank top-k documents returned by embedding models. \n    - **update embedding model**: release `bge-*-v1.5` embedding model to alleviate the issue of the similarity distribution, and enhance its retrieval ability without instruction.\n \n\n<details>\n  <summary>More</summary>\n<!-- ### More -->\n    \n- 09/07/2023: Update [fine-tune code](https://github.com/FlagOpen/FlagEmbedding/blob/master/FlagEmbedding/baai_general_embedding/README.md): Add script to mine hard negatives and support adding instruction during fine-tuning. \n- 08/09/2023: BGE Models are integrated into **Langchain**, you can use it like [this](#using-langchain); C-MTEB **leaderboard** is [available](https://huggingface.co/spaces/mteb/leaderboard).  \n- 08/05/2023: Release base-scale and small-scale models, **best performance among the models of the same size \ud83e\udd17**  \n- 08/02/2023: Release `bge-large-*`(short for BAAI General Embedding) Models, **rank 1st on MTEB and C-MTEB benchmark!** :tada: :tada:   \n- 08/01/2023: We release the [Chinese Massive Text Embedding Benchmark](https://github.com/FlagOpen/FlagEmbedding/blob/master/C_MTEB) (**C-MTEB**), consisting of 31 test dataset.  \n  \n</details>\n\n\n## Model List\n\n`bge` is short for `BAAI general embedding`.\n\n|              Model              | Language | | Description | query instruction for retrieval [1] |\n|:-------------------------------|:--------:| :--------:| :--------:|:--------:|\n| [BAAI/bge-m3](https://huggingface.co/BAAI/bge-m3)                   |    Multilingual     |    [Inference](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/BGE_M3#usage) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/BGE_M3)    | Multi-Functionality(dense retrieval, sparse retrieval, multi-vector(colbert)), Multi-Linguality, and Multi-Granularity(8192 tokens) |  |\n|  [BAAI/llm-embedder](https://huggingface.co/BAAI/llm-embedder)  |   English | [Inference](./FlagEmbedding/llm_embedder/README.md) [Fine-tune](./FlagEmbedding/llm_embedder/README.md) | a unified embedding model to support diverse retrieval augmentation needs for LLMs | See [README](./FlagEmbedding/llm_embedder/README.md) |\n|  [BAAI/bge-reranker-large](https://huggingface.co/BAAI/bge-reranker-large)  |   Chinese and English | [Inference](#usage-for-reranker) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/reranker) | a cross-encoder model which is more accurate but less efficient [2] |   |\n|  [BAAI/bge-reranker-base](https://huggingface.co/BAAI/bge-reranker-base) |   Chinese and English | [Inference](#usage-for-reranker) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/reranker) | a cross-encoder model which is more accurate but less efficient [2] |   |\n|  [BAAI/bge-large-en-v1.5](https://huggingface.co/BAAI/bge-large-en-v1.5) |   English | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | version 1.5 with more reasonable similarity distribution | `Represent this sentence for searching relevant passages: `  |\n|  [BAAI/bge-base-en-v1.5](https://huggingface.co/BAAI/bge-base-en-v1.5) |   English | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | version 1.5 with more reasonable similarity distribution | `Represent this sentence for searching relevant passages: `  |\n|  [BAAI/bge-small-en-v1.5](https://huggingface.co/BAAI/bge-small-en-v1.5) |   English | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | version 1.5 with more reasonable similarity distribution  | `Represent this sentence for searching relevant passages: `  |\n|  [BAAI/bge-large-zh-v1.5](https://huggingface.co/BAAI/bge-large-zh-v1.5) |   Chinese | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | version 1.5 with more reasonable similarity distribution | `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`  |\n|  [BAAI/bge-base-zh-v1.5](https://huggingface.co/BAAI/bge-base-zh-v1.5) |   Chinese |  [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | version 1.5 with more reasonable similarity distribution | `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`  |\n|  [BAAI/bge-small-zh-v1.5](https://huggingface.co/BAAI/bge-small-zh-v1.5) |   Chinese | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | version 1.5 with more reasonable similarity distribution | `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`  |\n|  [BAAI/bge-large-en](https://huggingface.co/BAAI/bge-large-en) |   English | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | :trophy: rank **1st** in [MTEB](https://huggingface.co/spaces/mteb/leaderboard) leaderboard | `Represent this sentence for searching relevant passages: `  |\n|  [BAAI/bge-base-en](https://huggingface.co/BAAI/bge-base-en) |   English | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | a base-scale model but with similar ability to `bge-large-en` | `Represent this sentence for searching relevant passages: `  |\n|  [BAAI/bge-small-en](https://huggingface.co/BAAI/bge-small-en) |   English | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) |a small-scale model but with competitive performance  | `Represent this sentence for searching relevant passages: `  |\n|  [BAAI/bge-large-zh](https://huggingface.co/BAAI/bge-large-zh) |   Chinese | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | :trophy: rank **1st** in [C-MTEB](https://github.com/FlagOpen/FlagEmbedding/tree/master/C_MTEB) benchmark | `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`  |\n|  [BAAI/bge-base-zh](https://huggingface.co/BAAI/bge-base-zh) |   Chinese |  [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | a base-scale model but with similar ability to `bge-large-zh` | `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`  |\n|  [BAAI/bge-small-zh](https://huggingface.co/BAAI/bge-small-zh) |   Chinese | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | a small-scale model but with competitive performance | `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`  |\n\n\n[1\\]: If you need to search the relevant passages to a query, we suggest to add the instruction to the query; in other cases, no instruction is needed, just use the original query directly. In all cases, **no instruction** needs to be added to passages.\n\n[2\\]: Different from embedding model, reranker uses question and document as input and directly output similarity instead of embedding. To balance the accuracy and time cost, cross-encoder is widely used to re-rank top-k documents retrieved by other simple models. \nFor examples, use bge embedding model to retrieve top 100 relevant documents, and then use bge reranker to re-rank the top 100 document to get the final top-3 results.\n\nAll models have been uploaded to Huggingface Hub, and you can see them at https://huggingface.co/BAAI. \nIf you cannot open the Huggingface Hub, you also can download the models at https://model.baai.ac.cn/models .\n\n\n## Frequently asked questions\n\n<details>\n  <summary>1. How to fine-tune bge embedding model?</summary>\n\n  <!-- ### How to fine-tune bge embedding model? -->\nFollowing this [example](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) to prepare data and fine-tune your model. \nSome suggestions:\n- Mine hard negatives following this [example](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune#hard-negatives), which can improve the retrieval performance.\n- If you pre-train bge on your data, the pre-trained model cannot be directly used to calculate similarity, and it must be fine-tuned with contrastive learning before computing similarity.\n- If the accuracy of the fine-tuned model is still not high, it is recommended to use/fine-tune the cross-encoder model (bge-reranker) to re-rank top-k results.\n  Hard negatives also are needed to fine-tune reranker. Refer to this [example](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/reranker) for the fine-tuning for reranker\n\n  \n</details>\n\n<details>\n  <summary>2. The similarity score between two dissimilar sentences is higher than 0.5</summary>\n\n  <!-- ### The similarity score between two dissimilar sentences is higher than 0.5 -->\n**Suggest to use bge v1.5, which alleviates the issue of the similarity distribution.** \n\nSince we finetune the models by contrastive learning with a temperature of 0.01, \nthe similarity distribution of the current BGE model is about in the interval \\[0.6, 1\\].\nSo a similarity score greater than 0.5 does not indicate that the two sentences are similar.\n\nFor downstream tasks, such as passage retrieval or semantic similarity, \n**what matters is the relative order of the scores, not the absolute value.**\nIf you need to filter similar sentences based on a similarity threshold, \nplease select an appropriate similarity threshold based on the similarity distribution on your data (such as 0.8, 0.85, or even 0.9).\n\n</details>\n\n<details>\n  <summary>3. When does the query instruction need to be used</summary>\n\n  <!-- ### When does the query instruction need to be used -->\n\nFor the `bge-*-v1.5`, we improve its retrieval ability when not using instruction. \nNo instruction only has a slight degradation in retrieval performance compared with using instruction. \nSo you can generate embedding without instruction in all cases for convenience.\n \nFor a retrieval task that uses short queries to find long related documents, \nit is recommended to add instructions for these short queries.\n**The best method to decide whether to add instructions for queries is choosing the setting that achieves better performance on your task.**\nIn all cases, the documents/passages do not need to add the instruction. \n\n</details>\n\n\n## Usage \n\n### Usage for Embedding Model\n\nHere are some examples for using `bge` models with \n[FlagEmbedding](#using-flagembedding), [Sentence-Transformers](#using-sentence-transformers), [Langchain](#using-langchain), or [Huggingface Transformers](#using-huggingface-transformers).\n\n#### Using FlagEmbedding\n```\npip install -U FlagEmbedding\n```\nIf it doesn't work for you, you can see [FlagEmbedding](https://github.com/FlagOpen/FlagEmbedding/blob/master/FlagEmbedding/baai_general_embedding/README.md) for more methods to install FlagEmbedding.\n\n```python\nfrom FlagEmbedding import FlagModel\nsentences_1 = [\"\u6837\u4f8b\u6570\u636e-1\", \"\u6837\u4f8b\u6570\u636e-2\"]\nsentences_2 = [\"\u6837\u4f8b\u6570\u636e-3\", \"\u6837\u4f8b\u6570\u636e-4\"]\nmodel = FlagModel('BAAI/bge-large-zh-v1.5', \n                  query_instruction_for_retrieval=\"\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a\",\n                  use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\nembeddings_1 = model.encode(sentences_1)\nembeddings_2 = model.encode(sentences_2)\nsimilarity = embeddings_1 @ embeddings_2.T\nprint(similarity)\n\n# for s2p(short query to long passage) retrieval task, suggest to use encode_queries() which will automatically add the instruction to each query\n# corpus in retrieval task can still use encode() or encode_corpus(), since they don't need instruction\nqueries = ['query_1', 'query_2']\npassages = [\"\u6837\u4f8b\u6587\u6863-1\", \"\u6837\u4f8b\u6587\u6863-2\"]\nq_embeddings = model.encode_queries(queries)\np_embeddings = model.encode(passages)\nscores = q_embeddings @ p_embeddings.T\n```\nFor the value of the argument `query_instruction_for_retrieval`, see [Model List](https://github.com/FlagOpen/FlagEmbedding/tree/master#model-list). \n\nBy default, FlagModel will use all available GPUs when encoding. Please set `os.environ[\"CUDA_VISIBLE_DEVICES\"]` to select specific GPUs.\nYou also can set `os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\"` to make all GPUs unavailable.\n\n\n#### Using Sentence-Transformers\n\nYou can also use the `bge` models with [sentence-transformers](https://www.SBERT.net):\n\n```\npip install -U sentence-transformers\n```\n```python\nfrom sentence_transformers import SentenceTransformer\nsentences_1 = [\"\u6837\u4f8b\u6570\u636e-1\", \"\u6837\u4f8b\u6570\u636e-2\"]\nsentences_2 = [\"\u6837\u4f8b\u6570\u636e-3\", \"\u6837\u4f8b\u6570\u636e-4\"]\nmodel = SentenceTransformer('BAAI/bge-large-zh-v1.5')\nembeddings_1 = model.encode(sentences_1, normalize_embeddings=True)\nembeddings_2 = model.encode(sentences_2, normalize_embeddings=True)\nsimilarity = embeddings_1 @ embeddings_2.T\nprint(similarity)\n```\nFor s2p(short query to long passage) retrieval task, \neach short query should start with an instruction (instructions see [Model List](https://github.com/FlagOpen/FlagEmbedding/tree/master#model-list)). \nBut the instruction is not needed for passages.\n```python\nfrom sentence_transformers import SentenceTransformer\nqueries = ['query_1', 'query_2']\npassages = [\"\u6837\u4f8b\u6587\u6863-1\", \"\u6837\u4f8b\u6587\u6863-2\"]\ninstruction = \"\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a\"\n\nmodel = SentenceTransformer('BAAI/bge-large-zh-v1.5')\nq_embeddings = model.encode([instruction+q for q in queries], normalize_embeddings=True)\np_embeddings = model.encode(passages, normalize_embeddings=True)\nscores = q_embeddings @ p_embeddings.T\n```\n\n#### Using Langchain \n\nYou can use `bge` in langchain like this:\n```python\nfrom langchain.embeddings import HuggingFaceBgeEmbeddings\nmodel_name = \"BAAI/bge-large-en-v1.5\"\nmodel_kwargs = {'device': 'cuda'}\nencode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\nmodel = HuggingFaceBgeEmbeddings(\n    model_name=model_name,\n    model_kwargs=model_kwargs,\n    encode_kwargs=encode_kwargs,\n    query_instruction=\"\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a\"\n)\nmodel.query_instruction = \"\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a\"\n```\n\n\n#### Using HuggingFace Transformers\n\nWith the transformers package, you can use the model like this: First, you pass your input through the transformer model, then you select the last hidden state of the first token (i.e., [CLS]) as the sentence embedding.\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\n# Sentences we want sentence embeddings for\nsentences = [\"\u6837\u4f8b\u6570\u636e-1\", \"\u6837\u4f8b\u6570\u636e-2\"]\n\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained('BAAI/bge-large-zh-v1.5')\nmodel = AutoModel.from_pretrained('BAAI/bge-large-zh-v1.5')\nmodel.eval()\n\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n# for s2p(short query to long passage) retrieval task, add an instruction to query (not add instruction for passages)\n# encoded_input = tokenizer([instruction + q for q in queries], padding=True, truncation=True, return_tensors='pt')\n\n# Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n    # Perform pooling. In this case, cls pooling.\n    sentence_embeddings = model_output[0][:, 0]\n# normalize embeddings\nsentence_embeddings = torch.nn.functional.normalize(sentence_embeddings, p=2, dim=1)\nprint(\"Sentence embeddings:\", sentence_embeddings)\n```\n\n### Usage for Reranker\n\nDifferent from embedding model, reranker uses question and document as input and directly output similarity instead of embedding. \nYou can get a relevance score by inputting query and passage to the reranker. \nThe reranker is optimized based cross-entropy loss, so the relevance score is not bounded to a specific range.\n\n\n#### Using FlagEmbedding\n```\npip install -U FlagEmbedding\n```\n\nGet relevance scores (higher scores indicate more relevance):\n```python\nfrom FlagEmbedding import FlagReranker\nreranker = FlagReranker('BAAI/bge-reranker-large', use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\n\nscore = reranker.compute_score(['query', 'passage'])\nprint(score)\n\nscores = reranker.compute_score([['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']])\nprint(scores)\n```\n\n\n#### Using Huggingface transformers\n\n```python\nimport torch\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained('BAAI/bge-reranker-large')\nmodel = AutoModelForSequenceClassification.from_pretrained('BAAI/bge-reranker-large')\nmodel.eval()\n\npairs = [['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']]\nwith torch.no_grad():\n    inputs = tokenizer(pairs, padding=True, truncation=True, return_tensors='pt', max_length=512)\n    scores = model(**inputs, return_dict=True).logits.view(-1, ).float()\n    print(scores)\n```\n\n#### Usage reranker with the ONNX files\n\n```python\nfrom optimum.onnxruntime import ORTModelForSequenceClassification  # type: ignore\n\nimport torch\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained('BAAI/bge-reranker-large')\nmodel = AutoModelForSequenceClassification.from_pretrained('BAAI/bge-reranker-base')\nmodel_ort = ORTModelForSequenceClassification.from_pretrained('BAAI/bge-reranker-base', file_name=\"onnx/model.onnx\")\n\n# Sentences we want sentence embeddings for\npairs = [['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']]\n\n# Tokenize sentences\nencoded_input = tokenizer(pairs, padding=True, truncation=True, return_tensors='pt')\n\nscores_ort = model_ort(**encoded_input, return_dict=True).logits.view(-1, ).float()\n# Compute token embeddings\nwith torch.inference_mode():\n    scores = model_ort(**encoded_input, return_dict=True).logits.view(-1, ).float()\n\n# scores and scores_ort are identical\n```\n#### Usage reranker with infinity\n\nIts also possible to deploy the onnx/torch files with the [infinity_emb](https://github.com/michaelfeil/infinity) pip package.\n```python\nimport asyncio\nfrom infinity_emb import AsyncEmbeddingEngine, EngineArgs\n\nquery='what is a panda?'\ndocs = ['The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear', \"Paris is in France.\"]\n\nengine = AsyncEmbeddingEngine.from_args(\n    EngineArgs(model_name_or_path = \"BAAI/bge-reranker-base\", device=\"cpu\", engine=\"torch\" # or engine=\"optimum\" for onnx\n))\n\nasync def main(): \n    async with engine:\n        ranking, usage = await engine.rerank(query=query, docs=docs)\n        print(list(zip(ranking, docs)))\nasyncio.run(main())\n```\n\n## Evaluation  \n\n`baai-general-embedding` models achieve **state-of-the-art performance on both MTEB and C-MTEB leaderboard!**\nFor more details and evaluation tools see our [scripts](https://github.com/FlagOpen/FlagEmbedding/blob/master/C_MTEB/README.md). \n\n- **MTEB**:   \n\n| Model Name |  Dimension | Sequence Length | Average (56) | Retrieval (15) |Clustering (11) | Pair Classification (3) | Reranking (4) |  STS (10) | Summarization (1) | Classification (12) |\n|:----:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n| [BAAI/bge-large-en-v1.5](https://huggingface.co/BAAI/bge-large-en-v1.5) | 1024 | 512 |  **64.23** | **54.29** |  46.08 | 87.12 | 60.03 | 83.11 | 31.61 | 75.97 |  \n| [BAAI/bge-base-en-v1.5](https://huggingface.co/BAAI/bge-base-en-v1.5) |  768 | 512 | 63.55 | 53.25 |   45.77 | 86.55 | 58.86 | 82.4 | 31.07 | 75.53 |  \n| [BAAI/bge-small-en-v1.5](https://huggingface.co/BAAI/bge-small-en-v1.5) |  384 | 512 | 62.17 |51.68 | 43.82 |  84.92 | 58.36 | 81.59 | 30.12 | 74.14 |  \n| [bge-large-en](https://huggingface.co/BAAI/bge-large-en) |  1024 | 512 | 63.98 |  53.9 | 46.98 | 85.8 | 59.48 | 81.56 | 32.06 | 76.21 | \n| [bge-base-en](https://huggingface.co/BAAI/bge-base-en) |  768 | 512 |  63.36 | 53.0 | 46.32 | 85.86 | 58.7 | 81.84 | 29.27 | 75.27 | \n| [gte-large](https://huggingface.co/thenlper/gte-large) |  1024 | 512 | 63.13 | 52.22 | 46.84 | 85.00 | 59.13 | 83.35 | 31.66 | 73.33 |\n| [gte-base](https://huggingface.co/thenlper/gte-base) \t|  768 | 512 | 62.39 | 51.14 | 46.2 | 84.57 | 58.61 | 82.3 | 31.17 | 73.01 |\n| [e5-large-v2](https://huggingface.co/intfloat/e5-large-v2) |  1024| 512 | 62.25 | 50.56 | 44.49 | 86.03 | 56.61 | 82.05 | 30.19 | 75.24 |\n| [bge-small-en](https://huggingface.co/BAAI/bge-small-en) |  384 | 512 | 62.11 |  51.82 | 44.31 | 83.78 | 57.97 | 80.72 | 30.53 | 74.37 |  \n| [instructor-xl](https://huggingface.co/hkunlp/instructor-xl) |  768 | 512 | 61.79 | 49.26 | 44.74 | 86.62 | 57.29 | 83.06 | 32.32 | 61.79 |\n| [e5-base-v2](https://huggingface.co/intfloat/e5-base-v2) |  768 | 512 | 61.5 | 50.29 | 43.80 | 85.73 | 55.91 | 81.05 | 30.28 | 73.84 |\n| [gte-small](https://huggingface.co/thenlper/gte-small) |  384 | 512 | 61.36 | 49.46 | 44.89 | 83.54 | 57.7 | 82.07 | 30.42 | 72.31 |\n| [text-embedding-ada-002](https://platform.openai.com/docs/guides/embeddings) | 1536 | 8192 | 60.99 | 49.25 | 45.9 | 84.89 | 56.32 | 80.97 | 30.8 | 70.93 |\n| [e5-small-v2](https://huggingface.co/intfloat/e5-base-v2) | 384 | 512 | 59.93 | 49.04 | 39.92 | 84.67 | 54.32 | 80.39 | 31.16 | 72.94 |\n| [sentence-t5-xxl](https://huggingface.co/sentence-transformers/sentence-t5-xxl) |  768 | 512 | 59.51 | 42.24 | 43.72 | 85.06 | 56.42 | 82.63 | 30.08 | 73.42 |\n| [all-mpnet-base-v2](https://huggingface.co/sentence-transformers/all-mpnet-base-v2) \t|  768 | 514 \t| 57.78 | 43.81 | 43.69 | 83.04 | 59.36 | 80.28 | 27.49 | 65.07 |\n| [sgpt-bloom-7b1-msmarco](https://huggingface.co/bigscience/sgpt-bloom-7b1-msmarco) \t|  4096 | 2048 | 57.59 | 48.22 | 38.93 | 81.9 | 55.65 | 77.74 | 33.6 | 66.19 |\n\n\n\n- **C-MTEB**:  \nWe create the benchmark C-MTEB for Chinese text embedding which consists of 31 datasets from 6 tasks. \nPlease refer to [C_MTEB](https://github.com/FlagOpen/FlagEmbedding/blob/master/C_MTEB/README.md) for a detailed introduction.\n \n| Model | Embedding dimension | Avg | Retrieval | STS | PairClassification | Classification | Reranking | Clustering |\n|:-------------------------------|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|\n| [**BAAI/bge-large-zh-v1.5**](https://huggingface.co/BAAI/bge-large-zh-v1.5) | 1024 |  **64.53** | 70.46 | 56.25 | 81.6 | 69.13 | 65.84 | 48.99 |  \n| [BAAI/bge-base-zh-v1.5](https://huggingface.co/BAAI/bge-base-zh-v1.5) | 768 |  63.13 | 69.49 | 53.72 | 79.75 | 68.07 | 65.39 | 47.53 |  \n| [BAAI/bge-small-zh-v1.5](https://huggingface.co/BAAI/bge-small-zh-v1.5) | 512 | 57.82 | 61.77 | 49.11 | 70.41 | 63.96 | 60.92 | 44.18 |   \n| [BAAI/bge-large-zh](https://huggingface.co/BAAI/bge-large-zh) | 1024 | 64.20 | 71.53 | 54.98 | 78.94 | 68.32 | 65.11 | 48.39 |\n| [bge-large-zh-noinstruct](https://huggingface.co/BAAI/bge-large-zh-noinstruct) | 1024 | 63.53 | 70.55 | 53 | 76.77 | 68.58 | 64.91 | 50.01 |\n| [BAAI/bge-base-zh](https://huggingface.co/BAAI/bge-base-zh) | 768 | 62.96 | 69.53 | 54.12 | 77.5 | 67.07 | 64.91 | 47.63 |\n| [multilingual-e5-large](https://huggingface.co/intfloat/multilingual-e5-large) | 1024 | 58.79 | 63.66 | 48.44 | 69.89 | 67.34 | 56.00 | 48.23 |\n| [BAAI/bge-small-zh](https://huggingface.co/BAAI/bge-small-zh) | 512 | 58.27 |  63.07 | 49.45 | 70.35 | 63.64 | 61.48 | 45.09 |\n| [m3e-base](https://huggingface.co/moka-ai/m3e-base) | 768 | 57.10 | 56.91 | 50.47 | 63.99 | 67.52 | 59.34 | 47.68 |\n| [m3e-large](https://huggingface.co/moka-ai/m3e-large) | 1024 |  57.05 | 54.75 | 50.42 | 64.3 | 68.2 | 59.66 | 48.88 |\n| [multilingual-e5-base](https://huggingface.co/intfloat/multilingual-e5-base) | 768 | 55.48 | 61.63 | 46.49 | 67.07 | 65.35 | 54.35 | 40.68 |\n| [multilingual-e5-small](https://huggingface.co/intfloat/multilingual-e5-small) | 384 | 55.38 | 59.95 | 45.27 | 66.45 | 65.85 | 53.86 | 45.26 |\n| [text-embedding-ada-002(OpenAI)](https://platform.openai.com/docs/guides/embeddings/what-are-embeddings) | 1536 |  53.02 | 52.0 | 43.35 | 69.56 | 64.31 | 54.28 | 45.68 |\n| [luotuo](https://huggingface.co/silk-road/luotuo-bert-medium) | 1024 | 49.37 |  44.4 | 42.78 | 66.62 | 61 | 49.25 | 44.39 |\n| [text2vec-base](https://huggingface.co/shibing624/text2vec-base-chinese) | 768 |  47.63 | 38.79 | 43.41 | 67.41 | 62.19 | 49.45 | 37.66 |\n| [text2vec-large](https://huggingface.co/GanymedeNil/text2vec-large-chinese) | 1024 | 47.36 | 41.94 | 44.97 | 70.86 | 60.66 | 49.16 | 30.02 |\n\n\n- **Reranking**:\nSee [C_MTEB](https://github.com/FlagOpen/FlagEmbedding/blob/master/C_MTEB/) for evaluation script.\n\n| Model | T2Reranking | T2RerankingZh2En\\* | T2RerankingEn2Zh\\* | MMarcoReranking | CMedQAv1 | CMedQAv2 | Avg |  \n|:-------------------------------|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|  \n| text2vec-base-multilingual | 64.66 | 62.94 | 62.51 | 14.37 | 48.46 | 48.6 | 50.26 |  \n| multilingual-e5-small | 65.62 | 60.94 | 56.41 | 29.91 | 67.26 | 66.54 | 57.78 |  \n| multilingual-e5-large | 64.55 | 61.61 | 54.28 | 28.6 | 67.42 | 67.92 | 57.4 |  \n| multilingual-e5-base | 64.21 | 62.13 | 54.68 | 29.5 | 66.23 | 66.98 | 57.29 |  \n| m3e-base | 66.03 | 62.74 | 56.07 | 17.51 | 77.05 | 76.76 | 59.36 |  \n| m3e-large | 66.13 | 62.72 | 56.1 | 16.46 | 77.76 | 78.27 | 59.57 |  \n| bge-base-zh-v1.5 | 66.49 | 63.25 | 57.02 | 29.74 | 80.47 | 84.88 | 63.64 |  \n| bge-large-zh-v1.5 | 65.74 | 63.39 | 57.03 | 28.74 | 83.45 | 85.44 | 63.97 |  \n| [BAAI/bge-reranker-base](https://huggingface.co/BAAI/bge-reranker-base) | 67.28 | 63.95 | 60.45 | 35.46 | 81.26 | 84.1 | 65.42 |  \n| [BAAI/bge-reranker-large](https://huggingface.co/BAAI/bge-reranker-large) | 67.6 | 64.03 | 61.44 | 37.16 | 82.15 | 84.18 | 66.09 |  \n\n\\* : T2RerankingZh2En and T2RerankingEn2Zh are cross-language retrieval tasks\n\n## Train\n\n### BAAI Embedding \n\nWe pre-train the models using [retromae](https://github.com/staoxiao/RetroMAE) and train them on large-scale pairs data using contrastive learning. \n**You can fine-tune the embedding model on your data following our [examples](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune).**\nWe also provide a [pre-train example](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/pretrain).\nNote that the goal of pre-training is to reconstruct the text, and the pre-trained model cannot be used for similarity calculation directly, it needs to be fine-tuned.\nMore training details for bge see [baai_general_embedding](https://github.com/FlagOpen/FlagEmbedding/blob/master/FlagEmbedding/baai_general_embedding/README.md).\n\n\n\n### BGE Reranker\n\nCross-encoder will perform full-attention over the input pair, \nwhich is more accurate than embedding model (i.e., bi-encoder) but more time-consuming than embedding model.\nTherefore, it can be used to re-rank the top-k documents returned by embedding model.\nWe train the cross-encoder on a multilingual pair data, \nThe data format is the same as embedding model, so you can fine-tune it easily following our [example](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/reranker). \nMore details please refer to [./FlagEmbedding/reranker/README.md](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/reranker)\n\n\n\n## Citation\n\nIf you find this repository useful, please consider giving a star :star: and citation\n\n```\n@misc{bge_embedding,\n      title={C-Pack: Packaged Resources To Advance General Chinese Embedding}, \n      author={Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff},\n      year={2023},\n      eprint={2309.07597},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n\n## License\nFlagEmbedding is licensed under the [MIT License](https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE). The released models can be used for commercial purposes free of charge.",
    "card_content": "---\nlanguage:\n- en\n- zh\nlicense: mit\nlibrary_name: sentence-transformers\ntags:\n- mteb\n- text-embeddings-inference\npipeline_tag: text-classification\nmodel-index:\n- name: bge-reranker-base\n  results:\n  - task:\n      type: Reranking\n    dataset:\n      name: MTEB CMedQAv1\n      type: C-MTEB/CMedQAv1-reranking\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map\n      value: 81.27206722525007\n    - type: mrr\n      value: 84.14238095238095\n  - task:\n      type: Reranking\n    dataset:\n      name: MTEB CMedQAv2\n      type: C-MTEB/CMedQAv2-reranking\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map\n      value: 84.10369934291236\n    - type: mrr\n      value: 86.79376984126984\n  - task:\n      type: Reranking\n    dataset:\n      name: MTEB MMarcoReranking\n      type: C-MTEB/Mmarco-reranking\n      config: default\n      split: dev\n      revision: None\n    metrics:\n    - type: map\n      value: 35.4600511272538\n    - type: mrr\n      value: 34.60238095238095\n  - task:\n      type: Reranking\n    dataset:\n      name: MTEB T2Reranking\n      type: C-MTEB/T2Reranking\n      config: default\n      split: dev\n      revision: None\n    metrics:\n    - type: map\n      value: 67.27728847727172\n    - type: mrr\n      value: 77.1315192743764\n---\n\n**We have updated the [new reranker](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/llm_reranker), supporting larger lengths, more languages, and achieving better performance.**\n\n<h1 align=\"center\">FlagEmbedding</h1>\n\n\n<h4 align=\"center\">\n    <p>\n        <a href=#model-list>Model List</a> | \n        <a href=#frequently-asked-questions>FAQ</a> |\n        <a href=#usage>Usage</a>  |\n        <a href=\"#evaluation\">Evaluation</a> |\n        <a href=\"#train\">Train</a> |\n        <a href=\"#citation\">Citation</a> |\n        <a href=\"#license\">License</a> \n    <p>\n</h4>\n\n**More details please refer to our Github: [FlagEmbedding](https://github.com/FlagOpen/FlagEmbedding).**\n\n\n[English](README.md) | [\u4e2d\u6587](https://github.com/FlagOpen/FlagEmbedding/blob/master/README_zh.md)\n\n\nFlagEmbedding focuses on retrieval-augmented LLMs, consisting of the following projects currently:\n\n- **Long-Context LLM**: [Activation Beacon](https://github.com/FlagOpen/FlagEmbedding/tree/master/Long_LLM/activation_beacon)\n- **Fine-tuning of LM** : [LM-Cocktail](https://github.com/FlagOpen/FlagEmbedding/tree/master/LM_Cocktail)\n- **Embedding Model**: [Visualized-BGE](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/visual), [BGE-M3](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/BGE_M3), [LLM Embedder](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/llm_embedder), [BGE Embedding](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/baai_general_embedding)\n- **Reranker Model**: [llm rerankers](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/llm_reranker), [BGE Reranker](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/reranker)\n- **Benchmark**: [C-MTEB](https://github.com/FlagOpen/FlagEmbedding/tree/master/C_MTEB)\n\n## News \n- 3/18/2024: Release new [rerankers](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/llm_reranker), built upon powerful M3 and LLM (GEMMA and MiniCPM, not so large actually) backbones, supporitng multi-lingual processing and larger inputs, massive improvements of ranking performances on BEIR, C-MTEB/Retrieval, MIRACL, LlamaIndex Evaluation.\n- 3/18/2024: Release [Visualized-BGE](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/visual), equipping BGE with visual capabilities. Visualized-BGE can be utilized to generate embeddings for hybrid image-text data.\n- 1/30/2024: Release **BGE-M3**, a new member to BGE model series! M3 stands for **M**ulti-linguality (100+ languages), **M**ulti-granularities (input length up to 8192), **M**ulti-Functionality (unification of dense, lexical, multi-vec/colbert retrieval). \nIt is the first embedding model which supports all three retrieval methods, achieving new SOTA on multi-lingual (MIRACL) and cross-lingual (MKQA) benchmarks.\n[Technical Report](https://github.com/FlagOpen/FlagEmbedding/blob/master/FlagEmbedding/BGE_M3/BGE_M3.pdf) and [Code](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/BGE_M3). :fire:\n- 1/9/2024: Release [Activation-Beacon](https://github.com/FlagOpen/FlagEmbedding/tree/master/Long_LLM/activation_beacon), an effective, efficient, compatible, and low-cost (training) method to extend the context length of LLM. [Technical Report](https://arxiv.org/abs/2401.03462) :fire:\n- 12/24/2023: Release **LLaRA**, a LLaMA-7B based dense retriever, leading to state-of-the-art performances on MS MARCO and BEIR. Model and code will be open-sourced. Please stay tuned. [Technical Report](https://arxiv.org/abs/2312.15503)\n- 11/23/2023: Release [LM-Cocktail](https://github.com/FlagOpen/FlagEmbedding/tree/master/LM_Cocktail), a method to maintain general capabilities during fine-tuning by merging multiple language models. [Technical Report](https://arxiv.org/abs/2311.13534) :fire:  \n- 10/12/2023: Release [LLM-Embedder](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/llm_embedder), a unified embedding model to support diverse retrieval augmentation needs for LLMs. [Technical Report](https://arxiv.org/pdf/2310.07554.pdf)\n- 09/15/2023: The [technical report](https://arxiv.org/pdf/2309.07597.pdf) of BGE has been released \n- 09/15/2023: The [massive training data](https://data.baai.ac.cn/details/BAAI-MTP) of BGE has been released \n- 09/12/2023: New models: \n    - **New reranker model**: release cross-encoder models `BAAI/bge-reranker-base` and `BAAI/bge-reranker-large`, which are more powerful than embedding model. We recommend to use/fine-tune them to re-rank top-k documents returned by embedding models. \n    - **update embedding model**: release `bge-*-v1.5` embedding model to alleviate the issue of the similarity distribution, and enhance its retrieval ability without instruction.\n \n\n<details>\n  <summary>More</summary>\n<!-- ### More -->\n    \n- 09/07/2023: Update [fine-tune code](https://github.com/FlagOpen/FlagEmbedding/blob/master/FlagEmbedding/baai_general_embedding/README.md): Add script to mine hard negatives and support adding instruction during fine-tuning. \n- 08/09/2023: BGE Models are integrated into **Langchain**, you can use it like [this](#using-langchain); C-MTEB **leaderboard** is [available](https://huggingface.co/spaces/mteb/leaderboard).  \n- 08/05/2023: Release base-scale and small-scale models, **best performance among the models of the same size \ud83e\udd17**  \n- 08/02/2023: Release `bge-large-*`(short for BAAI General Embedding) Models, **rank 1st on MTEB and C-MTEB benchmark!** :tada: :tada:   \n- 08/01/2023: We release the [Chinese Massive Text Embedding Benchmark](https://github.com/FlagOpen/FlagEmbedding/blob/master/C_MTEB) (**C-MTEB**), consisting of 31 test dataset.  \n  \n</details>\n\n\n## Model List\n\n`bge` is short for `BAAI general embedding`.\n\n|              Model              | Language | | Description | query instruction for retrieval [1] |\n|:-------------------------------|:--------:| :--------:| :--------:|:--------:|\n| [BAAI/bge-m3](https://huggingface.co/BAAI/bge-m3)                   |    Multilingual     |    [Inference](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/BGE_M3#usage) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/BGE_M3)    | Multi-Functionality(dense retrieval, sparse retrieval, multi-vector(colbert)), Multi-Linguality, and Multi-Granularity(8192 tokens) |  |\n|  [BAAI/llm-embedder](https://huggingface.co/BAAI/llm-embedder)  |   English | [Inference](./FlagEmbedding/llm_embedder/README.md) [Fine-tune](./FlagEmbedding/llm_embedder/README.md) | a unified embedding model to support diverse retrieval augmentation needs for LLMs | See [README](./FlagEmbedding/llm_embedder/README.md) |\n|  [BAAI/bge-reranker-large](https://huggingface.co/BAAI/bge-reranker-large)  |   Chinese and English | [Inference](#usage-for-reranker) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/reranker) | a cross-encoder model which is more accurate but less efficient [2] |   |\n|  [BAAI/bge-reranker-base](https://huggingface.co/BAAI/bge-reranker-base) |   Chinese and English | [Inference](#usage-for-reranker) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/reranker) | a cross-encoder model which is more accurate but less efficient [2] |   |\n|  [BAAI/bge-large-en-v1.5](https://huggingface.co/BAAI/bge-large-en-v1.5) |   English | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | version 1.5 with more reasonable similarity distribution | `Represent this sentence for searching relevant passages: `  |\n|  [BAAI/bge-base-en-v1.5](https://huggingface.co/BAAI/bge-base-en-v1.5) |   English | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | version 1.5 with more reasonable similarity distribution | `Represent this sentence for searching relevant passages: `  |\n|  [BAAI/bge-small-en-v1.5](https://huggingface.co/BAAI/bge-small-en-v1.5) |   English | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | version 1.5 with more reasonable similarity distribution  | `Represent this sentence for searching relevant passages: `  |\n|  [BAAI/bge-large-zh-v1.5](https://huggingface.co/BAAI/bge-large-zh-v1.5) |   Chinese | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | version 1.5 with more reasonable similarity distribution | `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`  |\n|  [BAAI/bge-base-zh-v1.5](https://huggingface.co/BAAI/bge-base-zh-v1.5) |   Chinese |  [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | version 1.5 with more reasonable similarity distribution | `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`  |\n|  [BAAI/bge-small-zh-v1.5](https://huggingface.co/BAAI/bge-small-zh-v1.5) |   Chinese | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | version 1.5 with more reasonable similarity distribution | `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`  |\n|  [BAAI/bge-large-en](https://huggingface.co/BAAI/bge-large-en) |   English | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | :trophy: rank **1st** in [MTEB](https://huggingface.co/spaces/mteb/leaderboard) leaderboard | `Represent this sentence for searching relevant passages: `  |\n|  [BAAI/bge-base-en](https://huggingface.co/BAAI/bge-base-en) |   English | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | a base-scale model but with similar ability to `bge-large-en` | `Represent this sentence for searching relevant passages: `  |\n|  [BAAI/bge-small-en](https://huggingface.co/BAAI/bge-small-en) |   English | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) |a small-scale model but with competitive performance  | `Represent this sentence for searching relevant passages: `  |\n|  [BAAI/bge-large-zh](https://huggingface.co/BAAI/bge-large-zh) |   Chinese | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | :trophy: rank **1st** in [C-MTEB](https://github.com/FlagOpen/FlagEmbedding/tree/master/C_MTEB) benchmark | `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`  |\n|  [BAAI/bge-base-zh](https://huggingface.co/BAAI/bge-base-zh) |   Chinese |  [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | a base-scale model but with similar ability to `bge-large-zh` | `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`  |\n|  [BAAI/bge-small-zh](https://huggingface.co/BAAI/bge-small-zh) |   Chinese | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | a small-scale model but with competitive performance | `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`  |\n\n\n[1\\]: If you need to search the relevant passages to a query, we suggest to add the instruction to the query; in other cases, no instruction is needed, just use the original query directly. In all cases, **no instruction** needs to be added to passages.\n\n[2\\]: Different from embedding model, reranker uses question and document as input and directly output similarity instead of embedding. To balance the accuracy and time cost, cross-encoder is widely used to re-rank top-k documents retrieved by other simple models. \nFor examples, use bge embedding model to retrieve top 100 relevant documents, and then use bge reranker to re-rank the top 100 document to get the final top-3 results.\n\nAll models have been uploaded to Huggingface Hub, and you can see them at https://huggingface.co/BAAI. \nIf you cannot open the Huggingface Hub, you also can download the models at https://model.baai.ac.cn/models .\n\n\n## Frequently asked questions\n\n<details>\n  <summary>1. How to fine-tune bge embedding model?</summary>\n\n  <!-- ### How to fine-tune bge embedding model? -->\nFollowing this [example](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) to prepare data and fine-tune your model. \nSome suggestions:\n- Mine hard negatives following this [example](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune#hard-negatives), which can improve the retrieval performance.\n- If you pre-train bge on your data, the pre-trained model cannot be directly used to calculate similarity, and it must be fine-tuned with contrastive learning before computing similarity.\n- If the accuracy of the fine-tuned model is still not high, it is recommended to use/fine-tune the cross-encoder model (bge-reranker) to re-rank top-k results.\n  Hard negatives also are needed to fine-tune reranker. Refer to this [example](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/reranker) for the fine-tuning for reranker\n\n  \n</details>\n\n<details>\n  <summary>2. The similarity score between two dissimilar sentences is higher than 0.5</summary>\n\n  <!-- ### The similarity score between two dissimilar sentences is higher than 0.5 -->\n**Suggest to use bge v1.5, which alleviates the issue of the similarity distribution.** \n\nSince we finetune the models by contrastive learning with a temperature of 0.01, \nthe similarity distribution of the current BGE model is about in the interval \\[0.6, 1\\].\nSo a similarity score greater than 0.5 does not indicate that the two sentences are similar.\n\nFor downstream tasks, such as passage retrieval or semantic similarity, \n**what matters is the relative order of the scores, not the absolute value.**\nIf you need to filter similar sentences based on a similarity threshold, \nplease select an appropriate similarity threshold based on the similarity distribution on your data (such as 0.8, 0.85, or even 0.9).\n\n</details>\n\n<details>\n  <summary>3. When does the query instruction need to be used</summary>\n\n  <!-- ### When does the query instruction need to be used -->\n\nFor the `bge-*-v1.5`, we improve its retrieval ability when not using instruction. \nNo instruction only has a slight degradation in retrieval performance compared with using instruction. \nSo you can generate embedding without instruction in all cases for convenience.\n \nFor a retrieval task that uses short queries to find long related documents, \nit is recommended to add instructions for these short queries.\n**The best method to decide whether to add instructions for queries is choosing the setting that achieves better performance on your task.**\nIn all cases, the documents/passages do not need to add the instruction. \n\n</details>\n\n\n## Usage \n\n### Usage for Embedding Model\n\nHere are some examples for using `bge` models with \n[FlagEmbedding](#using-flagembedding), [Sentence-Transformers](#using-sentence-transformers), [Langchain](#using-langchain), or [Huggingface Transformers](#using-huggingface-transformers).\n\n#### Using FlagEmbedding\n```\npip install -U FlagEmbedding\n```\nIf it doesn't work for you, you can see [FlagEmbedding](https://github.com/FlagOpen/FlagEmbedding/blob/master/FlagEmbedding/baai_general_embedding/README.md) for more methods to install FlagEmbedding.\n\n```python\nfrom FlagEmbedding import FlagModel\nsentences_1 = [\"\u6837\u4f8b\u6570\u636e-1\", \"\u6837\u4f8b\u6570\u636e-2\"]\nsentences_2 = [\"\u6837\u4f8b\u6570\u636e-3\", \"\u6837\u4f8b\u6570\u636e-4\"]\nmodel = FlagModel('BAAI/bge-large-zh-v1.5', \n                  query_instruction_for_retrieval=\"\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a\",\n                  use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\nembeddings_1 = model.encode(sentences_1)\nembeddings_2 = model.encode(sentences_2)\nsimilarity = embeddings_1 @ embeddings_2.T\nprint(similarity)\n\n# for s2p(short query to long passage) retrieval task, suggest to use encode_queries() which will automatically add the instruction to each query\n# corpus in retrieval task can still use encode() or encode_corpus(), since they don't need instruction\nqueries = ['query_1', 'query_2']\npassages = [\"\u6837\u4f8b\u6587\u6863-1\", \"\u6837\u4f8b\u6587\u6863-2\"]\nq_embeddings = model.encode_queries(queries)\np_embeddings = model.encode(passages)\nscores = q_embeddings @ p_embeddings.T\n```\nFor the value of the argument `query_instruction_for_retrieval`, see [Model List](https://github.com/FlagOpen/FlagEmbedding/tree/master#model-list). \n\nBy default, FlagModel will use all available GPUs when encoding. Please set `os.environ[\"CUDA_VISIBLE_DEVICES\"]` to select specific GPUs.\nYou also can set `os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\"` to make all GPUs unavailable.\n\n\n#### Using Sentence-Transformers\n\nYou can also use the `bge` models with [sentence-transformers](https://www.SBERT.net):\n\n```\npip install -U sentence-transformers\n```\n```python\nfrom sentence_transformers import SentenceTransformer\nsentences_1 = [\"\u6837\u4f8b\u6570\u636e-1\", \"\u6837\u4f8b\u6570\u636e-2\"]\nsentences_2 = [\"\u6837\u4f8b\u6570\u636e-3\", \"\u6837\u4f8b\u6570\u636e-4\"]\nmodel = SentenceTransformer('BAAI/bge-large-zh-v1.5')\nembeddings_1 = model.encode(sentences_1, normalize_embeddings=True)\nembeddings_2 = model.encode(sentences_2, normalize_embeddings=True)\nsimilarity = embeddings_1 @ embeddings_2.T\nprint(similarity)\n```\nFor s2p(short query to long passage) retrieval task, \neach short query should start with an instruction (instructions see [Model List](https://github.com/FlagOpen/FlagEmbedding/tree/master#model-list)). \nBut the instruction is not needed for passages.\n```python\nfrom sentence_transformers import SentenceTransformer\nqueries = ['query_1', 'query_2']\npassages = [\"\u6837\u4f8b\u6587\u6863-1\", \"\u6837\u4f8b\u6587\u6863-2\"]\ninstruction = \"\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a\"\n\nmodel = SentenceTransformer('BAAI/bge-large-zh-v1.5')\nq_embeddings = model.encode([instruction+q for q in queries], normalize_embeddings=True)\np_embeddings = model.encode(passages, normalize_embeddings=True)\nscores = q_embeddings @ p_embeddings.T\n```\n\n#### Using Langchain \n\nYou can use `bge` in langchain like this:\n```python\nfrom langchain.embeddings import HuggingFaceBgeEmbeddings\nmodel_name = \"BAAI/bge-large-en-v1.5\"\nmodel_kwargs = {'device': 'cuda'}\nencode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\nmodel = HuggingFaceBgeEmbeddings(\n    model_name=model_name,\n    model_kwargs=model_kwargs,\n    encode_kwargs=encode_kwargs,\n    query_instruction=\"\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a\"\n)\nmodel.query_instruction = \"\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a\"\n```\n\n\n#### Using HuggingFace Transformers\n\nWith the transformers package, you can use the model like this: First, you pass your input through the transformer model, then you select the last hidden state of the first token (i.e., [CLS]) as the sentence embedding.\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\n# Sentences we want sentence embeddings for\nsentences = [\"\u6837\u4f8b\u6570\u636e-1\", \"\u6837\u4f8b\u6570\u636e-2\"]\n\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained('BAAI/bge-large-zh-v1.5')\nmodel = AutoModel.from_pretrained('BAAI/bge-large-zh-v1.5')\nmodel.eval()\n\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n# for s2p(short query to long passage) retrieval task, add an instruction to query (not add instruction for passages)\n# encoded_input = tokenizer([instruction + q for q in queries], padding=True, truncation=True, return_tensors='pt')\n\n# Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n    # Perform pooling. In this case, cls pooling.\n    sentence_embeddings = model_output[0][:, 0]\n# normalize embeddings\nsentence_embeddings = torch.nn.functional.normalize(sentence_embeddings, p=2, dim=1)\nprint(\"Sentence embeddings:\", sentence_embeddings)\n```\n\n### Usage for Reranker\n\nDifferent from embedding model, reranker uses question and document as input and directly output similarity instead of embedding. \nYou can get a relevance score by inputting query and passage to the reranker. \nThe reranker is optimized based cross-entropy loss, so the relevance score is not bounded to a specific range.\n\n\n#### Using FlagEmbedding\n```\npip install -U FlagEmbedding\n```\n\nGet relevance scores (higher scores indicate more relevance):\n```python\nfrom FlagEmbedding import FlagReranker\nreranker = FlagReranker('BAAI/bge-reranker-large', use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\n\nscore = reranker.compute_score(['query', 'passage'])\nprint(score)\n\nscores = reranker.compute_score([['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']])\nprint(scores)\n```\n\n\n#### Using Huggingface transformers\n\n```python\nimport torch\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained('BAAI/bge-reranker-large')\nmodel = AutoModelForSequenceClassification.from_pretrained('BAAI/bge-reranker-large')\nmodel.eval()\n\npairs = [['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']]\nwith torch.no_grad():\n    inputs = tokenizer(pairs, padding=True, truncation=True, return_tensors='pt', max_length=512)\n    scores = model(**inputs, return_dict=True).logits.view(-1, ).float()\n    print(scores)\n```\n\n#### Usage reranker with the ONNX files\n\n```python\nfrom optimum.onnxruntime import ORTModelForSequenceClassification  # type: ignore\n\nimport torch\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained('BAAI/bge-reranker-large')\nmodel = AutoModelForSequenceClassification.from_pretrained('BAAI/bge-reranker-base')\nmodel_ort = ORTModelForSequenceClassification.from_pretrained('BAAI/bge-reranker-base', file_name=\"onnx/model.onnx\")\n\n# Sentences we want sentence embeddings for\npairs = [['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']]\n\n# Tokenize sentences\nencoded_input = tokenizer(pairs, padding=True, truncation=True, return_tensors='pt')\n\nscores_ort = model_ort(**encoded_input, return_dict=True).logits.view(-1, ).float()\n# Compute token embeddings\nwith torch.inference_mode():\n    scores = model_ort(**encoded_input, return_dict=True).logits.view(-1, ).float()\n\n# scores and scores_ort are identical\n```\n#### Usage reranker with infinity\n\nIts also possible to deploy the onnx/torch files with the [infinity_emb](https://github.com/michaelfeil/infinity) pip package.\n```python\nimport asyncio\nfrom infinity_emb import AsyncEmbeddingEngine, EngineArgs\n\nquery='what is a panda?'\ndocs = ['The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear', \"Paris is in France.\"]\n\nengine = AsyncEmbeddingEngine.from_args(\n    EngineArgs(model_name_or_path = \"BAAI/bge-reranker-base\", device=\"cpu\", engine=\"torch\" # or engine=\"optimum\" for onnx\n))\n\nasync def main(): \n    async with engine:\n        ranking, usage = await engine.rerank(query=query, docs=docs)\n        print(list(zip(ranking, docs)))\nasyncio.run(main())\n```\n\n## Evaluation  \n\n`baai-general-embedding` models achieve **state-of-the-art performance on both MTEB and C-MTEB leaderboard!**\nFor more details and evaluation tools see our [scripts](https://github.com/FlagOpen/FlagEmbedding/blob/master/C_MTEB/README.md). \n\n- **MTEB**:   \n\n| Model Name |  Dimension | Sequence Length | Average (56) | Retrieval (15) |Clustering (11) | Pair Classification (3) | Reranking (4) |  STS (10) | Summarization (1) | Classification (12) |\n|:----:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n| [BAAI/bge-large-en-v1.5](https://huggingface.co/BAAI/bge-large-en-v1.5) | 1024 | 512 |  **64.23** | **54.29** |  46.08 | 87.12 | 60.03 | 83.11 | 31.61 | 75.97 |  \n| [BAAI/bge-base-en-v1.5](https://huggingface.co/BAAI/bge-base-en-v1.5) |  768 | 512 | 63.55 | 53.25 |   45.77 | 86.55 | 58.86 | 82.4 | 31.07 | 75.53 |  \n| [BAAI/bge-small-en-v1.5](https://huggingface.co/BAAI/bge-small-en-v1.5) |  384 | 512 | 62.17 |51.68 | 43.82 |  84.92 | 58.36 | 81.59 | 30.12 | 74.14 |  \n| [bge-large-en](https://huggingface.co/BAAI/bge-large-en) |  1024 | 512 | 63.98 |  53.9 | 46.98 | 85.8 | 59.48 | 81.56 | 32.06 | 76.21 | \n| [bge-base-en](https://huggingface.co/BAAI/bge-base-en) |  768 | 512 |  63.36 | 53.0 | 46.32 | 85.86 | 58.7 | 81.84 | 29.27 | 75.27 | \n| [gte-large](https://huggingface.co/thenlper/gte-large) |  1024 | 512 | 63.13 | 52.22 | 46.84 | 85.00 | 59.13 | 83.35 | 31.66 | 73.33 |\n| [gte-base](https://huggingface.co/thenlper/gte-base) \t|  768 | 512 | 62.39 | 51.14 | 46.2 | 84.57 | 58.61 | 82.3 | 31.17 | 73.01 |\n| [e5-large-v2](https://huggingface.co/intfloat/e5-large-v2) |  1024| 512 | 62.25 | 50.56 | 44.49 | 86.03 | 56.61 | 82.05 | 30.19 | 75.24 |\n| [bge-small-en](https://huggingface.co/BAAI/bge-small-en) |  384 | 512 | 62.11 |  51.82 | 44.31 | 83.78 | 57.97 | 80.72 | 30.53 | 74.37 |  \n| [instructor-xl](https://huggingface.co/hkunlp/instructor-xl) |  768 | 512 | 61.79 | 49.26 | 44.74 | 86.62 | 57.29 | 83.06 | 32.32 | 61.79 |\n| [e5-base-v2](https://huggingface.co/intfloat/e5-base-v2) |  768 | 512 | 61.5 | 50.29 | 43.80 | 85.73 | 55.91 | 81.05 | 30.28 | 73.84 |\n| [gte-small](https://huggingface.co/thenlper/gte-small) |  384 | 512 | 61.36 | 49.46 | 44.89 | 83.54 | 57.7 | 82.07 | 30.42 | 72.31 |\n| [text-embedding-ada-002](https://platform.openai.com/docs/guides/embeddings) | 1536 | 8192 | 60.99 | 49.25 | 45.9 | 84.89 | 56.32 | 80.97 | 30.8 | 70.93 |\n| [e5-small-v2](https://huggingface.co/intfloat/e5-base-v2) | 384 | 512 | 59.93 | 49.04 | 39.92 | 84.67 | 54.32 | 80.39 | 31.16 | 72.94 |\n| [sentence-t5-xxl](https://huggingface.co/sentence-transformers/sentence-t5-xxl) |  768 | 512 | 59.51 | 42.24 | 43.72 | 85.06 | 56.42 | 82.63 | 30.08 | 73.42 |\n| [all-mpnet-base-v2](https://huggingface.co/sentence-transformers/all-mpnet-base-v2) \t|  768 | 514 \t| 57.78 | 43.81 | 43.69 | 83.04 | 59.36 | 80.28 | 27.49 | 65.07 |\n| [sgpt-bloom-7b1-msmarco](https://huggingface.co/bigscience/sgpt-bloom-7b1-msmarco) \t|  4096 | 2048 | 57.59 | 48.22 | 38.93 | 81.9 | 55.65 | 77.74 | 33.6 | 66.19 |\n\n\n\n- **C-MTEB**:  \nWe create the benchmark C-MTEB for Chinese text embedding which consists of 31 datasets from 6 tasks. \nPlease refer to [C_MTEB](https://github.com/FlagOpen/FlagEmbedding/blob/master/C_MTEB/README.md) for a detailed introduction.\n \n| Model | Embedding dimension | Avg | Retrieval | STS | PairClassification | Classification | Reranking | Clustering |\n|:-------------------------------|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|\n| [**BAAI/bge-large-zh-v1.5**](https://huggingface.co/BAAI/bge-large-zh-v1.5) | 1024 |  **64.53** | 70.46 | 56.25 | 81.6 | 69.13 | 65.84 | 48.99 |  \n| [BAAI/bge-base-zh-v1.5](https://huggingface.co/BAAI/bge-base-zh-v1.5) | 768 |  63.13 | 69.49 | 53.72 | 79.75 | 68.07 | 65.39 | 47.53 |  \n| [BAAI/bge-small-zh-v1.5](https://huggingface.co/BAAI/bge-small-zh-v1.5) | 512 | 57.82 | 61.77 | 49.11 | 70.41 | 63.96 | 60.92 | 44.18 |   \n| [BAAI/bge-large-zh](https://huggingface.co/BAAI/bge-large-zh) | 1024 | 64.20 | 71.53 | 54.98 | 78.94 | 68.32 | 65.11 | 48.39 |\n| [bge-large-zh-noinstruct](https://huggingface.co/BAAI/bge-large-zh-noinstruct) | 1024 | 63.53 | 70.55 | 53 | 76.77 | 68.58 | 64.91 | 50.01 |\n| [BAAI/bge-base-zh](https://huggingface.co/BAAI/bge-base-zh) | 768 | 62.96 | 69.53 | 54.12 | 77.5 | 67.07 | 64.91 | 47.63 |\n| [multilingual-e5-large](https://huggingface.co/intfloat/multilingual-e5-large) | 1024 | 58.79 | 63.66 | 48.44 | 69.89 | 67.34 | 56.00 | 48.23 |\n| [BAAI/bge-small-zh](https://huggingface.co/BAAI/bge-small-zh) | 512 | 58.27 |  63.07 | 49.45 | 70.35 | 63.64 | 61.48 | 45.09 |\n| [m3e-base](https://huggingface.co/moka-ai/m3e-base) | 768 | 57.10 | 56.91 | 50.47 | 63.99 | 67.52 | 59.34 | 47.68 |\n| [m3e-large](https://huggingface.co/moka-ai/m3e-large) | 1024 |  57.05 | 54.75 | 50.42 | 64.3 | 68.2 | 59.66 | 48.88 |\n| [multilingual-e5-base](https://huggingface.co/intfloat/multilingual-e5-base) | 768 | 55.48 | 61.63 | 46.49 | 67.07 | 65.35 | 54.35 | 40.68 |\n| [multilingual-e5-small](https://huggingface.co/intfloat/multilingual-e5-small) | 384 | 55.38 | 59.95 | 45.27 | 66.45 | 65.85 | 53.86 | 45.26 |\n| [text-embedding-ada-002(OpenAI)](https://platform.openai.com/docs/guides/embeddings/what-are-embeddings) | 1536 |  53.02 | 52.0 | 43.35 | 69.56 | 64.31 | 54.28 | 45.68 |\n| [luotuo](https://huggingface.co/silk-road/luotuo-bert-medium) | 1024 | 49.37 |  44.4 | 42.78 | 66.62 | 61 | 49.25 | 44.39 |\n| [text2vec-base](https://huggingface.co/shibing624/text2vec-base-chinese) | 768 |  47.63 | 38.79 | 43.41 | 67.41 | 62.19 | 49.45 | 37.66 |\n| [text2vec-large](https://huggingface.co/GanymedeNil/text2vec-large-chinese) | 1024 | 47.36 | 41.94 | 44.97 | 70.86 | 60.66 | 49.16 | 30.02 |\n\n\n- **Reranking**:\nSee [C_MTEB](https://github.com/FlagOpen/FlagEmbedding/blob/master/C_MTEB/) for evaluation script.\n\n| Model | T2Reranking | T2RerankingZh2En\\* | T2RerankingEn2Zh\\* | MMarcoReranking | CMedQAv1 | CMedQAv2 | Avg |  \n|:-------------------------------|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|  \n| text2vec-base-multilingual | 64.66 | 62.94 | 62.51 | 14.37 | 48.46 | 48.6 | 50.26 |  \n| multilingual-e5-small | 65.62 | 60.94 | 56.41 | 29.91 | 67.26 | 66.54 | 57.78 |  \n| multilingual-e5-large | 64.55 | 61.61 | 54.28 | 28.6 | 67.42 | 67.92 | 57.4 |  \n| multilingual-e5-base | 64.21 | 62.13 | 54.68 | 29.5 | 66.23 | 66.98 | 57.29 |  \n| m3e-base | 66.03 | 62.74 | 56.07 | 17.51 | 77.05 | 76.76 | 59.36 |  \n| m3e-large | 66.13 | 62.72 | 56.1 | 16.46 | 77.76 | 78.27 | 59.57 |  \n| bge-base-zh-v1.5 | 66.49 | 63.25 | 57.02 | 29.74 | 80.47 | 84.88 | 63.64 |  \n| bge-large-zh-v1.5 | 65.74 | 63.39 | 57.03 | 28.74 | 83.45 | 85.44 | 63.97 |  \n| [BAAI/bge-reranker-base](https://huggingface.co/BAAI/bge-reranker-base) | 67.28 | 63.95 | 60.45 | 35.46 | 81.26 | 84.1 | 65.42 |  \n| [BAAI/bge-reranker-large](https://huggingface.co/BAAI/bge-reranker-large) | 67.6 | 64.03 | 61.44 | 37.16 | 82.15 | 84.18 | 66.09 |  \n\n\\* : T2RerankingZh2En and T2RerankingEn2Zh are cross-language retrieval tasks\n\n## Train\n\n### BAAI Embedding \n\nWe pre-train the models using [retromae](https://github.com/staoxiao/RetroMAE) and train them on large-scale pairs data using contrastive learning. \n**You can fine-tune the embedding model on your data following our [examples](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune).**\nWe also provide a [pre-train example](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/pretrain).\nNote that the goal of pre-training is to reconstruct the text, and the pre-trained model cannot be used for similarity calculation directly, it needs to be fine-tuned.\nMore training details for bge see [baai_general_embedding](https://github.com/FlagOpen/FlagEmbedding/blob/master/FlagEmbedding/baai_general_embedding/README.md).\n\n\n\n### BGE Reranker\n\nCross-encoder will perform full-attention over the input pair, \nwhich is more accurate than embedding model (i.e., bi-encoder) but more time-consuming than embedding model.\nTherefore, it can be used to re-rank the top-k documents returned by embedding model.\nWe train the cross-encoder on a multilingual pair data, \nThe data format is the same as embedding model, so you can fine-tune it easily following our [example](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/reranker). \nMore details please refer to [./FlagEmbedding/reranker/README.md](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/reranker)\n\n\n\n## Citation\n\nIf you find this repository useful, please consider giving a star :star: and citation\n\n```\n@misc{bge_embedding,\n      title={C-Pack: Packaged Resources To Advance General Chinese Embedding}, \n      author={Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff},\n      year={2023},\n      eprint={2309.07597},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n\n## License\nFlagEmbedding is licensed under the [MIT License](https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE). The released models can be used for commercial purposes free of charge.",
    "library_name": "sentence-transformers"
  },
  {
    "model_id": "yiyanghkust/finbert-tone",
    "model_name": "yiyanghkust/finbert-tone",
    "author": "yiyanghkust",
    "downloads": 942768,
    "likes": 174,
    "tags": [
      "transformers",
      "pytorch",
      "tf",
      "text-classification",
      "financial-sentiment-analysis",
      "sentiment-analysis",
      "en",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/yiyanghkust/finbert-tone",
    "dependencies": [
      [
        "transformers",
        "4.30.2"
      ],
      [
        "torch",
        "2.0.1"
      ],
      [
        "numpy",
        "1.24.3"
      ],
      [
        "xformers",
        "0.0.22"
      ]
    ],
    "analysis_date": "2025-03-26T00:47:36.316228",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "unknown",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": "en",
      "tags": [
        "financial-sentiment-analysis",
        "sentiment-analysis"
      ],
      "widget": [
        {
          "text": "growth is strong and we have plenty of liquidity"
        }
      ]
    },
    "card_text": "\n`FinBERT` is a BERT model pre-trained on financial communication text. The purpose is to enhance financial NLP research and practice. It is trained on the following three financial communication corpus. The total corpora size is 4.9B tokens.\n- Corporate Reports 10-K & 10-Q: 2.5B tokens\n- Earnings Call Transcripts: 1.3B tokens\n- Analyst Reports: 1.1B tokens\n\nMore technical details on `FinBERT`: [Click Link](https://github.com/yya518/FinBERT)\n\nThis released `finbert-tone` model is the `FinBERT` model fine-tuned on 10,000 manually annotated (positive, negative, neutral) sentences from analyst reports. This model achieves superior performance on financial tone analysis task. If you are simply interested in using `FinBERT` for financial tone analysis, give it a try.\n\nIf you use the model in your academic work, please cite the following paper:\n\nHuang, Allen H., Hui Wang, and Yi Yang. \"FinBERT: A Large Language Model for Extracting Information from Financial Text.\" *Contemporary Accounting Research* (2022).\n\n\n# How to use \nYou can use this model with Transformers pipeline for sentiment analysis.\n```python\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom transformers import pipeline\n\nfinbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone',num_labels=3)\ntokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')\n\nnlp = pipeline(\"sentiment-analysis\", model=finbert, tokenizer=tokenizer)\n\nsentences = [\"there is a shortage of capital, and we need extra financing\",  \n             \"growth is strong and we have plenty of liquidity\", \n             \"there are doubts about our finances\", \n             \"profits are flat\"]\nresults = nlp(sentences)\nprint(results)  #LABEL_0: neutral; LABEL_1: positive; LABEL_2: negative\n\n```",
    "card_content": "---\nlanguage: en\ntags:\n- financial-sentiment-analysis\n- sentiment-analysis\nwidget:\n- text: growth is strong and we have plenty of liquidity\n---\n\n`FinBERT` is a BERT model pre-trained on financial communication text. The purpose is to enhance financial NLP research and practice. It is trained on the following three financial communication corpus. The total corpora size is 4.9B tokens.\n- Corporate Reports 10-K & 10-Q: 2.5B tokens\n- Earnings Call Transcripts: 1.3B tokens\n- Analyst Reports: 1.1B tokens\n\nMore technical details on `FinBERT`: [Click Link](https://github.com/yya518/FinBERT)\n\nThis released `finbert-tone` model is the `FinBERT` model fine-tuned on 10,000 manually annotated (positive, negative, neutral) sentences from analyst reports. This model achieves superior performance on financial tone analysis task. If you are simply interested in using `FinBERT` for financial tone analysis, give it a try.\n\nIf you use the model in your academic work, please cite the following paper:\n\nHuang, Allen H., Hui Wang, and Yi Yang. \"FinBERT: A Large Language Model for Extracting Information from Financial Text.\" *Contemporary Accounting Research* (2022).\n\n\n# How to use \nYou can use this model with Transformers pipeline for sentiment analysis.\n```python\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom transformers import pipeline\n\nfinbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone',num_labels=3)\ntokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')\n\nnlp = pipeline(\"sentiment-analysis\", model=finbert, tokenizer=tokenizer)\n\nsentences = [\"there is a shortage of capital, and we need extra financing\",  \n             \"growth is strong and we have plenty of liquidity\", \n             \"there are doubts about our finances\", \n             \"profits are flat\"]\nresults = nlp(sentences)\nprint(results)  #LABEL_0: neutral; LABEL_1: positive; LABEL_2: negative\n\n```",
    "library_name": "transformers"
  },
  {
    "model_id": "pysentimiento/robertuito-sentiment-analysis",
    "model_name": "pysentimiento/robertuito-sentiment-analysis",
    "author": "pysentimiento",
    "downloads": 899187,
    "likes": 87,
    "tags": [
      "pysentimiento",
      "pytorch",
      "tf",
      "safetensors",
      "roberta",
      "twitter",
      "sentiment-analysis",
      "text-classification",
      "es",
      "region:us"
    ],
    "card_url": "https://huggingface.co/pysentimiento/robertuito-sentiment-analysis",
    "dependencies": [
      [
        "pysentimiento",
        "0.7.2"
      ]
    ],
    "analysis_date": "2025-03-26T00:47:37.335268",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "roberta",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": [
        "es"
      ],
      "library_name": "pysentimiento",
      "tags": [
        "twitter",
        "sentiment-analysis"
      ],
      "pipeline_tag": "text-classification"
    },
    "card_text": "# Sentiment Analysis in Spanish\n## robertuito-sentiment-analysis\n\nRepository: [https://github.com/pysentimiento/pysentimiento/](https://github.com/finiteautomata/pysentimiento/)\n\n\nModel trained with TASS 2020 corpus (around ~5k tweets) of several dialects of Spanish. Base model is [RoBERTuito](https://github.com/pysentimiento/robertuito), a RoBERTa model trained in Spanish tweets.\n\nUses `POS`, `NEG`, `NEU` labels.\n\n## Usage\n\nUse it directly with [pysentimiento](https://github.com/pysentimiento/pysentimiento)\n\n```python\nfrom pysentimiento import create_analyzer\nanalyzer = create_analyzer(task=\"sentiment\", lang=\"es\")\n\nanalyzer.predict(\"Qu\u00e9 gran jugador es Messi\")\n# returns AnalyzerOutput(output=POS, probas={POS: 0.998, NEG: 0.002, NEU: 0.000})\n```\n\n\n## Results\n\nResults for the four tasks evaluated in `pysentimiento`. Results are expressed as Macro F1 scores\n\n\n| model         | emotion       | hate_speech   | irony         | sentiment     |\n|:--------------|:--------------|:--------------|:--------------|:--------------|\n| robertuito    | 0.560 \u00b1 0.010 | 0.759 \u00b1 0.007 | 0.739 \u00b1 0.005 | 0.705 \u00b1 0.003 |\n| roberta       | 0.527 \u00b1 0.015 | 0.741 \u00b1 0.012 | 0.721 \u00b1 0.008 | 0.670 \u00b1 0.006 |\n| bertin        | 0.524 \u00b1 0.007 | 0.738 \u00b1 0.007 | 0.713 \u00b1 0.012 | 0.666 \u00b1 0.005 |\n| beto_uncased  | 0.532 \u00b1 0.012 | 0.727 \u00b1 0.016 | 0.701 \u00b1 0.007 | 0.651 \u00b1 0.006 |\n| beto_cased    | 0.516 \u00b1 0.012 | 0.724 \u00b1 0.012 | 0.705 \u00b1 0.009 | 0.662 \u00b1 0.005 |\n| mbert_uncased | 0.493 \u00b1 0.010 | 0.718 \u00b1 0.011 | 0.681 \u00b1 0.010 | 0.617 \u00b1 0.003 |\n| biGRU         | 0.264 \u00b1 0.007 | 0.592 \u00b1 0.018 | 0.631 \u00b1 0.011 | 0.585 \u00b1 0.011 |\n\n\nNote that for Hate Speech, these are the results for Semeval 2019, Task 5 Subtask B\n\n## Citation\n\nIf you use this model in your research, please cite pysentimiento, RoBERTuito and TASS papers:\n\n```latex\n\n@article{perez2021pysentimiento,\n  title={pysentimiento: a python toolkit for opinion mining and social NLP tasks},\n  author={P{\\'e}rez, Juan Manuel and Rajngewerc, Mariela and Giudici, Juan Carlos and Furman, Dami{\\'a}n A and Luque, Franco and Alemany, Laura Alonso and Mart{\\'\\i}nez, Mar{\\'\\i}a Vanina},\n  journal={arXiv preprint arXiv:2106.09462},\n  year={2021}\n}\n\n@inproceedings{perez-etal-2022-robertuito,\n    title = \"{R}o{BERT}uito: a pre-trained language model for social media text in {S}panish\",\n    author = \"P{\\'e}rez, Juan Manuel  and\n      Furman, Dami{\\'a}n Ariel  and\n      Alonso Alemany, Laura  and\n      Luque, Franco M.\",\n    booktitle = \"Proceedings of the Thirteenth Language Resources and Evaluation Conference\",\n    month = jun,\n    year = \"2022\",\n    address = \"Marseille, France\",\n    publisher = \"European Language Resources Association\",\n    url = \"https://aclanthology.org/2022.lrec-1.785\",\n    pages = \"7235--7243\",\n    abstract = \"Since BERT appeared, Transformer language models and transfer learning have become state-of-the-art for natural language processing tasks. Recently, some works geared towards pre-training specially-crafted models for particular domains, such as scientific papers, medical documents, user-generated texts, among others. These domain-specific models have been shown to improve performance significantly in most tasks; however, for languages other than English, such models are not widely available. In this work, we present RoBERTuito, a pre-trained language model for user-generated text in Spanish, trained on over 500 million tweets. Experiments on a benchmark of tasks involving user-generated text showed that RoBERTuito outperformed other pre-trained language models in Spanish. In addition to this, our model has some cross-lingual abilities, achieving top results for English-Spanish tasks of the Linguistic Code-Switching Evaluation benchmark (LinCE) and also competitive performance against monolingual models in English Twitter tasks. To facilitate further research, we make RoBERTuito publicly available at the HuggingFace model hub together with the dataset used to pre-train it.\",\n}\n\n@inproceedings{garcia2020overview,\n  title={Overview of TASS 2020: Introducing emotion detection},\n  author={Garc{\\'\\i}a-Vega, Manuel and D{\\'\\i}az-Galiano, MC and Garc{\\'\\i}a-Cumbreras, MA and Del Arco, FMP and Montejo-R{\\'a}ez, A and Jim{\\'e}nez-Zafra, SM and Mart{\\'\\i}nez C{\\'a}mara, E and Aguilar, CA and Cabezudo, MAS and Chiruzzo, L and others},\n  booktitle={Proceedings of the Iberian Languages Evaluation Forum (IberLEF 2020) Co-Located with 36th Conference of the Spanish Society for Natural Language Processing (SEPLN 2020), M{\\'a}laga, Spain},\n  pages={163--170},\n  year={2020}\n}\n```",
    "card_content": "---\nlanguage:\n- es\nlibrary_name: pysentimiento\ntags:\n- twitter\n- sentiment-analysis\npipeline_tag: text-classification\n---\n# Sentiment Analysis in Spanish\n## robertuito-sentiment-analysis\n\nRepository: [https://github.com/pysentimiento/pysentimiento/](https://github.com/finiteautomata/pysentimiento/)\n\n\nModel trained with TASS 2020 corpus (around ~5k tweets) of several dialects of Spanish. Base model is [RoBERTuito](https://github.com/pysentimiento/robertuito), a RoBERTa model trained in Spanish tweets.\n\nUses `POS`, `NEG`, `NEU` labels.\n\n## Usage\n\nUse it directly with [pysentimiento](https://github.com/pysentimiento/pysentimiento)\n\n```python\nfrom pysentimiento import create_analyzer\nanalyzer = create_analyzer(task=\"sentiment\", lang=\"es\")\n\nanalyzer.predict(\"Qu\u00e9 gran jugador es Messi\")\n# returns AnalyzerOutput(output=POS, probas={POS: 0.998, NEG: 0.002, NEU: 0.000})\n```\n\n\n## Results\n\nResults for the four tasks evaluated in `pysentimiento`. Results are expressed as Macro F1 scores\n\n\n| model         | emotion       | hate_speech   | irony         | sentiment     |\n|:--------------|:--------------|:--------------|:--------------|:--------------|\n| robertuito    | 0.560 \u00b1 0.010 | 0.759 \u00b1 0.007 | 0.739 \u00b1 0.005 | 0.705 \u00b1 0.003 |\n| roberta       | 0.527 \u00b1 0.015 | 0.741 \u00b1 0.012 | 0.721 \u00b1 0.008 | 0.670 \u00b1 0.006 |\n| bertin        | 0.524 \u00b1 0.007 | 0.738 \u00b1 0.007 | 0.713 \u00b1 0.012 | 0.666 \u00b1 0.005 |\n| beto_uncased  | 0.532 \u00b1 0.012 | 0.727 \u00b1 0.016 | 0.701 \u00b1 0.007 | 0.651 \u00b1 0.006 |\n| beto_cased    | 0.516 \u00b1 0.012 | 0.724 \u00b1 0.012 | 0.705 \u00b1 0.009 | 0.662 \u00b1 0.005 |\n| mbert_uncased | 0.493 \u00b1 0.010 | 0.718 \u00b1 0.011 | 0.681 \u00b1 0.010 | 0.617 \u00b1 0.003 |\n| biGRU         | 0.264 \u00b1 0.007 | 0.592 \u00b1 0.018 | 0.631 \u00b1 0.011 | 0.585 \u00b1 0.011 |\n\n\nNote that for Hate Speech, these are the results for Semeval 2019, Task 5 Subtask B\n\n## Citation\n\nIf you use this model in your research, please cite pysentimiento, RoBERTuito and TASS papers:\n\n```latex\n\n@article{perez2021pysentimiento,\n  title={pysentimiento: a python toolkit for opinion mining and social NLP tasks},\n  author={P{\\'e}rez, Juan Manuel and Rajngewerc, Mariela and Giudici, Juan Carlos and Furman, Dami{\\'a}n A and Luque, Franco and Alemany, Laura Alonso and Mart{\\'\\i}nez, Mar{\\'\\i}a Vanina},\n  journal={arXiv preprint arXiv:2106.09462},\n  year={2021}\n}\n\n@inproceedings{perez-etal-2022-robertuito,\n    title = \"{R}o{BERT}uito: a pre-trained language model for social media text in {S}panish\",\n    author = \"P{\\'e}rez, Juan Manuel  and\n      Furman, Dami{\\'a}n Ariel  and\n      Alonso Alemany, Laura  and\n      Luque, Franco M.\",\n    booktitle = \"Proceedings of the Thirteenth Language Resources and Evaluation Conference\",\n    month = jun,\n    year = \"2022\",\n    address = \"Marseille, France\",\n    publisher = \"European Language Resources Association\",\n    url = \"https://aclanthology.org/2022.lrec-1.785\",\n    pages = \"7235--7243\",\n    abstract = \"Since BERT appeared, Transformer language models and transfer learning have become state-of-the-art for natural language processing tasks. Recently, some works geared towards pre-training specially-crafted models for particular domains, such as scientific papers, medical documents, user-generated texts, among others. These domain-specific models have been shown to improve performance significantly in most tasks; however, for languages other than English, such models are not widely available. In this work, we present RoBERTuito, a pre-trained language model for user-generated text in Spanish, trained on over 500 million tweets. Experiments on a benchmark of tasks involving user-generated text showed that RoBERTuito outperformed other pre-trained language models in Spanish. In addition to this, our model has some cross-lingual abilities, achieving top results for English-Spanish tasks of the Linguistic Code-Switching Evaluation benchmark (LinCE) and also competitive performance against monolingual models in English Twitter tasks. To facilitate further research, we make RoBERTuito publicly available at the HuggingFace model hub together with the dataset used to pre-train it.\",\n}\n\n@inproceedings{garcia2020overview,\n  title={Overview of TASS 2020: Introducing emotion detection},\n  author={Garc{\\'\\i}a-Vega, Manuel and D{\\'\\i}az-Galiano, MC and Garc{\\'\\i}a-Cumbreras, MA and Del Arco, FMP and Montejo-R{\\'a}ez, A and Jim{\\'e}nez-Zafra, SM and Mart{\\'\\i}nez C{\\'a}mara, E and Aguilar, CA and Cabezudo, MAS and Chiruzzo, L and others},\n  booktitle={Proceedings of the Iberian Languages Evaluation Forum (IberLEF 2020) Co-Located with 36th Conference of the Spanish Society for Natural Language Processing (SEPLN 2020), M{\\'a}laga, Spain},\n  pages={163--170},\n  year={2020}\n}\n```",
    "library_name": "pysentimiento"
  },
  {
    "model_id": "mixedbread-ai/mxbai-rerank-base-v1",
    "model_name": "mixedbread-ai/mxbai-rerank-base-v1",
    "author": "mixedbread-ai",
    "downloads": 880320,
    "likes": 41,
    "tags": [
      "transformers",
      "onnx",
      "safetensors",
      "deberta-v2",
      "text-classification",
      "reranker",
      "transformers.js",
      "en",
      "license:apache-2.0",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/mixedbread-ai/mxbai-rerank-base-v1",
    "dependencies": null,
    "analysis_date": "2025-03-26T00:47:41.964876",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "deberta-v2",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": [
        "en"
      ],
      "license": "apache-2.0",
      "library_name": "transformers",
      "tags": [
        "reranker",
        "transformers.js"
      ]
    },
    "card_text": "<br><br>\n\n<p align=\"center\">\n<svg xmlns=\"http://www.w3.org/2000/svg\" xml:space=\"preserve\" viewBox=\"0 0 2020 1130\" width=\"150\" height=\"150\" aria-hidden=\"true\"><path fill=\"#e95a0f\" d=\"M398.167 621.992c-1.387-20.362-4.092-40.739-3.851-61.081.355-30.085 6.873-59.139 21.253-85.976 10.487-19.573 24.09-36.822 40.662-51.515 16.394-14.535 34.338-27.046 54.336-36.182 15.224-6.955 31.006-12.609 47.829-14.168 11.809-1.094 23.753-2.514 35.524-1.836 23.033 1.327 45.131 7.255 66.255 16.75 16.24 7.3 31.497 16.165 45.651 26.969 12.997 9.921 24.412 21.37 34.158 34.509 11.733 15.817 20.849 33.037 25.987 52.018 3.468 12.81 6.438 25.928 7.779 39.097 1.722 16.908 1.642 34.003 2.235 51.021.427 12.253.224 24.547 1.117 36.762 1.677 22.93 4.062 45.764 11.8 67.7 5.376 15.239 12.499 29.55 20.846 43.681l-18.282 20.328c-1.536 1.71-2.795 3.665-4.254 5.448l-19.323 23.533c-13.859-5.449-27.446-11.803-41.657-16.086-13.622-4.106-27.793-6.765-41.905-8.775-15.256-2.173-30.701-3.475-46.105-4.049-23.571-.879-47.178-1.056-70.769-1.029-10.858.013-21.723 1.116-32.57 1.926-5.362.4-10.69 1.255-16.464 1.477-2.758-7.675-5.284-14.865-7.367-22.181-3.108-10.92-4.325-22.554-13.16-31.095-2.598-2.512-5.069-5.341-6.883-8.443-6.366-10.884-12.48-21.917-18.571-32.959-4.178-7.573-8.411-14.375-17.016-18.559-10.34-5.028-19.538-12.387-29.311-18.611-3.173-2.021-6.414-4.312-9.952-5.297-5.857-1.63-11.98-2.301-17.991-3.376z\"></path><path fill=\"#ed6d7b\" d=\"M1478.998 758.842c-12.025.042-24.05.085-36.537-.373-.14-8.536.231-16.569.453-24.607.033-1.179-.315-2.986-1.081-3.4-.805-.434-2.376.338-3.518.81-.856.354-1.562 1.069-3.589 2.521-.239-3.308-.664-5.586-.519-7.827.488-7.544 2.212-15.166 1.554-22.589-1.016-11.451 1.397-14.592-12.332-14.419-3.793.048-3.617-2.803-3.332-5.331.499-4.422 1.45-8.803 1.77-13.233.311-4.316.068-8.672.068-12.861-2.554-.464-4.326-.86-6.12-1.098-4.415-.586-6.051-2.251-5.065-7.31 1.224-6.279.848-12.862 1.276-19.306.19-2.86-.971-4.473-3.794-4.753-4.113-.407-8.242-1.057-12.352-.975-4.663.093-5.192-2.272-4.751-6.012.733-6.229 1.252-12.483 1.875-18.726l1.102-10.495c-5.905-.309-11.146-.805-16.385-.778-3.32.017-5.174-1.4-5.566-4.4-1.172-8.968-2.479-17.944-3.001-26.96-.26-4.484-1.936-5.705-6.005-5.774-9.284-.158-18.563-.594-27.843-.953-7.241-.28-10.137-2.764-11.3-9.899-.746-4.576-2.715-7.801-7.777-8.207-7.739-.621-15.511-.992-23.207-1.961-7.327-.923-14.587-2.415-21.853-3.777-5.021-.941-10.003-2.086-15.003-3.14 4.515-22.952 13.122-44.382 26.284-63.587 18.054-26.344 41.439-47.239 69.102-63.294 15.847-9.197 32.541-16.277 50.376-20.599 16.655-4.036 33.617-5.715 50.622-4.385 33.334 2.606 63.836 13.955 92.415 31.15 15.864 9.545 30.241 20.86 42.269 34.758 8.113 9.374 15.201 19.78 21.718 30.359 10.772 17.484 16.846 36.922 20.611 56.991 1.783 9.503 2.815 19.214 3.318 28.876.758 14.578.755 29.196.65 44.311l-51.545 20.013c-7.779 3.059-15.847 5.376-21.753 12.365-4.73 5.598-10.658 10.316-16.547 14.774-9.9 7.496-18.437 15.988-25.083 26.631-3.333 5.337-7.901 10.381-12.999 14.038-11.355 8.144-17.397 18.973-19.615 32.423l-6.988 41.011z\"></path><path fill=\"#ec663e\" d=\"M318.11 923.047c-.702 17.693-.832 35.433-2.255 53.068-1.699 21.052-6.293 41.512-14.793 61.072-9.001 20.711-21.692 38.693-38.496 53.583-16.077 14.245-34.602 24.163-55.333 30.438-21.691 6.565-43.814 8.127-66.013 6.532-22.771-1.636-43.88-9.318-62.74-22.705-20.223-14.355-35.542-32.917-48.075-54.096-9.588-16.203-16.104-33.55-19.201-52.015-2.339-13.944-2.307-28.011-.403-42.182 2.627-19.545 9.021-37.699 17.963-55.067 11.617-22.564 27.317-41.817 48.382-56.118 15.819-10.74 33.452-17.679 52.444-20.455 8.77-1.282 17.696-1.646 26.568-2.055 11.755-.542 23.534-.562 35.289-1.11 8.545-.399 17.067-1.291 26.193-1.675 1.349 1.77 2.24 3.199 2.835 4.742 4.727 12.261 10.575 23.865 18.636 34.358 7.747 10.084 14.83 20.684 22.699 30.666 3.919 4.972 8.37 9.96 13.609 13.352 7.711 4.994 16.238 8.792 24.617 12.668 5.852 2.707 12.037 4.691 18.074 6.998z\"></path><path fill=\"#ea580e\" d=\"M1285.167 162.995c3.796-29.75 13.825-56.841 32.74-80.577 16.339-20.505 36.013-36.502 59.696-47.614 14.666-6.881 29.971-11.669 46.208-12.749 10.068-.669 20.239-1.582 30.255-.863 16.6 1.191 32.646 5.412 47.9 12.273 19.39 8.722 36.44 20.771 50.582 36.655 15.281 17.162 25.313 37.179 31.49 59.286 5.405 19.343 6.31 39.161 4.705 58.825-2.37 29.045-11.836 55.923-30.451 78.885-10.511 12.965-22.483 24.486-37.181 33.649-5.272-5.613-10.008-11.148-14.539-16.846-5.661-7.118-10.958-14.533-16.78-21.513-4.569-5.478-9.548-10.639-14.624-15.658-3.589-3.549-7.411-6.963-11.551-9.827-5.038-3.485-10.565-6.254-15.798-9.468-8.459-5.195-17.011-9.669-26.988-11.898-12.173-2.72-24.838-4.579-35.622-11.834-1.437-.967-3.433-1.192-5.213-1.542-12.871-2.529-25.454-5.639-36.968-12.471-5.21-3.091-11.564-4.195-17.011-6.965-4.808-2.445-8.775-6.605-13.646-8.851-8.859-4.085-18.114-7.311-27.204-10.896z\"></path><path fill=\"#f8ab00\" d=\"M524.963 311.12c-9.461-5.684-19.513-10.592-28.243-17.236-12.877-9.801-24.031-21.578-32.711-35.412-11.272-17.965-19.605-37.147-21.902-58.403-1.291-11.951-2.434-24.073-1.87-36.034.823-17.452 4.909-34.363 11.581-50.703 8.82-21.603 22.25-39.792 39.568-55.065 18.022-15.894 39.162-26.07 62.351-32.332 19.22-5.19 38.842-6.177 58.37-4.674 23.803 1.831 45.56 10.663 65.062 24.496 17.193 12.195 31.688 27.086 42.894 45.622-11.403 8.296-22.633 16.117-34.092 23.586-17.094 11.142-34.262 22.106-48.036 37.528-8.796 9.848-17.201 20.246-27.131 28.837-16.859 14.585-27.745 33.801-41.054 51.019-11.865 15.349-20.663 33.117-30.354 50.08-5.303 9.283-9.654 19.11-14.434 28.692z\"></path><path fill=\"#ea5227\" d=\"M1060.11 1122.049c-7.377 1.649-14.683 4.093-22.147 4.763-11.519 1.033-23.166 1.441-34.723 1.054-19.343-.647-38.002-4.7-55.839-12.65-15.078-6.72-28.606-15.471-40.571-26.836-24.013-22.81-42.053-49.217-49.518-81.936-1.446-6.337-1.958-12.958-2.235-19.477-.591-13.926-.219-27.909-1.237-41.795-.916-12.5-3.16-24.904-4.408-37.805 1.555-1.381 3.134-2.074 3.778-3.27 4.729-8.79 12.141-15.159 19.083-22.03 5.879-5.818 10.688-12.76 16.796-18.293 6.993-6.335 11.86-13.596 14.364-22.612l8.542-29.993c8.015 1.785 15.984 3.821 24.057 5.286 8.145 1.478 16.371 2.59 24.602 3.493 8.453.927 16.956 1.408 25.891 2.609 1.119 16.09 1.569 31.667 2.521 47.214.676 11.045 1.396 22.154 3.234 33.043 2.418 14.329 5.708 28.527 9.075 42.674 3.499 14.705 4.028 29.929 10.415 44.188 10.157 22.674 18.29 46.25 28.281 69.004 7.175 16.341 12.491 32.973 15.078 50.615.645 4.4 3.256 8.511 4.963 12.755z\"></path><path fill=\"#ea5330\" d=\"M1060.512 1122.031c-2.109-4.226-4.72-8.337-5.365-12.737-2.587-17.642-7.904-34.274-15.078-50.615-9.991-22.755-18.124-46.33-28.281-69.004-6.387-14.259-6.916-29.482-10.415-44.188-3.366-14.147-6.656-28.346-9.075-42.674-1.838-10.889-2.558-21.999-3.234-33.043-.951-15.547-1.401-31.124-2.068-47.146 8.568-.18 17.146.487 25.704.286l41.868-1.4c.907 3.746 1.245 7.04 1.881 10.276l8.651 42.704c.903 4.108 2.334 8.422 4.696 11.829 7.165 10.338 14.809 20.351 22.456 30.345 4.218 5.512 8.291 11.304 13.361 15.955 8.641 7.927 18.065 14.995 27.071 22.532 12.011 10.052 24.452 19.302 40.151 22.854-1.656 11.102-2.391 22.44-5.172 33.253-4.792 18.637-12.38 36.209-23.412 52.216-13.053 18.94-29.086 34.662-49.627 45.055-10.757 5.443-22.443 9.048-34.111 13.501z\"></path><path fill=\"#f8aa05\" d=\"M1989.106 883.951c5.198 8.794 11.46 17.148 15.337 26.491 5.325 12.833 9.744 26.207 12.873 39.737 2.95 12.757 3.224 25.908 1.987 39.219-1.391 14.973-4.643 29.268-10.349 43.034-5.775 13.932-13.477 26.707-23.149 38.405-14.141 17.104-31.215 30.458-50.807 40.488-14.361 7.352-29.574 12.797-45.741 14.594-10.297 1.144-20.732 2.361-31.031 1.894-24.275-1.1-47.248-7.445-68.132-20.263-6.096-3.741-11.925-7.917-17.731-12.342 5.319-5.579 10.361-10.852 15.694-15.811l37.072-34.009c.975-.892 2.113-1.606 3.08-2.505 6.936-6.448 14.765-12.2 20.553-19.556 8.88-11.285 20.064-19.639 31.144-28.292 4.306-3.363 9.06-6.353 12.673-10.358 5.868-6.504 10.832-13.814 16.422-20.582 6.826-8.264 13.727-16.481 20.943-24.401 4.065-4.461 8.995-8.121 13.249-12.424 14.802-14.975 28.77-30.825 45.913-43.317z\"></path><path fill=\"#ed6876\" d=\"M1256.099 523.419c5.065.642 10.047 1.787 15.068 2.728 7.267 1.362 14.526 2.854 21.853 3.777 7.696.97 15.468 1.34 23.207 1.961 5.062.406 7.031 3.631 7.777 8.207 1.163 7.135 4.059 9.62 11.3 9.899l27.843.953c4.069.069 5.745 1.291 6.005 5.774.522 9.016 1.829 17.992 3.001 26.96.392 3 2.246 4.417 5.566 4.4 5.239-.026 10.48.469 16.385.778l-1.102 10.495-1.875 18.726c-.44 3.74.088 6.105 4.751 6.012 4.11-.082 8.239.568 12.352.975 2.823.28 3.984 1.892 3.794 4.753-.428 6.444-.052 13.028-1.276 19.306-.986 5.059.651 6.724 5.065 7.31 1.793.238 3.566.634 6.12 1.098 0 4.189.243 8.545-.068 12.861-.319 4.43-1.27 8.811-1.77 13.233-.285 2.528-.461 5.379 3.332 5.331 13.729-.173 11.316 2.968 12.332 14.419.658 7.423-1.066 15.045-1.554 22.589-.145 2.241.28 4.519.519 7.827 2.026-1.452 2.733-2.167 3.589-2.521 1.142-.472 2.713-1.244 3.518-.81.767.414 1.114 2.221 1.081 3.4l-.917 24.539c-11.215.82-22.45.899-33.636 1.674l-43.952 3.436c-1.086-3.01-2.319-5.571-2.296-8.121.084-9.297-4.468-16.583-9.091-24.116-3.872-6.308-8.764-13.052-9.479-19.987-1.071-10.392-5.716-15.936-14.889-18.979-1.097-.364-2.16-.844-3.214-1.327-7.478-3.428-15.548-5.918-19.059-14.735-.904-2.27-3.657-3.775-5.461-5.723-2.437-2.632-4.615-5.525-7.207-7.987-2.648-2.515-5.352-5.346-8.589-6.777-4.799-2.121-10.074-3.185-15.175-4.596l-15.785-4.155c.274-12.896 1.722-25.901.54-38.662-1.647-17.783-3.457-35.526-2.554-53.352.528-10.426 2.539-20.777 3.948-31.574z\"></path><path fill=\"#f6a200\" d=\"M525.146 311.436c4.597-9.898 8.947-19.725 14.251-29.008 9.691-16.963 18.49-34.73 30.354-50.08 13.309-17.218 24.195-36.434 41.054-51.019 9.93-8.591 18.335-18.989 27.131-28.837 13.774-15.422 30.943-26.386 48.036-37.528 11.459-7.469 22.688-15.29 34.243-23.286 11.705 16.744 19.716 35.424 22.534 55.717 2.231 16.066 2.236 32.441 2.753 49.143-4.756 1.62-9.284 2.234-13.259 4.056-6.43 2.948-12.193 7.513-18.774 9.942-19.863 7.331-33.806 22.349-47.926 36.784-7.86 8.035-13.511 18.275-19.886 27.705-4.434 6.558-9.345 13.037-12.358 20.254-4.249 10.177-6.94 21.004-10.296 31.553-12.33.053-24.741 1.027-36.971-.049-20.259-1.783-40.227-5.567-58.755-14.69-.568-.28-1.295-.235-2.132-.658z\"></path><path fill=\"#f7a80d\" d=\"M1989.057 883.598c-17.093 12.845-31.061 28.695-45.863 43.67-4.254 4.304-9.184 7.963-13.249 12.424-7.216 7.92-14.117 16.137-20.943 24.401-5.59 6.768-10.554 14.078-16.422 20.582-3.614 4.005-8.367 6.995-12.673 10.358-11.08 8.653-22.264 17.007-31.144 28.292-5.788 7.356-13.617 13.108-20.553 19.556-.967.899-2.105 1.614-3.08 2.505l-37.072 34.009c-5.333 4.96-10.375 10.232-15.859 15.505-21.401-17.218-37.461-38.439-48.623-63.592 3.503-1.781 7.117-2.604 9.823-4.637 8.696-6.536 20.392-8.406 27.297-17.714.933-1.258 2.646-1.973 4.065-2.828 17.878-10.784 36.338-20.728 53.441-32.624 10.304-7.167 18.637-17.23 27.583-26.261 3.819-3.855 7.436-8.091 10.3-12.681 12.283-19.68 24.43-39.446 40.382-56.471 12.224-13.047 17.258-29.524 22.539-45.927 15.85 4.193 29.819 12.129 42.632 22.08 10.583 8.219 19.782 17.883 27.42 29.351z\"></path><path fill=\"#ef7a72\" d=\"M1479.461 758.907c1.872-13.734 4.268-27.394 6.525-41.076 2.218-13.45 8.26-24.279 19.615-32.423 5.099-3.657 9.667-8.701 12.999-14.038 6.646-10.643 15.183-19.135 25.083-26.631 5.888-4.459 11.817-9.176 16.547-14.774 5.906-6.99 13.974-9.306 21.753-12.365l51.48-19.549c.753 11.848.658 23.787 1.641 35.637 1.771 21.353 4.075 42.672 11.748 62.955.17.449.107.985-.019 2.158-6.945 4.134-13.865 7.337-20.437 11.143-3.935 2.279-7.752 5.096-10.869 8.384-6.011 6.343-11.063 13.624-17.286 19.727-9.096 8.92-12.791 20.684-18.181 31.587-.202.409-.072.984-.096 1.481-8.488-1.72-16.937-3.682-25.476-5.094-9.689-1.602-19.426-3.084-29.201-3.949-15.095-1.335-30.241-2.1-45.828-3.172z\"></path><path fill=\"#e94e3b\" d=\"M957.995 766.838c-20.337-5.467-38.791-14.947-55.703-27.254-8.2-5.967-15.451-13.238-22.958-20.37 2.969-3.504 5.564-6.772 8.598-9.563 7.085-6.518 11.283-14.914 15.8-23.153 4.933-8.996 10.345-17.743 14.966-26.892 2.642-5.231 5.547-11.01 5.691-16.611.12-4.651.194-8.932 2.577-12.742 8.52-13.621 15.483-28.026 18.775-43.704 2.11-10.049 7.888-18.774 7.81-29.825-.064-9.089 4.291-18.215 6.73-27.313 3.212-11.983 7.369-23.797 9.492-35.968 3.202-18.358 5.133-36.945 7.346-55.466l4.879-45.8c6.693.288 13.386.575 20.54 1.365.13 3.458-.41 6.407-.496 9.37l-1.136 42.595c-.597 11.552-2.067 23.058-3.084 34.59l-3.845 44.478c-.939 10.202-1.779 20.432-3.283 30.557-.96 6.464-4.46 12.646-1.136 19.383.348.706-.426 1.894-.448 2.864-.224 9.918-5.99 19.428-2.196 29.646.103.279-.033.657-.092.983l-8.446 46.205c-1.231 6.469-2.936 12.846-4.364 19.279-1.5 6.757-2.602 13.621-4.456 20.277-3.601 12.93-10.657 25.3-5.627 39.47.368 1.036.234 2.352.017 3.476l-5.949 30.123z\"></path><path fill=\"#ea5043\" d=\"M958.343 767.017c1.645-10.218 3.659-20.253 5.602-30.302.217-1.124.351-2.44-.017-3.476-5.03-14.17 2.026-26.539 5.627-39.47 1.854-6.656 2.956-13.52 4.456-20.277 1.428-6.433 3.133-12.81 4.364-19.279l8.446-46.205c.059-.326.196-.705.092-.983-3.794-10.218 1.972-19.728 2.196-29.646.022-.97.796-2.158.448-2.864-3.324-6.737.176-12.919 1.136-19.383 1.504-10.125 2.344-20.355 3.283-30.557l3.845-44.478c1.017-11.532 2.488-23.038 3.084-34.59.733-14.18.722-28.397 1.136-42.595.086-2.963.626-5.912.956-9.301 5.356-.48 10.714-.527 16.536-.081 2.224 15.098 1.855 29.734 1.625 44.408-.157 10.064 1.439 20.142 1.768 30.23.334 10.235-.035 20.49.116 30.733.084 5.713.789 11.418.861 17.13.054 4.289-.469 8.585-.702 12.879-.072 1.323-.138 2.659-.031 3.975l2.534 34.405-1.707 36.293-1.908 48.69c-.182 8.103.993 16.237.811 24.34-.271 12.076-1.275 24.133-1.787 36.207-.102 2.414-.101 5.283 1.06 7.219 4.327 7.22 4.463 15.215 4.736 23.103.365 10.553.088 21.128.086 31.693-11.44 2.602-22.84.688-34.106-.916-11.486-1.635-22.806-4.434-34.546-6.903z\"></path><path fill=\"#eb5d19\" d=\"M398.091 622.45c6.086.617 12.21 1.288 18.067 2.918 3.539.985 6.779 3.277 9.952 5.297 9.773 6.224 18.971 13.583 29.311 18.611 8.606 4.184 12.839 10.986 17.016 18.559l18.571 32.959c1.814 3.102 4.285 5.931 6.883 8.443 8.835 8.542 10.052 20.175 13.16 31.095 2.082 7.317 4.609 14.507 6.946 22.127-29.472 3.021-58.969 5.582-87.584 15.222-1.185-2.302-1.795-4.362-2.769-6.233-4.398-8.449-6.703-18.174-14.942-24.299-2.511-1.866-5.103-3.814-7.047-6.218-8.358-10.332-17.028-20.276-28.772-26.973 4.423-11.478 9.299-22.806 13.151-34.473 4.406-13.348 6.724-27.18 6.998-41.313.098-5.093.643-10.176 1.06-15.722z\"></path><path fill=\"#e94c32\" d=\"M981.557 392.109c-1.172 15.337-2.617 30.625-4.438 45.869-2.213 18.521-4.144 37.108-7.346 55.466-2.123 12.171-6.28 23.985-9.492 35.968-2.439 9.098-6.794 18.224-6.73 27.313.078 11.051-5.7 19.776-7.81 29.825-3.292 15.677-10.255 30.082-18.775 43.704-2.383 3.81-2.458 8.091-2.577 12.742-.144 5.6-3.049 11.38-5.691 16.611-4.621 9.149-10.033 17.896-14.966 26.892-4.517 8.239-8.715 16.635-15.8 23.153-3.034 2.791-5.629 6.06-8.735 9.255-12.197-10.595-21.071-23.644-29.301-37.24-7.608-12.569-13.282-25.962-17.637-40.37 13.303-6.889 25.873-13.878 35.311-25.315.717-.869 1.934-1.312 2.71-2.147 5.025-5.405 10.515-10.481 14.854-16.397 6.141-8.374 10.861-17.813 17.206-26.008 8.22-10.618 13.657-22.643 20.024-34.466 4.448-.626 6.729-3.21 8.114-6.89 1.455-3.866 2.644-7.895 4.609-11.492 4.397-8.05 9.641-15.659 13.708-23.86 3.354-6.761 5.511-14.116 8.203-21.206 5.727-15.082 7.277-31.248 12.521-46.578 3.704-10.828 3.138-23.116 4.478-34.753l7.56-.073z\"></path><path fill=\"#f7a617\" d=\"M1918.661 831.99c-4.937 16.58-9.971 33.057-22.196 46.104-15.952 17.025-28.099 36.791-40.382 56.471-2.864 4.59-6.481 8.825-10.3 12.681-8.947 9.031-17.279 19.094-27.583 26.261-17.103 11.896-35.564 21.84-53.441 32.624-1.419.856-3.132 1.571-4.065 2.828-6.904 9.308-18.6 11.178-27.297 17.714-2.705 2.033-6.319 2.856-9.874 4.281-3.413-9.821-6.916-19.583-9.36-29.602-1.533-6.284-1.474-12.957-1.665-19.913 1.913-.78 3.374-1.057 4.81-1.431 15.822-4.121 31.491-8.029 43.818-20.323 9.452-9.426 20.371-17.372 30.534-26.097 6.146-5.277 13.024-10.052 17.954-16.326 14.812-18.848 28.876-38.285 43.112-57.581 2.624-3.557 5.506-7.264 6.83-11.367 2.681-8.311 4.375-16.94 6.476-25.438 17.89.279 35.333 3.179 52.629 9.113z\"></path><path fill=\"#ea553a\" d=\"M1172.91 977.582c-15.775-3.127-28.215-12.377-40.227-22.43-9.005-7.537-18.43-14.605-27.071-22.532-5.07-4.651-9.143-10.443-13.361-15.955-7.647-9.994-15.291-20.007-22.456-30.345-2.361-3.407-3.792-7.72-4.696-11.829-3.119-14.183-5.848-28.453-8.651-42.704-.636-3.236-.974-6.53-1.452-10.209 15.234-2.19 30.471-3.969 46.408-5.622 2.692 5.705 4.882 11.222 6.63 16.876 2.9 9.381 7.776 17.194 15.035 24.049 7.056 6.662 13.305 14.311 19.146 22.099 9.509 12.677 23.01 19.061 36.907 25.054-1.048 7.441-2.425 14.854-3.066 22.33-.956 11.162-1.393 22.369-2.052 33.557l-1.096 17.661z\"></path><path fill=\"#ea5453\" d=\"M1163.123 704.036c-4.005 5.116-7.685 10.531-12.075 15.293-12.842 13.933-27.653 25.447-44.902 34.538-3.166-5.708-5.656-11.287-8.189-17.251-3.321-12.857-6.259-25.431-9.963-37.775-4.6-15.329-10.6-30.188-11.349-46.562-.314-6.871-1.275-14.287-7.114-19.644-1.047-.961-1.292-3.053-1.465-4.67l-4.092-39.927c-.554-5.245-.383-10.829-2.21-15.623-3.622-9.503-4.546-19.253-4.688-29.163-.088-6.111 1.068-12.256.782-18.344-.67-14.281-1.76-28.546-2.9-42.8-.657-8.222-1.951-16.395-2.564-24.62-.458-6.137-.285-12.322-.104-18.21.959 5.831 1.076 11.525 2.429 16.909 2.007 7.986 5.225 15.664 7.324 23.632 3.222 12.23 1.547 25.219 6.728 37.355 4.311 10.099 6.389 21.136 9.732 31.669 2.228 7.02 6.167 13.722 7.121 20.863 1.119 8.376 6.1 13.974 10.376 20.716l2.026 10.576c1.711 9.216 3.149 18.283 8.494 26.599 6.393 9.946 11.348 20.815 16.943 31.276 4.021 7.519 6.199 16.075 12.925 22.065l24.462 22.26c.556.503 1.507.571 2.274.841z\"></path><path fill=\"#ea5b15\" d=\"M1285.092 163.432c9.165 3.148 18.419 6.374 27.279 10.459 4.871 2.246 8.838 6.406 13.646 8.851 5.446 2.77 11.801 3.874 17.011 6.965 11.514 6.831 24.097 9.942 36.968 12.471 1.78.35 3.777.576 5.213 1.542 10.784 7.255 23.448 9.114 35.622 11.834 9.977 2.23 18.529 6.703 26.988 11.898 5.233 3.214 10.76 5.983 15.798 9.468 4.14 2.864 7.962 6.279 11.551 9.827 5.076 5.02 10.056 10.181 14.624 15.658 5.822 6.98 11.119 14.395 16.78 21.513 4.531 5.698 9.267 11.233 14.222 16.987-10.005 5.806-20.07 12.004-30.719 16.943-7.694 3.569-16.163 5.464-24.688 7.669-2.878-7.088-5.352-13.741-7.833-20.392-.802-2.15-1.244-4.55-2.498-6.396-4.548-6.7-9.712-12.999-14.011-19.847-6.672-10.627-15.34-18.93-26.063-25.376-9.357-5.625-18.367-11.824-27.644-17.587-6.436-3.997-12.902-8.006-19.659-11.405-5.123-2.577-11.107-3.536-16.046-6.37-17.187-9.863-35.13-17.887-54.031-23.767-4.403-1.37-8.953-2.267-13.436-3.382l.926-27.565z\"></path><path fill=\"#ea504b\" d=\"M1098 737l7.789 16.893c-15.04 9.272-31.679 15.004-49.184 17.995-9.464 1.617-19.122 2.097-29.151 3.019-.457-10.636-.18-21.211-.544-31.764-.273-7.888-.409-15.883-4.736-23.103-1.16-1.936-1.162-4.805-1.06-7.219l1.787-36.207c.182-8.103-.993-16.237-.811-24.34.365-16.236 1.253-32.461 1.908-48.69.484-12 .942-24.001 1.98-36.069 5.57 10.19 10.632 20.42 15.528 30.728 1.122 2.362 2.587 5.09 2.339 7.488-1.536 14.819 5.881 26.839 12.962 38.33 10.008 16.241 16.417 33.54 20.331 51.964 2.285 10.756 4.729 21.394 11.958 30.165L1098 737z\"></path><path fill=\"#f6a320\" d=\"M1865.78 822.529c-1.849 8.846-3.544 17.475-6.224 25.786-1.323 4.102-4.206 7.81-6.83 11.367l-43.112 57.581c-4.93 6.273-11.808 11.049-17.954 16.326-10.162 8.725-21.082 16.671-30.534 26.097-12.327 12.294-27.997 16.202-43.818 20.323-1.436.374-2.897.651-4.744.986-1.107-17.032-1.816-34.076-2.079-51.556 1.265-.535 2.183-.428 2.888-.766 10.596-5.072 20.8-11.059 32.586-13.273 1.69-.317 3.307-1.558 4.732-2.662l26.908-21.114c4.992-4.003 11.214-7.393 14.381-12.585 11.286-18.5 22.363-37.263 27.027-58.87l36.046 1.811c3.487.165 6.983.14 10.727.549z\"></path><path fill=\"#ec6333\" d=\"M318.448 922.814c-6.374-2.074-12.56-4.058-18.412-6.765-8.379-3.876-16.906-7.675-24.617-12.668-5.239-3.392-9.69-8.381-13.609-13.352-7.87-9.983-14.953-20.582-22.699-30.666-8.061-10.493-13.909-22.097-18.636-34.358-.595-1.543-1.486-2.972-2.382-4.783 6.84-1.598 13.797-3.023 20.807-4.106 18.852-2.912 36.433-9.493 53.737-17.819.697.888.889 1.555 1.292 2.051l17.921 21.896c4.14 4.939 8.06 10.191 12.862 14.412 5.67 4.984 12.185 9.007 18.334 13.447-8.937 16.282-16.422 33.178-20.696 51.31-1.638 6.951-2.402 14.107-3.903 21.403z\"></path><path fill=\"#f49700\" d=\"M623.467 326.903c2.893-10.618 5.584-21.446 9.833-31.623 3.013-7.217 7.924-13.696 12.358-20.254 6.375-9.43 12.026-19.67 19.886-27.705 14.12-14.434 28.063-29.453 47.926-36.784 6.581-2.429 12.344-6.994 18.774-9.942 3.975-1.822 8.503-2.436 13.186-3.592 1.947 18.557 3.248 37.15 8.307 55.686-15.453 7.931-28.853 18.092-40.46 29.996-10.417 10.683-19.109 23.111-28.013 35.175-3.238 4.388-4.888 9.948-7.262 14.973-17.803-3.987-35.767-6.498-54.535-5.931z\"></path><path fill=\"#ea544c\" d=\"M1097.956 736.615c-2.925-3.218-5.893-6.822-8.862-10.425-7.229-8.771-9.672-19.409-11.958-30.165-3.914-18.424-10.323-35.722-20.331-51.964-7.081-11.491-14.498-23.511-12.962-38.33.249-2.398-1.217-5.126-2.339-7.488l-15.232-31.019-3.103-34.338c-.107-1.316-.041-2.653.031-3.975.233-4.294.756-8.59.702-12.879-.072-5.713-.776-11.417-.861-17.13l-.116-30.733c-.329-10.088-1.926-20.166-1.768-30.23.23-14.674.599-29.31-1.162-44.341 9.369-.803 18.741-1.179 28.558-1.074 1.446 15.814 2.446 31.146 3.446 46.478.108 6.163-.064 12.348.393 18.485.613 8.225 1.907 16.397 2.564 24.62l2.9 42.8c.286 6.088-.869 12.234-.782 18.344.142 9.91 1.066 19.661 4.688 29.163 1.827 4.794 1.657 10.377 2.21 15.623l4.092 39.927c.172 1.617.417 3.71 1.465 4.67 5.839 5.357 6.8 12.773 7.114 19.644.749 16.374 6.749 31.233 11.349 46.562 3.704 12.344 6.642 24.918 9.963 37.775z\"></path><path fill=\"#ec5c61\" d=\"M1204.835 568.008c1.254 25.351-1.675 50.16-10.168 74.61-8.598-4.883-18.177-8.709-24.354-15.59-7.44-8.289-13.929-17.442-21.675-25.711-8.498-9.072-16.731-18.928-21.084-31.113-.54-1.513-1.691-2.807-2.594-4.564-4.605-9.247-7.706-18.544-7.96-29.09-.835-7.149-1.214-13.944-2.609-20.523-2.215-10.454-5.626-20.496-7.101-31.302-2.513-18.419-7.207-36.512-5.347-55.352.24-2.43-.17-4.949-.477-7.402l-4.468-34.792c2.723-.379 5.446-.757 8.585-.667 1.749 8.781 2.952 17.116 4.448 25.399 1.813 10.037 3.64 20.084 5.934 30.017 1.036 4.482 3.953 8.573 4.73 13.064 1.794 10.377 4.73 20.253 9.272 29.771 2.914 6.105 4.761 12.711 7.496 18.912 2.865 6.496 6.264 12.755 9.35 19.156 3.764 7.805 7.667 15.013 16.1 19.441 7.527 3.952 13.713 10.376 20.983 14.924 6.636 4.152 13.932 7.25 20.937 10.813z\"></path><path fill=\"#ed676f\" d=\"M1140.75 379.231c18.38-4.858 36.222-11.21 53.979-18.971 3.222 3.368 5.693 6.744 8.719 9.512 2.333 2.134 5.451 5.07 8.067 4.923 7.623-.429 12.363 2.688 17.309 8.215 5.531 6.18 12.744 10.854 19.224 16.184-5.121 7.193-10.461 14.241-15.323 21.606-13.691 20.739-22.99 43.255-26.782 67.926-.543 3.536-1.281 7.043-2.366 10.925-14.258-6.419-26.411-14.959-32.731-29.803-1.087-2.553-2.596-4.93-3.969-7.355-1.694-2.993-3.569-5.89-5.143-8.943-1.578-3.062-2.922-6.249-4.295-9.413-1.57-3.621-3.505-7.163-4.47-10.946-1.257-4.93-.636-10.572-2.725-15.013-5.831-12.397-7.467-25.628-9.497-38.847z\"></path><path fill=\"#ed656e\" d=\"M1254.103 647.439c5.325.947 10.603 2.272 15.847 3.722 5.101 1.41 10.376 2.475 15.175 4.596 3.237 1.431 5.942 4.262 8.589 6.777 2.592 2.462 4.77 5.355 7.207 7.987 1.804 1.948 4.557 3.453 5.461 5.723 3.51 8.817 11.581 11.307 19.059 14.735 1.053.483 2.116.963 3.214 1.327 9.172 3.043 13.818 8.587 14.889 18.979.715 6.935 5.607 13.679 9.479 19.987 4.623 7.533 9.175 14.819 9.091 24.116-.023 2.55 1.21 5.111 1.874 8.055-19.861 2.555-39.795 4.296-59.597 9.09l-11.596-23.203c-1.107-2.169-2.526-4.353-4.307-5.975-7.349-6.694-14.863-13.209-22.373-19.723l-17.313-14.669c-2.776-2.245-5.935-4.017-8.92-6.003l11.609-38.185c1.508-5.453 1.739-11.258 2.613-17.336z\"></path><path fill=\"#ec6168\" d=\"M1140.315 379.223c2.464 13.227 4.101 26.459 9.931 38.856 2.089 4.441 1.468 10.083 2.725 15.013.965 3.783 2.9 7.325 4.47 10.946 1.372 3.164 2.716 6.351 4.295 9.413 1.574 3.053 3.449 5.95 5.143 8.943 1.372 2.425 2.882 4.803 3.969 7.355 6.319 14.844 18.473 23.384 32.641 30.212.067 5.121-.501 10.201-.435 15.271l.985 38.117c.151 4.586.616 9.162.868 14.201-7.075-3.104-14.371-6.202-21.007-10.354-7.269-4.548-13.456-10.972-20.983-14.924-8.434-4.428-12.337-11.637-16.1-19.441-3.087-6.401-6.485-12.66-9.35-19.156-2.735-6.201-4.583-12.807-7.496-18.912-4.542-9.518-7.477-19.394-9.272-29.771-.777-4.491-3.694-8.581-4.73-13.064-2.294-9.933-4.121-19.98-5.934-30.017-1.496-8.283-2.699-16.618-4.036-25.335 10.349-2.461 20.704-4.511 31.054-6.582.957-.191 1.887-.515 3.264-.769z\"></path><path fill=\"#e94c28\" d=\"M922 537c-6.003 11.784-11.44 23.81-19.66 34.428-6.345 8.196-11.065 17.635-17.206 26.008-4.339 5.916-9.828 10.992-14.854 16.397-.776.835-1.993 1.279-2.71 2.147-9.439 11.437-22.008 18.427-35.357 24.929-4.219-10.885-6.942-22.155-7.205-33.905l-.514-49.542c7.441-2.893 14.452-5.197 21.334-7.841 1.749-.672 3.101-2.401 4.604-3.681 6.749-5.745 12.845-12.627 20.407-16.944 7.719-4.406 14.391-9.101 18.741-16.889.626-1.122 1.689-2.077 2.729-2.877 7.197-5.533 12.583-12.51 16.906-20.439.68-1.247 2.495-1.876 4.105-2.651 2.835 1.408 5.267 2.892 7.884 3.892 3.904 1.491 4.392 3.922 2.833 7.439-1.47 3.318-2.668 6.756-4.069 10.106-1.247 2.981-.435 5.242 2.413 6.544 2.805 1.282 3.125 3.14 1.813 5.601l-6.907 12.799L922 537z\"></path><path fill=\"#eb5659\" d=\"M1124.995 566c.868 1.396 2.018 2.691 2.559 4.203 4.353 12.185 12.586 22.041 21.084 31.113 7.746 8.269 14.235 17.422 21.675 25.711 6.176 6.881 15.756 10.707 24.174 15.932-6.073 22.316-16.675 42.446-31.058 60.937-1.074-.131-2.025-.199-2.581-.702l-24.462-22.26c-6.726-5.99-8.904-14.546-12.925-22.065-5.594-10.461-10.55-21.33-16.943-31.276-5.345-8.315-6.783-17.383-8.494-26.599-.63-3.394-1.348-6.772-1.738-10.848-.371-6.313-1.029-11.934-1.745-18.052l6.34 4.04 1.288-.675-2.143-15.385 9.454 1.208v-8.545L1124.995 566z\"></path><path fill=\"#f5a02d\" d=\"M1818.568 820.096c-4.224 21.679-15.302 40.442-26.587 58.942-3.167 5.192-9.389 8.582-14.381 12.585l-26.908 21.114c-1.425 1.104-3.042 2.345-4.732 2.662-11.786 2.214-21.99 8.201-32.586 13.273-.705.338-1.624.231-2.824.334a824.35 824.35 0 0 1-8.262-42.708c4.646-2.14 9.353-3.139 13.269-5.47 5.582-3.323 11.318-6.942 15.671-11.652 7.949-8.6 14.423-18.572 22.456-27.081 8.539-9.046 13.867-19.641 18.325-30.922l46.559 8.922z\"></path><path fill=\"#eb5a57\" d=\"M1124.96 565.639c-5.086-4.017-10.208-8.395-15.478-12.901v8.545l-9.454-1.208 2.143 15.385-1.288.675-6.34-4.04c.716 6.118 1.375 11.74 1.745 17.633-4.564-6.051-9.544-11.649-10.663-20.025-.954-7.141-4.892-13.843-7.121-20.863-3.344-10.533-5.421-21.57-9.732-31.669-5.181-12.135-3.506-25.125-6.728-37.355-2.099-7.968-5.317-15.646-7.324-23.632-1.353-5.384-1.47-11.078-2.429-16.909l-3.294-46.689a278.63 278.63 0 0 1 27.57-2.084c2.114 12.378 3.647 24.309 5.479 36.195 1.25 8.111 2.832 16.175 4.422 24.23 1.402 7.103 2.991 14.169 4.55 21.241 1.478 6.706.273 14.002 4.6 20.088 5.401 7.597 7.176 16.518 9.467 25.337 1.953 7.515 5.804 14.253 11.917 19.406.254 10.095 3.355 19.392 7.96 28.639z\"></path><path fill=\"#ea541c\" d=\"M911.651 810.999c-2.511 10.165-5.419 20.146-8.2 30.162-2.503 9.015-7.37 16.277-14.364 22.612-6.108 5.533-10.917 12.475-16.796 18.293-6.942 6.871-14.354 13.24-19.083 22.03-.644 1.196-2.222 1.889-3.705 2.857-2.39-7.921-4.101-15.991-6.566-23.823-5.451-17.323-12.404-33.976-23.414-48.835l21.627-21.095c3.182-3.29 5.532-7.382 8.295-11.083l10.663-14.163c9.528 4.78 18.925 9.848 28.625 14.247 7.324 3.321 15.036 5.785 22.917 8.799z\"></path><path fill=\"#eb5d19\" d=\"M1284.092 191.421c4.557.69 9.107 1.587 13.51 2.957 18.901 5.881 36.844 13.904 54.031 23.767 4.938 2.834 10.923 3.792 16.046 6.37 6.757 3.399 13.224 7.408 19.659 11.405l27.644 17.587c10.723 6.446 19.392 14.748 26.063 25.376 4.299 6.848 9.463 13.147 14.011 19.847 1.254 1.847 1.696 4.246 2.498 6.396l7.441 20.332c-11.685 1.754-23.379 3.133-35.533 4.037-.737-2.093-.995-3.716-1.294-5.33-3.157-17.057-14.048-30.161-23.034-44.146-3.027-4.71-7.786-8.529-12.334-11.993-9.346-7.116-19.004-13.834-28.688-20.491-6.653-4.573-13.311-9.251-20.431-13.002-8.048-4.24-16.479-7.85-24.989-11.091-11.722-4.465-23.673-8.328-35.527-12.449l.927-19.572z\"></path><path fill=\"#eb5e24\" d=\"M1283.09 211.415c11.928 3.699 23.88 7.562 35.602 12.027 8.509 3.241 16.941 6.852 24.989 11.091 7.12 3.751 13.778 8.429 20.431 13.002 9.684 6.657 19.342 13.375 28.688 20.491 4.548 3.463 9.307 7.283 12.334 11.993 8.986 13.985 19.877 27.089 23.034 44.146.299 1.615.557 3.237.836 5.263-13.373-.216-26.749-.839-40.564-1.923-2.935-9.681-4.597-18.92-12.286-26.152-15.577-14.651-30.4-30.102-45.564-45.193-.686-.683-1.626-1.156-2.516-1.584l-47.187-22.615 2.203-20.546z\"></path><path fill=\"#e9511f\" d=\"M913 486.001c-1.29.915-3.105 1.543-3.785 2.791-4.323 7.929-9.709 14.906-16.906 20.439-1.04.8-2.103 1.755-2.729 2.877-4.35 7.788-11.022 12.482-18.741 16.889-7.562 4.317-13.658 11.199-20.407 16.944-1.503 1.28-2.856 3.009-4.604 3.681-6.881 2.643-13.893 4.948-21.262 7.377-.128-11.151.202-22.302.378-33.454.03-1.892-.6-3.795-.456-6.12 13.727-1.755 23.588-9.527 33.278-17.663 2.784-2.337 6.074-4.161 8.529-6.784l29.057-31.86c1.545-1.71 3.418-3.401 4.221-5.459 5.665-14.509 11.49-28.977 16.436-43.736 2.817-8.407 4.074-17.338 6.033-26.032 5.039.714 10.078 1.427 15.536 2.629-.909 8.969-2.31 17.438-3.546 25.931-2.41 16.551-5.84 32.839-11.991 48.461L913 486.001z\"></path><path fill=\"#ea5741\" d=\"M1179.451 903.828c-14.224-5.787-27.726-12.171-37.235-24.849-5.841-7.787-12.09-15.436-19.146-22.099-7.259-6.854-12.136-14.667-15.035-24.049-1.748-5.654-3.938-11.171-6.254-17.033 15.099-4.009 30.213-8.629 44.958-15.533l28.367 36.36c6.09 8.015 13.124 14.75 22.72 18.375-7.404 14.472-13.599 29.412-17.48 45.244-.271 1.106-.382 2.25-.895 3.583z\"></path><path fill=\"#ea522a\" d=\"M913.32 486.141c2.693-7.837 5.694-15.539 8.722-23.231 6.151-15.622 9.581-31.91 11.991-48.461l3.963-25.861c7.582.317 15.168 1.031 22.748 1.797 4.171.421 8.333.928 12.877 1.596-.963 11.836-.398 24.125-4.102 34.953-5.244 15.33-6.794 31.496-12.521 46.578-2.692 7.09-4.849 14.445-8.203 21.206-4.068 8.201-9.311 15.81-13.708 23.86-1.965 3.597-3.154 7.627-4.609 11.492-1.385 3.68-3.666 6.265-8.114 6.89-1.994-1.511-3.624-3.059-5.077-4.44l6.907-12.799c1.313-2.461.993-4.318-1.813-5.601-2.849-1.302-3.66-3.563-2.413-6.544 1.401-3.35 2.599-6.788 4.069-10.106 1.558-3.517 1.071-5.948-2.833-7.439-2.617-1-5.049-2.484-7.884-3.892z\"></path><path fill=\"#eb5e24\" d=\"M376.574 714.118c12.053 6.538 20.723 16.481 29.081 26.814 1.945 2.404 4.537 4.352 7.047 6.218 8.24 6.125 10.544 15.85 14.942 24.299.974 1.871 1.584 3.931 2.376 6.29-7.145 3.719-14.633 6.501-21.386 10.517-9.606 5.713-18.673 12.334-28.425 18.399-3.407-3.73-6.231-7.409-9.335-10.834l-30.989-33.862c11.858-11.593 22.368-24.28 31.055-38.431 1.86-3.031 3.553-6.164 5.632-9.409z\"></path><path fill=\"#e95514\" d=\"M859.962 787.636c-3.409 5.037-6.981 9.745-10.516 14.481-2.763 3.701-5.113 7.792-8.295 11.083-6.885 7.118-14.186 13.834-21.65 20.755-13.222-17.677-29.417-31.711-48.178-42.878-.969-.576-2.068-.934-3.27-1.709 6.28-8.159 12.733-15.993 19.16-23.849 1.459-1.783 2.718-3.738 4.254-5.448l18.336-19.969c4.909 5.34 9.619 10.738 14.081 16.333 9.72 12.19 21.813 21.566 34.847 29.867.411.262.725.674 1.231 1.334z\"></path><path fill=\"#eb5f2d\" d=\"M339.582 762.088l31.293 33.733c3.104 3.425 5.928 7.104 9.024 10.979-12.885 11.619-24.548 24.139-33.899 38.704-.872 1.359-1.56 2.837-2.644 4.428-6.459-4.271-12.974-8.294-18.644-13.278-4.802-4.221-8.722-9.473-12.862-14.412l-17.921-21.896c-.403-.496-.595-1.163-.926-2.105 16.738-10.504 32.58-21.87 46.578-36.154z\"></path><path fill=\"#f28d00\" d=\"M678.388 332.912c1.989-5.104 3.638-10.664 6.876-15.051 8.903-12.064 17.596-24.492 28.013-35.175 11.607-11.904 25.007-22.064 40.507-29.592 4.873 11.636 9.419 23.412 13.67 35.592-5.759 4.084-11.517 7.403-16.594 11.553-4.413 3.607-8.124 8.092-12.023 12.301-5.346 5.772-10.82 11.454-15.782 17.547-3.929 4.824-7.17 10.208-10.716 15.344l-33.95-12.518z\"></path><path fill=\"#f08369\" d=\"M1580.181 771.427c-.191-.803-.322-1.377-.119-1.786 5.389-10.903 9.084-22.666 18.181-31.587 6.223-6.103 11.276-13.385 17.286-19.727 3.117-3.289 6.933-6.105 10.869-8.384 6.572-3.806 13.492-7.009 20.461-10.752 1.773 3.23 3.236 6.803 4.951 10.251l12.234 24.993c-1.367 1.966-2.596 3.293-3.935 4.499-7.845 7.07-16.315 13.564-23.407 21.32-6.971 7.623-12.552 16.517-18.743 24.854l-37.777-13.68z\"></path><path fill=\"#f18b5e\" d=\"M1618.142 785.4c6.007-8.63 11.588-17.524 18.559-25.147 7.092-7.755 15.562-14.249 23.407-21.32 1.338-1.206 2.568-2.534 3.997-4.162l28.996 33.733c1.896 2.205 4.424 3.867 6.66 6.394-6.471 7.492-12.967 14.346-19.403 21.255l-18.407 19.953c-12.958-12.409-27.485-22.567-43.809-30.706z\"></path><path fill=\"#f49c3a\" d=\"M1771.617 811.1c-4.066 11.354-9.394 21.949-17.933 30.995-8.032 8.509-14.507 18.481-22.456 27.081-4.353 4.71-10.089 8.329-15.671 11.652-3.915 2.331-8.623 3.331-13.318 5.069-4.298-9.927-8.255-19.998-12.1-30.743 4.741-4.381 9.924-7.582 13.882-11.904 7.345-8.021 14.094-16.603 20.864-25.131 4.897-6.168 9.428-12.626 14.123-18.955l32.61 11.936z\"></path><path fill=\"#f08000\" d=\"M712.601 345.675c3.283-5.381 6.524-10.765 10.453-15.589 4.962-6.093 10.435-11.774 15.782-17.547 3.899-4.21 7.61-8.695 12.023-12.301 5.078-4.15 10.836-7.469 16.636-11.19a934.12 934.12 0 0 1 23.286 35.848c-4.873 6.234-9.676 11.895-14.63 17.421l-25.195 27.801c-11.713-9.615-24.433-17.645-38.355-24.443z\"></path><path fill=\"#ed6e04\" d=\"M751.11 370.42c8.249-9.565 16.693-18.791 25.041-28.103 4.954-5.526 9.757-11.187 14.765-17.106 7.129 6.226 13.892 13.041 21.189 19.225 5.389 4.567 11.475 8.312 17.53 12.92-5.51 7.863-10.622 15.919-17.254 22.427-8.881 8.716-18.938 16.233-28.49 24.264-5.703-6.587-11.146-13.427-17.193-19.682-4.758-4.921-10.261-9.121-15.587-13.944z\"></path><path fill=\"#ea541c\" d=\"M921.823 385.544c-1.739 9.04-2.995 17.971-5.813 26.378-4.946 14.759-10.771 29.227-16.436 43.736-.804 2.058-2.676 3.749-4.221 5.459l-29.057 31.86c-2.455 2.623-5.745 4.447-8.529 6.784-9.69 8.135-19.551 15.908-33.208 17.237-1.773-9.728-3.147-19.457-4.091-29.6l36.13-16.763c.581-.267 1.046-.812 1.525-1.269 8.033-7.688 16.258-15.19 24.011-23.152 4.35-4.467 9.202-9.144 11.588-14.69 6.638-15.425 15.047-30.299 17.274-47.358 3.536.344 7.072.688 10.829 1.377z\"></path><path fill=\"#f3944d\" d=\"M1738.688 798.998c-4.375 6.495-8.906 12.953-13.803 19.121-6.771 8.528-13.519 17.11-20.864 25.131-3.958 4.322-9.141 7.523-13.925 11.54-8.036-13.464-16.465-26.844-27.999-38.387 5.988-6.951 12.094-13.629 18.261-20.25l19.547-20.95 38.783 23.794z\"></path><path fill=\"#ec6168\" d=\"M1239.583 703.142c3.282 1.805 6.441 3.576 9.217 5.821 5.88 4.755 11.599 9.713 17.313 14.669l22.373 19.723c1.781 1.622 3.2 3.806 4.307 5.975 3.843 7.532 7.477 15.171 11.194 23.136-10.764 4.67-21.532 8.973-32.69 12.982l-22.733-27.366c-2.003-2.416-4.096-4.758-6.194-7.093-3.539-3.94-6.927-8.044-10.74-11.701-2.57-2.465-5.762-4.283-8.675-6.39l16.627-29.755z\"></path><path fill=\"#ec663e\" d=\"M1351.006 332.839l-28.499 10.33c-.294.107-.533.367-1.194.264-11.067-19.018-27.026-32.559-44.225-44.855-4.267-3.051-8.753-5.796-13.138-8.682l9.505-24.505c10.055 4.069 19.821 8.227 29.211 13.108 3.998 2.078 7.299 5.565 10.753 8.598 3.077 2.701 5.743 5.891 8.926 8.447 4.116 3.304 9.787 5.345 12.62 9.432 6.083 8.777 10.778 18.517 16.041 27.863z\"></path><path fill=\"#eb5e5b\" d=\"M1222.647 733.051c3.223 1.954 6.415 3.771 8.985 6.237 3.813 3.658 7.201 7.761 10.74 11.701l6.194 7.093 22.384 27.409c-13.056 6.836-25.309 14.613-36.736 24.161l-39.323-44.7 24.494-27.846c1.072-1.224 1.974-2.598 3.264-4.056z\"></path><path fill=\"#ea580e\" d=\"M876.001 376.171c5.874 1.347 11.748 2.694 17.812 4.789-.81 5.265-2.687 9.791-2.639 14.296.124 11.469-4.458 20.383-12.73 27.863-2.075 1.877-3.659 4.286-5.668 6.248l-22.808 21.967c-.442.422-1.212.488-1.813.757l-23.113 10.389-9.875 4.514c-2.305-6.09-4.609-12.181-6.614-18.676 7.64-4.837 15.567-8.54 22.18-13.873 9.697-7.821 18.931-16.361 27.443-25.455 5.613-5.998 12.679-11.331 14.201-20.475.699-4.2 2.384-8.235 3.623-12.345z\"></path><path fill=\"#e95514\" d=\"M815.103 467.384c3.356-1.894 6.641-3.415 9.94-4.903l23.113-10.389c.6-.269 1.371-.335 1.813-.757l22.808-21.967c2.008-1.962 3.593-4.371 5.668-6.248 8.272-7.48 12.854-16.394 12.73-27.863-.049-4.505 1.828-9.031 2.847-13.956 5.427.559 10.836 1.526 16.609 2.68-1.863 17.245-10.272 32.119-16.91 47.544-2.387 5.546-7.239 10.223-11.588 14.69-7.753 7.962-15.978 15.464-24.011 23.152-.478.458-.944 1.002-1.525 1.269l-36.069 16.355c-2.076-6.402-3.783-12.81-5.425-19.607z\"></path><path fill=\"#eb620b\" d=\"M783.944 404.402c9.499-8.388 19.556-15.905 28.437-24.621 6.631-6.508 11.744-14.564 17.575-22.273 9.271 4.016 18.501 8.375 27.893 13.43-4.134 7.07-8.017 13.778-12.833 19.731-5.785 7.15-12.109 13.917-18.666 20.376-7.99 7.869-16.466 15.244-24.731 22.832l-17.674-29.475z\"></path><path fill=\"#ea544c\" d=\"M1197.986 854.686c-9.756-3.309-16.79-10.044-22.88-18.059l-28.001-36.417c8.601-5.939 17.348-11.563 26.758-17.075 1.615 1.026 2.639 1.876 3.505 2.865l26.664 30.44c3.723 4.139 7.995 7.785 12.017 11.656l-18.064 26.591z\"></path><path fill=\"#ec6333\" d=\"M1351.41 332.903c-5.667-9.409-10.361-19.149-16.445-27.926-2.833-4.087-8.504-6.128-12.62-9.432-3.184-2.555-5.849-5.745-8.926-8.447-3.454-3.033-6.756-6.52-10.753-8.598-9.391-4.88-19.157-9.039-29.138-13.499 1.18-5.441 2.727-10.873 4.81-16.607 11.918 4.674 24.209 8.261 34.464 14.962 14.239 9.304 29.011 18.453 39.595 32.464 2.386 3.159 5.121 6.077 7.884 8.923 6.564 6.764 10.148 14.927 11.723 24.093l-20.594 4.067z\"></path><path fill=\"#eb5e5b\" d=\"M1117 536.549c-6.113-4.702-9.965-11.44-11.917-18.955-2.292-8.819-4.066-17.74-9.467-25.337-4.327-6.085-3.122-13.382-4.6-20.088l-4.55-21.241c-1.59-8.054-3.172-16.118-4.422-24.23l-5.037-36.129c6.382-1.43 12.777-2.462 19.582-3.443 1.906 11.646 3.426 23.24 4.878 34.842.307 2.453.717 4.973.477 7.402-1.86 18.84 2.834 36.934 5.347 55.352 1.474 10.806 4.885 20.848 7.101 31.302 1.394 6.579 1.774 13.374 2.609 20.523z\"></path><path fill=\"#ec644b\" d=\"M1263.638 290.071c4.697 2.713 9.183 5.458 13.45 8.509 17.199 12.295 33.158 25.836 43.873 44.907-8.026 4.725-16.095 9.106-24.83 13.372-11.633-15.937-25.648-28.515-41.888-38.689-1.609-1.008-3.555-1.48-5.344-2.2 2.329-3.852 4.766-7.645 6.959-11.573l7.78-14.326z\"></path><path fill=\"#eb5f2d\" d=\"M1372.453 328.903c-2.025-9.233-5.608-17.396-12.172-24.16-2.762-2.846-5.498-5.764-7.884-8.923-10.584-14.01-25.356-23.16-39.595-32.464-10.256-6.701-22.546-10.289-34.284-15.312.325-5.246 1.005-10.444 2.027-15.863l47.529 22.394c.89.428 1.83.901 2.516 1.584l45.564 45.193c7.69 7.233 9.352 16.472 11.849 26.084-5.032.773-10.066 1.154-15.55 1.466z\"></path><path fill=\"#e95a0f\" d=\"M801.776 434.171c8.108-7.882 16.584-15.257 24.573-23.126 6.558-6.459 12.881-13.226 18.666-20.376 4.817-5.953 8.7-12.661 13.011-19.409 5.739 1.338 11.463 3.051 17.581 4.838-.845 4.183-2.53 8.219-3.229 12.418-1.522 9.144-8.588 14.477-14.201 20.475-8.512 9.094-17.745 17.635-27.443 25.455-6.613 5.333-14.54 9.036-22.223 13.51-2.422-4.469-4.499-8.98-6.735-13.786z\"></path><path fill=\"#eb5e5b\" d=\"M1248.533 316.002c2.155.688 4.101 1.159 5.71 2.168 16.24 10.174 30.255 22.752 41.532 38.727-7.166 5.736-14.641 11.319-22.562 16.731-1.16-1.277-1.684-2.585-2.615-3.46l-38.694-36.2 14.203-15.029c.803-.86 1.38-1.93 2.427-2.936z\"></path><path fill=\"#eb5a57\" d=\"M1216.359 827.958c-4.331-3.733-8.603-7.379-12.326-11.518l-26.664-30.44c-.866-.989-1.89-1.839-3.152-2.902 6.483-6.054 13.276-11.959 20.371-18.005l39.315 44.704c-5.648 6.216-11.441 12.12-17.544 18.161z\"></path><path fill=\"#ec6168\" d=\"M1231.598 334.101l38.999 36.066c.931.876 1.456 2.183 2.303 3.608-4.283 4.279-8.7 8.24-13.769 12.091-4.2-3.051-7.512-6.349-11.338-8.867-12.36-8.136-22.893-18.27-32.841-29.093l16.646-13.805z\"></path><path fill=\"#ed656e\" d=\"M1214.597 347.955c10.303 10.775 20.836 20.908 33.196 29.044 3.825 2.518 7.137 5.816 10.992 8.903-3.171 4.397-6.65 8.648-10.432 13.046-6.785-5.184-13.998-9.858-19.529-16.038-4.946-5.527-9.687-8.644-17.309-8.215-2.616.147-5.734-2.788-8.067-4.923-3.026-2.769-5.497-6.144-8.35-9.568 6.286-4.273 12.715-8.237 19.499-12.25z\"></path></svg>\n</p>\n\n<p align=\"center\">\n<b>The crispy rerank family from <a href=\"https://mixedbread.ai\"><b>Mixedbread</b></a>.</b>\n</p>\n\n<p align=\"center\">\n<sup> \ud83c\udf5e Looking for a simple end-to-end retrieval solution? Meet Omni, our multimodal and multilingual model. <a href=\"https://mixedbread.com\"><b>Get in touch for access.</a> </sup>\n</p>\n\n# mxbai-rerank-base-v1\n\nThis is the base model in our family of powerful reranker models. You can learn more about the models in our [blog post](https://www.mixedbread.ai/blog/mxbai-rerank-v1).\n\nWe have three models:\n\n- [mxbai-rerank-xsmall-v1](https://huggingface.co/mixedbread-ai/mxbai-rerank-xsmall-v1)\n- [mxbai-rerank-base-v1](https://huggingface.co/mixedbread-ai/mxbai-rerank-base-v1) (\ud83c\udf5e)\n- [mxbai-rerank-large-v1](https://huggingface.co/mixedbread-ai/mxbai-rerank-large-v1)\n\n## Quickstart\n\nCurrently, the best way to use our models is with the most recent version of sentence-transformers.\n\n`pip install -U sentence-transformers`\n\nLet's say you have a query, and you want to rerank a set of documents. You can do that with only one line of code:\n\n```python\nfrom sentence_transformers import CrossEncoder\n\n# Load the model, here we use our base sized model\nmodel = CrossEncoder(\"mixedbread-ai/mxbai-rerank-base-v1\")\n\n\n# Example query and documents\nquery = \"Who wrote 'To Kill a Mockingbird'?\"\ndocuments = [\n    \"'To Kill a Mockingbird' is a novel by Harper Lee published in 1960. It was immediately successful, winning the Pulitzer Prize, and has become a classic of modern American literature.\",\n    \"The novel 'Moby-Dick' was written by Herman Melville and first published in 1851. It is considered a masterpiece of American literature and deals with complex themes of obsession, revenge, and the conflict between good and evil.\",\n    \"Harper Lee, an American novelist widely known for her novel 'To Kill a Mockingbird', was born in 1926 in Monroeville, Alabama. She received the Pulitzer Prize for Fiction in 1961.\",\n    \"Jane Austen was an English novelist known primarily for her six major novels, which interpret, critique and comment upon the British landed gentry at the end of the 18th century.\",\n    \"The 'Harry Potter' series, which consists of seven fantasy novels written by British author J.K. Rowling, is among the most popular and critically acclaimed books of the modern era.\",\n    \"'The Great Gatsby', a novel written by American author F. Scott Fitzgerald, was published in 1925. The story is set in the Jazz Age and follows the life of millionaire Jay Gatsby and his pursuit of Daisy Buchanan.\"\n]\n\n# Lets get the scores\nresults = model.rank(query, documents, return_documents=True, top_k=3)\n```\n\n\n<details>\n  <summary>JavaScript Example</summary>\n\nInstall [transformers.js](https://github.com/xenova/transformers.js)\n\n`npm i @xenova/transformers`\n\nLet's say you have a query, and you want to rerank a set of documents. In JavaScript, you need to add a function:\n\n```javascript\nimport { AutoTokenizer, AutoModelForSequenceClassification } from '@xenova/transformers';\n\nconst model_id = 'mixedbread-ai/mxbai-rerank-base-v1';\nconst model = await AutoModelForSequenceClassification.from_pretrained(model_id);\nconst tokenizer = await AutoTokenizer.from_pretrained(model_id);\n\n/**\n * Performs ranking with the CrossEncoder on the given query and documents. Returns a sorted list with the document indices and scores.\n * @param {string} query A single query\n * @param {string[]} documents A list of documents\n * @param {Object} options Options for ranking\n * @param {number} [options.top_k=undefined] Return the top-k documents. If undefined, all documents are returned.\n * @param {number} [options.return_documents=false] If true, also returns the documents. If false, only returns the indices and scores.\n */\nasync function rank(query, documents, {\n    top_k = undefined,\n    return_documents = false,\n} = {}) {\n    const inputs = tokenizer(\n        new Array(documents.length).fill(query),\n        {\n            text_pair: documents,\n            padding: true,\n            truncation: true,\n        }\n    )\n    const { logits } = await model(inputs);\n    return logits\n        .sigmoid()\n        .tolist()\n        .map(([score], i) => ({\n            corpus_id: i,\n            score,\n            ...(return_documents ? { text: documents[i] } : {})\n        }))\n        .sort((a, b) => b.score - a.score)\n        .slice(0, top_k);\n}\n\n// Example usage:\nconst query = \"Who wrote 'To Kill a Mockingbird'?\"\nconst documents = [\n    \"'To Kill a Mockingbird' is a novel by Harper Lee published in 1960. It was immediately successful, winning the Pulitzer Prize, and has become a classic of modern American literature.\",\n    \"The novel 'Moby-Dick' was written by Herman Melville and first published in 1851. It is considered a masterpiece of American literature and deals with complex themes of obsession, revenge, and the conflict between good and evil.\",\n    \"Harper Lee, an American novelist widely known for her novel 'To Kill a Mockingbird', was born in 1926 in Monroeville, Alabama. She received the Pulitzer Prize for Fiction in 1961.\",\n    \"Jane Austen was an English novelist known primarily for her six major novels, which interpret, critique and comment upon the British landed gentry at the end of the 18th century.\",\n    \"The 'Harry Potter' series, which consists of seven fantasy novels written by British author J.K. Rowling, is among the most popular and critically acclaimed books of the modern era.\",\n    \"'The Great Gatsby', a novel written by American author F. Scott Fitzgerald, was published in 1925. The story is set in the Jazz Age and follows the life of millionaire Jay Gatsby and his pursuit of Daisy Buchanan.\"\n]\n\nconst results = await rank(query, documents, { return_documents: true, top_k: 3 });\nconsole.log(results);\n```\n</details>\n\n## Using API\n\nYou can use the large model via our API as follows:\n\n```python\nfrom mixedbread_ai.client import MixedbreadAI\n\nmxbai = MixedbreadAI(api_key=\"{MIXEDBREAD_API_KEY}\")\n\nres = mxbai.reranking(\n  model=\"mixedbread-ai/mxbai-rerank-large-v1\",\n  query=\"Who is the author of To Kill a Mockingbird?\",\n  input=[\n    \"To Kill a Mockingbird is a novel by Harper Lee published in 1960. It was immediately successful, winning the Pulitzer Prize, and has become a classic of modern American literature.\",\n    \"The novel Moby-Dick was written by Herman Melville and first published in 1851. It is considered a masterpiece of American literature and deals with complex themes of obsession, revenge, and the conflict between good and evil.\",\n    \"Harper Lee, an American novelist widely known for her novel To Kill a Mockingbird, was born in 1926 in Monroeville, Alabama. She received the Pulitzer Prize for Fiction in 1961.\",\n    \"Jane Austen was an English novelist known primarily for her six major novels, which interpret, critique and comment upon the British landed gentry at the end of the 18th century.\",\n    \"The Harry Potter series, which consists of seven fantasy novels written by British author J.K. Rowling, is among the most popular and critically acclaimed books of the modern era.\",\n    \"The Great Gatsby, a novel written by American author F. Scott Fitzgerald, was published in 1925. The story is set in the Jazz Age and follows the life of millionaire Jay Gatsby and his pursuit of Daisy Buchanan.\"\n  ],\n  top_k=3,\n  return_input=false\n)\n\nprint(res.data)\n```\n\nThe API comes with additional features, such as a continous trained reranker! Check out the [docs](https://www.mixedbread.ai/docs) for more information.\n\n## Evaluation\n\nOur reranker models are designed to elevate your search. They work extremely well in combination with keyword search and can even outperform semantic search systems in many cases.\n\n| Model                                                                                 | NDCG@10  | Accuracy@3 |\n| ------------------------------------------------------------------------------------- | -------- | ---------- |\n| Lexical Search (Lucene)                                                               | 38.0     | 66.4       |\n| [BAAI/bge-reranker-base](https://huggingface.co/BAAI/bge-reranker-base)               | 41.6     | 66.9       |\n| [BAAI/bge-reranker-large](https://huggingface.co/BAAI/bge-reranker-large)             | 45.2     | 70.6       |\n| cohere-embed-v3 (semantic search)                                                     | 47.5     | 70.9       |\n| [mxbai-rerank-xsmall-v1](https://huggingface.co/mixedbread-ai/mxbai-rerank-xsmall-v1) | **43.9** | **70.0**   |\n| [mxbai-rerank-base-v1](https://huggingface.co/mixedbread-ai/mxbai-rerank-base-v1)     | **46.9** | **72.3**   |\n| [mxbai-rerank-large-v1](https://huggingface.co/mixedbread-ai/mxbai-rerank-large-v1)   | **48.8** | **74.9**   |\n\nThe reported results are aggregated from 11 datasets of BEIR. We used [Pyserini](https://github.com/castorini/pyserini/) to evaluate the models. Find more in our [blog-post](https://www.mixedbread.ai/blog/mxbai-rerank-v1) and on this [spreadsheet](https://docs.google.com/spreadsheets/d/15ELkSMFv-oHa5TRiIjDvhIstH9dlc3pnZeO-iGz4Ld4/edit?usp=sharing).\n\n## Community\nPlease join our [Discord Community](https://discord.gg/jDfMHzAVfU) and share your feedback and thoughts! We are here to help and also always happy to chat.\n\n## Citation\n\n```bibtex\n@online{rerank2024mxbai,\n  title={Boost Your Search With The Crispy Mixedbread Rerank Models},\n  author={Aamir Shakir and Darius Koenig and Julius Lipp and Sean Lee},\n  year={2024},\n  url={https://www.mixedbread.ai/blog/mxbai-rerank-v1},\n}\n```\n\n## License\nApache 2.0",
    "card_content": "---\nlanguage:\n- en\nlicense: apache-2.0\nlibrary_name: transformers\ntags:\n- reranker\n- transformers.js\n---\n<br><br>\n\n<p align=\"center\">\n<svg xmlns=\"http://www.w3.org/2000/svg\" xml:space=\"preserve\" viewBox=\"0 0 2020 1130\" width=\"150\" height=\"150\" aria-hidden=\"true\"><path fill=\"#e95a0f\" d=\"M398.167 621.992c-1.387-20.362-4.092-40.739-3.851-61.081.355-30.085 6.873-59.139 21.253-85.976 10.487-19.573 24.09-36.822 40.662-51.515 16.394-14.535 34.338-27.046 54.336-36.182 15.224-6.955 31.006-12.609 47.829-14.168 11.809-1.094 23.753-2.514 35.524-1.836 23.033 1.327 45.131 7.255 66.255 16.75 16.24 7.3 31.497 16.165 45.651 26.969 12.997 9.921 24.412 21.37 34.158 34.509 11.733 15.817 20.849 33.037 25.987 52.018 3.468 12.81 6.438 25.928 7.779 39.097 1.722 16.908 1.642 34.003 2.235 51.021.427 12.253.224 24.547 1.117 36.762 1.677 22.93 4.062 45.764 11.8 67.7 5.376 15.239 12.499 29.55 20.846 43.681l-18.282 20.328c-1.536 1.71-2.795 3.665-4.254 5.448l-19.323 23.533c-13.859-5.449-27.446-11.803-41.657-16.086-13.622-4.106-27.793-6.765-41.905-8.775-15.256-2.173-30.701-3.475-46.105-4.049-23.571-.879-47.178-1.056-70.769-1.029-10.858.013-21.723 1.116-32.57 1.926-5.362.4-10.69 1.255-16.464 1.477-2.758-7.675-5.284-14.865-7.367-22.181-3.108-10.92-4.325-22.554-13.16-31.095-2.598-2.512-5.069-5.341-6.883-8.443-6.366-10.884-12.48-21.917-18.571-32.959-4.178-7.573-8.411-14.375-17.016-18.559-10.34-5.028-19.538-12.387-29.311-18.611-3.173-2.021-6.414-4.312-9.952-5.297-5.857-1.63-11.98-2.301-17.991-3.376z\"></path><path fill=\"#ed6d7b\" d=\"M1478.998 758.842c-12.025.042-24.05.085-36.537-.373-.14-8.536.231-16.569.453-24.607.033-1.179-.315-2.986-1.081-3.4-.805-.434-2.376.338-3.518.81-.856.354-1.562 1.069-3.589 2.521-.239-3.308-.664-5.586-.519-7.827.488-7.544 2.212-15.166 1.554-22.589-1.016-11.451 1.397-14.592-12.332-14.419-3.793.048-3.617-2.803-3.332-5.331.499-4.422 1.45-8.803 1.77-13.233.311-4.316.068-8.672.068-12.861-2.554-.464-4.326-.86-6.12-1.098-4.415-.586-6.051-2.251-5.065-7.31 1.224-6.279.848-12.862 1.276-19.306.19-2.86-.971-4.473-3.794-4.753-4.113-.407-8.242-1.057-12.352-.975-4.663.093-5.192-2.272-4.751-6.012.733-6.229 1.252-12.483 1.875-18.726l1.102-10.495c-5.905-.309-11.146-.805-16.385-.778-3.32.017-5.174-1.4-5.566-4.4-1.172-8.968-2.479-17.944-3.001-26.96-.26-4.484-1.936-5.705-6.005-5.774-9.284-.158-18.563-.594-27.843-.953-7.241-.28-10.137-2.764-11.3-9.899-.746-4.576-2.715-7.801-7.777-8.207-7.739-.621-15.511-.992-23.207-1.961-7.327-.923-14.587-2.415-21.853-3.777-5.021-.941-10.003-2.086-15.003-3.14 4.515-22.952 13.122-44.382 26.284-63.587 18.054-26.344 41.439-47.239 69.102-63.294 15.847-9.197 32.541-16.277 50.376-20.599 16.655-4.036 33.617-5.715 50.622-4.385 33.334 2.606 63.836 13.955 92.415 31.15 15.864 9.545 30.241 20.86 42.269 34.758 8.113 9.374 15.201 19.78 21.718 30.359 10.772 17.484 16.846 36.922 20.611 56.991 1.783 9.503 2.815 19.214 3.318 28.876.758 14.578.755 29.196.65 44.311l-51.545 20.013c-7.779 3.059-15.847 5.376-21.753 12.365-4.73 5.598-10.658 10.316-16.547 14.774-9.9 7.496-18.437 15.988-25.083 26.631-3.333 5.337-7.901 10.381-12.999 14.038-11.355 8.144-17.397 18.973-19.615 32.423l-6.988 41.011z\"></path><path fill=\"#ec663e\" d=\"M318.11 923.047c-.702 17.693-.832 35.433-2.255 53.068-1.699 21.052-6.293 41.512-14.793 61.072-9.001 20.711-21.692 38.693-38.496 53.583-16.077 14.245-34.602 24.163-55.333 30.438-21.691 6.565-43.814 8.127-66.013 6.532-22.771-1.636-43.88-9.318-62.74-22.705-20.223-14.355-35.542-32.917-48.075-54.096-9.588-16.203-16.104-33.55-19.201-52.015-2.339-13.944-2.307-28.011-.403-42.182 2.627-19.545 9.021-37.699 17.963-55.067 11.617-22.564 27.317-41.817 48.382-56.118 15.819-10.74 33.452-17.679 52.444-20.455 8.77-1.282 17.696-1.646 26.568-2.055 11.755-.542 23.534-.562 35.289-1.11 8.545-.399 17.067-1.291 26.193-1.675 1.349 1.77 2.24 3.199 2.835 4.742 4.727 12.261 10.575 23.865 18.636 34.358 7.747 10.084 14.83 20.684 22.699 30.666 3.919 4.972 8.37 9.96 13.609 13.352 7.711 4.994 16.238 8.792 24.617 12.668 5.852 2.707 12.037 4.691 18.074 6.998z\"></path><path fill=\"#ea580e\" d=\"M1285.167 162.995c3.796-29.75 13.825-56.841 32.74-80.577 16.339-20.505 36.013-36.502 59.696-47.614 14.666-6.881 29.971-11.669 46.208-12.749 10.068-.669 20.239-1.582 30.255-.863 16.6 1.191 32.646 5.412 47.9 12.273 19.39 8.722 36.44 20.771 50.582 36.655 15.281 17.162 25.313 37.179 31.49 59.286 5.405 19.343 6.31 39.161 4.705 58.825-2.37 29.045-11.836 55.923-30.451 78.885-10.511 12.965-22.483 24.486-37.181 33.649-5.272-5.613-10.008-11.148-14.539-16.846-5.661-7.118-10.958-14.533-16.78-21.513-4.569-5.478-9.548-10.639-14.624-15.658-3.589-3.549-7.411-6.963-11.551-9.827-5.038-3.485-10.565-6.254-15.798-9.468-8.459-5.195-17.011-9.669-26.988-11.898-12.173-2.72-24.838-4.579-35.622-11.834-1.437-.967-3.433-1.192-5.213-1.542-12.871-2.529-25.454-5.639-36.968-12.471-5.21-3.091-11.564-4.195-17.011-6.965-4.808-2.445-8.775-6.605-13.646-8.851-8.859-4.085-18.114-7.311-27.204-10.896z\"></path><path fill=\"#f8ab00\" d=\"M524.963 311.12c-9.461-5.684-19.513-10.592-28.243-17.236-12.877-9.801-24.031-21.578-32.711-35.412-11.272-17.965-19.605-37.147-21.902-58.403-1.291-11.951-2.434-24.073-1.87-36.034.823-17.452 4.909-34.363 11.581-50.703 8.82-21.603 22.25-39.792 39.568-55.065 18.022-15.894 39.162-26.07 62.351-32.332 19.22-5.19 38.842-6.177 58.37-4.674 23.803 1.831 45.56 10.663 65.062 24.496 17.193 12.195 31.688 27.086 42.894 45.622-11.403 8.296-22.633 16.117-34.092 23.586-17.094 11.142-34.262 22.106-48.036 37.528-8.796 9.848-17.201 20.246-27.131 28.837-16.859 14.585-27.745 33.801-41.054 51.019-11.865 15.349-20.663 33.117-30.354 50.08-5.303 9.283-9.654 19.11-14.434 28.692z\"></path><path fill=\"#ea5227\" d=\"M1060.11 1122.049c-7.377 1.649-14.683 4.093-22.147 4.763-11.519 1.033-23.166 1.441-34.723 1.054-19.343-.647-38.002-4.7-55.839-12.65-15.078-6.72-28.606-15.471-40.571-26.836-24.013-22.81-42.053-49.217-49.518-81.936-1.446-6.337-1.958-12.958-2.235-19.477-.591-13.926-.219-27.909-1.237-41.795-.916-12.5-3.16-24.904-4.408-37.805 1.555-1.381 3.134-2.074 3.778-3.27 4.729-8.79 12.141-15.159 19.083-22.03 5.879-5.818 10.688-12.76 16.796-18.293 6.993-6.335 11.86-13.596 14.364-22.612l8.542-29.993c8.015 1.785 15.984 3.821 24.057 5.286 8.145 1.478 16.371 2.59 24.602 3.493 8.453.927 16.956 1.408 25.891 2.609 1.119 16.09 1.569 31.667 2.521 47.214.676 11.045 1.396 22.154 3.234 33.043 2.418 14.329 5.708 28.527 9.075 42.674 3.499 14.705 4.028 29.929 10.415 44.188 10.157 22.674 18.29 46.25 28.281 69.004 7.175 16.341 12.491 32.973 15.078 50.615.645 4.4 3.256 8.511 4.963 12.755z\"></path><path fill=\"#ea5330\" d=\"M1060.512 1122.031c-2.109-4.226-4.72-8.337-5.365-12.737-2.587-17.642-7.904-34.274-15.078-50.615-9.991-22.755-18.124-46.33-28.281-69.004-6.387-14.259-6.916-29.482-10.415-44.188-3.366-14.147-6.656-28.346-9.075-42.674-1.838-10.889-2.558-21.999-3.234-33.043-.951-15.547-1.401-31.124-2.068-47.146 8.568-.18 17.146.487 25.704.286l41.868-1.4c.907 3.746 1.245 7.04 1.881 10.276l8.651 42.704c.903 4.108 2.334 8.422 4.696 11.829 7.165 10.338 14.809 20.351 22.456 30.345 4.218 5.512 8.291 11.304 13.361 15.955 8.641 7.927 18.065 14.995 27.071 22.532 12.011 10.052 24.452 19.302 40.151 22.854-1.656 11.102-2.391 22.44-5.172 33.253-4.792 18.637-12.38 36.209-23.412 52.216-13.053 18.94-29.086 34.662-49.627 45.055-10.757 5.443-22.443 9.048-34.111 13.501z\"></path><path fill=\"#f8aa05\" d=\"M1989.106 883.951c5.198 8.794 11.46 17.148 15.337 26.491 5.325 12.833 9.744 26.207 12.873 39.737 2.95 12.757 3.224 25.908 1.987 39.219-1.391 14.973-4.643 29.268-10.349 43.034-5.775 13.932-13.477 26.707-23.149 38.405-14.141 17.104-31.215 30.458-50.807 40.488-14.361 7.352-29.574 12.797-45.741 14.594-10.297 1.144-20.732 2.361-31.031 1.894-24.275-1.1-47.248-7.445-68.132-20.263-6.096-3.741-11.925-7.917-17.731-12.342 5.319-5.579 10.361-10.852 15.694-15.811l37.072-34.009c.975-.892 2.113-1.606 3.08-2.505 6.936-6.448 14.765-12.2 20.553-19.556 8.88-11.285 20.064-19.639 31.144-28.292 4.306-3.363 9.06-6.353 12.673-10.358 5.868-6.504 10.832-13.814 16.422-20.582 6.826-8.264 13.727-16.481 20.943-24.401 4.065-4.461 8.995-8.121 13.249-12.424 14.802-14.975 28.77-30.825 45.913-43.317z\"></path><path fill=\"#ed6876\" d=\"M1256.099 523.419c5.065.642 10.047 1.787 15.068 2.728 7.267 1.362 14.526 2.854 21.853 3.777 7.696.97 15.468 1.34 23.207 1.961 5.062.406 7.031 3.631 7.777 8.207 1.163 7.135 4.059 9.62 11.3 9.899l27.843.953c4.069.069 5.745 1.291 6.005 5.774.522 9.016 1.829 17.992 3.001 26.96.392 3 2.246 4.417 5.566 4.4 5.239-.026 10.48.469 16.385.778l-1.102 10.495-1.875 18.726c-.44 3.74.088 6.105 4.751 6.012 4.11-.082 8.239.568 12.352.975 2.823.28 3.984 1.892 3.794 4.753-.428 6.444-.052 13.028-1.276 19.306-.986 5.059.651 6.724 5.065 7.31 1.793.238 3.566.634 6.12 1.098 0 4.189.243 8.545-.068 12.861-.319 4.43-1.27 8.811-1.77 13.233-.285 2.528-.461 5.379 3.332 5.331 13.729-.173 11.316 2.968 12.332 14.419.658 7.423-1.066 15.045-1.554 22.589-.145 2.241.28 4.519.519 7.827 2.026-1.452 2.733-2.167 3.589-2.521 1.142-.472 2.713-1.244 3.518-.81.767.414 1.114 2.221 1.081 3.4l-.917 24.539c-11.215.82-22.45.899-33.636 1.674l-43.952 3.436c-1.086-3.01-2.319-5.571-2.296-8.121.084-9.297-4.468-16.583-9.091-24.116-3.872-6.308-8.764-13.052-9.479-19.987-1.071-10.392-5.716-15.936-14.889-18.979-1.097-.364-2.16-.844-3.214-1.327-7.478-3.428-15.548-5.918-19.059-14.735-.904-2.27-3.657-3.775-5.461-5.723-2.437-2.632-4.615-5.525-7.207-7.987-2.648-2.515-5.352-5.346-8.589-6.777-4.799-2.121-10.074-3.185-15.175-4.596l-15.785-4.155c.274-12.896 1.722-25.901.54-38.662-1.647-17.783-3.457-35.526-2.554-53.352.528-10.426 2.539-20.777 3.948-31.574z\"></path><path fill=\"#f6a200\" d=\"M525.146 311.436c4.597-9.898 8.947-19.725 14.251-29.008 9.691-16.963 18.49-34.73 30.354-50.08 13.309-17.218 24.195-36.434 41.054-51.019 9.93-8.591 18.335-18.989 27.131-28.837 13.774-15.422 30.943-26.386 48.036-37.528 11.459-7.469 22.688-15.29 34.243-23.286 11.705 16.744 19.716 35.424 22.534 55.717 2.231 16.066 2.236 32.441 2.753 49.143-4.756 1.62-9.284 2.234-13.259 4.056-6.43 2.948-12.193 7.513-18.774 9.942-19.863 7.331-33.806 22.349-47.926 36.784-7.86 8.035-13.511 18.275-19.886 27.705-4.434 6.558-9.345 13.037-12.358 20.254-4.249 10.177-6.94 21.004-10.296 31.553-12.33.053-24.741 1.027-36.971-.049-20.259-1.783-40.227-5.567-58.755-14.69-.568-.28-1.295-.235-2.132-.658z\"></path><path fill=\"#f7a80d\" d=\"M1989.057 883.598c-17.093 12.845-31.061 28.695-45.863 43.67-4.254 4.304-9.184 7.963-13.249 12.424-7.216 7.92-14.117 16.137-20.943 24.401-5.59 6.768-10.554 14.078-16.422 20.582-3.614 4.005-8.367 6.995-12.673 10.358-11.08 8.653-22.264 17.007-31.144 28.292-5.788 7.356-13.617 13.108-20.553 19.556-.967.899-2.105 1.614-3.08 2.505l-37.072 34.009c-5.333 4.96-10.375 10.232-15.859 15.505-21.401-17.218-37.461-38.439-48.623-63.592 3.503-1.781 7.117-2.604 9.823-4.637 8.696-6.536 20.392-8.406 27.297-17.714.933-1.258 2.646-1.973 4.065-2.828 17.878-10.784 36.338-20.728 53.441-32.624 10.304-7.167 18.637-17.23 27.583-26.261 3.819-3.855 7.436-8.091 10.3-12.681 12.283-19.68 24.43-39.446 40.382-56.471 12.224-13.047 17.258-29.524 22.539-45.927 15.85 4.193 29.819 12.129 42.632 22.08 10.583 8.219 19.782 17.883 27.42 29.351z\"></path><path fill=\"#ef7a72\" d=\"M1479.461 758.907c1.872-13.734 4.268-27.394 6.525-41.076 2.218-13.45 8.26-24.279 19.615-32.423 5.099-3.657 9.667-8.701 12.999-14.038 6.646-10.643 15.183-19.135 25.083-26.631 5.888-4.459 11.817-9.176 16.547-14.774 5.906-6.99 13.974-9.306 21.753-12.365l51.48-19.549c.753 11.848.658 23.787 1.641 35.637 1.771 21.353 4.075 42.672 11.748 62.955.17.449.107.985-.019 2.158-6.945 4.134-13.865 7.337-20.437 11.143-3.935 2.279-7.752 5.096-10.869 8.384-6.011 6.343-11.063 13.624-17.286 19.727-9.096 8.92-12.791 20.684-18.181 31.587-.202.409-.072.984-.096 1.481-8.488-1.72-16.937-3.682-25.476-5.094-9.689-1.602-19.426-3.084-29.201-3.949-15.095-1.335-30.241-2.1-45.828-3.172z\"></path><path fill=\"#e94e3b\" d=\"M957.995 766.838c-20.337-5.467-38.791-14.947-55.703-27.254-8.2-5.967-15.451-13.238-22.958-20.37 2.969-3.504 5.564-6.772 8.598-9.563 7.085-6.518 11.283-14.914 15.8-23.153 4.933-8.996 10.345-17.743 14.966-26.892 2.642-5.231 5.547-11.01 5.691-16.611.12-4.651.194-8.932 2.577-12.742 8.52-13.621 15.483-28.026 18.775-43.704 2.11-10.049 7.888-18.774 7.81-29.825-.064-9.089 4.291-18.215 6.73-27.313 3.212-11.983 7.369-23.797 9.492-35.968 3.202-18.358 5.133-36.945 7.346-55.466l4.879-45.8c6.693.288 13.386.575 20.54 1.365.13 3.458-.41 6.407-.496 9.37l-1.136 42.595c-.597 11.552-2.067 23.058-3.084 34.59l-3.845 44.478c-.939 10.202-1.779 20.432-3.283 30.557-.96 6.464-4.46 12.646-1.136 19.383.348.706-.426 1.894-.448 2.864-.224 9.918-5.99 19.428-2.196 29.646.103.279-.033.657-.092.983l-8.446 46.205c-1.231 6.469-2.936 12.846-4.364 19.279-1.5 6.757-2.602 13.621-4.456 20.277-3.601 12.93-10.657 25.3-5.627 39.47.368 1.036.234 2.352.017 3.476l-5.949 30.123z\"></path><path fill=\"#ea5043\" d=\"M958.343 767.017c1.645-10.218 3.659-20.253 5.602-30.302.217-1.124.351-2.44-.017-3.476-5.03-14.17 2.026-26.539 5.627-39.47 1.854-6.656 2.956-13.52 4.456-20.277 1.428-6.433 3.133-12.81 4.364-19.279l8.446-46.205c.059-.326.196-.705.092-.983-3.794-10.218 1.972-19.728 2.196-29.646.022-.97.796-2.158.448-2.864-3.324-6.737.176-12.919 1.136-19.383 1.504-10.125 2.344-20.355 3.283-30.557l3.845-44.478c1.017-11.532 2.488-23.038 3.084-34.59.733-14.18.722-28.397 1.136-42.595.086-2.963.626-5.912.956-9.301 5.356-.48 10.714-.527 16.536-.081 2.224 15.098 1.855 29.734 1.625 44.408-.157 10.064 1.439 20.142 1.768 30.23.334 10.235-.035 20.49.116 30.733.084 5.713.789 11.418.861 17.13.054 4.289-.469 8.585-.702 12.879-.072 1.323-.138 2.659-.031 3.975l2.534 34.405-1.707 36.293-1.908 48.69c-.182 8.103.993 16.237.811 24.34-.271 12.076-1.275 24.133-1.787 36.207-.102 2.414-.101 5.283 1.06 7.219 4.327 7.22 4.463 15.215 4.736 23.103.365 10.553.088 21.128.086 31.693-11.44 2.602-22.84.688-34.106-.916-11.486-1.635-22.806-4.434-34.546-6.903z\"></path><path fill=\"#eb5d19\" d=\"M398.091 622.45c6.086.617 12.21 1.288 18.067 2.918 3.539.985 6.779 3.277 9.952 5.297 9.773 6.224 18.971 13.583 29.311 18.611 8.606 4.184 12.839 10.986 17.016 18.559l18.571 32.959c1.814 3.102 4.285 5.931 6.883 8.443 8.835 8.542 10.052 20.175 13.16 31.095 2.082 7.317 4.609 14.507 6.946 22.127-29.472 3.021-58.969 5.582-87.584 15.222-1.185-2.302-1.795-4.362-2.769-6.233-4.398-8.449-6.703-18.174-14.942-24.299-2.511-1.866-5.103-3.814-7.047-6.218-8.358-10.332-17.028-20.276-28.772-26.973 4.423-11.478 9.299-22.806 13.151-34.473 4.406-13.348 6.724-27.18 6.998-41.313.098-5.093.643-10.176 1.06-15.722z\"></path><path fill=\"#e94c32\" d=\"M981.557 392.109c-1.172 15.337-2.617 30.625-4.438 45.869-2.213 18.521-4.144 37.108-7.346 55.466-2.123 12.171-6.28 23.985-9.492 35.968-2.439 9.098-6.794 18.224-6.73 27.313.078 11.051-5.7 19.776-7.81 29.825-3.292 15.677-10.255 30.082-18.775 43.704-2.383 3.81-2.458 8.091-2.577 12.742-.144 5.6-3.049 11.38-5.691 16.611-4.621 9.149-10.033 17.896-14.966 26.892-4.517 8.239-8.715 16.635-15.8 23.153-3.034 2.791-5.629 6.06-8.735 9.255-12.197-10.595-21.071-23.644-29.301-37.24-7.608-12.569-13.282-25.962-17.637-40.37 13.303-6.889 25.873-13.878 35.311-25.315.717-.869 1.934-1.312 2.71-2.147 5.025-5.405 10.515-10.481 14.854-16.397 6.141-8.374 10.861-17.813 17.206-26.008 8.22-10.618 13.657-22.643 20.024-34.466 4.448-.626 6.729-3.21 8.114-6.89 1.455-3.866 2.644-7.895 4.609-11.492 4.397-8.05 9.641-15.659 13.708-23.86 3.354-6.761 5.511-14.116 8.203-21.206 5.727-15.082 7.277-31.248 12.521-46.578 3.704-10.828 3.138-23.116 4.478-34.753l7.56-.073z\"></path><path fill=\"#f7a617\" d=\"M1918.661 831.99c-4.937 16.58-9.971 33.057-22.196 46.104-15.952 17.025-28.099 36.791-40.382 56.471-2.864 4.59-6.481 8.825-10.3 12.681-8.947 9.031-17.279 19.094-27.583 26.261-17.103 11.896-35.564 21.84-53.441 32.624-1.419.856-3.132 1.571-4.065 2.828-6.904 9.308-18.6 11.178-27.297 17.714-2.705 2.033-6.319 2.856-9.874 4.281-3.413-9.821-6.916-19.583-9.36-29.602-1.533-6.284-1.474-12.957-1.665-19.913 1.913-.78 3.374-1.057 4.81-1.431 15.822-4.121 31.491-8.029 43.818-20.323 9.452-9.426 20.371-17.372 30.534-26.097 6.146-5.277 13.024-10.052 17.954-16.326 14.812-18.848 28.876-38.285 43.112-57.581 2.624-3.557 5.506-7.264 6.83-11.367 2.681-8.311 4.375-16.94 6.476-25.438 17.89.279 35.333 3.179 52.629 9.113z\"></path><path fill=\"#ea553a\" d=\"M1172.91 977.582c-15.775-3.127-28.215-12.377-40.227-22.43-9.005-7.537-18.43-14.605-27.071-22.532-5.07-4.651-9.143-10.443-13.361-15.955-7.647-9.994-15.291-20.007-22.456-30.345-2.361-3.407-3.792-7.72-4.696-11.829-3.119-14.183-5.848-28.453-8.651-42.704-.636-3.236-.974-6.53-1.452-10.209 15.234-2.19 30.471-3.969 46.408-5.622 2.692 5.705 4.882 11.222 6.63 16.876 2.9 9.381 7.776 17.194 15.035 24.049 7.056 6.662 13.305 14.311 19.146 22.099 9.509 12.677 23.01 19.061 36.907 25.054-1.048 7.441-2.425 14.854-3.066 22.33-.956 11.162-1.393 22.369-2.052 33.557l-1.096 17.661z\"></path><path fill=\"#ea5453\" d=\"M1163.123 704.036c-4.005 5.116-7.685 10.531-12.075 15.293-12.842 13.933-27.653 25.447-44.902 34.538-3.166-5.708-5.656-11.287-8.189-17.251-3.321-12.857-6.259-25.431-9.963-37.775-4.6-15.329-10.6-30.188-11.349-46.562-.314-6.871-1.275-14.287-7.114-19.644-1.047-.961-1.292-3.053-1.465-4.67l-4.092-39.927c-.554-5.245-.383-10.829-2.21-15.623-3.622-9.503-4.546-19.253-4.688-29.163-.088-6.111 1.068-12.256.782-18.344-.67-14.281-1.76-28.546-2.9-42.8-.657-8.222-1.951-16.395-2.564-24.62-.458-6.137-.285-12.322-.104-18.21.959 5.831 1.076 11.525 2.429 16.909 2.007 7.986 5.225 15.664 7.324 23.632 3.222 12.23 1.547 25.219 6.728 37.355 4.311 10.099 6.389 21.136 9.732 31.669 2.228 7.02 6.167 13.722 7.121 20.863 1.119 8.376 6.1 13.974 10.376 20.716l2.026 10.576c1.711 9.216 3.149 18.283 8.494 26.599 6.393 9.946 11.348 20.815 16.943 31.276 4.021 7.519 6.199 16.075 12.925 22.065l24.462 22.26c.556.503 1.507.571 2.274.841z\"></path><path fill=\"#ea5b15\" d=\"M1285.092 163.432c9.165 3.148 18.419 6.374 27.279 10.459 4.871 2.246 8.838 6.406 13.646 8.851 5.446 2.77 11.801 3.874 17.011 6.965 11.514 6.831 24.097 9.942 36.968 12.471 1.78.35 3.777.576 5.213 1.542 10.784 7.255 23.448 9.114 35.622 11.834 9.977 2.23 18.529 6.703 26.988 11.898 5.233 3.214 10.76 5.983 15.798 9.468 4.14 2.864 7.962 6.279 11.551 9.827 5.076 5.02 10.056 10.181 14.624 15.658 5.822 6.98 11.119 14.395 16.78 21.513 4.531 5.698 9.267 11.233 14.222 16.987-10.005 5.806-20.07 12.004-30.719 16.943-7.694 3.569-16.163 5.464-24.688 7.669-2.878-7.088-5.352-13.741-7.833-20.392-.802-2.15-1.244-4.55-2.498-6.396-4.548-6.7-9.712-12.999-14.011-19.847-6.672-10.627-15.34-18.93-26.063-25.376-9.357-5.625-18.367-11.824-27.644-17.587-6.436-3.997-12.902-8.006-19.659-11.405-5.123-2.577-11.107-3.536-16.046-6.37-17.187-9.863-35.13-17.887-54.031-23.767-4.403-1.37-8.953-2.267-13.436-3.382l.926-27.565z\"></path><path fill=\"#ea504b\" d=\"M1098 737l7.789 16.893c-15.04 9.272-31.679 15.004-49.184 17.995-9.464 1.617-19.122 2.097-29.151 3.019-.457-10.636-.18-21.211-.544-31.764-.273-7.888-.409-15.883-4.736-23.103-1.16-1.936-1.162-4.805-1.06-7.219l1.787-36.207c.182-8.103-.993-16.237-.811-24.34.365-16.236 1.253-32.461 1.908-48.69.484-12 .942-24.001 1.98-36.069 5.57 10.19 10.632 20.42 15.528 30.728 1.122 2.362 2.587 5.09 2.339 7.488-1.536 14.819 5.881 26.839 12.962 38.33 10.008 16.241 16.417 33.54 20.331 51.964 2.285 10.756 4.729 21.394 11.958 30.165L1098 737z\"></path><path fill=\"#f6a320\" d=\"M1865.78 822.529c-1.849 8.846-3.544 17.475-6.224 25.786-1.323 4.102-4.206 7.81-6.83 11.367l-43.112 57.581c-4.93 6.273-11.808 11.049-17.954 16.326-10.162 8.725-21.082 16.671-30.534 26.097-12.327 12.294-27.997 16.202-43.818 20.323-1.436.374-2.897.651-4.744.986-1.107-17.032-1.816-34.076-2.079-51.556 1.265-.535 2.183-.428 2.888-.766 10.596-5.072 20.8-11.059 32.586-13.273 1.69-.317 3.307-1.558 4.732-2.662l26.908-21.114c4.992-4.003 11.214-7.393 14.381-12.585 11.286-18.5 22.363-37.263 27.027-58.87l36.046 1.811c3.487.165 6.983.14 10.727.549z\"></path><path fill=\"#ec6333\" d=\"M318.448 922.814c-6.374-2.074-12.56-4.058-18.412-6.765-8.379-3.876-16.906-7.675-24.617-12.668-5.239-3.392-9.69-8.381-13.609-13.352-7.87-9.983-14.953-20.582-22.699-30.666-8.061-10.493-13.909-22.097-18.636-34.358-.595-1.543-1.486-2.972-2.382-4.783 6.84-1.598 13.797-3.023 20.807-4.106 18.852-2.912 36.433-9.493 53.737-17.819.697.888.889 1.555 1.292 2.051l17.921 21.896c4.14 4.939 8.06 10.191 12.862 14.412 5.67 4.984 12.185 9.007 18.334 13.447-8.937 16.282-16.422 33.178-20.696 51.31-1.638 6.951-2.402 14.107-3.903 21.403z\"></path><path fill=\"#f49700\" d=\"M623.467 326.903c2.893-10.618 5.584-21.446 9.833-31.623 3.013-7.217 7.924-13.696 12.358-20.254 6.375-9.43 12.026-19.67 19.886-27.705 14.12-14.434 28.063-29.453 47.926-36.784 6.581-2.429 12.344-6.994 18.774-9.942 3.975-1.822 8.503-2.436 13.186-3.592 1.947 18.557 3.248 37.15 8.307 55.686-15.453 7.931-28.853 18.092-40.46 29.996-10.417 10.683-19.109 23.111-28.013 35.175-3.238 4.388-4.888 9.948-7.262 14.973-17.803-3.987-35.767-6.498-54.535-5.931z\"></path><path fill=\"#ea544c\" d=\"M1097.956 736.615c-2.925-3.218-5.893-6.822-8.862-10.425-7.229-8.771-9.672-19.409-11.958-30.165-3.914-18.424-10.323-35.722-20.331-51.964-7.081-11.491-14.498-23.511-12.962-38.33.249-2.398-1.217-5.126-2.339-7.488l-15.232-31.019-3.103-34.338c-.107-1.316-.041-2.653.031-3.975.233-4.294.756-8.59.702-12.879-.072-5.713-.776-11.417-.861-17.13l-.116-30.733c-.329-10.088-1.926-20.166-1.768-30.23.23-14.674.599-29.31-1.162-44.341 9.369-.803 18.741-1.179 28.558-1.074 1.446 15.814 2.446 31.146 3.446 46.478.108 6.163-.064 12.348.393 18.485.613 8.225 1.907 16.397 2.564 24.62l2.9 42.8c.286 6.088-.869 12.234-.782 18.344.142 9.91 1.066 19.661 4.688 29.163 1.827 4.794 1.657 10.377 2.21 15.623l4.092 39.927c.172 1.617.417 3.71 1.465 4.67 5.839 5.357 6.8 12.773 7.114 19.644.749 16.374 6.749 31.233 11.349 46.562 3.704 12.344 6.642 24.918 9.963 37.775z\"></path><path fill=\"#ec5c61\" d=\"M1204.835 568.008c1.254 25.351-1.675 50.16-10.168 74.61-8.598-4.883-18.177-8.709-24.354-15.59-7.44-8.289-13.929-17.442-21.675-25.711-8.498-9.072-16.731-18.928-21.084-31.113-.54-1.513-1.691-2.807-2.594-4.564-4.605-9.247-7.706-18.544-7.96-29.09-.835-7.149-1.214-13.944-2.609-20.523-2.215-10.454-5.626-20.496-7.101-31.302-2.513-18.419-7.207-36.512-5.347-55.352.24-2.43-.17-4.949-.477-7.402l-4.468-34.792c2.723-.379 5.446-.757 8.585-.667 1.749 8.781 2.952 17.116 4.448 25.399 1.813 10.037 3.64 20.084 5.934 30.017 1.036 4.482 3.953 8.573 4.73 13.064 1.794 10.377 4.73 20.253 9.272 29.771 2.914 6.105 4.761 12.711 7.496 18.912 2.865 6.496 6.264 12.755 9.35 19.156 3.764 7.805 7.667 15.013 16.1 19.441 7.527 3.952 13.713 10.376 20.983 14.924 6.636 4.152 13.932 7.25 20.937 10.813z\"></path><path fill=\"#ed676f\" d=\"M1140.75 379.231c18.38-4.858 36.222-11.21 53.979-18.971 3.222 3.368 5.693 6.744 8.719 9.512 2.333 2.134 5.451 5.07 8.067 4.923 7.623-.429 12.363 2.688 17.309 8.215 5.531 6.18 12.744 10.854 19.224 16.184-5.121 7.193-10.461 14.241-15.323 21.606-13.691 20.739-22.99 43.255-26.782 67.926-.543 3.536-1.281 7.043-2.366 10.925-14.258-6.419-26.411-14.959-32.731-29.803-1.087-2.553-2.596-4.93-3.969-7.355-1.694-2.993-3.569-5.89-5.143-8.943-1.578-3.062-2.922-6.249-4.295-9.413-1.57-3.621-3.505-7.163-4.47-10.946-1.257-4.93-.636-10.572-2.725-15.013-5.831-12.397-7.467-25.628-9.497-38.847z\"></path><path fill=\"#ed656e\" d=\"M1254.103 647.439c5.325.947 10.603 2.272 15.847 3.722 5.101 1.41 10.376 2.475 15.175 4.596 3.237 1.431 5.942 4.262 8.589 6.777 2.592 2.462 4.77 5.355 7.207 7.987 1.804 1.948 4.557 3.453 5.461 5.723 3.51 8.817 11.581 11.307 19.059 14.735 1.053.483 2.116.963 3.214 1.327 9.172 3.043 13.818 8.587 14.889 18.979.715 6.935 5.607 13.679 9.479 19.987 4.623 7.533 9.175 14.819 9.091 24.116-.023 2.55 1.21 5.111 1.874 8.055-19.861 2.555-39.795 4.296-59.597 9.09l-11.596-23.203c-1.107-2.169-2.526-4.353-4.307-5.975-7.349-6.694-14.863-13.209-22.373-19.723l-17.313-14.669c-2.776-2.245-5.935-4.017-8.92-6.003l11.609-38.185c1.508-5.453 1.739-11.258 2.613-17.336z\"></path><path fill=\"#ec6168\" d=\"M1140.315 379.223c2.464 13.227 4.101 26.459 9.931 38.856 2.089 4.441 1.468 10.083 2.725 15.013.965 3.783 2.9 7.325 4.47 10.946 1.372 3.164 2.716 6.351 4.295 9.413 1.574 3.053 3.449 5.95 5.143 8.943 1.372 2.425 2.882 4.803 3.969 7.355 6.319 14.844 18.473 23.384 32.641 30.212.067 5.121-.501 10.201-.435 15.271l.985 38.117c.151 4.586.616 9.162.868 14.201-7.075-3.104-14.371-6.202-21.007-10.354-7.269-4.548-13.456-10.972-20.983-14.924-8.434-4.428-12.337-11.637-16.1-19.441-3.087-6.401-6.485-12.66-9.35-19.156-2.735-6.201-4.583-12.807-7.496-18.912-4.542-9.518-7.477-19.394-9.272-29.771-.777-4.491-3.694-8.581-4.73-13.064-2.294-9.933-4.121-19.98-5.934-30.017-1.496-8.283-2.699-16.618-4.036-25.335 10.349-2.461 20.704-4.511 31.054-6.582.957-.191 1.887-.515 3.264-.769z\"></path><path fill=\"#e94c28\" d=\"M922 537c-6.003 11.784-11.44 23.81-19.66 34.428-6.345 8.196-11.065 17.635-17.206 26.008-4.339 5.916-9.828 10.992-14.854 16.397-.776.835-1.993 1.279-2.71 2.147-9.439 11.437-22.008 18.427-35.357 24.929-4.219-10.885-6.942-22.155-7.205-33.905l-.514-49.542c7.441-2.893 14.452-5.197 21.334-7.841 1.749-.672 3.101-2.401 4.604-3.681 6.749-5.745 12.845-12.627 20.407-16.944 7.719-4.406 14.391-9.101 18.741-16.889.626-1.122 1.689-2.077 2.729-2.877 7.197-5.533 12.583-12.51 16.906-20.439.68-1.247 2.495-1.876 4.105-2.651 2.835 1.408 5.267 2.892 7.884 3.892 3.904 1.491 4.392 3.922 2.833 7.439-1.47 3.318-2.668 6.756-4.069 10.106-1.247 2.981-.435 5.242 2.413 6.544 2.805 1.282 3.125 3.14 1.813 5.601l-6.907 12.799L922 537z\"></path><path fill=\"#eb5659\" d=\"M1124.995 566c.868 1.396 2.018 2.691 2.559 4.203 4.353 12.185 12.586 22.041 21.084 31.113 7.746 8.269 14.235 17.422 21.675 25.711 6.176 6.881 15.756 10.707 24.174 15.932-6.073 22.316-16.675 42.446-31.058 60.937-1.074-.131-2.025-.199-2.581-.702l-24.462-22.26c-6.726-5.99-8.904-14.546-12.925-22.065-5.594-10.461-10.55-21.33-16.943-31.276-5.345-8.315-6.783-17.383-8.494-26.599-.63-3.394-1.348-6.772-1.738-10.848-.371-6.313-1.029-11.934-1.745-18.052l6.34 4.04 1.288-.675-2.143-15.385 9.454 1.208v-8.545L1124.995 566z\"></path><path fill=\"#f5a02d\" d=\"M1818.568 820.096c-4.224 21.679-15.302 40.442-26.587 58.942-3.167 5.192-9.389 8.582-14.381 12.585l-26.908 21.114c-1.425 1.104-3.042 2.345-4.732 2.662-11.786 2.214-21.99 8.201-32.586 13.273-.705.338-1.624.231-2.824.334a824.35 824.35 0 0 1-8.262-42.708c4.646-2.14 9.353-3.139 13.269-5.47 5.582-3.323 11.318-6.942 15.671-11.652 7.949-8.6 14.423-18.572 22.456-27.081 8.539-9.046 13.867-19.641 18.325-30.922l46.559 8.922z\"></path><path fill=\"#eb5a57\" d=\"M1124.96 565.639c-5.086-4.017-10.208-8.395-15.478-12.901v8.545l-9.454-1.208 2.143 15.385-1.288.675-6.34-4.04c.716 6.118 1.375 11.74 1.745 17.633-4.564-6.051-9.544-11.649-10.663-20.025-.954-7.141-4.892-13.843-7.121-20.863-3.344-10.533-5.421-21.57-9.732-31.669-5.181-12.135-3.506-25.125-6.728-37.355-2.099-7.968-5.317-15.646-7.324-23.632-1.353-5.384-1.47-11.078-2.429-16.909l-3.294-46.689a278.63 278.63 0 0 1 27.57-2.084c2.114 12.378 3.647 24.309 5.479 36.195 1.25 8.111 2.832 16.175 4.422 24.23 1.402 7.103 2.991 14.169 4.55 21.241 1.478 6.706.273 14.002 4.6 20.088 5.401 7.597 7.176 16.518 9.467 25.337 1.953 7.515 5.804 14.253 11.917 19.406.254 10.095 3.355 19.392 7.96 28.639z\"></path><path fill=\"#ea541c\" d=\"M911.651 810.999c-2.511 10.165-5.419 20.146-8.2 30.162-2.503 9.015-7.37 16.277-14.364 22.612-6.108 5.533-10.917 12.475-16.796 18.293-6.942 6.871-14.354 13.24-19.083 22.03-.644 1.196-2.222 1.889-3.705 2.857-2.39-7.921-4.101-15.991-6.566-23.823-5.451-17.323-12.404-33.976-23.414-48.835l21.627-21.095c3.182-3.29 5.532-7.382 8.295-11.083l10.663-14.163c9.528 4.78 18.925 9.848 28.625 14.247 7.324 3.321 15.036 5.785 22.917 8.799z\"></path><path fill=\"#eb5d19\" d=\"M1284.092 191.421c4.557.69 9.107 1.587 13.51 2.957 18.901 5.881 36.844 13.904 54.031 23.767 4.938 2.834 10.923 3.792 16.046 6.37 6.757 3.399 13.224 7.408 19.659 11.405l27.644 17.587c10.723 6.446 19.392 14.748 26.063 25.376 4.299 6.848 9.463 13.147 14.011 19.847 1.254 1.847 1.696 4.246 2.498 6.396l7.441 20.332c-11.685 1.754-23.379 3.133-35.533 4.037-.737-2.093-.995-3.716-1.294-5.33-3.157-17.057-14.048-30.161-23.034-44.146-3.027-4.71-7.786-8.529-12.334-11.993-9.346-7.116-19.004-13.834-28.688-20.491-6.653-4.573-13.311-9.251-20.431-13.002-8.048-4.24-16.479-7.85-24.989-11.091-11.722-4.465-23.673-8.328-35.527-12.449l.927-19.572z\"></path><path fill=\"#eb5e24\" d=\"M1283.09 211.415c11.928 3.699 23.88 7.562 35.602 12.027 8.509 3.241 16.941 6.852 24.989 11.091 7.12 3.751 13.778 8.429 20.431 13.002 9.684 6.657 19.342 13.375 28.688 20.491 4.548 3.463 9.307 7.283 12.334 11.993 8.986 13.985 19.877 27.089 23.034 44.146.299 1.615.557 3.237.836 5.263-13.373-.216-26.749-.839-40.564-1.923-2.935-9.681-4.597-18.92-12.286-26.152-15.577-14.651-30.4-30.102-45.564-45.193-.686-.683-1.626-1.156-2.516-1.584l-47.187-22.615 2.203-20.546z\"></path><path fill=\"#e9511f\" d=\"M913 486.001c-1.29.915-3.105 1.543-3.785 2.791-4.323 7.929-9.709 14.906-16.906 20.439-1.04.8-2.103 1.755-2.729 2.877-4.35 7.788-11.022 12.482-18.741 16.889-7.562 4.317-13.658 11.199-20.407 16.944-1.503 1.28-2.856 3.009-4.604 3.681-6.881 2.643-13.893 4.948-21.262 7.377-.128-11.151.202-22.302.378-33.454.03-1.892-.6-3.795-.456-6.12 13.727-1.755 23.588-9.527 33.278-17.663 2.784-2.337 6.074-4.161 8.529-6.784l29.057-31.86c1.545-1.71 3.418-3.401 4.221-5.459 5.665-14.509 11.49-28.977 16.436-43.736 2.817-8.407 4.074-17.338 6.033-26.032 5.039.714 10.078 1.427 15.536 2.629-.909 8.969-2.31 17.438-3.546 25.931-2.41 16.551-5.84 32.839-11.991 48.461L913 486.001z\"></path><path fill=\"#ea5741\" d=\"M1179.451 903.828c-14.224-5.787-27.726-12.171-37.235-24.849-5.841-7.787-12.09-15.436-19.146-22.099-7.259-6.854-12.136-14.667-15.035-24.049-1.748-5.654-3.938-11.171-6.254-17.033 15.099-4.009 30.213-8.629 44.958-15.533l28.367 36.36c6.09 8.015 13.124 14.75 22.72 18.375-7.404 14.472-13.599 29.412-17.48 45.244-.271 1.106-.382 2.25-.895 3.583z\"></path><path fill=\"#ea522a\" d=\"M913.32 486.141c2.693-7.837 5.694-15.539 8.722-23.231 6.151-15.622 9.581-31.91 11.991-48.461l3.963-25.861c7.582.317 15.168 1.031 22.748 1.797 4.171.421 8.333.928 12.877 1.596-.963 11.836-.398 24.125-4.102 34.953-5.244 15.33-6.794 31.496-12.521 46.578-2.692 7.09-4.849 14.445-8.203 21.206-4.068 8.201-9.311 15.81-13.708 23.86-1.965 3.597-3.154 7.627-4.609 11.492-1.385 3.68-3.666 6.265-8.114 6.89-1.994-1.511-3.624-3.059-5.077-4.44l6.907-12.799c1.313-2.461.993-4.318-1.813-5.601-2.849-1.302-3.66-3.563-2.413-6.544 1.401-3.35 2.599-6.788 4.069-10.106 1.558-3.517 1.071-5.948-2.833-7.439-2.617-1-5.049-2.484-7.884-3.892z\"></path><path fill=\"#eb5e24\" d=\"M376.574 714.118c12.053 6.538 20.723 16.481 29.081 26.814 1.945 2.404 4.537 4.352 7.047 6.218 8.24 6.125 10.544 15.85 14.942 24.299.974 1.871 1.584 3.931 2.376 6.29-7.145 3.719-14.633 6.501-21.386 10.517-9.606 5.713-18.673 12.334-28.425 18.399-3.407-3.73-6.231-7.409-9.335-10.834l-30.989-33.862c11.858-11.593 22.368-24.28 31.055-38.431 1.86-3.031 3.553-6.164 5.632-9.409z\"></path><path fill=\"#e95514\" d=\"M859.962 787.636c-3.409 5.037-6.981 9.745-10.516 14.481-2.763 3.701-5.113 7.792-8.295 11.083-6.885 7.118-14.186 13.834-21.65 20.755-13.222-17.677-29.417-31.711-48.178-42.878-.969-.576-2.068-.934-3.27-1.709 6.28-8.159 12.733-15.993 19.16-23.849 1.459-1.783 2.718-3.738 4.254-5.448l18.336-19.969c4.909 5.34 9.619 10.738 14.081 16.333 9.72 12.19 21.813 21.566 34.847 29.867.411.262.725.674 1.231 1.334z\"></path><path fill=\"#eb5f2d\" d=\"M339.582 762.088l31.293 33.733c3.104 3.425 5.928 7.104 9.024 10.979-12.885 11.619-24.548 24.139-33.899 38.704-.872 1.359-1.56 2.837-2.644 4.428-6.459-4.271-12.974-8.294-18.644-13.278-4.802-4.221-8.722-9.473-12.862-14.412l-17.921-21.896c-.403-.496-.595-1.163-.926-2.105 16.738-10.504 32.58-21.87 46.578-36.154z\"></path><path fill=\"#f28d00\" d=\"M678.388 332.912c1.989-5.104 3.638-10.664 6.876-15.051 8.903-12.064 17.596-24.492 28.013-35.175 11.607-11.904 25.007-22.064 40.507-29.592 4.873 11.636 9.419 23.412 13.67 35.592-5.759 4.084-11.517 7.403-16.594 11.553-4.413 3.607-8.124 8.092-12.023 12.301-5.346 5.772-10.82 11.454-15.782 17.547-3.929 4.824-7.17 10.208-10.716 15.344l-33.95-12.518z\"></path><path fill=\"#f08369\" d=\"M1580.181 771.427c-.191-.803-.322-1.377-.119-1.786 5.389-10.903 9.084-22.666 18.181-31.587 6.223-6.103 11.276-13.385 17.286-19.727 3.117-3.289 6.933-6.105 10.869-8.384 6.572-3.806 13.492-7.009 20.461-10.752 1.773 3.23 3.236 6.803 4.951 10.251l12.234 24.993c-1.367 1.966-2.596 3.293-3.935 4.499-7.845 7.07-16.315 13.564-23.407 21.32-6.971 7.623-12.552 16.517-18.743 24.854l-37.777-13.68z\"></path><path fill=\"#f18b5e\" d=\"M1618.142 785.4c6.007-8.63 11.588-17.524 18.559-25.147 7.092-7.755 15.562-14.249 23.407-21.32 1.338-1.206 2.568-2.534 3.997-4.162l28.996 33.733c1.896 2.205 4.424 3.867 6.66 6.394-6.471 7.492-12.967 14.346-19.403 21.255l-18.407 19.953c-12.958-12.409-27.485-22.567-43.809-30.706z\"></path><path fill=\"#f49c3a\" d=\"M1771.617 811.1c-4.066 11.354-9.394 21.949-17.933 30.995-8.032 8.509-14.507 18.481-22.456 27.081-4.353 4.71-10.089 8.329-15.671 11.652-3.915 2.331-8.623 3.331-13.318 5.069-4.298-9.927-8.255-19.998-12.1-30.743 4.741-4.381 9.924-7.582 13.882-11.904 7.345-8.021 14.094-16.603 20.864-25.131 4.897-6.168 9.428-12.626 14.123-18.955l32.61 11.936z\"></path><path fill=\"#f08000\" d=\"M712.601 345.675c3.283-5.381 6.524-10.765 10.453-15.589 4.962-6.093 10.435-11.774 15.782-17.547 3.899-4.21 7.61-8.695 12.023-12.301 5.078-4.15 10.836-7.469 16.636-11.19a934.12 934.12 0 0 1 23.286 35.848c-4.873 6.234-9.676 11.895-14.63 17.421l-25.195 27.801c-11.713-9.615-24.433-17.645-38.355-24.443z\"></path><path fill=\"#ed6e04\" d=\"M751.11 370.42c8.249-9.565 16.693-18.791 25.041-28.103 4.954-5.526 9.757-11.187 14.765-17.106 7.129 6.226 13.892 13.041 21.189 19.225 5.389 4.567 11.475 8.312 17.53 12.92-5.51 7.863-10.622 15.919-17.254 22.427-8.881 8.716-18.938 16.233-28.49 24.264-5.703-6.587-11.146-13.427-17.193-19.682-4.758-4.921-10.261-9.121-15.587-13.944z\"></path><path fill=\"#ea541c\" d=\"M921.823 385.544c-1.739 9.04-2.995 17.971-5.813 26.378-4.946 14.759-10.771 29.227-16.436 43.736-.804 2.058-2.676 3.749-4.221 5.459l-29.057 31.86c-2.455 2.623-5.745 4.447-8.529 6.784-9.69 8.135-19.551 15.908-33.208 17.237-1.773-9.728-3.147-19.457-4.091-29.6l36.13-16.763c.581-.267 1.046-.812 1.525-1.269 8.033-7.688 16.258-15.19 24.011-23.152 4.35-4.467 9.202-9.144 11.588-14.69 6.638-15.425 15.047-30.299 17.274-47.358 3.536.344 7.072.688 10.829 1.377z\"></path><path fill=\"#f3944d\" d=\"M1738.688 798.998c-4.375 6.495-8.906 12.953-13.803 19.121-6.771 8.528-13.519 17.11-20.864 25.131-3.958 4.322-9.141 7.523-13.925 11.54-8.036-13.464-16.465-26.844-27.999-38.387 5.988-6.951 12.094-13.629 18.261-20.25l19.547-20.95 38.783 23.794z\"></path><path fill=\"#ec6168\" d=\"M1239.583 703.142c3.282 1.805 6.441 3.576 9.217 5.821 5.88 4.755 11.599 9.713 17.313 14.669l22.373 19.723c1.781 1.622 3.2 3.806 4.307 5.975 3.843 7.532 7.477 15.171 11.194 23.136-10.764 4.67-21.532 8.973-32.69 12.982l-22.733-27.366c-2.003-2.416-4.096-4.758-6.194-7.093-3.539-3.94-6.927-8.044-10.74-11.701-2.57-2.465-5.762-4.283-8.675-6.39l16.627-29.755z\"></path><path fill=\"#ec663e\" d=\"M1351.006 332.839l-28.499 10.33c-.294.107-.533.367-1.194.264-11.067-19.018-27.026-32.559-44.225-44.855-4.267-3.051-8.753-5.796-13.138-8.682l9.505-24.505c10.055 4.069 19.821 8.227 29.211 13.108 3.998 2.078 7.299 5.565 10.753 8.598 3.077 2.701 5.743 5.891 8.926 8.447 4.116 3.304 9.787 5.345 12.62 9.432 6.083 8.777 10.778 18.517 16.041 27.863z\"></path><path fill=\"#eb5e5b\" d=\"M1222.647 733.051c3.223 1.954 6.415 3.771 8.985 6.237 3.813 3.658 7.201 7.761 10.74 11.701l6.194 7.093 22.384 27.409c-13.056 6.836-25.309 14.613-36.736 24.161l-39.323-44.7 24.494-27.846c1.072-1.224 1.974-2.598 3.264-4.056z\"></path><path fill=\"#ea580e\" d=\"M876.001 376.171c5.874 1.347 11.748 2.694 17.812 4.789-.81 5.265-2.687 9.791-2.639 14.296.124 11.469-4.458 20.383-12.73 27.863-2.075 1.877-3.659 4.286-5.668 6.248l-22.808 21.967c-.442.422-1.212.488-1.813.757l-23.113 10.389-9.875 4.514c-2.305-6.09-4.609-12.181-6.614-18.676 7.64-4.837 15.567-8.54 22.18-13.873 9.697-7.821 18.931-16.361 27.443-25.455 5.613-5.998 12.679-11.331 14.201-20.475.699-4.2 2.384-8.235 3.623-12.345z\"></path><path fill=\"#e95514\" d=\"M815.103 467.384c3.356-1.894 6.641-3.415 9.94-4.903l23.113-10.389c.6-.269 1.371-.335 1.813-.757l22.808-21.967c2.008-1.962 3.593-4.371 5.668-6.248 8.272-7.48 12.854-16.394 12.73-27.863-.049-4.505 1.828-9.031 2.847-13.956 5.427.559 10.836 1.526 16.609 2.68-1.863 17.245-10.272 32.119-16.91 47.544-2.387 5.546-7.239 10.223-11.588 14.69-7.753 7.962-15.978 15.464-24.011 23.152-.478.458-.944 1.002-1.525 1.269l-36.069 16.355c-2.076-6.402-3.783-12.81-5.425-19.607z\"></path><path fill=\"#eb620b\" d=\"M783.944 404.402c9.499-8.388 19.556-15.905 28.437-24.621 6.631-6.508 11.744-14.564 17.575-22.273 9.271 4.016 18.501 8.375 27.893 13.43-4.134 7.07-8.017 13.778-12.833 19.731-5.785 7.15-12.109 13.917-18.666 20.376-7.99 7.869-16.466 15.244-24.731 22.832l-17.674-29.475z\"></path><path fill=\"#ea544c\" d=\"M1197.986 854.686c-9.756-3.309-16.79-10.044-22.88-18.059l-28.001-36.417c8.601-5.939 17.348-11.563 26.758-17.075 1.615 1.026 2.639 1.876 3.505 2.865l26.664 30.44c3.723 4.139 7.995 7.785 12.017 11.656l-18.064 26.591z\"></path><path fill=\"#ec6333\" d=\"M1351.41 332.903c-5.667-9.409-10.361-19.149-16.445-27.926-2.833-4.087-8.504-6.128-12.62-9.432-3.184-2.555-5.849-5.745-8.926-8.447-3.454-3.033-6.756-6.52-10.753-8.598-9.391-4.88-19.157-9.039-29.138-13.499 1.18-5.441 2.727-10.873 4.81-16.607 11.918 4.674 24.209 8.261 34.464 14.962 14.239 9.304 29.011 18.453 39.595 32.464 2.386 3.159 5.121 6.077 7.884 8.923 6.564 6.764 10.148 14.927 11.723 24.093l-20.594 4.067z\"></path><path fill=\"#eb5e5b\" d=\"M1117 536.549c-6.113-4.702-9.965-11.44-11.917-18.955-2.292-8.819-4.066-17.74-9.467-25.337-4.327-6.085-3.122-13.382-4.6-20.088l-4.55-21.241c-1.59-8.054-3.172-16.118-4.422-24.23l-5.037-36.129c6.382-1.43 12.777-2.462 19.582-3.443 1.906 11.646 3.426 23.24 4.878 34.842.307 2.453.717 4.973.477 7.402-1.86 18.84 2.834 36.934 5.347 55.352 1.474 10.806 4.885 20.848 7.101 31.302 1.394 6.579 1.774 13.374 2.609 20.523z\"></path><path fill=\"#ec644b\" d=\"M1263.638 290.071c4.697 2.713 9.183 5.458 13.45 8.509 17.199 12.295 33.158 25.836 43.873 44.907-8.026 4.725-16.095 9.106-24.83 13.372-11.633-15.937-25.648-28.515-41.888-38.689-1.609-1.008-3.555-1.48-5.344-2.2 2.329-3.852 4.766-7.645 6.959-11.573l7.78-14.326z\"></path><path fill=\"#eb5f2d\" d=\"M1372.453 328.903c-2.025-9.233-5.608-17.396-12.172-24.16-2.762-2.846-5.498-5.764-7.884-8.923-10.584-14.01-25.356-23.16-39.595-32.464-10.256-6.701-22.546-10.289-34.284-15.312.325-5.246 1.005-10.444 2.027-15.863l47.529 22.394c.89.428 1.83.901 2.516 1.584l45.564 45.193c7.69 7.233 9.352 16.472 11.849 26.084-5.032.773-10.066 1.154-15.55 1.466z\"></path><path fill=\"#e95a0f\" d=\"M801.776 434.171c8.108-7.882 16.584-15.257 24.573-23.126 6.558-6.459 12.881-13.226 18.666-20.376 4.817-5.953 8.7-12.661 13.011-19.409 5.739 1.338 11.463 3.051 17.581 4.838-.845 4.183-2.53 8.219-3.229 12.418-1.522 9.144-8.588 14.477-14.201 20.475-8.512 9.094-17.745 17.635-27.443 25.455-6.613 5.333-14.54 9.036-22.223 13.51-2.422-4.469-4.499-8.98-6.735-13.786z\"></path><path fill=\"#eb5e5b\" d=\"M1248.533 316.002c2.155.688 4.101 1.159 5.71 2.168 16.24 10.174 30.255 22.752 41.532 38.727-7.166 5.736-14.641 11.319-22.562 16.731-1.16-1.277-1.684-2.585-2.615-3.46l-38.694-36.2 14.203-15.029c.803-.86 1.38-1.93 2.427-2.936z\"></path><path fill=\"#eb5a57\" d=\"M1216.359 827.958c-4.331-3.733-8.603-7.379-12.326-11.518l-26.664-30.44c-.866-.989-1.89-1.839-3.152-2.902 6.483-6.054 13.276-11.959 20.371-18.005l39.315 44.704c-5.648 6.216-11.441 12.12-17.544 18.161z\"></path><path fill=\"#ec6168\" d=\"M1231.598 334.101l38.999 36.066c.931.876 1.456 2.183 2.303 3.608-4.283 4.279-8.7 8.24-13.769 12.091-4.2-3.051-7.512-6.349-11.338-8.867-12.36-8.136-22.893-18.27-32.841-29.093l16.646-13.805z\"></path><path fill=\"#ed656e\" d=\"M1214.597 347.955c10.303 10.775 20.836 20.908 33.196 29.044 3.825 2.518 7.137 5.816 10.992 8.903-3.171 4.397-6.65 8.648-10.432 13.046-6.785-5.184-13.998-9.858-19.529-16.038-4.946-5.527-9.687-8.644-17.309-8.215-2.616.147-5.734-2.788-8.067-4.923-3.026-2.769-5.497-6.144-8.35-9.568 6.286-4.273 12.715-8.237 19.499-12.25z\"></path></svg>\n</p>\n\n<p align=\"center\">\n<b>The crispy rerank family from <a href=\"https://mixedbread.ai\"><b>Mixedbread</b></a>.</b>\n</p>\n\n<p align=\"center\">\n<sup> \ud83c\udf5e Looking for a simple end-to-end retrieval solution? Meet Omni, our multimodal and multilingual model. <a href=\"https://mixedbread.com\"><b>Get in touch for access.</a> </sup>\n</p>\n\n# mxbai-rerank-base-v1\n\nThis is the base model in our family of powerful reranker models. You can learn more about the models in our [blog post](https://www.mixedbread.ai/blog/mxbai-rerank-v1).\n\nWe have three models:\n\n- [mxbai-rerank-xsmall-v1](https://huggingface.co/mixedbread-ai/mxbai-rerank-xsmall-v1)\n- [mxbai-rerank-base-v1](https://huggingface.co/mixedbread-ai/mxbai-rerank-base-v1) (\ud83c\udf5e)\n- [mxbai-rerank-large-v1](https://huggingface.co/mixedbread-ai/mxbai-rerank-large-v1)\n\n## Quickstart\n\nCurrently, the best way to use our models is with the most recent version of sentence-transformers.\n\n`pip install -U sentence-transformers`\n\nLet's say you have a query, and you want to rerank a set of documents. You can do that with only one line of code:\n\n```python\nfrom sentence_transformers import CrossEncoder\n\n# Load the model, here we use our base sized model\nmodel = CrossEncoder(\"mixedbread-ai/mxbai-rerank-base-v1\")\n\n\n# Example query and documents\nquery = \"Who wrote 'To Kill a Mockingbird'?\"\ndocuments = [\n    \"'To Kill a Mockingbird' is a novel by Harper Lee published in 1960. It was immediately successful, winning the Pulitzer Prize, and has become a classic of modern American literature.\",\n    \"The novel 'Moby-Dick' was written by Herman Melville and first published in 1851. It is considered a masterpiece of American literature and deals with complex themes of obsession, revenge, and the conflict between good and evil.\",\n    \"Harper Lee, an American novelist widely known for her novel 'To Kill a Mockingbird', was born in 1926 in Monroeville, Alabama. She received the Pulitzer Prize for Fiction in 1961.\",\n    \"Jane Austen was an English novelist known primarily for her six major novels, which interpret, critique and comment upon the British landed gentry at the end of the 18th century.\",\n    \"The 'Harry Potter' series, which consists of seven fantasy novels written by British author J.K. Rowling, is among the most popular and critically acclaimed books of the modern era.\",\n    \"'The Great Gatsby', a novel written by American author F. Scott Fitzgerald, was published in 1925. The story is set in the Jazz Age and follows the life of millionaire Jay Gatsby and his pursuit of Daisy Buchanan.\"\n]\n\n# Lets get the scores\nresults = model.rank(query, documents, return_documents=True, top_k=3)\n```\n\n\n<details>\n  <summary>JavaScript Example</summary>\n\nInstall [transformers.js](https://github.com/xenova/transformers.js)\n\n`npm i @xenova/transformers`\n\nLet's say you have a query, and you want to rerank a set of documents. In JavaScript, you need to add a function:\n\n```javascript\nimport { AutoTokenizer, AutoModelForSequenceClassification } from '@xenova/transformers';\n\nconst model_id = 'mixedbread-ai/mxbai-rerank-base-v1';\nconst model = await AutoModelForSequenceClassification.from_pretrained(model_id);\nconst tokenizer = await AutoTokenizer.from_pretrained(model_id);\n\n/**\n * Performs ranking with the CrossEncoder on the given query and documents. Returns a sorted list with the document indices and scores.\n * @param {string} query A single query\n * @param {string[]} documents A list of documents\n * @param {Object} options Options for ranking\n * @param {number} [options.top_k=undefined] Return the top-k documents. If undefined, all documents are returned.\n * @param {number} [options.return_documents=false] If true, also returns the documents. If false, only returns the indices and scores.\n */\nasync function rank(query, documents, {\n    top_k = undefined,\n    return_documents = false,\n} = {}) {\n    const inputs = tokenizer(\n        new Array(documents.length).fill(query),\n        {\n            text_pair: documents,\n            padding: true,\n            truncation: true,\n        }\n    )\n    const { logits } = await model(inputs);\n    return logits\n        .sigmoid()\n        .tolist()\n        .map(([score], i) => ({\n            corpus_id: i,\n            score,\n            ...(return_documents ? { text: documents[i] } : {})\n        }))\n        .sort((a, b) => b.score - a.score)\n        .slice(0, top_k);\n}\n\n// Example usage:\nconst query = \"Who wrote 'To Kill a Mockingbird'?\"\nconst documents = [\n    \"'To Kill a Mockingbird' is a novel by Harper Lee published in 1960. It was immediately successful, winning the Pulitzer Prize, and has become a classic of modern American literature.\",\n    \"The novel 'Moby-Dick' was written by Herman Melville and first published in 1851. It is considered a masterpiece of American literature and deals with complex themes of obsession, revenge, and the conflict between good and evil.\",\n    \"Harper Lee, an American novelist widely known for her novel 'To Kill a Mockingbird', was born in 1926 in Monroeville, Alabama. She received the Pulitzer Prize for Fiction in 1961.\",\n    \"Jane Austen was an English novelist known primarily for her six major novels, which interpret, critique and comment upon the British landed gentry at the end of the 18th century.\",\n    \"The 'Harry Potter' series, which consists of seven fantasy novels written by British author J.K. Rowling, is among the most popular and critically acclaimed books of the modern era.\",\n    \"'The Great Gatsby', a novel written by American author F. Scott Fitzgerald, was published in 1925. The story is set in the Jazz Age and follows the life of millionaire Jay Gatsby and his pursuit of Daisy Buchanan.\"\n]\n\nconst results = await rank(query, documents, { return_documents: true, top_k: 3 });\nconsole.log(results);\n```\n</details>\n\n## Using API\n\nYou can use the large model via our API as follows:\n\n```python\nfrom mixedbread_ai.client import MixedbreadAI\n\nmxbai = MixedbreadAI(api_key=\"{MIXEDBREAD_API_KEY}\")\n\nres = mxbai.reranking(\n  model=\"mixedbread-ai/mxbai-rerank-large-v1\",\n  query=\"Who is the author of To Kill a Mockingbird?\",\n  input=[\n    \"To Kill a Mockingbird is a novel by Harper Lee published in 1960. It was immediately successful, winning the Pulitzer Prize, and has become a classic of modern American literature.\",\n    \"The novel Moby-Dick was written by Herman Melville and first published in 1851. It is considered a masterpiece of American literature and deals with complex themes of obsession, revenge, and the conflict between good and evil.\",\n    \"Harper Lee, an American novelist widely known for her novel To Kill a Mockingbird, was born in 1926 in Monroeville, Alabama. She received the Pulitzer Prize for Fiction in 1961.\",\n    \"Jane Austen was an English novelist known primarily for her six major novels, which interpret, critique and comment upon the British landed gentry at the end of the 18th century.\",\n    \"The Harry Potter series, which consists of seven fantasy novels written by British author J.K. Rowling, is among the most popular and critically acclaimed books of the modern era.\",\n    \"The Great Gatsby, a novel written by American author F. Scott Fitzgerald, was published in 1925. The story is set in the Jazz Age and follows the life of millionaire Jay Gatsby and his pursuit of Daisy Buchanan.\"\n  ],\n  top_k=3,\n  return_input=false\n)\n\nprint(res.data)\n```\n\nThe API comes with additional features, such as a continous trained reranker! Check out the [docs](https://www.mixedbread.ai/docs) for more information.\n\n## Evaluation\n\nOur reranker models are designed to elevate your search. They work extremely well in combination with keyword search and can even outperform semantic search systems in many cases.\n\n| Model                                                                                 | NDCG@10  | Accuracy@3 |\n| ------------------------------------------------------------------------------------- | -------- | ---------- |\n| Lexical Search (Lucene)                                                               | 38.0     | 66.4       |\n| [BAAI/bge-reranker-base](https://huggingface.co/BAAI/bge-reranker-base)               | 41.6     | 66.9       |\n| [BAAI/bge-reranker-large](https://huggingface.co/BAAI/bge-reranker-large)             | 45.2     | 70.6       |\n| cohere-embed-v3 (semantic search)                                                     | 47.5     | 70.9       |\n| [mxbai-rerank-xsmall-v1](https://huggingface.co/mixedbread-ai/mxbai-rerank-xsmall-v1) | **43.9** | **70.0**   |\n| [mxbai-rerank-base-v1](https://huggingface.co/mixedbread-ai/mxbai-rerank-base-v1)     | **46.9** | **72.3**   |\n| [mxbai-rerank-large-v1](https://huggingface.co/mixedbread-ai/mxbai-rerank-large-v1)   | **48.8** | **74.9**   |\n\nThe reported results are aggregated from 11 datasets of BEIR. We used [Pyserini](https://github.com/castorini/pyserini/) to evaluate the models. Find more in our [blog-post](https://www.mixedbread.ai/blog/mxbai-rerank-v1) and on this [spreadsheet](https://docs.google.com/spreadsheets/d/15ELkSMFv-oHa5TRiIjDvhIstH9dlc3pnZeO-iGz4Ld4/edit?usp=sharing).\n\n## Community\nPlease join our [Discord Community](https://discord.gg/jDfMHzAVfU) and share your feedback and thoughts! We are here to help and also always happy to chat.\n\n## Citation\n\n```bibtex\n@online{rerank2024mxbai,\n  title={Boost Your Search With The Crispy Mixedbread Rerank Models},\n  author={Aamir Shakir and Darius Koenig and Julius Lipp and Sean Lee},\n  year={2024},\n  url={https://www.mixedbread.ai/blog/mxbai-rerank-v1},\n}\n```\n\n## License\nApache 2.0",
    "library_name": "transformers"
  },
  {
    "model_id": "michellejieli/emotion_text_classifier",
    "model_name": "michellejieli/emotion_text_classifier",
    "author": "michellejieli",
    "downloads": 872983,
    "likes": 123,
    "tags": [
      "transformers",
      "pytorch",
      "roberta",
      "text-classification",
      "distilroberta",
      "sentiment",
      "emotion",
      "twitter",
      "reddit",
      "en",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/michellejieli/emotion_text_classifier",
    "dependencies": [
      [
        "transformers",
        "4.30.2"
      ],
      [
        "torch",
        "2.0.1"
      ],
      [
        "numpy",
        "1.24.3"
      ],
      [
        "hf_xet",
        "0.1.0"
      ],
      [
        "xformers",
        "0.0.20"
      ]
    ],
    "analysis_date": "2025-03-26T00:47:43.227191",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "roberta",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": "en",
      "tags": [
        "distilroberta",
        "sentiment",
        "emotion",
        "twitter",
        "reddit"
      ],
      "widget": [
        {
          "text": "Oh my God, he's lost it. He's totally lost it."
        },
        {
          "text": "What?"
        },
        {
          "text": "Wow, congratulations! So excited for you!"
        }
      ]
    },
    "card_text": "\n# Fine-tuned DistilRoBERTa-base for Emotion Classification \ud83e\udd2c\ud83e\udd22\ud83d\ude00\ud83d\ude10\ud83d\ude2d\ud83d\ude32\n\n# Model Description \n\nDistilRoBERTa-base is a transformer model that performs sentiment analysis. I fine-tuned the model on transcripts from the Friends show with the goal of classifying emotions from text data, specifically dialogue from Netflix shows or movies. The model predicts 6 Ekman emotions and a neutral class. These emotions include anger, disgust, fear, joy, neutrality, sadness, and surprise.\n\nThe model is a fine-tuned version of [Emotion English DistilRoBERTa-base](https://huggingface.co/j-hartmann/emotion-english-distilroberta-base/) and [DistilRoBERTa-base](https://huggingface.co/j-hartmann/emotion-english-distilroberta-base). This model was initially trained on the following table from [Emotion English DistilRoBERTa-base](https://huggingface.co/j-hartmann/emotion-english-distilroberta-base/):\n\n|Name|anger|disgust|fear|joy|neutral|sadness|surprise|\n|---|---|---|---|---|---|---|---|\n|Crowdflower (2016)|Yes|-|-|Yes|Yes|Yes|Yes|\n|Emotion Dataset, Elvis et al. (2018)|Yes|-|Yes|Yes|-|Yes|Yes|\n|GoEmotions, Demszky et al. (2020)|Yes|Yes|Yes|Yes|Yes|Yes|Yes|\n|ISEAR, Vikash (2018)|Yes|Yes|Yes|Yes|-|Yes|-|\n|MELD, Poria et al. (2019)|Yes|Yes|Yes|Yes|Yes|Yes|Yes|\n|SemEval-2018, EI-reg, Mohammad et al. (2018) |Yes|-|Yes|Yes|-|Yes|-|\n\nIt was fine-tuned on:\n|Name|anger|disgust|fear|joy|neutral|sadness|surprise|\n|---|---|---|---|---|---|---|---|\n|Emotion Lines (Friends)|Yes|Yes|Yes|Yes|Yes|Yes|Yes|\n\n# How to Use \n\n```python\nfrom transformers import pipeline\nclassifier = pipeline(\"sentiment-analysis\", model=\"michellejieli/emotion_text_classifier\")\nclassifier(\"I love this!\")\n```\n\n```python\nOutput:\n[{'label': 'joy', 'score': 0.9887555241584778}]\n```\n\n# Contact\n\nPlease reach out to [michelleli1999@gmail.com](mailto:michelleli1999@gmail.com) if you have any questions or feedback.\n\n\n# Reference\n\n```\nJochen Hartmann, \"Emotion English DistilRoBERTa-base\". https://huggingface.co/j-hartmann/emotion-english-distilroberta-base/, 2022.\nAshritha R Murthy and K M Anil Kumar 2021 IOP Conf. Ser.: Mater. Sci. Eng. 1110 012009\n```",
    "card_content": "---\nlanguage: en\ntags:\n- distilroberta\n- sentiment\n- emotion\n- twitter\n- reddit\nwidget:\n- text: Oh my God, he's lost it. He's totally lost it.\n- text: What?\n- text: Wow, congratulations! So excited for you!\n---\n\n# Fine-tuned DistilRoBERTa-base for Emotion Classification \ud83e\udd2c\ud83e\udd22\ud83d\ude00\ud83d\ude10\ud83d\ude2d\ud83d\ude32\n\n# Model Description \n\nDistilRoBERTa-base is a transformer model that performs sentiment analysis. I fine-tuned the model on transcripts from the Friends show with the goal of classifying emotions from text data, specifically dialogue from Netflix shows or movies. The model predicts 6 Ekman emotions and a neutral class. These emotions include anger, disgust, fear, joy, neutrality, sadness, and surprise.\n\nThe model is a fine-tuned version of [Emotion English DistilRoBERTa-base](https://huggingface.co/j-hartmann/emotion-english-distilroberta-base/) and [DistilRoBERTa-base](https://huggingface.co/j-hartmann/emotion-english-distilroberta-base). This model was initially trained on the following table from [Emotion English DistilRoBERTa-base](https://huggingface.co/j-hartmann/emotion-english-distilroberta-base/):\n\n|Name|anger|disgust|fear|joy|neutral|sadness|surprise|\n|---|---|---|---|---|---|---|---|\n|Crowdflower (2016)|Yes|-|-|Yes|Yes|Yes|Yes|\n|Emotion Dataset, Elvis et al. (2018)|Yes|-|Yes|Yes|-|Yes|Yes|\n|GoEmotions, Demszky et al. (2020)|Yes|Yes|Yes|Yes|Yes|Yes|Yes|\n|ISEAR, Vikash (2018)|Yes|Yes|Yes|Yes|-|Yes|-|\n|MELD, Poria et al. (2019)|Yes|Yes|Yes|Yes|Yes|Yes|Yes|\n|SemEval-2018, EI-reg, Mohammad et al. (2018) |Yes|-|Yes|Yes|-|Yes|-|\n\nIt was fine-tuned on:\n|Name|anger|disgust|fear|joy|neutral|sadness|surprise|\n|---|---|---|---|---|---|---|---|\n|Emotion Lines (Friends)|Yes|Yes|Yes|Yes|Yes|Yes|Yes|\n\n# How to Use \n\n```python\nfrom transformers import pipeline\nclassifier = pipeline(\"sentiment-analysis\", model=\"michellejieli/emotion_text_classifier\")\nclassifier(\"I love this!\")\n```\n\n```python\nOutput:\n[{'label': 'joy', 'score': 0.9887555241584778}]\n```\n\n# Contact\n\nPlease reach out to [michelleli1999@gmail.com](mailto:michelleli1999@gmail.com) if you have any questions or feedback.\n\n\n# Reference\n\n```\nJochen Hartmann, \"Emotion English DistilRoBERTa-base\". https://huggingface.co/j-hartmann/emotion-english-distilroberta-base/, 2022.\nAshritha R Murthy and K M Anil Kumar 2021 IOP Conf. Ser.: Mater. Sci. Eng. 1110 012009\n```",
    "library_name": "transformers"
  },
  {
    "model_id": "BAAI/bge-reranker-large",
    "model_name": "BAAI/bge-reranker-large",
    "author": "BAAI",
    "downloads": 843418,
    "likes": 385,
    "tags": [
      "transformers",
      "pytorch",
      "onnx",
      "safetensors",
      "xlm-roberta",
      "text-classification",
      "mteb",
      "feature-extraction",
      "en",
      "zh",
      "arxiv:2401.03462",
      "arxiv:2312.15503",
      "arxiv:2311.13534",
      "arxiv:2310.07554",
      "arxiv:2309.07597",
      "license:mit",
      "model-index",
      "autotrain_compatible",
      "text-embeddings-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/BAAI/bge-reranker-large",
    "dependencies": null,
    "analysis_date": "2025-03-26T00:47:46.496373",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "xlm-roberta",
    "pipeline_tag": "feature-extraction",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": [
        "en",
        "zh"
      ],
      "license": "mit",
      "tags": [
        "mteb"
      ],
      "pipeline_tag": "feature-extraction",
      "model-index": [
        {
          "name": "bge-reranker-base",
          "results": [
            {
              "task": {
                "type": "Reranking"
              },
              "dataset": {
                "name": "MTEB CMedQAv1",
                "type": "C-MTEB/CMedQAv1-reranking",
                "config": "default",
                "split": "test",
                "revision": "None"
              },
              "metrics": [
                {
                  "type": "map",
                  "value": 81.27206722525007
                },
                {
                  "type": "mrr",
                  "value": 84.14238095238095
                }
              ]
            },
            {
              "task": {
                "type": "Reranking"
              },
              "dataset": {
                "name": "MTEB CMedQAv2",
                "type": "C-MTEB/CMedQAv2-reranking",
                "config": "default",
                "split": "test",
                "revision": "None"
              },
              "metrics": [
                {
                  "type": "map",
                  "value": 84.10369934291236
                },
                {
                  "type": "mrr",
                  "value": 86.79376984126984
                }
              ]
            },
            {
              "task": {
                "type": "Reranking"
              },
              "dataset": {
                "name": "MTEB MMarcoReranking",
                "type": "C-MTEB/Mmarco-reranking",
                "config": "default",
                "split": "dev",
                "revision": "None"
              },
              "metrics": [
                {
                  "type": "map",
                  "value": 35.4600511272538
                },
                {
                  "type": "mrr",
                  "value": 34.60238095238095
                }
              ]
            },
            {
              "task": {
                "type": "Reranking"
              },
              "dataset": {
                "name": "MTEB T2Reranking",
                "type": "C-MTEB/T2Reranking",
                "config": "default",
                "split": "dev",
                "revision": "None"
              },
              "metrics": [
                {
                  "type": "map",
                  "value": 67.27728847727172
                },
                {
                  "type": "mrr",
                  "value": 77.1315192743764
                }
              ]
            }
          ]
        }
      ]
    },
    "card_text": "\n**We have updated the [new reranker](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/llm_reranker), supporting larger lengths, more languages, and achieving better performance.**\n\n<h1 align=\"center\">FlagEmbedding</h1>\n\n\n<h4 align=\"center\">\n    <p>\n        <a href=#model-list>Model List</a> | \n        <a href=#frequently-asked-questions>FAQ</a> |\n        <a href=#usage>Usage</a>  |\n        <a href=\"#evaluation\">Evaluation</a> |\n        <a href=\"#train\">Train</a> |\n        <a href=\"#citation\">Citation</a> |\n        <a href=\"#license\">License</a> \n    <p>\n</h4>\n\n**More details please refer to our Github: [FlagEmbedding](https://github.com/FlagOpen/FlagEmbedding).**\n\n\n[English](README.md) | [\u4e2d\u6587](https://github.com/FlagOpen/FlagEmbedding/blob/master/README_zh.md)\n\n\nFlagEmbedding focuses on retrieval-augmented LLMs, consisting of the following projects currently:\n\n- **Long-Context LLM**: [Activation Beacon](https://github.com/FlagOpen/FlagEmbedding/tree/master/Long_LLM/activation_beacon)\n- **Fine-tuning of LM** : [LM-Cocktail](https://github.com/FlagOpen/FlagEmbedding/tree/master/LM_Cocktail)\n- **Embedding Model**: [Visualized-BGE](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/visual), [BGE-M3](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/BGE_M3), [LLM Embedder](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/llm_embedder), [BGE Embedding](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/baai_general_embedding)\n- **Reranker Model**: [llm rerankers](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/llm_reranker), [BGE Reranker](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/reranker)\n- **Benchmark**: [C-MTEB](https://github.com/FlagOpen/FlagEmbedding/tree/master/C_MTEB)\n\n## News \n- 3/18/2024: Release new [rerankers](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/llm_reranker), built upon powerful M3 and LLM (GEMMA and MiniCPM, not so large actually) backbones, supporitng multi-lingual processing and larger inputs, massive improvements of ranking performances on BEIR, C-MTEB/Retrieval, MIRACL, LlamaIndex Evaluation.\n- 3/18/2024: Release [Visualized-BGE](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/visual), equipping BGE with visual capabilities. Visualized-BGE can be utilized to generate embeddings for hybrid image-text data.\n- 1/30/2024: Release **BGE-M3**, a new member to BGE model series! M3 stands for **M**ulti-linguality (100+ languages), **M**ulti-granularities (input length up to 8192), **M**ulti-Functionality (unification of dense, lexical, multi-vec/colbert retrieval). \nIt is the first embedding model which supports all three retrieval methods, achieving new SOTA on multi-lingual (MIRACL) and cross-lingual (MKQA) benchmarks.\n[Technical Report](https://github.com/FlagOpen/FlagEmbedding/blob/master/FlagEmbedding/BGE_M3/BGE_M3.pdf) and [Code](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/BGE_M3). :fire:\n- 1/9/2024: Release [Activation-Beacon](https://github.com/FlagOpen/FlagEmbedding/tree/master/Long_LLM/activation_beacon), an effective, efficient, compatible, and low-cost (training) method to extend the context length of LLM. [Technical Report](https://arxiv.org/abs/2401.03462) :fire:\n- 12/24/2023: Release **LLaRA**, a LLaMA-7B based dense retriever, leading to state-of-the-art performances on MS MARCO and BEIR. Model and code will be open-sourced. Please stay tuned. [Technical Report](https://arxiv.org/abs/2312.15503)\n- 11/23/2023: Release [LM-Cocktail](https://github.com/FlagOpen/FlagEmbedding/tree/master/LM_Cocktail), a method to maintain general capabilities during fine-tuning by merging multiple language models. [Technical Report](https://arxiv.org/abs/2311.13534) :fire:  \n- 10/12/2023: Release [LLM-Embedder](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/llm_embedder), a unified embedding model to support diverse retrieval augmentation needs for LLMs. [Technical Report](https://arxiv.org/pdf/2310.07554.pdf)\n- 09/15/2023: The [technical report](https://arxiv.org/pdf/2309.07597.pdf) of BGE has been released \n- 09/15/2023: The [massive training data](https://data.baai.ac.cn/details/BAAI-MTP) of BGE has been released \n- 09/12/2023: New models: \n    - **New reranker model**: release cross-encoder models `BAAI/bge-reranker-base` and `BAAI/bge-reranker-large`, which are more powerful than embedding model. We recommend to use/fine-tune them to re-rank top-k documents returned by embedding models. \n    - **update embedding model**: release `bge-*-v1.5` embedding model to alleviate the issue of the similarity distribution, and enhance its retrieval ability without instruction.\n \n\n<details>\n  <summary>More</summary>\n<!-- ### More -->\n    \n- 09/07/2023: Update [fine-tune code](https://github.com/FlagOpen/FlagEmbedding/blob/master/FlagEmbedding/baai_general_embedding/README.md): Add script to mine hard negatives and support adding instruction during fine-tuning. \n- 08/09/2023: BGE Models are integrated into **Langchain**, you can use it like [this](#using-langchain); C-MTEB **leaderboard** is [available](https://huggingface.co/spaces/mteb/leaderboard).  \n- 08/05/2023: Release base-scale and small-scale models, **best performance among the models of the same size \ud83e\udd17**  \n- 08/02/2023: Release `bge-large-*`(short for BAAI General Embedding) Models, **rank 1st on MTEB and C-MTEB benchmark!** :tada: :tada:   \n- 08/01/2023: We release the [Chinese Massive Text Embedding Benchmark](https://github.com/FlagOpen/FlagEmbedding/blob/master/C_MTEB) (**C-MTEB**), consisting of 31 test dataset.  \n  \n</details>\n\n\n## Model List\n\n`bge` is short for `BAAI general embedding`.\n\n|              Model              | Language | | Description | query instruction for retrieval [1] |\n|:-------------------------------|:--------:| :--------:| :--------:|:--------:|\n| [BAAI/bge-m3](https://huggingface.co/BAAI/bge-m3)                   |    Multilingual     |    [Inference](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/BGE_M3#usage) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/BGE_M3)    | Multi-Functionality(dense retrieval, sparse retrieval, multi-vector(colbert)), Multi-Linguality, and Multi-Granularity(8192 tokens) |  |\n|  [BAAI/llm-embedder](https://huggingface.co/BAAI/llm-embedder)  |   English | [Inference](./FlagEmbedding/llm_embedder/README.md) [Fine-tune](./FlagEmbedding/llm_embedder/README.md) | a unified embedding model to support diverse retrieval augmentation needs for LLMs | See [README](./FlagEmbedding/llm_embedder/README.md) |\n|  [BAAI/bge-reranker-large](https://huggingface.co/BAAI/bge-reranker-large)  |   Chinese and English | [Inference](#usage-for-reranker) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/reranker) | a cross-encoder model which is more accurate but less efficient [2] |   |\n|  [BAAI/bge-reranker-base](https://huggingface.co/BAAI/bge-reranker-base) |   Chinese and English | [Inference](#usage-for-reranker) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/reranker) | a cross-encoder model which is more accurate but less efficient [2] |   |\n|  [BAAI/bge-large-en-v1.5](https://huggingface.co/BAAI/bge-large-en-v1.5) |   English | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | version 1.5 with more reasonable similarity distribution | `Represent this sentence for searching relevant passages: `  |\n|  [BAAI/bge-base-en-v1.5](https://huggingface.co/BAAI/bge-base-en-v1.5) |   English | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | version 1.5 with more reasonable similarity distribution | `Represent this sentence for searching relevant passages: `  |\n|  [BAAI/bge-small-en-v1.5](https://huggingface.co/BAAI/bge-small-en-v1.5) |   English | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | version 1.5 with more reasonable similarity distribution  | `Represent this sentence for searching relevant passages: `  |\n|  [BAAI/bge-large-zh-v1.5](https://huggingface.co/BAAI/bge-large-zh-v1.5) |   Chinese | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | version 1.5 with more reasonable similarity distribution | `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`  |\n|  [BAAI/bge-base-zh-v1.5](https://huggingface.co/BAAI/bge-base-zh-v1.5) |   Chinese |  [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | version 1.5 with more reasonable similarity distribution | `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`  |\n|  [BAAI/bge-small-zh-v1.5](https://huggingface.co/BAAI/bge-small-zh-v1.5) |   Chinese | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | version 1.5 with more reasonable similarity distribution | `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`  |\n|  [BAAI/bge-large-en](https://huggingface.co/BAAI/bge-large-en) |   English | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | :trophy: rank **1st** in [MTEB](https://huggingface.co/spaces/mteb/leaderboard) leaderboard | `Represent this sentence for searching relevant passages: `  |\n|  [BAAI/bge-base-en](https://huggingface.co/BAAI/bge-base-en) |   English | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | a base-scale model but with similar ability to `bge-large-en` | `Represent this sentence for searching relevant passages: `  |\n|  [BAAI/bge-small-en](https://huggingface.co/BAAI/bge-small-en) |   English | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) |a small-scale model but with competitive performance  | `Represent this sentence for searching relevant passages: `  |\n|  [BAAI/bge-large-zh](https://huggingface.co/BAAI/bge-large-zh) |   Chinese | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | :trophy: rank **1st** in [C-MTEB](https://github.com/FlagOpen/FlagEmbedding/tree/master/C_MTEB) benchmark | `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`  |\n|  [BAAI/bge-base-zh](https://huggingface.co/BAAI/bge-base-zh) |   Chinese |  [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | a base-scale model but with similar ability to `bge-large-zh` | `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`  |\n|  [BAAI/bge-small-zh](https://huggingface.co/BAAI/bge-small-zh) |   Chinese | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | a small-scale model but with competitive performance | `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`  |\n\n\n[1\\]: If you need to search the relevant passages to a query, we suggest to add the instruction to the query; in other cases, no instruction is needed, just use the original query directly. In all cases, **no instruction** needs to be added to passages.\n\n[2\\]: Different from embedding model, reranker uses question and document as input and directly output similarity instead of embedding. To balance the accuracy and time cost, cross-encoder is widely used to re-rank top-k documents retrieved by other simple models. \nFor examples, use bge embedding model to retrieve top 100 relevant documents, and then use bge reranker to re-rank the top 100 document to get the final top-3 results.\n\nAll models have been uploaded to Huggingface Hub, and you can see them at https://huggingface.co/BAAI. \nIf you cannot open the Huggingface Hub, you also can download the models at https://model.baai.ac.cn/models .\n\n\n## Frequently asked questions\n\n<details>\n  <summary>1. How to fine-tune bge embedding model?</summary>\n\n  <!-- ### How to fine-tune bge embedding model? -->\nFollowing this [example](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) to prepare data and fine-tune your model. \nSome suggestions:\n- Mine hard negatives following this [example](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune#hard-negatives), which can improve the retrieval performance.\n- If you pre-train bge on your data, the pre-trained model cannot be directly used to calculate similarity, and it must be fine-tuned with contrastive learning before computing similarity.\n- If the accuracy of the fine-tuned model is still not high, it is recommended to use/fine-tune the cross-encoder model (bge-reranker) to re-rank top-k results.\n  Hard negatives also are needed to fine-tune reranker. Refer to this [example](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/reranker) for the fine-tuning for reranker\n\n  \n</details>\n\n<details>\n  <summary>2. The similarity score between two dissimilar sentences is higher than 0.5</summary>\n\n  <!-- ### The similarity score between two dissimilar sentences is higher than 0.5 -->\n**Suggest to use bge v1.5, which alleviates the issue of the similarity distribution.** \n\nSince we finetune the models by contrastive learning with a temperature of 0.01, \nthe similarity distribution of the current BGE model is about in the interval \\[0.6, 1\\].\nSo a similarity score greater than 0.5 does not indicate that the two sentences are similar.\n\nFor downstream tasks, such as passage retrieval or semantic similarity, \n**what matters is the relative order of the scores, not the absolute value.**\nIf you need to filter similar sentences based on a similarity threshold, \nplease select an appropriate similarity threshold based on the similarity distribution on your data (such as 0.8, 0.85, or even 0.9).\n\n</details>\n\n<details>\n  <summary>3. When does the query instruction need to be used</summary>\n\n  <!-- ### When does the query instruction need to be used -->\n\nFor the `bge-*-v1.5`, we improve its retrieval ability when not using instruction. \nNo instruction only has a slight degradation in retrieval performance compared with using instruction. \nSo you can generate embedding without instruction in all cases for convenience.\n \nFor a retrieval task that uses short queries to find long related documents, \nit is recommended to add instructions for these short queries.\n**The best method to decide whether to add instructions for queries is choosing the setting that achieves better performance on your task.**\nIn all cases, the documents/passages do not need to add the instruction. \n\n</details>\n\n\n## Usage \n\n### Usage for Embedding Model\n\nHere are some examples for using `bge` models with \n[FlagEmbedding](#using-flagembedding), [Sentence-Transformers](#using-sentence-transformers), [Langchain](#using-langchain), or [Huggingface Transformers](#using-huggingface-transformers).\n\n#### Using FlagEmbedding\n```\npip install -U FlagEmbedding\n```\nIf it doesn't work for you, you can see [FlagEmbedding](https://github.com/FlagOpen/FlagEmbedding/blob/master/FlagEmbedding/baai_general_embedding/README.md) for more methods to install FlagEmbedding.\n\n```python\nfrom FlagEmbedding import FlagModel\nsentences_1 = [\"\u6837\u4f8b\u6570\u636e-1\", \"\u6837\u4f8b\u6570\u636e-2\"]\nsentences_2 = [\"\u6837\u4f8b\u6570\u636e-3\", \"\u6837\u4f8b\u6570\u636e-4\"]\nmodel = FlagModel('BAAI/bge-large-zh-v1.5', \n                  query_instruction_for_retrieval=\"\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a\",\n                  use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\nembeddings_1 = model.encode(sentences_1)\nembeddings_2 = model.encode(sentences_2)\nsimilarity = embeddings_1 @ embeddings_2.T\nprint(similarity)\n\n# for s2p(short query to long passage) retrieval task, suggest to use encode_queries() which will automatically add the instruction to each query\n# corpus in retrieval task can still use encode() or encode_corpus(), since they don't need instruction\nqueries = ['query_1', 'query_2']\npassages = [\"\u6837\u4f8b\u6587\u6863-1\", \"\u6837\u4f8b\u6587\u6863-2\"]\nq_embeddings = model.encode_queries(queries)\np_embeddings = model.encode(passages)\nscores = q_embeddings @ p_embeddings.T\n```\nFor the value of the argument `query_instruction_for_retrieval`, see [Model List](https://github.com/FlagOpen/FlagEmbedding/tree/master#model-list). \n\nBy default, FlagModel will use all available GPUs when encoding. Please set `os.environ[\"CUDA_VISIBLE_DEVICES\"]` to select specific GPUs.\nYou also can set `os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\"` to make all GPUs unavailable.\n\n\n#### Using Sentence-Transformers\n\nYou can also use the `bge` models with [sentence-transformers](https://www.SBERT.net):\n\n```\npip install -U sentence-transformers\n```\n```python\nfrom sentence_transformers import SentenceTransformer\nsentences_1 = [\"\u6837\u4f8b\u6570\u636e-1\", \"\u6837\u4f8b\u6570\u636e-2\"]\nsentences_2 = [\"\u6837\u4f8b\u6570\u636e-3\", \"\u6837\u4f8b\u6570\u636e-4\"]\nmodel = SentenceTransformer('BAAI/bge-large-zh-v1.5')\nembeddings_1 = model.encode(sentences_1, normalize_embeddings=True)\nembeddings_2 = model.encode(sentences_2, normalize_embeddings=True)\nsimilarity = embeddings_1 @ embeddings_2.T\nprint(similarity)\n```\nFor s2p(short query to long passage) retrieval task, \neach short query should start with an instruction (instructions see [Model List](https://github.com/FlagOpen/FlagEmbedding/tree/master#model-list)). \nBut the instruction is not needed for passages.\n```python\nfrom sentence_transformers import SentenceTransformer\nqueries = ['query_1', 'query_2']\npassages = [\"\u6837\u4f8b\u6587\u6863-1\", \"\u6837\u4f8b\u6587\u6863-2\"]\ninstruction = \"\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a\"\n\nmodel = SentenceTransformer('BAAI/bge-large-zh-v1.5')\nq_embeddings = model.encode([instruction+q for q in queries], normalize_embeddings=True)\np_embeddings = model.encode(passages, normalize_embeddings=True)\nscores = q_embeddings @ p_embeddings.T\n```\n\n#### Using Langchain \n\nYou can use `bge` in langchain like this:\n```python\nfrom langchain.embeddings import HuggingFaceBgeEmbeddings\nmodel_name = \"BAAI/bge-large-en-v1.5\"\nmodel_kwargs = {'device': 'cuda'}\nencode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\nmodel = HuggingFaceBgeEmbeddings(\n    model_name=model_name,\n    model_kwargs=model_kwargs,\n    encode_kwargs=encode_kwargs,\n    query_instruction=\"\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a\"\n)\nmodel.query_instruction = \"\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a\"\n```\n\n\n#### Using HuggingFace Transformers\n\nWith the transformers package, you can use the model like this: First, you pass your input through the transformer model, then you select the last hidden state of the first token (i.e., [CLS]) as the sentence embedding.\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\n# Sentences we want sentence embeddings for\nsentences = [\"\u6837\u4f8b\u6570\u636e-1\", \"\u6837\u4f8b\u6570\u636e-2\"]\n\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained('BAAI/bge-large-zh-v1.5')\nmodel = AutoModel.from_pretrained('BAAI/bge-large-zh-v1.5')\nmodel.eval()\n\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n# for s2p(short query to long passage) retrieval task, add an instruction to query (not add instruction for passages)\n# encoded_input = tokenizer([instruction + q for q in queries], padding=True, truncation=True, return_tensors='pt')\n\n# Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n    # Perform pooling. In this case, cls pooling.\n    sentence_embeddings = model_output[0][:, 0]\n# normalize embeddings\nsentence_embeddings = torch.nn.functional.normalize(sentence_embeddings, p=2, dim=1)\nprint(\"Sentence embeddings:\", sentence_embeddings)\n```\n\n### Usage for Reranker\n\nDifferent from embedding model, reranker uses question and document as input and directly output similarity instead of embedding. \nYou can get a relevance score by inputting query and passage to the reranker. \nThe reranker is optimized based cross-entropy loss, so the relevance score is not bounded to a specific range.\n\n\n#### Using FlagEmbedding\n```\npip install -U FlagEmbedding\n```\n\nGet relevance scores (higher scores indicate more relevance):\n```python\nfrom FlagEmbedding import FlagReranker\nreranker = FlagReranker('BAAI/bge-reranker-large', use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\n\nscore = reranker.compute_score(['query', 'passage'])\nprint(score)\n\nscores = reranker.compute_score([['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']])\nprint(scores)\n```\n\n\n#### Using Huggingface transformers\n\n```python\nimport torch\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained('BAAI/bge-reranker-large')\nmodel = AutoModelForSequenceClassification.from_pretrained('BAAI/bge-reranker-large')\nmodel.eval()\n\npairs = [['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']]\nwith torch.no_grad():\n    inputs = tokenizer(pairs, padding=True, truncation=True, return_tensors='pt', max_length=512)\n    scores = model(**inputs, return_dict=True).logits.view(-1, ).float()\n    print(scores)\n```\n\n#### Usage reranker with the ONNX files\n\n```python\nfrom optimum.onnxruntime import ORTModelForSequenceClassification  # type: ignore\n\nimport torch\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained('BAAI/bge-reranker-large')\nmodel = AutoModelForSequenceClassification.from_pretrained('BAAI/bge-reranker-base')\nmodel_ort = ORTModelForSequenceClassification.from_pretrained('BAAI/bge-reranker-base', file_name=\"onnx/model.onnx\")\n\n# Sentences we want sentence embeddings for\npairs = [['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']]\n\n# Tokenize sentences\nencoded_input = tokenizer(pairs, padding=True, truncation=True, return_tensors='pt')\n\nscores_ort = model_ort(**encoded_input, return_dict=True).logits.view(-1, ).float()\n# Compute token embeddings\nwith torch.inference_mode():\n    scores = model_ort(**encoded_input, return_dict=True).logits.view(-1, ).float()\n\n# scores and scores_ort are identical\n```\n#### Usage reranker with infinity\n\nIts also possible to deploy the onnx/torch files with the [infinity_emb](https://github.com/michaelfeil/infinity) pip package.\n```python\nimport asyncio\nfrom infinity_emb import AsyncEmbeddingEngine, EngineArgs\n\nquery='what is a panda?'\ndocs = ['The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear', \"Paris is in France.\"]\n\nengine = AsyncEmbeddingEngine.from_args(\n    EngineArgs(model_name_or_path = \"BAAI/bge-reranker-base\", device=\"cpu\", engine=\"torch\" # or engine=\"optimum\" for onnx\n))\n\nasync def main(): \n    async with engine:\n        ranking, usage = await engine.rerank(query=query, docs=docs)\n        print(list(zip(ranking, docs)))\nasyncio.run(main())\n```\n\n## Evaluation  \n\n`baai-general-embedding` models achieve **state-of-the-art performance on both MTEB and C-MTEB leaderboard!**\nFor more details and evaluation tools see our [scripts](https://github.com/FlagOpen/FlagEmbedding/blob/master/C_MTEB/README.md). \n\n- **MTEB**:   \n\n| Model Name |  Dimension | Sequence Length | Average (56) | Retrieval (15) |Clustering (11) | Pair Classification (3) | Reranking (4) |  STS (10) | Summarization (1) | Classification (12) |\n|:----:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n| [BAAI/bge-large-en-v1.5](https://huggingface.co/BAAI/bge-large-en-v1.5) | 1024 | 512 |  **64.23** | **54.29** |  46.08 | 87.12 | 60.03 | 83.11 | 31.61 | 75.97 |  \n| [BAAI/bge-base-en-v1.5](https://huggingface.co/BAAI/bge-base-en-v1.5) |  768 | 512 | 63.55 | 53.25 |   45.77 | 86.55 | 58.86 | 82.4 | 31.07 | 75.53 |  \n| [BAAI/bge-small-en-v1.5](https://huggingface.co/BAAI/bge-small-en-v1.5) |  384 | 512 | 62.17 |51.68 | 43.82 |  84.92 | 58.36 | 81.59 | 30.12 | 74.14 |  \n| [bge-large-en](https://huggingface.co/BAAI/bge-large-en) |  1024 | 512 | 63.98 |  53.9 | 46.98 | 85.8 | 59.48 | 81.56 | 32.06 | 76.21 | \n| [bge-base-en](https://huggingface.co/BAAI/bge-base-en) |  768 | 512 |  63.36 | 53.0 | 46.32 | 85.86 | 58.7 | 81.84 | 29.27 | 75.27 | \n| [gte-large](https://huggingface.co/thenlper/gte-large) |  1024 | 512 | 63.13 | 52.22 | 46.84 | 85.00 | 59.13 | 83.35 | 31.66 | 73.33 |\n| [gte-base](https://huggingface.co/thenlper/gte-base) \t|  768 | 512 | 62.39 | 51.14 | 46.2 | 84.57 | 58.61 | 82.3 | 31.17 | 73.01 |\n| [e5-large-v2](https://huggingface.co/intfloat/e5-large-v2) |  1024| 512 | 62.25 | 50.56 | 44.49 | 86.03 | 56.61 | 82.05 | 30.19 | 75.24 |\n| [bge-small-en](https://huggingface.co/BAAI/bge-small-en) |  384 | 512 | 62.11 |  51.82 | 44.31 | 83.78 | 57.97 | 80.72 | 30.53 | 74.37 |  \n| [instructor-xl](https://huggingface.co/hkunlp/instructor-xl) |  768 | 512 | 61.79 | 49.26 | 44.74 | 86.62 | 57.29 | 83.06 | 32.32 | 61.79 |\n| [e5-base-v2](https://huggingface.co/intfloat/e5-base-v2) |  768 | 512 | 61.5 | 50.29 | 43.80 | 85.73 | 55.91 | 81.05 | 30.28 | 73.84 |\n| [gte-small](https://huggingface.co/thenlper/gte-small) |  384 | 512 | 61.36 | 49.46 | 44.89 | 83.54 | 57.7 | 82.07 | 30.42 | 72.31 |\n| [text-embedding-ada-002](https://platform.openai.com/docs/guides/embeddings) | 1536 | 8192 | 60.99 | 49.25 | 45.9 | 84.89 | 56.32 | 80.97 | 30.8 | 70.93 |\n| [e5-small-v2](https://huggingface.co/intfloat/e5-base-v2) | 384 | 512 | 59.93 | 49.04 | 39.92 | 84.67 | 54.32 | 80.39 | 31.16 | 72.94 |\n| [sentence-t5-xxl](https://huggingface.co/sentence-transformers/sentence-t5-xxl) |  768 | 512 | 59.51 | 42.24 | 43.72 | 85.06 | 56.42 | 82.63 | 30.08 | 73.42 |\n| [all-mpnet-base-v2](https://huggingface.co/sentence-transformers/all-mpnet-base-v2) \t|  768 | 514 \t| 57.78 | 43.81 | 43.69 | 83.04 | 59.36 | 80.28 | 27.49 | 65.07 |\n| [sgpt-bloom-7b1-msmarco](https://huggingface.co/bigscience/sgpt-bloom-7b1-msmarco) \t|  4096 | 2048 | 57.59 | 48.22 | 38.93 | 81.9 | 55.65 | 77.74 | 33.6 | 66.19 |\n\n\n\n- **C-MTEB**:  \nWe create the benchmark C-MTEB for Chinese text embedding which consists of 31 datasets from 6 tasks. \nPlease refer to [C_MTEB](https://github.com/FlagOpen/FlagEmbedding/blob/master/C_MTEB/README.md) for a detailed introduction.\n \n| Model | Embedding dimension | Avg | Retrieval | STS | PairClassification | Classification | Reranking | Clustering |\n|:-------------------------------|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|\n| [**BAAI/bge-large-zh-v1.5**](https://huggingface.co/BAAI/bge-large-zh-v1.5) | 1024 |  **64.53** | 70.46 | 56.25 | 81.6 | 69.13 | 65.84 | 48.99 |  \n| [BAAI/bge-base-zh-v1.5](https://huggingface.co/BAAI/bge-base-zh-v1.5) | 768 |  63.13 | 69.49 | 53.72 | 79.75 | 68.07 | 65.39 | 47.53 |  \n| [BAAI/bge-small-zh-v1.5](https://huggingface.co/BAAI/bge-small-zh-v1.5) | 512 | 57.82 | 61.77 | 49.11 | 70.41 | 63.96 | 60.92 | 44.18 |   \n| [BAAI/bge-large-zh](https://huggingface.co/BAAI/bge-large-zh) | 1024 | 64.20 | 71.53 | 54.98 | 78.94 | 68.32 | 65.11 | 48.39 |\n| [bge-large-zh-noinstruct](https://huggingface.co/BAAI/bge-large-zh-noinstruct) | 1024 | 63.53 | 70.55 | 53 | 76.77 | 68.58 | 64.91 | 50.01 |\n| [BAAI/bge-base-zh](https://huggingface.co/BAAI/bge-base-zh) | 768 | 62.96 | 69.53 | 54.12 | 77.5 | 67.07 | 64.91 | 47.63 |\n| [multilingual-e5-large](https://huggingface.co/intfloat/multilingual-e5-large) | 1024 | 58.79 | 63.66 | 48.44 | 69.89 | 67.34 | 56.00 | 48.23 |\n| [BAAI/bge-small-zh](https://huggingface.co/BAAI/bge-small-zh) | 512 | 58.27 |  63.07 | 49.45 | 70.35 | 63.64 | 61.48 | 45.09 |\n| [m3e-base](https://huggingface.co/moka-ai/m3e-base) | 768 | 57.10 | 56.91 | 50.47 | 63.99 | 67.52 | 59.34 | 47.68 |\n| [m3e-large](https://huggingface.co/moka-ai/m3e-large) | 1024 |  57.05 | 54.75 | 50.42 | 64.3 | 68.2 | 59.66 | 48.88 |\n| [multilingual-e5-base](https://huggingface.co/intfloat/multilingual-e5-base) | 768 | 55.48 | 61.63 | 46.49 | 67.07 | 65.35 | 54.35 | 40.68 |\n| [multilingual-e5-small](https://huggingface.co/intfloat/multilingual-e5-small) | 384 | 55.38 | 59.95 | 45.27 | 66.45 | 65.85 | 53.86 | 45.26 |\n| [text-embedding-ada-002(OpenAI)](https://platform.openai.com/docs/guides/embeddings/what-are-embeddings) | 1536 |  53.02 | 52.0 | 43.35 | 69.56 | 64.31 | 54.28 | 45.68 |\n| [luotuo](https://huggingface.co/silk-road/luotuo-bert-medium) | 1024 | 49.37 |  44.4 | 42.78 | 66.62 | 61 | 49.25 | 44.39 |\n| [text2vec-base](https://huggingface.co/shibing624/text2vec-base-chinese) | 768 |  47.63 | 38.79 | 43.41 | 67.41 | 62.19 | 49.45 | 37.66 |\n| [text2vec-large](https://huggingface.co/GanymedeNil/text2vec-large-chinese) | 1024 | 47.36 | 41.94 | 44.97 | 70.86 | 60.66 | 49.16 | 30.02 |\n\n\n- **Reranking**:\nSee [C_MTEB](https://github.com/FlagOpen/FlagEmbedding/blob/master/C_MTEB/) for evaluation script.\n\n| Model | T2Reranking | T2RerankingZh2En\\* | T2RerankingEn2Zh\\* | MMarcoReranking | CMedQAv1 | CMedQAv2 | Avg |  \n|:-------------------------------|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|  \n| text2vec-base-multilingual | 64.66 | 62.94 | 62.51 | 14.37 | 48.46 | 48.6 | 50.26 |  \n| multilingual-e5-small | 65.62 | 60.94 | 56.41 | 29.91 | 67.26 | 66.54 | 57.78 |  \n| multilingual-e5-large | 64.55 | 61.61 | 54.28 | 28.6 | 67.42 | 67.92 | 57.4 |  \n| multilingual-e5-base | 64.21 | 62.13 | 54.68 | 29.5 | 66.23 | 66.98 | 57.29 |  \n| m3e-base | 66.03 | 62.74 | 56.07 | 17.51 | 77.05 | 76.76 | 59.36 |  \n| m3e-large | 66.13 | 62.72 | 56.1 | 16.46 | 77.76 | 78.27 | 59.57 |  \n| bge-base-zh-v1.5 | 66.49 | 63.25 | 57.02 | 29.74 | 80.47 | 84.88 | 63.64 |  \n| bge-large-zh-v1.5 | 65.74 | 63.39 | 57.03 | 28.74 | 83.45 | 85.44 | 63.97 |  \n| [BAAI/bge-reranker-base](https://huggingface.co/BAAI/bge-reranker-base) | 67.28 | 63.95 | 60.45 | 35.46 | 81.26 | 84.1 | 65.42 |  \n| [BAAI/bge-reranker-large](https://huggingface.co/BAAI/bge-reranker-large) | 67.6 | 64.03 | 61.44 | 37.16 | 82.15 | 84.18 | 66.09 |  \n\n\\* : T2RerankingZh2En and T2RerankingEn2Zh are cross-language retrieval tasks\n\n## Train\n\n### BAAI Embedding \n\nWe pre-train the models using [retromae](https://github.com/staoxiao/RetroMAE) and train them on large-scale pairs data using contrastive learning. \n**You can fine-tune the embedding model on your data following our [examples](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune).**\nWe also provide a [pre-train example](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/pretrain).\nNote that the goal of pre-training is to reconstruct the text, and the pre-trained model cannot be used for similarity calculation directly, it needs to be fine-tuned.\nMore training details for bge see [baai_general_embedding](https://github.com/FlagOpen/FlagEmbedding/blob/master/FlagEmbedding/baai_general_embedding/README.md).\n\n\n\n### BGE Reranker\n\nCross-encoder will perform full-attention over the input pair, \nwhich is more accurate than embedding model (i.e., bi-encoder) but more time-consuming than embedding model.\nTherefore, it can be used to re-rank the top-k documents returned by embedding model.\nWe train the cross-encoder on a multilingual pair data, \nThe data format is the same as embedding model, so you can fine-tune it easily following our [example](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/reranker). \nMore details please refer to [./FlagEmbedding/reranker/README.md](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/reranker)\n\n\n\n## Citation\n\nIf you find this repository useful, please consider giving a star :star: and citation\n\n```\n@misc{bge_embedding,\n      title={C-Pack: Packaged Resources To Advance General Chinese Embedding}, \n      author={Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff},\n      year={2023},\n      eprint={2309.07597},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n\n## License\nFlagEmbedding is licensed under the [MIT License](https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE). The released models can be used for commercial purposes free of charge.",
    "card_content": "---\nlanguage:\n- en\n- zh\nlicense: mit\ntags:\n- mteb\npipeline_tag: feature-extraction\nmodel-index:\n- name: bge-reranker-base\n  results:\n  - task:\n      type: Reranking\n    dataset:\n      name: MTEB CMedQAv1\n      type: C-MTEB/CMedQAv1-reranking\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map\n      value: 81.27206722525007\n    - type: mrr\n      value: 84.14238095238095\n  - task:\n      type: Reranking\n    dataset:\n      name: MTEB CMedQAv2\n      type: C-MTEB/CMedQAv2-reranking\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map\n      value: 84.10369934291236\n    - type: mrr\n      value: 86.79376984126984\n  - task:\n      type: Reranking\n    dataset:\n      name: MTEB MMarcoReranking\n      type: C-MTEB/Mmarco-reranking\n      config: default\n      split: dev\n      revision: None\n    metrics:\n    - type: map\n      value: 35.4600511272538\n    - type: mrr\n      value: 34.60238095238095\n  - task:\n      type: Reranking\n    dataset:\n      name: MTEB T2Reranking\n      type: C-MTEB/T2Reranking\n      config: default\n      split: dev\n      revision: None\n    metrics:\n    - type: map\n      value: 67.27728847727172\n    - type: mrr\n      value: 77.1315192743764\n---\n\n**We have updated the [new reranker](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/llm_reranker), supporting larger lengths, more languages, and achieving better performance.**\n\n<h1 align=\"center\">FlagEmbedding</h1>\n\n\n<h4 align=\"center\">\n    <p>\n        <a href=#model-list>Model List</a> | \n        <a href=#frequently-asked-questions>FAQ</a> |\n        <a href=#usage>Usage</a>  |\n        <a href=\"#evaluation\">Evaluation</a> |\n        <a href=\"#train\">Train</a> |\n        <a href=\"#citation\">Citation</a> |\n        <a href=\"#license\">License</a> \n    <p>\n</h4>\n\n**More details please refer to our Github: [FlagEmbedding](https://github.com/FlagOpen/FlagEmbedding).**\n\n\n[English](README.md) | [\u4e2d\u6587](https://github.com/FlagOpen/FlagEmbedding/blob/master/README_zh.md)\n\n\nFlagEmbedding focuses on retrieval-augmented LLMs, consisting of the following projects currently:\n\n- **Long-Context LLM**: [Activation Beacon](https://github.com/FlagOpen/FlagEmbedding/tree/master/Long_LLM/activation_beacon)\n- **Fine-tuning of LM** : [LM-Cocktail](https://github.com/FlagOpen/FlagEmbedding/tree/master/LM_Cocktail)\n- **Embedding Model**: [Visualized-BGE](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/visual), [BGE-M3](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/BGE_M3), [LLM Embedder](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/llm_embedder), [BGE Embedding](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/baai_general_embedding)\n- **Reranker Model**: [llm rerankers](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/llm_reranker), [BGE Reranker](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/reranker)\n- **Benchmark**: [C-MTEB](https://github.com/FlagOpen/FlagEmbedding/tree/master/C_MTEB)\n\n## News \n- 3/18/2024: Release new [rerankers](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/llm_reranker), built upon powerful M3 and LLM (GEMMA and MiniCPM, not so large actually) backbones, supporitng multi-lingual processing and larger inputs, massive improvements of ranking performances on BEIR, C-MTEB/Retrieval, MIRACL, LlamaIndex Evaluation.\n- 3/18/2024: Release [Visualized-BGE](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/visual), equipping BGE with visual capabilities. Visualized-BGE can be utilized to generate embeddings for hybrid image-text data.\n- 1/30/2024: Release **BGE-M3**, a new member to BGE model series! M3 stands for **M**ulti-linguality (100+ languages), **M**ulti-granularities (input length up to 8192), **M**ulti-Functionality (unification of dense, lexical, multi-vec/colbert retrieval). \nIt is the first embedding model which supports all three retrieval methods, achieving new SOTA on multi-lingual (MIRACL) and cross-lingual (MKQA) benchmarks.\n[Technical Report](https://github.com/FlagOpen/FlagEmbedding/blob/master/FlagEmbedding/BGE_M3/BGE_M3.pdf) and [Code](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/BGE_M3). :fire:\n- 1/9/2024: Release [Activation-Beacon](https://github.com/FlagOpen/FlagEmbedding/tree/master/Long_LLM/activation_beacon), an effective, efficient, compatible, and low-cost (training) method to extend the context length of LLM. [Technical Report](https://arxiv.org/abs/2401.03462) :fire:\n- 12/24/2023: Release **LLaRA**, a LLaMA-7B based dense retriever, leading to state-of-the-art performances on MS MARCO and BEIR. Model and code will be open-sourced. Please stay tuned. [Technical Report](https://arxiv.org/abs/2312.15503)\n- 11/23/2023: Release [LM-Cocktail](https://github.com/FlagOpen/FlagEmbedding/tree/master/LM_Cocktail), a method to maintain general capabilities during fine-tuning by merging multiple language models. [Technical Report](https://arxiv.org/abs/2311.13534) :fire:  \n- 10/12/2023: Release [LLM-Embedder](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/llm_embedder), a unified embedding model to support diverse retrieval augmentation needs for LLMs. [Technical Report](https://arxiv.org/pdf/2310.07554.pdf)\n- 09/15/2023: The [technical report](https://arxiv.org/pdf/2309.07597.pdf) of BGE has been released \n- 09/15/2023: The [massive training data](https://data.baai.ac.cn/details/BAAI-MTP) of BGE has been released \n- 09/12/2023: New models: \n    - **New reranker model**: release cross-encoder models `BAAI/bge-reranker-base` and `BAAI/bge-reranker-large`, which are more powerful than embedding model. We recommend to use/fine-tune them to re-rank top-k documents returned by embedding models. \n    - **update embedding model**: release `bge-*-v1.5` embedding model to alleviate the issue of the similarity distribution, and enhance its retrieval ability without instruction.\n \n\n<details>\n  <summary>More</summary>\n<!-- ### More -->\n    \n- 09/07/2023: Update [fine-tune code](https://github.com/FlagOpen/FlagEmbedding/blob/master/FlagEmbedding/baai_general_embedding/README.md): Add script to mine hard negatives and support adding instruction during fine-tuning. \n- 08/09/2023: BGE Models are integrated into **Langchain**, you can use it like [this](#using-langchain); C-MTEB **leaderboard** is [available](https://huggingface.co/spaces/mteb/leaderboard).  \n- 08/05/2023: Release base-scale and small-scale models, **best performance among the models of the same size \ud83e\udd17**  \n- 08/02/2023: Release `bge-large-*`(short for BAAI General Embedding) Models, **rank 1st on MTEB and C-MTEB benchmark!** :tada: :tada:   \n- 08/01/2023: We release the [Chinese Massive Text Embedding Benchmark](https://github.com/FlagOpen/FlagEmbedding/blob/master/C_MTEB) (**C-MTEB**), consisting of 31 test dataset.  \n  \n</details>\n\n\n## Model List\n\n`bge` is short for `BAAI general embedding`.\n\n|              Model              | Language | | Description | query instruction for retrieval [1] |\n|:-------------------------------|:--------:| :--------:| :--------:|:--------:|\n| [BAAI/bge-m3](https://huggingface.co/BAAI/bge-m3)                   |    Multilingual     |    [Inference](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/BGE_M3#usage) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/BGE_M3)    | Multi-Functionality(dense retrieval, sparse retrieval, multi-vector(colbert)), Multi-Linguality, and Multi-Granularity(8192 tokens) |  |\n|  [BAAI/llm-embedder](https://huggingface.co/BAAI/llm-embedder)  |   English | [Inference](./FlagEmbedding/llm_embedder/README.md) [Fine-tune](./FlagEmbedding/llm_embedder/README.md) | a unified embedding model to support diverse retrieval augmentation needs for LLMs | See [README](./FlagEmbedding/llm_embedder/README.md) |\n|  [BAAI/bge-reranker-large](https://huggingface.co/BAAI/bge-reranker-large)  |   Chinese and English | [Inference](#usage-for-reranker) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/reranker) | a cross-encoder model which is more accurate but less efficient [2] |   |\n|  [BAAI/bge-reranker-base](https://huggingface.co/BAAI/bge-reranker-base) |   Chinese and English | [Inference](#usage-for-reranker) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/reranker) | a cross-encoder model which is more accurate but less efficient [2] |   |\n|  [BAAI/bge-large-en-v1.5](https://huggingface.co/BAAI/bge-large-en-v1.5) |   English | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | version 1.5 with more reasonable similarity distribution | `Represent this sentence for searching relevant passages: `  |\n|  [BAAI/bge-base-en-v1.5](https://huggingface.co/BAAI/bge-base-en-v1.5) |   English | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | version 1.5 with more reasonable similarity distribution | `Represent this sentence for searching relevant passages: `  |\n|  [BAAI/bge-small-en-v1.5](https://huggingface.co/BAAI/bge-small-en-v1.5) |   English | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | version 1.5 with more reasonable similarity distribution  | `Represent this sentence for searching relevant passages: `  |\n|  [BAAI/bge-large-zh-v1.5](https://huggingface.co/BAAI/bge-large-zh-v1.5) |   Chinese | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | version 1.5 with more reasonable similarity distribution | `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`  |\n|  [BAAI/bge-base-zh-v1.5](https://huggingface.co/BAAI/bge-base-zh-v1.5) |   Chinese |  [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | version 1.5 with more reasonable similarity distribution | `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`  |\n|  [BAAI/bge-small-zh-v1.5](https://huggingface.co/BAAI/bge-small-zh-v1.5) |   Chinese | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | version 1.5 with more reasonable similarity distribution | `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`  |\n|  [BAAI/bge-large-en](https://huggingface.co/BAAI/bge-large-en) |   English | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | :trophy: rank **1st** in [MTEB](https://huggingface.co/spaces/mteb/leaderboard) leaderboard | `Represent this sentence for searching relevant passages: `  |\n|  [BAAI/bge-base-en](https://huggingface.co/BAAI/bge-base-en) |   English | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | a base-scale model but with similar ability to `bge-large-en` | `Represent this sentence for searching relevant passages: `  |\n|  [BAAI/bge-small-en](https://huggingface.co/BAAI/bge-small-en) |   English | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) |a small-scale model but with competitive performance  | `Represent this sentence for searching relevant passages: `  |\n|  [BAAI/bge-large-zh](https://huggingface.co/BAAI/bge-large-zh) |   Chinese | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | :trophy: rank **1st** in [C-MTEB](https://github.com/FlagOpen/FlagEmbedding/tree/master/C_MTEB) benchmark | `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`  |\n|  [BAAI/bge-base-zh](https://huggingface.co/BAAI/bge-base-zh) |   Chinese |  [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | a base-scale model but with similar ability to `bge-large-zh` | `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`  |\n|  [BAAI/bge-small-zh](https://huggingface.co/BAAI/bge-small-zh) |   Chinese | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | a small-scale model but with competitive performance | `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`  |\n\n\n[1\\]: If you need to search the relevant passages to a query, we suggest to add the instruction to the query; in other cases, no instruction is needed, just use the original query directly. In all cases, **no instruction** needs to be added to passages.\n\n[2\\]: Different from embedding model, reranker uses question and document as input and directly output similarity instead of embedding. To balance the accuracy and time cost, cross-encoder is widely used to re-rank top-k documents retrieved by other simple models. \nFor examples, use bge embedding model to retrieve top 100 relevant documents, and then use bge reranker to re-rank the top 100 document to get the final top-3 results.\n\nAll models have been uploaded to Huggingface Hub, and you can see them at https://huggingface.co/BAAI. \nIf you cannot open the Huggingface Hub, you also can download the models at https://model.baai.ac.cn/models .\n\n\n## Frequently asked questions\n\n<details>\n  <summary>1. How to fine-tune bge embedding model?</summary>\n\n  <!-- ### How to fine-tune bge embedding model? -->\nFollowing this [example](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) to prepare data and fine-tune your model. \nSome suggestions:\n- Mine hard negatives following this [example](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune#hard-negatives), which can improve the retrieval performance.\n- If you pre-train bge on your data, the pre-trained model cannot be directly used to calculate similarity, and it must be fine-tuned with contrastive learning before computing similarity.\n- If the accuracy of the fine-tuned model is still not high, it is recommended to use/fine-tune the cross-encoder model (bge-reranker) to re-rank top-k results.\n  Hard negatives also are needed to fine-tune reranker. Refer to this [example](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/reranker) for the fine-tuning for reranker\n\n  \n</details>\n\n<details>\n  <summary>2. The similarity score between two dissimilar sentences is higher than 0.5</summary>\n\n  <!-- ### The similarity score between two dissimilar sentences is higher than 0.5 -->\n**Suggest to use bge v1.5, which alleviates the issue of the similarity distribution.** \n\nSince we finetune the models by contrastive learning with a temperature of 0.01, \nthe similarity distribution of the current BGE model is about in the interval \\[0.6, 1\\].\nSo a similarity score greater than 0.5 does not indicate that the two sentences are similar.\n\nFor downstream tasks, such as passage retrieval or semantic similarity, \n**what matters is the relative order of the scores, not the absolute value.**\nIf you need to filter similar sentences based on a similarity threshold, \nplease select an appropriate similarity threshold based on the similarity distribution on your data (such as 0.8, 0.85, or even 0.9).\n\n</details>\n\n<details>\n  <summary>3. When does the query instruction need to be used</summary>\n\n  <!-- ### When does the query instruction need to be used -->\n\nFor the `bge-*-v1.5`, we improve its retrieval ability when not using instruction. \nNo instruction only has a slight degradation in retrieval performance compared with using instruction. \nSo you can generate embedding without instruction in all cases for convenience.\n \nFor a retrieval task that uses short queries to find long related documents, \nit is recommended to add instructions for these short queries.\n**The best method to decide whether to add instructions for queries is choosing the setting that achieves better performance on your task.**\nIn all cases, the documents/passages do not need to add the instruction. \n\n</details>\n\n\n## Usage \n\n### Usage for Embedding Model\n\nHere are some examples for using `bge` models with \n[FlagEmbedding](#using-flagembedding), [Sentence-Transformers](#using-sentence-transformers), [Langchain](#using-langchain), or [Huggingface Transformers](#using-huggingface-transformers).\n\n#### Using FlagEmbedding\n```\npip install -U FlagEmbedding\n```\nIf it doesn't work for you, you can see [FlagEmbedding](https://github.com/FlagOpen/FlagEmbedding/blob/master/FlagEmbedding/baai_general_embedding/README.md) for more methods to install FlagEmbedding.\n\n```python\nfrom FlagEmbedding import FlagModel\nsentences_1 = [\"\u6837\u4f8b\u6570\u636e-1\", \"\u6837\u4f8b\u6570\u636e-2\"]\nsentences_2 = [\"\u6837\u4f8b\u6570\u636e-3\", \"\u6837\u4f8b\u6570\u636e-4\"]\nmodel = FlagModel('BAAI/bge-large-zh-v1.5', \n                  query_instruction_for_retrieval=\"\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a\",\n                  use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\nembeddings_1 = model.encode(sentences_1)\nembeddings_2 = model.encode(sentences_2)\nsimilarity = embeddings_1 @ embeddings_2.T\nprint(similarity)\n\n# for s2p(short query to long passage) retrieval task, suggest to use encode_queries() which will automatically add the instruction to each query\n# corpus in retrieval task can still use encode() or encode_corpus(), since they don't need instruction\nqueries = ['query_1', 'query_2']\npassages = [\"\u6837\u4f8b\u6587\u6863-1\", \"\u6837\u4f8b\u6587\u6863-2\"]\nq_embeddings = model.encode_queries(queries)\np_embeddings = model.encode(passages)\nscores = q_embeddings @ p_embeddings.T\n```\nFor the value of the argument `query_instruction_for_retrieval`, see [Model List](https://github.com/FlagOpen/FlagEmbedding/tree/master#model-list). \n\nBy default, FlagModel will use all available GPUs when encoding. Please set `os.environ[\"CUDA_VISIBLE_DEVICES\"]` to select specific GPUs.\nYou also can set `os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\"` to make all GPUs unavailable.\n\n\n#### Using Sentence-Transformers\n\nYou can also use the `bge` models with [sentence-transformers](https://www.SBERT.net):\n\n```\npip install -U sentence-transformers\n```\n```python\nfrom sentence_transformers import SentenceTransformer\nsentences_1 = [\"\u6837\u4f8b\u6570\u636e-1\", \"\u6837\u4f8b\u6570\u636e-2\"]\nsentences_2 = [\"\u6837\u4f8b\u6570\u636e-3\", \"\u6837\u4f8b\u6570\u636e-4\"]\nmodel = SentenceTransformer('BAAI/bge-large-zh-v1.5')\nembeddings_1 = model.encode(sentences_1, normalize_embeddings=True)\nembeddings_2 = model.encode(sentences_2, normalize_embeddings=True)\nsimilarity = embeddings_1 @ embeddings_2.T\nprint(similarity)\n```\nFor s2p(short query to long passage) retrieval task, \neach short query should start with an instruction (instructions see [Model List](https://github.com/FlagOpen/FlagEmbedding/tree/master#model-list)). \nBut the instruction is not needed for passages.\n```python\nfrom sentence_transformers import SentenceTransformer\nqueries = ['query_1', 'query_2']\npassages = [\"\u6837\u4f8b\u6587\u6863-1\", \"\u6837\u4f8b\u6587\u6863-2\"]\ninstruction = \"\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a\"\n\nmodel = SentenceTransformer('BAAI/bge-large-zh-v1.5')\nq_embeddings = model.encode([instruction+q for q in queries], normalize_embeddings=True)\np_embeddings = model.encode(passages, normalize_embeddings=True)\nscores = q_embeddings @ p_embeddings.T\n```\n\n#### Using Langchain \n\nYou can use `bge` in langchain like this:\n```python\nfrom langchain.embeddings import HuggingFaceBgeEmbeddings\nmodel_name = \"BAAI/bge-large-en-v1.5\"\nmodel_kwargs = {'device': 'cuda'}\nencode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\nmodel = HuggingFaceBgeEmbeddings(\n    model_name=model_name,\n    model_kwargs=model_kwargs,\n    encode_kwargs=encode_kwargs,\n    query_instruction=\"\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a\"\n)\nmodel.query_instruction = \"\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a\"\n```\n\n\n#### Using HuggingFace Transformers\n\nWith the transformers package, you can use the model like this: First, you pass your input through the transformer model, then you select the last hidden state of the first token (i.e., [CLS]) as the sentence embedding.\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\n# Sentences we want sentence embeddings for\nsentences = [\"\u6837\u4f8b\u6570\u636e-1\", \"\u6837\u4f8b\u6570\u636e-2\"]\n\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained('BAAI/bge-large-zh-v1.5')\nmodel = AutoModel.from_pretrained('BAAI/bge-large-zh-v1.5')\nmodel.eval()\n\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n# for s2p(short query to long passage) retrieval task, add an instruction to query (not add instruction for passages)\n# encoded_input = tokenizer([instruction + q for q in queries], padding=True, truncation=True, return_tensors='pt')\n\n# Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n    # Perform pooling. In this case, cls pooling.\n    sentence_embeddings = model_output[0][:, 0]\n# normalize embeddings\nsentence_embeddings = torch.nn.functional.normalize(sentence_embeddings, p=2, dim=1)\nprint(\"Sentence embeddings:\", sentence_embeddings)\n```\n\n### Usage for Reranker\n\nDifferent from embedding model, reranker uses question and document as input and directly output similarity instead of embedding. \nYou can get a relevance score by inputting query and passage to the reranker. \nThe reranker is optimized based cross-entropy loss, so the relevance score is not bounded to a specific range.\n\n\n#### Using FlagEmbedding\n```\npip install -U FlagEmbedding\n```\n\nGet relevance scores (higher scores indicate more relevance):\n```python\nfrom FlagEmbedding import FlagReranker\nreranker = FlagReranker('BAAI/bge-reranker-large', use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\n\nscore = reranker.compute_score(['query', 'passage'])\nprint(score)\n\nscores = reranker.compute_score([['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']])\nprint(scores)\n```\n\n\n#### Using Huggingface transformers\n\n```python\nimport torch\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained('BAAI/bge-reranker-large')\nmodel = AutoModelForSequenceClassification.from_pretrained('BAAI/bge-reranker-large')\nmodel.eval()\n\npairs = [['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']]\nwith torch.no_grad():\n    inputs = tokenizer(pairs, padding=True, truncation=True, return_tensors='pt', max_length=512)\n    scores = model(**inputs, return_dict=True).logits.view(-1, ).float()\n    print(scores)\n```\n\n#### Usage reranker with the ONNX files\n\n```python\nfrom optimum.onnxruntime import ORTModelForSequenceClassification  # type: ignore\n\nimport torch\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained('BAAI/bge-reranker-large')\nmodel = AutoModelForSequenceClassification.from_pretrained('BAAI/bge-reranker-base')\nmodel_ort = ORTModelForSequenceClassification.from_pretrained('BAAI/bge-reranker-base', file_name=\"onnx/model.onnx\")\n\n# Sentences we want sentence embeddings for\npairs = [['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']]\n\n# Tokenize sentences\nencoded_input = tokenizer(pairs, padding=True, truncation=True, return_tensors='pt')\n\nscores_ort = model_ort(**encoded_input, return_dict=True).logits.view(-1, ).float()\n# Compute token embeddings\nwith torch.inference_mode():\n    scores = model_ort(**encoded_input, return_dict=True).logits.view(-1, ).float()\n\n# scores and scores_ort are identical\n```\n#### Usage reranker with infinity\n\nIts also possible to deploy the onnx/torch files with the [infinity_emb](https://github.com/michaelfeil/infinity) pip package.\n```python\nimport asyncio\nfrom infinity_emb import AsyncEmbeddingEngine, EngineArgs\n\nquery='what is a panda?'\ndocs = ['The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear', \"Paris is in France.\"]\n\nengine = AsyncEmbeddingEngine.from_args(\n    EngineArgs(model_name_or_path = \"BAAI/bge-reranker-base\", device=\"cpu\", engine=\"torch\" # or engine=\"optimum\" for onnx\n))\n\nasync def main(): \n    async with engine:\n        ranking, usage = await engine.rerank(query=query, docs=docs)\n        print(list(zip(ranking, docs)))\nasyncio.run(main())\n```\n\n## Evaluation  \n\n`baai-general-embedding` models achieve **state-of-the-art performance on both MTEB and C-MTEB leaderboard!**\nFor more details and evaluation tools see our [scripts](https://github.com/FlagOpen/FlagEmbedding/blob/master/C_MTEB/README.md). \n\n- **MTEB**:   \n\n| Model Name |  Dimension | Sequence Length | Average (56) | Retrieval (15) |Clustering (11) | Pair Classification (3) | Reranking (4) |  STS (10) | Summarization (1) | Classification (12) |\n|:----:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n| [BAAI/bge-large-en-v1.5](https://huggingface.co/BAAI/bge-large-en-v1.5) | 1024 | 512 |  **64.23** | **54.29** |  46.08 | 87.12 | 60.03 | 83.11 | 31.61 | 75.97 |  \n| [BAAI/bge-base-en-v1.5](https://huggingface.co/BAAI/bge-base-en-v1.5) |  768 | 512 | 63.55 | 53.25 |   45.77 | 86.55 | 58.86 | 82.4 | 31.07 | 75.53 |  \n| [BAAI/bge-small-en-v1.5](https://huggingface.co/BAAI/bge-small-en-v1.5) |  384 | 512 | 62.17 |51.68 | 43.82 |  84.92 | 58.36 | 81.59 | 30.12 | 74.14 |  \n| [bge-large-en](https://huggingface.co/BAAI/bge-large-en) |  1024 | 512 | 63.98 |  53.9 | 46.98 | 85.8 | 59.48 | 81.56 | 32.06 | 76.21 | \n| [bge-base-en](https://huggingface.co/BAAI/bge-base-en) |  768 | 512 |  63.36 | 53.0 | 46.32 | 85.86 | 58.7 | 81.84 | 29.27 | 75.27 | \n| [gte-large](https://huggingface.co/thenlper/gte-large) |  1024 | 512 | 63.13 | 52.22 | 46.84 | 85.00 | 59.13 | 83.35 | 31.66 | 73.33 |\n| [gte-base](https://huggingface.co/thenlper/gte-base) \t|  768 | 512 | 62.39 | 51.14 | 46.2 | 84.57 | 58.61 | 82.3 | 31.17 | 73.01 |\n| [e5-large-v2](https://huggingface.co/intfloat/e5-large-v2) |  1024| 512 | 62.25 | 50.56 | 44.49 | 86.03 | 56.61 | 82.05 | 30.19 | 75.24 |\n| [bge-small-en](https://huggingface.co/BAAI/bge-small-en) |  384 | 512 | 62.11 |  51.82 | 44.31 | 83.78 | 57.97 | 80.72 | 30.53 | 74.37 |  \n| [instructor-xl](https://huggingface.co/hkunlp/instructor-xl) |  768 | 512 | 61.79 | 49.26 | 44.74 | 86.62 | 57.29 | 83.06 | 32.32 | 61.79 |\n| [e5-base-v2](https://huggingface.co/intfloat/e5-base-v2) |  768 | 512 | 61.5 | 50.29 | 43.80 | 85.73 | 55.91 | 81.05 | 30.28 | 73.84 |\n| [gte-small](https://huggingface.co/thenlper/gte-small) |  384 | 512 | 61.36 | 49.46 | 44.89 | 83.54 | 57.7 | 82.07 | 30.42 | 72.31 |\n| [text-embedding-ada-002](https://platform.openai.com/docs/guides/embeddings) | 1536 | 8192 | 60.99 | 49.25 | 45.9 | 84.89 | 56.32 | 80.97 | 30.8 | 70.93 |\n| [e5-small-v2](https://huggingface.co/intfloat/e5-base-v2) | 384 | 512 | 59.93 | 49.04 | 39.92 | 84.67 | 54.32 | 80.39 | 31.16 | 72.94 |\n| [sentence-t5-xxl](https://huggingface.co/sentence-transformers/sentence-t5-xxl) |  768 | 512 | 59.51 | 42.24 | 43.72 | 85.06 | 56.42 | 82.63 | 30.08 | 73.42 |\n| [all-mpnet-base-v2](https://huggingface.co/sentence-transformers/all-mpnet-base-v2) \t|  768 | 514 \t| 57.78 | 43.81 | 43.69 | 83.04 | 59.36 | 80.28 | 27.49 | 65.07 |\n| [sgpt-bloom-7b1-msmarco](https://huggingface.co/bigscience/sgpt-bloom-7b1-msmarco) \t|  4096 | 2048 | 57.59 | 48.22 | 38.93 | 81.9 | 55.65 | 77.74 | 33.6 | 66.19 |\n\n\n\n- **C-MTEB**:  \nWe create the benchmark C-MTEB for Chinese text embedding which consists of 31 datasets from 6 tasks. \nPlease refer to [C_MTEB](https://github.com/FlagOpen/FlagEmbedding/blob/master/C_MTEB/README.md) for a detailed introduction.\n \n| Model | Embedding dimension | Avg | Retrieval | STS | PairClassification | Classification | Reranking | Clustering |\n|:-------------------------------|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|\n| [**BAAI/bge-large-zh-v1.5**](https://huggingface.co/BAAI/bge-large-zh-v1.5) | 1024 |  **64.53** | 70.46 | 56.25 | 81.6 | 69.13 | 65.84 | 48.99 |  \n| [BAAI/bge-base-zh-v1.5](https://huggingface.co/BAAI/bge-base-zh-v1.5) | 768 |  63.13 | 69.49 | 53.72 | 79.75 | 68.07 | 65.39 | 47.53 |  \n| [BAAI/bge-small-zh-v1.5](https://huggingface.co/BAAI/bge-small-zh-v1.5) | 512 | 57.82 | 61.77 | 49.11 | 70.41 | 63.96 | 60.92 | 44.18 |   \n| [BAAI/bge-large-zh](https://huggingface.co/BAAI/bge-large-zh) | 1024 | 64.20 | 71.53 | 54.98 | 78.94 | 68.32 | 65.11 | 48.39 |\n| [bge-large-zh-noinstruct](https://huggingface.co/BAAI/bge-large-zh-noinstruct) | 1024 | 63.53 | 70.55 | 53 | 76.77 | 68.58 | 64.91 | 50.01 |\n| [BAAI/bge-base-zh](https://huggingface.co/BAAI/bge-base-zh) | 768 | 62.96 | 69.53 | 54.12 | 77.5 | 67.07 | 64.91 | 47.63 |\n| [multilingual-e5-large](https://huggingface.co/intfloat/multilingual-e5-large) | 1024 | 58.79 | 63.66 | 48.44 | 69.89 | 67.34 | 56.00 | 48.23 |\n| [BAAI/bge-small-zh](https://huggingface.co/BAAI/bge-small-zh) | 512 | 58.27 |  63.07 | 49.45 | 70.35 | 63.64 | 61.48 | 45.09 |\n| [m3e-base](https://huggingface.co/moka-ai/m3e-base) | 768 | 57.10 | 56.91 | 50.47 | 63.99 | 67.52 | 59.34 | 47.68 |\n| [m3e-large](https://huggingface.co/moka-ai/m3e-large) | 1024 |  57.05 | 54.75 | 50.42 | 64.3 | 68.2 | 59.66 | 48.88 |\n| [multilingual-e5-base](https://huggingface.co/intfloat/multilingual-e5-base) | 768 | 55.48 | 61.63 | 46.49 | 67.07 | 65.35 | 54.35 | 40.68 |\n| [multilingual-e5-small](https://huggingface.co/intfloat/multilingual-e5-small) | 384 | 55.38 | 59.95 | 45.27 | 66.45 | 65.85 | 53.86 | 45.26 |\n| [text-embedding-ada-002(OpenAI)](https://platform.openai.com/docs/guides/embeddings/what-are-embeddings) | 1536 |  53.02 | 52.0 | 43.35 | 69.56 | 64.31 | 54.28 | 45.68 |\n| [luotuo](https://huggingface.co/silk-road/luotuo-bert-medium) | 1024 | 49.37 |  44.4 | 42.78 | 66.62 | 61 | 49.25 | 44.39 |\n| [text2vec-base](https://huggingface.co/shibing624/text2vec-base-chinese) | 768 |  47.63 | 38.79 | 43.41 | 67.41 | 62.19 | 49.45 | 37.66 |\n| [text2vec-large](https://huggingface.co/GanymedeNil/text2vec-large-chinese) | 1024 | 47.36 | 41.94 | 44.97 | 70.86 | 60.66 | 49.16 | 30.02 |\n\n\n- **Reranking**:\nSee [C_MTEB](https://github.com/FlagOpen/FlagEmbedding/blob/master/C_MTEB/) for evaluation script.\n\n| Model | T2Reranking | T2RerankingZh2En\\* | T2RerankingEn2Zh\\* | MMarcoReranking | CMedQAv1 | CMedQAv2 | Avg |  \n|:-------------------------------|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|  \n| text2vec-base-multilingual | 64.66 | 62.94 | 62.51 | 14.37 | 48.46 | 48.6 | 50.26 |  \n| multilingual-e5-small | 65.62 | 60.94 | 56.41 | 29.91 | 67.26 | 66.54 | 57.78 |  \n| multilingual-e5-large | 64.55 | 61.61 | 54.28 | 28.6 | 67.42 | 67.92 | 57.4 |  \n| multilingual-e5-base | 64.21 | 62.13 | 54.68 | 29.5 | 66.23 | 66.98 | 57.29 |  \n| m3e-base | 66.03 | 62.74 | 56.07 | 17.51 | 77.05 | 76.76 | 59.36 |  \n| m3e-large | 66.13 | 62.72 | 56.1 | 16.46 | 77.76 | 78.27 | 59.57 |  \n| bge-base-zh-v1.5 | 66.49 | 63.25 | 57.02 | 29.74 | 80.47 | 84.88 | 63.64 |  \n| bge-large-zh-v1.5 | 65.74 | 63.39 | 57.03 | 28.74 | 83.45 | 85.44 | 63.97 |  \n| [BAAI/bge-reranker-base](https://huggingface.co/BAAI/bge-reranker-base) | 67.28 | 63.95 | 60.45 | 35.46 | 81.26 | 84.1 | 65.42 |  \n| [BAAI/bge-reranker-large](https://huggingface.co/BAAI/bge-reranker-large) | 67.6 | 64.03 | 61.44 | 37.16 | 82.15 | 84.18 | 66.09 |  \n\n\\* : T2RerankingZh2En and T2RerankingEn2Zh are cross-language retrieval tasks\n\n## Train\n\n### BAAI Embedding \n\nWe pre-train the models using [retromae](https://github.com/staoxiao/RetroMAE) and train them on large-scale pairs data using contrastive learning. \n**You can fine-tune the embedding model on your data following our [examples](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune).**\nWe also provide a [pre-train example](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/pretrain).\nNote that the goal of pre-training is to reconstruct the text, and the pre-trained model cannot be used for similarity calculation directly, it needs to be fine-tuned.\nMore training details for bge see [baai_general_embedding](https://github.com/FlagOpen/FlagEmbedding/blob/master/FlagEmbedding/baai_general_embedding/README.md).\n\n\n\n### BGE Reranker\n\nCross-encoder will perform full-attention over the input pair, \nwhich is more accurate than embedding model (i.e., bi-encoder) but more time-consuming than embedding model.\nTherefore, it can be used to re-rank the top-k documents returned by embedding model.\nWe train the cross-encoder on a multilingual pair data, \nThe data format is the same as embedding model, so you can fine-tune it easily following our [example](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/reranker). \nMore details please refer to [./FlagEmbedding/reranker/README.md](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/reranker)\n\n\n\n## Citation\n\nIf you find this repository useful, please consider giving a star :star: and citation\n\n```\n@misc{bge_embedding,\n      title={C-Pack: Packaged Resources To Advance General Chinese Embedding}, \n      author={Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff},\n      year={2023},\n      eprint={2309.07597},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n\n## License\nFlagEmbedding is licensed under the [MIT License](https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE). The released models can be used for commercial purposes free of charge.",
    "library_name": "transformers"
  },
  {
    "model_id": "j-hartmann/emotion-english-distilroberta-base",
    "model_name": "j-hartmann/emotion-english-distilroberta-base",
    "author": "j-hartmann",
    "downloads": 822357,
    "likes": 390,
    "tags": [
      "transformers",
      "pytorch",
      "tf",
      "roberta",
      "text-classification",
      "distilroberta",
      "sentiment",
      "emotion",
      "twitter",
      "reddit",
      "en",
      "arxiv:2210.00434",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/j-hartmann/emotion-english-distilroberta-base",
    "dependencies": [
      [
        "transformers",
        "4.30.2"
      ],
      [
        "torch",
        "2.0.1"
      ],
      [
        "numpy",
        "1.24.3"
      ],
      [
        "xformers",
        "0.0.20"
      ],
      [
        "hf_xet",
        "0.1.0"
      ]
    ],
    "analysis_date": "2025-03-26T00:47:49.145213",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "roberta",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": "en",
      "tags": [
        "distilroberta",
        "sentiment",
        "emotion",
        "twitter",
        "reddit"
      ],
      "widget": [
        {
          "text": "Oh wow. I didn't know that."
        },
        {
          "text": "This movie always makes me cry.."
        },
        {
          "text": "Oh Happy Day"
        }
      ]
    },
    "card_text": "\n# Emotion English DistilRoBERTa-base\n\n# Description \u2139\n\nWith this model, you can classify emotions in English text data. The model was trained on 6 diverse datasets (see Appendix below) and predicts Ekman's 6 basic emotions, plus a neutral class:\n\n1) anger \ud83e\udd2c\n2) disgust \ud83e\udd22\n3) fear \ud83d\ude28\n4) joy \ud83d\ude00\n5) neutral \ud83d\ude10\n6) sadness \ud83d\ude2d\n7) surprise \ud83d\ude32\n\nThe model is a fine-tuned checkpoint of [DistilRoBERTa-base](https://huggingface.co/distilroberta-base). For a 'non-distilled' emotion model, please refer to the model card of the [RoBERTa-large](https://huggingface.co/j-hartmann/emotion-english-roberta-large) version.\n\n# Application \ud83d\ude80\n\na) Run emotion model with 3 lines of code on single text example using Hugging Face's pipeline command on Google Colab:\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/j-hartmann/emotion-english-distilroberta-base/blob/main/simple_emotion_pipeline.ipynb)\n\n```python\nfrom transformers import pipeline\nclassifier = pipeline(\"text-classification\", model=\"j-hartmann/emotion-english-distilroberta-base\", return_all_scores=True)\nclassifier(\"I love this!\")\n```\n\n```python\nOutput:\n[[{'label': 'anger', 'score': 0.004419783595949411},\n  {'label': 'disgust', 'score': 0.0016119900392368436},\n  {'label': 'fear', 'score': 0.0004138521908316761},\n  {'label': 'joy', 'score': 0.9771687984466553},\n  {'label': 'neutral', 'score': 0.005764586851000786},\n  {'label': 'sadness', 'score': 0.002092392183840275},\n  {'label': 'surprise', 'score': 0.008528684265911579}]]\n```\n\nb) Run emotion model on multiple examples and full datasets (e.g., .csv files) on Google Colab:\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/j-hartmann/emotion-english-distilroberta-base/blob/main/emotion_prediction_example.ipynb)\n\n# Contact \ud83d\udcbb\n\nPlease reach out to [jochen.hartmann@tum.de](mailto:jochen.hartmann@tum.de) if you have any questions or feedback.\n\nThanks to Samuel Domdey and [chrsiebert](https://huggingface.co/siebert) for their support in making this model available.\n\n# Reference \u2705\n\nFor attribution, please cite the following reference if you use this model. A working paper will be available soon.\n\n```\nJochen Hartmann, \"Emotion English DistilRoBERTa-base\". https://huggingface.co/j-hartmann/emotion-english-distilroberta-base/, 2022.\n```\n\nBibTex citation:\n\n```\n@misc{hartmann2022emotionenglish,\n  author={Hartmann, Jochen},\n  title={Emotion English DistilRoBERTa-base},\n  year={2022},\n  howpublished = {\\url{https://huggingface.co/j-hartmann/emotion-english-distilroberta-base/}},\n}\n```\n\n# Appendix \ud83d\udcda\n\nPlease find an overview of the datasets used for training below. All datasets contain English text. The table summarizes which emotions are available in each of the datasets. The datasets represent a diverse collection of text types. Specifically, they contain emotion labels for texts from Twitter, Reddit, student self-reports, and utterances from TV dialogues. As MELD (Multimodal EmotionLines Dataset) extends the popular EmotionLines dataset, EmotionLines itself is not included here. \n\n|Name|anger|disgust|fear|joy|neutral|sadness|surprise|\n|---|---|---|---|---|---|---|---|\n|Crowdflower (2016)|Yes|-|-|Yes|Yes|Yes|Yes|\n|Emotion Dataset, Elvis et al. (2018)|Yes|-|Yes|Yes|-|Yes|Yes|\n|GoEmotions, Demszky et al. (2020)|Yes|Yes|Yes|Yes|Yes|Yes|Yes|\n|ISEAR, Vikash (2018)|Yes|Yes|Yes|Yes|-|Yes|-|\n|MELD, Poria et al. (2019)|Yes|Yes|Yes|Yes|Yes|Yes|Yes|\n|SemEval-2018, EI-reg, Mohammad et al. (2018) |Yes|-|Yes|Yes|-|Yes|-|\n\nThe model is trained on a balanced subset from the datasets listed above (2,811 observations per emotion, i.e., nearly 20k observations in total). 80% of this balanced subset is used for training and 20% for evaluation. The evaluation accuracy is 66% (vs. the random-chance baseline of 1/7 = 14%).\n\n# Scientific Applications \ud83d\udcd6\n\nBelow you can find a list of papers using \"Emotion English DistilRoBERTa-base\". If you would like your paper to be added to the list, please send me an email.\n\n- Butt, S., Sharma, S., Sharma, R., Sidorov, G., & Gelbukh, A. (2022). What goes on inside rumour and non-rumour tweets and their reactions: A Psycholinguistic Analyses. Computers in Human Behavior, 107345.\n- Kuang, Z., Zong, S., Zhang, J., Chen, J., & Liu, H. (2022). Music-to-Text Synaesthesia: Generating Descriptive Text from Music Recordings. arXiv preprint arXiv:2210.00434.\n- Rozado, D., Hughes, R., & Halberstadt, J. (2022). Longitudinal analysis of sentiment and emotion in news media headlines using automated labelling with Transformer language models. Plos one, 17(10), e0276367.",
    "card_content": "---\nlanguage: en\ntags:\n- distilroberta\n- sentiment\n- emotion\n- twitter\n- reddit\nwidget:\n- text: Oh wow. I didn't know that.\n- text: This movie always makes me cry..\n- text: Oh Happy Day\n---\n\n# Emotion English DistilRoBERTa-base\n\n# Description \u2139\n\nWith this model, you can classify emotions in English text data. The model was trained on 6 diverse datasets (see Appendix below) and predicts Ekman's 6 basic emotions, plus a neutral class:\n\n1) anger \ud83e\udd2c\n2) disgust \ud83e\udd22\n3) fear \ud83d\ude28\n4) joy \ud83d\ude00\n5) neutral \ud83d\ude10\n6) sadness \ud83d\ude2d\n7) surprise \ud83d\ude32\n\nThe model is a fine-tuned checkpoint of [DistilRoBERTa-base](https://huggingface.co/distilroberta-base). For a 'non-distilled' emotion model, please refer to the model card of the [RoBERTa-large](https://huggingface.co/j-hartmann/emotion-english-roberta-large) version.\n\n# Application \ud83d\ude80\n\na) Run emotion model with 3 lines of code on single text example using Hugging Face's pipeline command on Google Colab:\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/j-hartmann/emotion-english-distilroberta-base/blob/main/simple_emotion_pipeline.ipynb)\n\n```python\nfrom transformers import pipeline\nclassifier = pipeline(\"text-classification\", model=\"j-hartmann/emotion-english-distilroberta-base\", return_all_scores=True)\nclassifier(\"I love this!\")\n```\n\n```python\nOutput:\n[[{'label': 'anger', 'score': 0.004419783595949411},\n  {'label': 'disgust', 'score': 0.0016119900392368436},\n  {'label': 'fear', 'score': 0.0004138521908316761},\n  {'label': 'joy', 'score': 0.9771687984466553},\n  {'label': 'neutral', 'score': 0.005764586851000786},\n  {'label': 'sadness', 'score': 0.002092392183840275},\n  {'label': 'surprise', 'score': 0.008528684265911579}]]\n```\n\nb) Run emotion model on multiple examples and full datasets (e.g., .csv files) on Google Colab:\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/j-hartmann/emotion-english-distilroberta-base/blob/main/emotion_prediction_example.ipynb)\n\n# Contact \ud83d\udcbb\n\nPlease reach out to [jochen.hartmann@tum.de](mailto:jochen.hartmann@tum.de) if you have any questions or feedback.\n\nThanks to Samuel Domdey and [chrsiebert](https://huggingface.co/siebert) for their support in making this model available.\n\n# Reference \u2705\n\nFor attribution, please cite the following reference if you use this model. A working paper will be available soon.\n\n```\nJochen Hartmann, \"Emotion English DistilRoBERTa-base\". https://huggingface.co/j-hartmann/emotion-english-distilroberta-base/, 2022.\n```\n\nBibTex citation:\n\n```\n@misc{hartmann2022emotionenglish,\n  author={Hartmann, Jochen},\n  title={Emotion English DistilRoBERTa-base},\n  year={2022},\n  howpublished = {\\url{https://huggingface.co/j-hartmann/emotion-english-distilroberta-base/}},\n}\n```\n\n# Appendix \ud83d\udcda\n\nPlease find an overview of the datasets used for training below. All datasets contain English text. The table summarizes which emotions are available in each of the datasets. The datasets represent a diverse collection of text types. Specifically, they contain emotion labels for texts from Twitter, Reddit, student self-reports, and utterances from TV dialogues. As MELD (Multimodal EmotionLines Dataset) extends the popular EmotionLines dataset, EmotionLines itself is not included here. \n\n|Name|anger|disgust|fear|joy|neutral|sadness|surprise|\n|---|---|---|---|---|---|---|---|\n|Crowdflower (2016)|Yes|-|-|Yes|Yes|Yes|Yes|\n|Emotion Dataset, Elvis et al. (2018)|Yes|-|Yes|Yes|-|Yes|Yes|\n|GoEmotions, Demszky et al. (2020)|Yes|Yes|Yes|Yes|Yes|Yes|Yes|\n|ISEAR, Vikash (2018)|Yes|Yes|Yes|Yes|-|Yes|-|\n|MELD, Poria et al. (2019)|Yes|Yes|Yes|Yes|Yes|Yes|Yes|\n|SemEval-2018, EI-reg, Mohammad et al. (2018) |Yes|-|Yes|Yes|-|Yes|-|\n\nThe model is trained on a balanced subset from the datasets listed above (2,811 observations per emotion, i.e., nearly 20k observations in total). 80% of this balanced subset is used for training and 20% for evaluation. The evaluation accuracy is 66% (vs. the random-chance baseline of 1/7 = 14%).\n\n# Scientific Applications \ud83d\udcd6\n\nBelow you can find a list of papers using \"Emotion English DistilRoBERTa-base\". If you would like your paper to be added to the list, please send me an email.\n\n- Butt, S., Sharma, S., Sharma, R., Sidorov, G., & Gelbukh, A. (2022). What goes on inside rumour and non-rumour tweets and their reactions: A Psycholinguistic Analyses. Computers in Human Behavior, 107345.\n- Kuang, Z., Zong, S., Zhang, J., Chen, J., & Liu, H. (2022). Music-to-Text Synaesthesia: Generating Descriptive Text from Music Recordings. arXiv preprint arXiv:2210.00434.\n- Rozado, D., Hughes, R., & Halberstadt, J. (2022). Longitudinal analysis of sentiment and emotion in news media headlines using automated labelling with Transformer language models. Plos one, 17(10), e0276367.",
    "library_name": "transformers"
  },
  {
    "model_id": "TieIncred/distilbert-base-uncased-finetuned-emotional",
    "model_name": "TieIncred/distilbert-base-uncased-finetuned-emotional",
    "author": "TieIncred",
    "downloads": 778413,
    "likes": 1,
    "tags": [
      "transformers",
      "pytorch",
      "tensorboard",
      "distilbert",
      "text-classification",
      "generated_from_trainer",
      "dataset:emotion",
      "base_model:distilbert/distilbert-base-uncased",
      "base_model:finetune:distilbert/distilbert-base-uncased",
      "license:apache-2.0",
      "model-index",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/TieIncred/distilbert-base-uncased-finetuned-emotional",
    "dependencies": [
      [
        "transformers",
        "4.16.2"
      ],
      [
        "pytorch",
        "2.1.0+cu118"
      ],
      [
        "datasets",
        "2.9.0"
      ],
      [
        "tokenizers",
        "0.14.1"
      ],
      [
        "torch",
        "2.0.1"
      ]
    ],
    "analysis_date": "2025-03-26T00:47:50.448123",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "distilbert",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "license": "apache-2.0",
      "tags": [
        "generated_from_trainer"
      ],
      "datasets": [
        "emotion"
      ],
      "metrics": [
        "accuracy",
        "f1"
      ],
      "base_model": "distilbert-base-uncased",
      "model-index": [
        {
          "name": "distilbert-base-uncased-finetuned-emotional",
          "results": [
            {
              "task": {
                "type": "text-classification",
                "name": "Text Classification"
              },
              "dataset": {
                "name": "emotion",
                "type": "emotion",
                "args": "split"
              },
              "metrics": [
                {
                  "type": "accuracy",
                  "value": 0.9305,
                  "name": "Accuracy"
                },
                {
                  "type": "f1",
                  "value": 0.9309021978510164,
                  "name": "F1"
                }
              ]
            }
          ]
        }
      ]
    },
    "card_text": "\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# distilbert-base-uncased-finetuned-emotional\n\nThis model is a fine-tuned version of [distilbert-base-uncased](https://huggingface.co/distilbert-base-uncased) on the emotion dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.1658\n- Accuracy: 0.9305\n- F1: 0.9309\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 64\n- eval_batch_size: 64\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 2\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss | Accuracy | F1     |\n|:-------------:|:-----:|:----:|:---------------:|:--------:|:------:|\n| 0.1683        | 1.0   | 250  | 0.1738          | 0.9295   | 0.9294 |\n| 0.1085        | 2.0   | 500  | 0.1658          | 0.9305   | 0.9309 |\n\n\n### Framework versions\n\n- Transformers 4.16.2\n- Pytorch 2.1.0+cu118\n- Datasets 2.9.0\n- Tokenizers 0.14.1\n",
    "card_content": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\ndatasets:\n- emotion\nmetrics:\n- accuracy\n- f1\nbase_model: distilbert-base-uncased\nmodel-index:\n- name: distilbert-base-uncased-finetuned-emotional\n  results:\n  - task:\n      type: text-classification\n      name: Text Classification\n    dataset:\n      name: emotion\n      type: emotion\n      args: split\n    metrics:\n    - type: accuracy\n      value: 0.9305\n      name: Accuracy\n    - type: f1\n      value: 0.9309021978510164\n      name: F1\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# distilbert-base-uncased-finetuned-emotional\n\nThis model is a fine-tuned version of [distilbert-base-uncased](https://huggingface.co/distilbert-base-uncased) on the emotion dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.1658\n- Accuracy: 0.9305\n- F1: 0.9309\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 64\n- eval_batch_size: 64\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 2\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss | Accuracy | F1     |\n|:-------------:|:-----:|:----:|:---------------:|:--------:|:------:|\n| 0.1683        | 1.0   | 250  | 0.1738          | 0.9295   | 0.9294 |\n| 0.1085        | 2.0   | 500  | 0.1658          | 0.9305   | 0.9309 |\n\n\n### Framework versions\n\n- Transformers 4.16.2\n- Pytorch 2.1.0+cu118\n- Datasets 2.9.0\n- Tokenizers 0.14.1\n",
    "library_name": "transformers"
  },
  {
    "model_id": "lxyuan/distilbert-base-multilingual-cased-sentiments-student",
    "model_name": "lxyuan/distilbert-base-multilingual-cased-sentiments-student",
    "author": "lxyuan",
    "downloads": 720988,
    "likes": 277,
    "tags": [
      "transformers",
      "pytorch",
      "onnx",
      "safetensors",
      "distilbert",
      "text-classification",
      "sentiment-analysis",
      "zero-shot-distillation",
      "distillation",
      "zero-shot-classification",
      "debarta-v3",
      "en",
      "ar",
      "de",
      "es",
      "fr",
      "ja",
      "zh",
      "id",
      "hi",
      "it",
      "ms",
      "pt",
      "dataset:tyqiangz/multilingual-sentiments",
      "doi:10.57967/hf/1422",
      "license:apache-2.0",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/lxyuan/distilbert-base-multilingual-cased-sentiments-student",
    "dependencies": [
      [
        "transformers",
        "4.28.1"
      ],
      [
        "torch",
        "2.0.0"
      ],
      [
        "datasets",
        "2.11.0"
      ],
      [
        "tokenizers",
        "0.13.3"
      ],
      [
        "numpy",
        "1.24.3"
      ]
    ],
    "analysis_date": "2025-03-26T00:47:52.248545",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "distilbert",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": [
        "en",
        "ar",
        "de",
        "es",
        "fr",
        "ja",
        "zh",
        "id",
        "hi",
        "it",
        "ms",
        "pt"
      ],
      "license": "apache-2.0",
      "tags": [
        "sentiment-analysis",
        "text-classification",
        "zero-shot-distillation",
        "distillation",
        "zero-shot-classification",
        "debarta-v3"
      ],
      "datasets": [
        "tyqiangz/multilingual-sentiments"
      ],
      "model-index": [
        {
          "name": "distilbert-base-multilingual-cased-sentiments-student",
          "results": []
        }
      ]
    },
    "card_text": "\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# distilbert-base-multilingual-cased-sentiments-student\n\nThis model is distilled from the zero-shot classification pipeline on the Multilingual Sentiment \ndataset using this [script](https://github.com/huggingface/transformers/tree/main/examples/research_projects/zero-shot-distillation). \n\nIn reality the multilingual-sentiment dataset is annotated of course, \nbut we'll pretend and ignore the annotations for the sake of example.\n\n\n    Teacher model: MoritzLaurer/mDeBERTa-v3-base-mnli-xnli\n    Teacher hypothesis template: \"The sentiment of this text is {}.\"\n    Student model: distilbert-base-multilingual-cased\n\n\n## Inference example\n\n```python\nfrom transformers import pipeline\n\ndistilled_student_sentiment_classifier = pipeline(\n    model=\"lxyuan/distilbert-base-multilingual-cased-sentiments-student\", \n    return_all_scores=True\n)\n\n# english\ndistilled_student_sentiment_classifier (\"I love this movie and i would watch it again and again!\")\n>> [[{'label': 'positive', 'score': 0.9731044769287109},\n  {'label': 'neutral', 'score': 0.016910076141357422},\n  {'label': 'negative', 'score': 0.009985478594899178}]]\n\n# malay\ndistilled_student_sentiment_classifier(\"Saya suka filem ini dan saya akan menontonnya lagi dan lagi!\")\n[[{'label': 'positive', 'score': 0.9760093688964844},\n  {'label': 'neutral', 'score': 0.01804516464471817},\n  {'label': 'negative', 'score': 0.005945465061813593}]]\n\n# japanese\ndistilled_student_sentiment_classifier(\"\u79c1\u306f\u3053\u306e\u6620\u753b\u304c\u5927\u597d\u304d\u3067\u3001\u4f55\u5ea6\u3082\u898b\u307e\u3059\uff01\")\n>> [[{'label': 'positive', 'score': 0.9342429041862488},\n  {'label': 'neutral', 'score': 0.040193185210227966},\n  {'label': 'negative', 'score': 0.025563929229974747}]]\n\n\n```\n\n\n## Training procedure\n\nNotebook link: [here](https://github.com/LxYuan0420/nlp/blob/main/notebooks/Distilling_Zero_Shot_multilingual_distilbert_sentiments_student.ipynb)\n\n### Training hyperparameters\n\nResult can be reproduce using the following commands:\n\n```bash\npython transformers/examples/research_projects/zero-shot-distillation/distill_classifier.py \\\n--data_file ./multilingual-sentiments/train_unlabeled.txt \\\n--class_names_file ./multilingual-sentiments/class_names.txt \\\n--hypothesis_template \"The sentiment of this text is {}.\" \\\n--teacher_name_or_path MoritzLaurer/mDeBERTa-v3-base-mnli-xnli \\\n--teacher_batch_size 32 \\\n--student_name_or_path distilbert-base-multilingual-cased \\\n--output_dir ./distilbert-base-multilingual-cased-sentiments-student \\\n--per_device_train_batch_size 16 \\\n--fp16\n```\n\nIf you are training this model on Colab, make the following code changes to avoid Out-of-memory error message:\n```bash\n###### modify L78 to disable fast tokenizer \ndefault=False,\n\n###### update dataset map part at L313\ndataset = dataset.map(tokenizer, input_columns=\"text\", fn_kwargs={\"padding\": \"max_length\", \"truncation\": True, \"max_length\": 512})\n\n###### add following lines to L213\ndel model\nprint(f\"Manually deleted Teacher model, free some memory for student model.\")\n\n###### add following lines to L337\ntrainer.push_to_hub()\ntokenizer.push_to_hub(\"distilbert-base-multilingual-cased-sentiments-student\")\n  \n```\n\n### Training log\n```bash\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n{'train_runtime': 2009.8864, 'train_samples_per_second': 73.0, 'train_steps_per_second': 4.563, 'train_loss': 0.6473459283913797, 'epoch': 1.0}\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9171/9171 [33:29<00:00,  4.56it/s]\n[INFO|trainer.py:762] 2023-05-06 10:56:18,555 >> The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n[INFO|trainer.py:3129] 2023-05-06 10:56:18,557 >> ***** Running Evaluation *****\n[INFO|trainer.py:3131] 2023-05-06 10:56:18,557 >>   Num examples = 146721\n[INFO|trainer.py:3134] 2023-05-06 10:56:18,557 >>   Batch size = 128\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1147/1147 [08:59<00:00,  2.13it/s]\n05/06/2023 11:05:18 - INFO - __main__ - Agreement of student and teacher predictions: 88.29%\n[INFO|trainer.py:2868] 2023-05-06 11:05:18,251 >> Saving model checkpoint to ./distilbert-base-multilingual-cased-sentiments-student\n[INFO|configuration_utils.py:457] 2023-05-06 11:05:18,251 >> Configuration saved in ./distilbert-base-multilingual-cased-sentiments-student/config.json\n[INFO|modeling_utils.py:1847] 2023-05-06 11:05:18,905 >> Model weights saved in ./distilbert-base-multilingual-cased-sentiments-student/pytorch_model.bin\n[INFO|tokenization_utils_base.py:2171] 2023-05-06 11:05:18,905 >> tokenizer config file saved in ./distilbert-base-multilingual-cased-sentiments-student/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2178] 2023-05-06 11:05:18,905 >> Special tokens file saved in ./distilbert-base-multilingual-cased-sentiments-student/special_tokens_map.json\n\n```\n\n### Framework versions\n\n- Transformers 4.28.1\n- Pytorch 2.0.0+cu118\n- Datasets 2.11.0\n- Tokenizers 0.13.3",
    "card_content": "---\nlanguage:\n- en\n- ar\n- de\n- es\n- fr\n- ja\n- zh\n- id\n- hi\n- it\n- ms\n- pt\nlicense: apache-2.0\ntags:\n- sentiment-analysis\n- text-classification\n- zero-shot-distillation\n- distillation\n- zero-shot-classification\n- debarta-v3\ndatasets:\n- tyqiangz/multilingual-sentiments\nmodel-index:\n- name: distilbert-base-multilingual-cased-sentiments-student\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# distilbert-base-multilingual-cased-sentiments-student\n\nThis model is distilled from the zero-shot classification pipeline on the Multilingual Sentiment \ndataset using this [script](https://github.com/huggingface/transformers/tree/main/examples/research_projects/zero-shot-distillation). \n\nIn reality the multilingual-sentiment dataset is annotated of course, \nbut we'll pretend and ignore the annotations for the sake of example.\n\n\n    Teacher model: MoritzLaurer/mDeBERTa-v3-base-mnli-xnli\n    Teacher hypothesis template: \"The sentiment of this text is {}.\"\n    Student model: distilbert-base-multilingual-cased\n\n\n## Inference example\n\n```python\nfrom transformers import pipeline\n\ndistilled_student_sentiment_classifier = pipeline(\n    model=\"lxyuan/distilbert-base-multilingual-cased-sentiments-student\", \n    return_all_scores=True\n)\n\n# english\ndistilled_student_sentiment_classifier (\"I love this movie and i would watch it again and again!\")\n>> [[{'label': 'positive', 'score': 0.9731044769287109},\n  {'label': 'neutral', 'score': 0.016910076141357422},\n  {'label': 'negative', 'score': 0.009985478594899178}]]\n\n# malay\ndistilled_student_sentiment_classifier(\"Saya suka filem ini dan saya akan menontonnya lagi dan lagi!\")\n[[{'label': 'positive', 'score': 0.9760093688964844},\n  {'label': 'neutral', 'score': 0.01804516464471817},\n  {'label': 'negative', 'score': 0.005945465061813593}]]\n\n# japanese\ndistilled_student_sentiment_classifier(\"\u79c1\u306f\u3053\u306e\u6620\u753b\u304c\u5927\u597d\u304d\u3067\u3001\u4f55\u5ea6\u3082\u898b\u307e\u3059\uff01\")\n>> [[{'label': 'positive', 'score': 0.9342429041862488},\n  {'label': 'neutral', 'score': 0.040193185210227966},\n  {'label': 'negative', 'score': 0.025563929229974747}]]\n\n\n```\n\n\n## Training procedure\n\nNotebook link: [here](https://github.com/LxYuan0420/nlp/blob/main/notebooks/Distilling_Zero_Shot_multilingual_distilbert_sentiments_student.ipynb)\n\n### Training hyperparameters\n\nResult can be reproduce using the following commands:\n\n```bash\npython transformers/examples/research_projects/zero-shot-distillation/distill_classifier.py \\\n--data_file ./multilingual-sentiments/train_unlabeled.txt \\\n--class_names_file ./multilingual-sentiments/class_names.txt \\\n--hypothesis_template \"The sentiment of this text is {}.\" \\\n--teacher_name_or_path MoritzLaurer/mDeBERTa-v3-base-mnli-xnli \\\n--teacher_batch_size 32 \\\n--student_name_or_path distilbert-base-multilingual-cased \\\n--output_dir ./distilbert-base-multilingual-cased-sentiments-student \\\n--per_device_train_batch_size 16 \\\n--fp16\n```\n\nIf you are training this model on Colab, make the following code changes to avoid Out-of-memory error message:\n```bash\n###### modify L78 to disable fast tokenizer \ndefault=False,\n\n###### update dataset map part at L313\ndataset = dataset.map(tokenizer, input_columns=\"text\", fn_kwargs={\"padding\": \"max_length\", \"truncation\": True, \"max_length\": 512})\n\n###### add following lines to L213\ndel model\nprint(f\"Manually deleted Teacher model, free some memory for student model.\")\n\n###### add following lines to L337\ntrainer.push_to_hub()\ntokenizer.push_to_hub(\"distilbert-base-multilingual-cased-sentiments-student\")\n  \n```\n\n### Training log\n```bash\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n{'train_runtime': 2009.8864, 'train_samples_per_second': 73.0, 'train_steps_per_second': 4.563, 'train_loss': 0.6473459283913797, 'epoch': 1.0}\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9171/9171 [33:29<00:00,  4.56it/s]\n[INFO|trainer.py:762] 2023-05-06 10:56:18,555 >> The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n[INFO|trainer.py:3129] 2023-05-06 10:56:18,557 >> ***** Running Evaluation *****\n[INFO|trainer.py:3131] 2023-05-06 10:56:18,557 >>   Num examples = 146721\n[INFO|trainer.py:3134] 2023-05-06 10:56:18,557 >>   Batch size = 128\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1147/1147 [08:59<00:00,  2.13it/s]\n05/06/2023 11:05:18 - INFO - __main__ - Agreement of student and teacher predictions: 88.29%\n[INFO|trainer.py:2868] 2023-05-06 11:05:18,251 >> Saving model checkpoint to ./distilbert-base-multilingual-cased-sentiments-student\n[INFO|configuration_utils.py:457] 2023-05-06 11:05:18,251 >> Configuration saved in ./distilbert-base-multilingual-cased-sentiments-student/config.json\n[INFO|modeling_utils.py:1847] 2023-05-06 11:05:18,905 >> Model weights saved in ./distilbert-base-multilingual-cased-sentiments-student/pytorch_model.bin\n[INFO|tokenization_utils_base.py:2171] 2023-05-06 11:05:18,905 >> tokenizer config file saved in ./distilbert-base-multilingual-cased-sentiments-student/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2178] 2023-05-06 11:05:18,905 >> Special tokens file saved in ./distilbert-base-multilingual-cased-sentiments-student/special_tokens_map.json\n\n```\n\n### Framework versions\n\n- Transformers 4.28.1\n- Pytorch 2.0.0+cu118\n- Datasets 2.11.0\n- Tokenizers 0.13.3",
    "library_name": "transformers"
  },
  {
    "model_id": "microsoft/deberta-large-mnli",
    "model_name": "microsoft/deberta-large-mnli",
    "author": "microsoft",
    "downloads": 696508,
    "likes": 18,
    "tags": [
      "transformers",
      "pytorch",
      "deberta",
      "text-classification",
      "deberta-v1",
      "deberta-mnli",
      "en",
      "arxiv:2006.03654",
      "license:mit",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/microsoft/deberta-large-mnli",
    "dependencies": [
      [
        "transformers",
        "4.30.2"
      ],
      [
        "torch",
        "2.0.1"
      ],
      [
        "numpy",
        "1.24.3"
      ]
    ],
    "analysis_date": "2025-03-26T00:47:53.722871",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "deberta",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": "en",
      "license": "mit",
      "tags": [
        "deberta-v1",
        "deberta-mnli"
      ],
      "tasks": "mnli",
      "thumbnail": "https://huggingface.co/front/thumbnails/microsoft.png",
      "widget": [
        {
          "text": "[CLS] I love you. [SEP] I like you. [SEP]"
        }
      ]
    },
    "card_text": "\n## DeBERTa: Decoding-enhanced BERT with Disentangled Attention\n\n[DeBERTa](https://arxiv.org/abs/2006.03654) improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder. It outperforms BERT and RoBERTa on  majority of NLU tasks with 80GB training data. \n\nPlease check the [official repository](https://github.com/microsoft/DeBERTa) for more details and updates.\n\nThis is the DeBERTa large model fine-tuned with MNLI task.\n\n#### Fine-tuning on NLU tasks\n\nWe present the dev results on SQuAD 1.1/2.0 and several GLUE benchmark tasks.\n\n| Model                     | SQuAD 1.1 | SQuAD 2.0 | MNLI-m/mm   | SST-2 | QNLI | CoLA | RTE    | MRPC  | QQP   |STS-B |\n|---------------------------|-----------|-----------|-------------|-------|------|------|--------|-------|-------|------|\n|                           | F1/EM     | F1/EM     | Acc         | Acc   | Acc  | MCC  | Acc    |Acc/F1 |Acc/F1 |P/S   |\n| BERT-Large                | 90.9/84.1 | 81.8/79.0 | 86.6/-      | 93.2  | 92.3 | 60.6 | 70.4   | 88.0/-       | 91.3/- |90.0/- |\n| RoBERTa-Large             | 94.6/88.9 | 89.4/86.5 | 90.2/-      | 96.4  | 93.9 | 68.0 | 86.6   | 90.9/-       | 92.2/- |92.4/- |\n| XLNet-Large               | 95.1/89.7 | 90.6/87.9 | 90.8/-      | 97.0  | 94.9 | 69.0 | 85.9   | 90.8/-       | 92.3/- |92.5/- |\n| [DeBERTa-Large](https://huggingface.co/microsoft/deberta-large)<sup>1</sup> | 95.5/90.1 | 90.7/88.0 | 91.3/91.1| 96.5|95.3| 69.5| 91.0| 92.6/94.6| 92.3/- |92.8/92.5 |\n| [DeBERTa-XLarge](https://huggingface.co/microsoft/deberta-xlarge)<sup>1</sup> | -/-  | -/-  | 91.5/91.2| 97.0 | - | -    | 93.1   | 92.1/94.3    | -    |92.9/92.7|\n| [DeBERTa-V2-XLarge](https://huggingface.co/microsoft/deberta-v2-xlarge)<sup>1</sup>|95.8/90.8| 91.4/88.9|91.7/91.6| **97.5**| 95.8|71.1|**93.9**|92.0/94.2|92.3/89.8|92.9/92.9|\n|**[DeBERTa-V2-XXLarge](https://huggingface.co/microsoft/deberta-v2-xxlarge)<sup>1,2</sup>**|**96.1/91.4**|**92.2/89.7**|**91.7/91.9**|97.2|**96.0**|**72.0**| 93.5| **93.1/94.9**|**92.7/90.3** |**93.2/93.1** |\n--------\n#### Notes.\n - <sup>1</sup> Following RoBERTa, for RTE, MRPC, STS-B, we fine-tune the tasks based on [DeBERTa-Large-MNLI](https://huggingface.co/microsoft/deberta-large-mnli), [DeBERTa-XLarge-MNLI](https://huggingface.co/microsoft/deberta-xlarge-mnli), [DeBERTa-V2-XLarge-MNLI](https://huggingface.co/microsoft/deberta-v2-xlarge-mnli), [DeBERTa-V2-XXLarge-MNLI](https://huggingface.co/microsoft/deberta-v2-xxlarge-mnli). The results of SST-2/QQP/QNLI/SQuADv2 will also be slightly improved when start from MNLI fine-tuned models, however, we only report the numbers fine-tuned from pretrained base models for those 4 tasks.\n - <sup>2</sup> To try the **XXLarge** model with **[HF transformers](https://huggingface.co/transformers/main_classes/trainer.html)**, you need to specify **--sharded_ddp**\n \n```bash  \ncd transformers/examples/text-classification/\nexport TASK_NAME=mrpc\npython -m torch.distributed.launch --nproc_per_node=8 run_glue.py   --model_name_or_path microsoft/deberta-v2-xxlarge   \\\\\n--task_name $TASK_NAME   --do_train   --do_eval   --max_seq_length 128   --per_device_train_batch_size 4   \\\\\n--learning_rate 3e-6   --num_train_epochs 3   --output_dir /tmp/$TASK_NAME/ --overwrite_output_dir --sharded_ddp --fp16\n```\n\n### Citation\n\nIf you find DeBERTa useful for your work, please cite the following paper:\n\n``` latex\n@inproceedings{\nhe2021deberta,\ntitle={DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION},\nauthor={Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=XPZIaotutsD}\n}\n```\n\n",
    "card_content": "---\nlanguage: en\nlicense: mit\ntags:\n- deberta-v1\n- deberta-mnli\ntasks: mnli\nthumbnail: https://huggingface.co/front/thumbnails/microsoft.png\nwidget:\n- text: '[CLS] I love you. [SEP] I like you. [SEP]'\n---\n\n## DeBERTa: Decoding-enhanced BERT with Disentangled Attention\n\n[DeBERTa](https://arxiv.org/abs/2006.03654) improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder. It outperforms BERT and RoBERTa on  majority of NLU tasks with 80GB training data. \n\nPlease check the [official repository](https://github.com/microsoft/DeBERTa) for more details and updates.\n\nThis is the DeBERTa large model fine-tuned with MNLI task.\n\n#### Fine-tuning on NLU tasks\n\nWe present the dev results on SQuAD 1.1/2.0 and several GLUE benchmark tasks.\n\n| Model                     | SQuAD 1.1 | SQuAD 2.0 | MNLI-m/mm   | SST-2 | QNLI | CoLA | RTE    | MRPC  | QQP   |STS-B |\n|---------------------------|-----------|-----------|-------------|-------|------|------|--------|-------|-------|------|\n|                           | F1/EM     | F1/EM     | Acc         | Acc   | Acc  | MCC  | Acc    |Acc/F1 |Acc/F1 |P/S   |\n| BERT-Large                | 90.9/84.1 | 81.8/79.0 | 86.6/-      | 93.2  | 92.3 | 60.6 | 70.4   | 88.0/-       | 91.3/- |90.0/- |\n| RoBERTa-Large             | 94.6/88.9 | 89.4/86.5 | 90.2/-      | 96.4  | 93.9 | 68.0 | 86.6   | 90.9/-       | 92.2/- |92.4/- |\n| XLNet-Large               | 95.1/89.7 | 90.6/87.9 | 90.8/-      | 97.0  | 94.9 | 69.0 | 85.9   | 90.8/-       | 92.3/- |92.5/- |\n| [DeBERTa-Large](https://huggingface.co/microsoft/deberta-large)<sup>1</sup> | 95.5/90.1 | 90.7/88.0 | 91.3/91.1| 96.5|95.3| 69.5| 91.0| 92.6/94.6| 92.3/- |92.8/92.5 |\n| [DeBERTa-XLarge](https://huggingface.co/microsoft/deberta-xlarge)<sup>1</sup> | -/-  | -/-  | 91.5/91.2| 97.0 | - | -    | 93.1   | 92.1/94.3    | -    |92.9/92.7|\n| [DeBERTa-V2-XLarge](https://huggingface.co/microsoft/deberta-v2-xlarge)<sup>1</sup>|95.8/90.8| 91.4/88.9|91.7/91.6| **97.5**| 95.8|71.1|**93.9**|92.0/94.2|92.3/89.8|92.9/92.9|\n|**[DeBERTa-V2-XXLarge](https://huggingface.co/microsoft/deberta-v2-xxlarge)<sup>1,2</sup>**|**96.1/91.4**|**92.2/89.7**|**91.7/91.9**|97.2|**96.0**|**72.0**| 93.5| **93.1/94.9**|**92.7/90.3** |**93.2/93.1** |\n--------\n#### Notes.\n - <sup>1</sup> Following RoBERTa, for RTE, MRPC, STS-B, we fine-tune the tasks based on [DeBERTa-Large-MNLI](https://huggingface.co/microsoft/deberta-large-mnli), [DeBERTa-XLarge-MNLI](https://huggingface.co/microsoft/deberta-xlarge-mnli), [DeBERTa-V2-XLarge-MNLI](https://huggingface.co/microsoft/deberta-v2-xlarge-mnli), [DeBERTa-V2-XXLarge-MNLI](https://huggingface.co/microsoft/deberta-v2-xxlarge-mnli). The results of SST-2/QQP/QNLI/SQuADv2 will also be slightly improved when start from MNLI fine-tuned models, however, we only report the numbers fine-tuned from pretrained base models for those 4 tasks.\n - <sup>2</sup> To try the **XXLarge** model with **[HF transformers](https://huggingface.co/transformers/main_classes/trainer.html)**, you need to specify **--sharded_ddp**\n \n```bash  \ncd transformers/examples/text-classification/\nexport TASK_NAME=mrpc\npython -m torch.distributed.launch --nproc_per_node=8 run_glue.py   --model_name_or_path microsoft/deberta-v2-xxlarge   \\\\\n--task_name $TASK_NAME   --do_train   --do_eval   --max_seq_length 128   --per_device_train_batch_size 4   \\\\\n--learning_rate 3e-6   --num_train_epochs 3   --output_dir /tmp/$TASK_NAME/ --overwrite_output_dir --sharded_ddp --fp16\n```\n\n### Citation\n\nIf you find DeBERTa useful for your work, please cite the following paper:\n\n``` latex\n@inproceedings{\nhe2021deberta,\ntitle={DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION},\nauthor={Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=XPZIaotutsD}\n}\n```\n\n",
    "library_name": "transformers"
  },
  {
    "model_id": "unitary/toxic-bert",
    "model_name": "unitary/toxic-bert",
    "author": "unitary",
    "downloads": 685955,
    "likes": 164,
    "tags": [
      "transformers",
      "pytorch",
      "jax",
      "safetensors",
      "bert",
      "text-classification",
      "arxiv:1703.04009",
      "arxiv:1905.12516",
      "license:apache-2.0",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/unitary/toxic-bert",
    "dependencies": [
      [
        "detoxify",
        "0.5.1"
      ],
      [
        "transformers",
        "4.30.2"
      ],
      [
        "pytorch-lightning",
        "2.0.9"
      ],
      [
        "kaggle",
        "1.5.16"
      ],
      [
        "pandas",
        "2.0.3"
      ],
      [
        "torch",
        "2.0.1"
      ]
    ],
    "analysis_date": "2025-03-26T00:47:55.444439",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "bert",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "license": "apache-2.0"
    },
    "card_text": "\n      \n<div align=\"center\">    \n\n**\u26a0\ufe0f Disclaimer:**\nThe huggingface models currently give different results to the detoxify library (see issue [here](https://github.com/unitaryai/detoxify/issues/15)). For the most up to date models we recommend using the models from https://github.com/unitaryai/detoxify\n\n# \ud83d\ude4a Detoxify\n##  Toxic Comment Classification with \u26a1 Pytorch Lightning and \ud83e\udd17 Transformers   \n\n![CI testing](https://github.com/unitaryai/detoxify/workflows/CI%20testing/badge.svg)\n![Lint](https://github.com/unitaryai/detoxify/workflows/Lint/badge.svg)\n\n</div>\n \n![Examples image](examples.png)\n\n## Description   \n\nTrained models & code to predict toxic comments on 3 Jigsaw challenges: Toxic comment classification, Unintended\u00a0Bias in Toxic comments, Multilingual toxic comment classification.\n\nBuilt by [Laura Hanu](https://laurahanu.github.io/) at [Unitary](https://www.unitary.ai/), where we are working to stop harmful content online by interpreting visual content in context. \n\nDependencies:\n- For inference:\n  - \ud83e\udd17 Transformers\n  - \u26a1 Pytorch lightning \n- For training will also need:\n  - Kaggle API (to download data)\n\n\n| Challenge | Year | Goal | Original Data Source | Detoxify Model Name | Top Kaggle Leaderboard Score | Detoxify Score\n|-|-|-|-|-|-|-|\n| [Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge) | 2018 |  build a multi-headed model that\u2019s capable of detecting different types of of toxicity like threats, obscenity, insults, and identity-based hate. | Wikipedia Comments | `original` | 0.98856 | 0.98636\n| [Jigsaw Unintended Bias in Toxicity Classification](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification) | 2019 | build a model that recognizes toxicity and minimizes this type of unintended bias with respect to mentions of identities. You'll be using a dataset labeled for identity mentions and optimizing a metric designed to measure unintended bias. | Civil Comments | `unbiased` | 0.94734 | 0.93639\n| [Jigsaw Multilingual Toxic Comment Classification](https://www.kaggle.com/c/jigsaw-multilingual-toxic-comment-classification) | 2020 | build effective multilingual models | Wikipedia Comments + Civil Comments | `multilingual` | 0.9536 | 0.91655*\n\n*Score not directly comparable since it is obtained on the validation set provided and not on the test set. To update when the test labels are made available. \n\nIt is also noteworthy to mention that the top leadearboard scores have been achieved using model ensembles. The purpose of this library was to build something user-friendly and straightforward to use.\n\n## Limitations and ethical considerations\n\nIf words that are associated with swearing, insults or profanity are present in a comment, it is likely that it will be classified as toxic, regardless of the tone or the intent of the author e.g. humorous/self-deprecating. This could present some biases towards already vulnerable minority groups.\n\nThe intended use of this library is for research purposes, fine-tuning on carefully constructed datasets that reflect real world demographics  and/or to aid content moderators in flagging out harmful content quicker.\n\nSome useful resources about the risk of different biases in toxicity or hate speech detection are:\n- [The Risk of Racial Bias in Hate Speech Detection](https://homes.cs.washington.edu/~msap/pdfs/sap2019risk.pdf)\n- [Automated Hate Speech Detection and the Problem of Offensive Language](https://arxiv.org/pdf/1703.04009.pdf%201.pdf)\n- [Racial Bias in Hate Speech and Abusive Language Detection Datasets](https://arxiv.org/pdf/1905.12516.pdf)\n\n## Quick prediction\n\n\nThe `multilingual` model has been trained on 7 different languages so it should only be tested on: `english`, `french`, `spanish`, `italian`, `portuguese`, `turkish` or `russian`.\n\n```bash\n# install detoxify  \n\npip install detoxify\n\n```\n```python\n\nfrom detoxify import Detoxify\n\n# each model takes in either a string or a list of strings\n\nresults = Detoxify('original').predict('example text')\n\nresults = Detoxify('unbiased').predict(['example text 1','example text 2'])\n\nresults = Detoxify('multilingual').predict(['example text','exemple de texte','texto de ejemplo','testo di esempio','texto de exemplo','\u00f6rnek metin','\u043f\u0440\u0438\u043c\u0435\u0440 \u0442\u0435\u043a\u0441\u0442\u0430'])\n\n# optional to display results nicely (will need to pip install pandas)\n\nimport pandas as pd\n\nprint(pd.DataFrame(results, index=input_text).round(5))\n\n```\nFor more details check the Prediction section.\n\n\n## Labels\nAll challenges have a toxicity label. The toxicity labels represent the aggregate ratings of up to 10 annotators according the following schema:\n- **Very Toxic** (a very hateful, aggressive, or disrespectful comment that is very likely to make you leave a discussion or give up on sharing your perspective)\n- **Toxic** (a rude, disrespectful, or unreasonable comment that is somewhat likely to make you leave a discussion or give up on sharing your perspective)\n- **Hard to Say**\n- **Not Toxic**\n\nMore information about the labelling schema can be found [here](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/data).\n\n### Toxic Comment Classification Challenge\nThis challenge includes the following labels:\n\n- `toxic`\n- `severe_toxic`\n- `obscene`\n- `threat`\n- `insult`\n- `identity_hate`\n\n### Jigsaw Unintended Bias in Toxicity Classification\nThis challenge has 2 types of labels: the main toxicity labels and some additional identity labels that represent the identities mentioned in the comments. \n\nOnly identities with more than 500 examples in the test set (combined public and private) are included during training as additional labels and in the evaluation calculation.\n\n- `toxicity`\n- `severe_toxicity`\n- `obscene`\n- `threat`\n- `insult`\n- `identity_attack`\n- `sexual_explicit`\n\nIdentity labels used:\n- `male`\n- `female`\n- `homosexual_gay_or_lesbian`\n- `christian`\n- `jewish`\n- `muslim`\n- `black`\n- `white`\n- `psychiatric_or_mental_illness`\n\nA complete list of all the identity labels available can be found [here](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/data).\n\n\n### Jigsaw Multilingual Toxic Comment Classification\n\nSince this challenge combines the data from the previous 2 challenges, it includes all labels from above, however the final evaluation is only on:\n\n- `toxicity`\n\n## How to run   \n\nFirst, install dependencies   \n```bash\n# clone project   \n\ngit clone https://github.com/unitaryai/detoxify\n\n# create virtual env\n\npython3 -m venv toxic-env\nsource toxic-env/bin/activate\n\n# install project   \n\npip install -e detoxify\ncd detoxify\n\n# for training\npip install -r requirements.txt\n\n ```   \n\n## Prediction\n\nTrained models summary:\n\n|Model name| Transformer type| Data from\n|:--:|:--:|:--:|\n|`original`| `bert-base-uncased` | Toxic Comment Classification Challenge\n|`unbiased`| `roberta-base`| Unintended Bias in Toxicity Classification\n|`multilingual`| `xlm-roberta-base`| Multilingual Toxic Comment Classification\n\nFor a quick prediction can run the example script on a comment directly or from a txt containing a list of comments. \n```bash\n\n# load model via torch.hub\n\npython run_prediction.py --input 'example' --model_name original\n\n# load model from from checkpoint path\n\npython run_prediction.py --input 'example' --from_ckpt_path model_path\n\n# save results to a .csv file\n\npython run_prediction.py --input test_set.txt --model_name original --save_to results.csv\n\n# to see usage\n\npython run_prediction.py --help\n\n```\n\nCheckpoints can be downloaded from the latest release or via the Pytorch hub API with the following names:\n- `toxic_bert`\n- `unbiased_toxic_roberta`\n- `multilingual_toxic_xlm_r`\n```bash\nmodel = torch.hub.load('unitaryai/detoxify','toxic_bert')\n```\n\nImporting detoxify in python:\n\n```python\n\nfrom detoxify import Detoxify\n\nresults = Detoxify('original').predict('some text')\n\nresults = Detoxify('unbiased').predict(['example text 1','example text 2'])\n\nresults = Detoxify('multilingual').predict(['example text','exemple de texte','texto de ejemplo','testo di esempio','texto de exemplo','\u00f6rnek metin','\u043f\u0440\u0438\u043c\u0435\u0440 \u0442\u0435\u043a\u0441\u0442\u0430'])\n\n# to display results nicely\n\nimport pandas as pd\n\nprint(pd.DataFrame(results,index=input_text).round(5))\n\n```\n\n\n## Training\n\n If you do not already have a Kaggle account: \n - you need to create one to be able to download the data\n \n - go to My Account and click on Create New API Token - this will download a kaggle.json file\n\n - make sure this file is located in ~/.kaggle\n\n ```bash\n\n# create data directory\n\nmkdir jigsaw_data\ncd jigsaw_data\n\n# download data\n\nkaggle competitions download -c jigsaw-toxic-comment-classification-challenge\n\nkaggle competitions download -c jigsaw-unintended-bias-in-toxicity-classification\n\nkaggle competitions download -c jigsaw-multilingual-toxic-comment-classification\n\n```\n## Start Training\n ### Toxic Comment Classification Challenge\n\n ```bash\n\npython create_val_set.py\n\npython train.py --config configs/Toxic_comment_classification_BERT.json\n``` \n ### Unintended Bias in Toxicicity Challenge\n\n```bash\n\npython train.py --config configs/Unintended_bias_toxic_comment_classification_RoBERTa.json\n\n```\n ### Multilingual Toxic Comment Classification\n\n This is trained in 2 stages. First, train on all available data, and second, train only on the translated versions of the first challenge. \n \n The [translated data](https://www.kaggle.com/miklgr500/jigsaw-train-multilingual-coments-google-api) can be downloaded from Kaggle in french, spanish, italian, portuguese, turkish, and russian (the languages available in the test set).\n\n ```bash\n\n# stage 1\n\npython train.py --config configs/Multilingual_toxic_comment_classification_XLMR.json\n\n# stage 2\n\npython train.py --config configs/Multilingual_toxic_comment_classification_XLMR_stage2.json\n\n```\n### Monitor progress with tensorboard\n\n ```bash\n\ntensorboard --logdir=./saved\n\n```\n## Model Evaluation\n\n### Toxic Comment Classification Challenge\n\nThis challenge is evaluated on the mean AUC score of all the labels.\n\n```bash\n\npython evaluate.py --checkpoint saved/lightning_logs/checkpoints/example_checkpoint.pth --test_csv test.csv\n\n```\n### Unintended Bias in Toxicicity Challenge\n\nThis challenge is evaluated on a novel bias metric that combines different AUC scores to balance overall performance. More information on this metric [here](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/overview/evaluation).\n\n```bash\n\npython evaluate.py --checkpoint saved/lightning_logs/checkpoints/example_checkpoint.pth --test_csv test.csv\n\n# to get the final bias metric\npython model_eval/compute_bias_metric.py\n\n```\n### Multilingual Toxic Comment Classification\n\nThis challenge is evaluated on the AUC score of the main toxic label.\n\n```bash\n\npython evaluate.py --checkpoint saved/lightning_logs/checkpoints/example_checkpoint.pth --test_csv test.csv\n\n```\n\n### Citation   \n```\n@misc{Detoxify,\n  title={Detoxify},\n  author={Hanu, Laura and {Unitary team}},\n  howpublished={Github. https://github.com/unitaryai/detoxify},\n  year={2020}\n}\n```",
    "card_content": "---\nlicense: apache-2.0\n---\n\n      \n<div align=\"center\">    \n\n**\u26a0\ufe0f Disclaimer:**\nThe huggingface models currently give different results to the detoxify library (see issue [here](https://github.com/unitaryai/detoxify/issues/15)). For the most up to date models we recommend using the models from https://github.com/unitaryai/detoxify\n\n# \ud83d\ude4a Detoxify\n##  Toxic Comment Classification with \u26a1 Pytorch Lightning and \ud83e\udd17 Transformers   \n\n![CI testing](https://github.com/unitaryai/detoxify/workflows/CI%20testing/badge.svg)\n![Lint](https://github.com/unitaryai/detoxify/workflows/Lint/badge.svg)\n\n</div>\n \n![Examples image](examples.png)\n\n## Description   \n\nTrained models & code to predict toxic comments on 3 Jigsaw challenges: Toxic comment classification, Unintended\u00a0Bias in Toxic comments, Multilingual toxic comment classification.\n\nBuilt by [Laura Hanu](https://laurahanu.github.io/) at [Unitary](https://www.unitary.ai/), where we are working to stop harmful content online by interpreting visual content in context. \n\nDependencies:\n- For inference:\n  - \ud83e\udd17 Transformers\n  - \u26a1 Pytorch lightning \n- For training will also need:\n  - Kaggle API (to download data)\n\n\n| Challenge | Year | Goal | Original Data Source | Detoxify Model Name | Top Kaggle Leaderboard Score | Detoxify Score\n|-|-|-|-|-|-|-|\n| [Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge) | 2018 |  build a multi-headed model that\u2019s capable of detecting different types of of toxicity like threats, obscenity, insults, and identity-based hate. | Wikipedia Comments | `original` | 0.98856 | 0.98636\n| [Jigsaw Unintended Bias in Toxicity Classification](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification) | 2019 | build a model that recognizes toxicity and minimizes this type of unintended bias with respect to mentions of identities. You'll be using a dataset labeled for identity mentions and optimizing a metric designed to measure unintended bias. | Civil Comments | `unbiased` | 0.94734 | 0.93639\n| [Jigsaw Multilingual Toxic Comment Classification](https://www.kaggle.com/c/jigsaw-multilingual-toxic-comment-classification) | 2020 | build effective multilingual models | Wikipedia Comments + Civil Comments | `multilingual` | 0.9536 | 0.91655*\n\n*Score not directly comparable since it is obtained on the validation set provided and not on the test set. To update when the test labels are made available. \n\nIt is also noteworthy to mention that the top leadearboard scores have been achieved using model ensembles. The purpose of this library was to build something user-friendly and straightforward to use.\n\n## Limitations and ethical considerations\n\nIf words that are associated with swearing, insults or profanity are present in a comment, it is likely that it will be classified as toxic, regardless of the tone or the intent of the author e.g. humorous/self-deprecating. This could present some biases towards already vulnerable minority groups.\n\nThe intended use of this library is for research purposes, fine-tuning on carefully constructed datasets that reflect real world demographics  and/or to aid content moderators in flagging out harmful content quicker.\n\nSome useful resources about the risk of different biases in toxicity or hate speech detection are:\n- [The Risk of Racial Bias in Hate Speech Detection](https://homes.cs.washington.edu/~msap/pdfs/sap2019risk.pdf)\n- [Automated Hate Speech Detection and the Problem of Offensive Language](https://arxiv.org/pdf/1703.04009.pdf%201.pdf)\n- [Racial Bias in Hate Speech and Abusive Language Detection Datasets](https://arxiv.org/pdf/1905.12516.pdf)\n\n## Quick prediction\n\n\nThe `multilingual` model has been trained on 7 different languages so it should only be tested on: `english`, `french`, `spanish`, `italian`, `portuguese`, `turkish` or `russian`.\n\n```bash\n# install detoxify  \n\npip install detoxify\n\n```\n```python\n\nfrom detoxify import Detoxify\n\n# each model takes in either a string or a list of strings\n\nresults = Detoxify('original').predict('example text')\n\nresults = Detoxify('unbiased').predict(['example text 1','example text 2'])\n\nresults = Detoxify('multilingual').predict(['example text','exemple de texte','texto de ejemplo','testo di esempio','texto de exemplo','\u00f6rnek metin','\u043f\u0440\u0438\u043c\u0435\u0440 \u0442\u0435\u043a\u0441\u0442\u0430'])\n\n# optional to display results nicely (will need to pip install pandas)\n\nimport pandas as pd\n\nprint(pd.DataFrame(results, index=input_text).round(5))\n\n```\nFor more details check the Prediction section.\n\n\n## Labels\nAll challenges have a toxicity label. The toxicity labels represent the aggregate ratings of up to 10 annotators according the following schema:\n- **Very Toxic** (a very hateful, aggressive, or disrespectful comment that is very likely to make you leave a discussion or give up on sharing your perspective)\n- **Toxic** (a rude, disrespectful, or unreasonable comment that is somewhat likely to make you leave a discussion or give up on sharing your perspective)\n- **Hard to Say**\n- **Not Toxic**\n\nMore information about the labelling schema can be found [here](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/data).\n\n### Toxic Comment Classification Challenge\nThis challenge includes the following labels:\n\n- `toxic`\n- `severe_toxic`\n- `obscene`\n- `threat`\n- `insult`\n- `identity_hate`\n\n### Jigsaw Unintended Bias in Toxicity Classification\nThis challenge has 2 types of labels: the main toxicity labels and some additional identity labels that represent the identities mentioned in the comments. \n\nOnly identities with more than 500 examples in the test set (combined public and private) are included during training as additional labels and in the evaluation calculation.\n\n- `toxicity`\n- `severe_toxicity`\n- `obscene`\n- `threat`\n- `insult`\n- `identity_attack`\n- `sexual_explicit`\n\nIdentity labels used:\n- `male`\n- `female`\n- `homosexual_gay_or_lesbian`\n- `christian`\n- `jewish`\n- `muslim`\n- `black`\n- `white`\n- `psychiatric_or_mental_illness`\n\nA complete list of all the identity labels available can be found [here](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/data).\n\n\n### Jigsaw Multilingual Toxic Comment Classification\n\nSince this challenge combines the data from the previous 2 challenges, it includes all labels from above, however the final evaluation is only on:\n\n- `toxicity`\n\n## How to run   \n\nFirst, install dependencies   \n```bash\n# clone project   \n\ngit clone https://github.com/unitaryai/detoxify\n\n# create virtual env\n\npython3 -m venv toxic-env\nsource toxic-env/bin/activate\n\n# install project   \n\npip install -e detoxify\ncd detoxify\n\n# for training\npip install -r requirements.txt\n\n ```   \n\n## Prediction\n\nTrained models summary:\n\n|Model name| Transformer type| Data from\n|:--:|:--:|:--:|\n|`original`| `bert-base-uncased` | Toxic Comment Classification Challenge\n|`unbiased`| `roberta-base`| Unintended Bias in Toxicity Classification\n|`multilingual`| `xlm-roberta-base`| Multilingual Toxic Comment Classification\n\nFor a quick prediction can run the example script on a comment directly or from a txt containing a list of comments. \n```bash\n\n# load model via torch.hub\n\npython run_prediction.py --input 'example' --model_name original\n\n# load model from from checkpoint path\n\npython run_prediction.py --input 'example' --from_ckpt_path model_path\n\n# save results to a .csv file\n\npython run_prediction.py --input test_set.txt --model_name original --save_to results.csv\n\n# to see usage\n\npython run_prediction.py --help\n\n```\n\nCheckpoints can be downloaded from the latest release or via the Pytorch hub API with the following names:\n- `toxic_bert`\n- `unbiased_toxic_roberta`\n- `multilingual_toxic_xlm_r`\n```bash\nmodel = torch.hub.load('unitaryai/detoxify','toxic_bert')\n```\n\nImporting detoxify in python:\n\n```python\n\nfrom detoxify import Detoxify\n\nresults = Detoxify('original').predict('some text')\n\nresults = Detoxify('unbiased').predict(['example text 1','example text 2'])\n\nresults = Detoxify('multilingual').predict(['example text','exemple de texte','texto de ejemplo','testo di esempio','texto de exemplo','\u00f6rnek metin','\u043f\u0440\u0438\u043c\u0435\u0440 \u0442\u0435\u043a\u0441\u0442\u0430'])\n\n# to display results nicely\n\nimport pandas as pd\n\nprint(pd.DataFrame(results,index=input_text).round(5))\n\n```\n\n\n## Training\n\n If you do not already have a Kaggle account: \n - you need to create one to be able to download the data\n \n - go to My Account and click on Create New API Token - this will download a kaggle.json file\n\n - make sure this file is located in ~/.kaggle\n\n ```bash\n\n# create data directory\n\nmkdir jigsaw_data\ncd jigsaw_data\n\n# download data\n\nkaggle competitions download -c jigsaw-toxic-comment-classification-challenge\n\nkaggle competitions download -c jigsaw-unintended-bias-in-toxicity-classification\n\nkaggle competitions download -c jigsaw-multilingual-toxic-comment-classification\n\n```\n## Start Training\n ### Toxic Comment Classification Challenge\n\n ```bash\n\npython create_val_set.py\n\npython train.py --config configs/Toxic_comment_classification_BERT.json\n``` \n ### Unintended Bias in Toxicicity Challenge\n\n```bash\n\npython train.py --config configs/Unintended_bias_toxic_comment_classification_RoBERTa.json\n\n```\n ### Multilingual Toxic Comment Classification\n\n This is trained in 2 stages. First, train on all available data, and second, train only on the translated versions of the first challenge. \n \n The [translated data](https://www.kaggle.com/miklgr500/jigsaw-train-multilingual-coments-google-api) can be downloaded from Kaggle in french, spanish, italian, portuguese, turkish, and russian (the languages available in the test set).\n\n ```bash\n\n# stage 1\n\npython train.py --config configs/Multilingual_toxic_comment_classification_XLMR.json\n\n# stage 2\n\npython train.py --config configs/Multilingual_toxic_comment_classification_XLMR_stage2.json\n\n```\n### Monitor progress with tensorboard\n\n ```bash\n\ntensorboard --logdir=./saved\n\n```\n## Model Evaluation\n\n### Toxic Comment Classification Challenge\n\nThis challenge is evaluated on the mean AUC score of all the labels.\n\n```bash\n\npython evaluate.py --checkpoint saved/lightning_logs/checkpoints/example_checkpoint.pth --test_csv test.csv\n\n```\n### Unintended Bias in Toxicicity Challenge\n\nThis challenge is evaluated on a novel bias metric that combines different AUC scores to balance overall performance. More information on this metric [here](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/overview/evaluation).\n\n```bash\n\npython evaluate.py --checkpoint saved/lightning_logs/checkpoints/example_checkpoint.pth --test_csv test.csv\n\n# to get the final bias metric\npython model_eval/compute_bias_metric.py\n\n```\n### Multilingual Toxic Comment Classification\n\nThis challenge is evaluated on the AUC score of the main toxic label.\n\n```bash\n\npython evaluate.py --checkpoint saved/lightning_logs/checkpoints/example_checkpoint.pth --test_csv test.csv\n\n```\n\n### Citation   \n```\n@misc{Detoxify,\n  title={Detoxify},\n  author={Hanu, Laura and {Unitary team}},\n  howpublished={Github. https://github.com/unitaryai/detoxify},\n  year={2020}\n}\n```",
    "library_name": "transformers"
  },
  {
    "model_id": "facebook/fasttext-language-identification",
    "model_name": "facebook/fasttext-language-identification",
    "author": "facebook",
    "downloads": 642817,
    "likes": 224,
    "tags": [
      "fasttext",
      "text-classification",
      "language-identification",
      "arxiv:1607.04606",
      "arxiv:1802.06893",
      "arxiv:1607.01759",
      "arxiv:1612.03651",
      "license:cc-by-nc-4.0",
      "region:us"
    ],
    "card_url": "https://huggingface.co/facebook/fasttext-language-identification",
    "dependencies": [
      [
        "fasttext",
        "0.9.2"
      ],
      [
        "huggingface_hub",
        null
      ],
      [
        "numpy",
        null
      ],
      [
        "python3-dev",
        "3.8.10"
      ],
      [
        "pybind11",
        "2.10.4"
      ]
    ],
    "analysis_date": "2025-03-26T00:47:57.150590",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "unknown",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "license": "cc-by-nc-4.0",
      "library_name": "fasttext",
      "tags": [
        "text-classification",
        "language-identification"
      ]
    },
    "card_text": "\n# fastText (Language Identification)\n\nfastText is an open-source, free, lightweight library that allows users to learn text representations and text classifiers. It works on standard, generic hardware. Models can later be reduced in size to even fit on mobile devices. It was introduced in [this paper](https://arxiv.org/abs/1607.04606). The official website can be found [here](https://fasttext.cc/).\n\nThis LID (Language IDentification) model is used to predict the language of the input text, and the hosted version (`lid218e`) was [released as part of the NLLB project](https://github.com/facebookresearch/fairseq/blob/nllb/README.md#lid-model) and can detect 217 languages. You can find older versions (ones that can identify 157 languages) on the [official fastText website](https://fasttext.cc/docs/en/language-identification.html).\n\n## Model description\n\nfastText is a library for efficient learning of word representations and sentence classification. fastText is designed to be simple to use for developers, domain experts, and students. It's dedicated to text classification and learning word representations, and was designed to allow for quick model iteration and refinement without specialized hardware. fastText models can be trained on more than a billion words on any multicore CPU in less than a few minutes.\n\nIt includes pre-trained models learned on Wikipedia and in over 157 different languages. fastText can be used as a command line, linked to a C++ application, or used as a library for use cases from experimentation and prototyping to production.\n\n## Intended uses & limitations\n\nYou can use pre-trained word vectors for text classification or language identification. See the [tutorials](https://fasttext.cc/docs/en/supervised-tutorial.html) and [resources](https://fasttext.cc/docs/en/english-vectors.html) on its official website to look for tasks that interest you.\n\n### How to use\n\nHere is how to use this model to detect the language of a given text:\n\n```python\n>>> import fasttext\n>>> from huggingface_hub import hf_hub_download\n\n>>> model_path = hf_hub_download(repo_id=\"facebook/fasttext-language-identification\", filename=\"model.bin\")\n>>> model = fasttext.load_model(model_path)\n>>> model.predict(\"Hello, world!\")\n\n(('__label__eng_Latn',), array([0.81148803]))\n\n>>> model.predict(\"Hello, world!\", k=5)\n\n(('__label__eng_Latn', '__label__vie_Latn', '__label__nld_Latn', '__label__pol_Latn', '__label__deu_Latn'), \n array([0.61224753, 0.21323682, 0.09696738, 0.01359863, 0.01319415]))\n```\n\n### Limitations and bias\n\nEven if the training data used for this model could be characterized as fairly neutral, this model can have biased predictions. \n\nCosine similarity can be used to measure the similarity between two different word vectors. If two two vectors are identical, the cosine similarity will be 1. For two completely unrelated vectors, the value will be 0. If two vectors have an opposite relationship, the value will be -1.\n\n```python\n>>> import numpy as np\n\n>>> def cosine_similarity(word1, word2):\n>>>     return np.dot(model[word1], model[word2]) / (np.linalg.norm(model[word1]) * np.linalg.norm(model[word2]))\n\n>>> cosine_similarity(\"man\", \"boy\")\n\n0.061653383\n\n>>> cosine_similarity(\"man\", \"ceo\")\n\n0.11989131\n\n>>> cosine_similarity(\"woman\", \"ceo\")\n\n-0.08834904\n```\n\n## Training data\n\nPre-trained word vectors for 157 languages were trained on [Common Crawl](http://commoncrawl.org/) and [Wikipedia](https://www.wikipedia.org/) using fastText. These models were trained using CBOW with position-weights, in dimension 300, with character n-grams of length 5, a window of size 5 and 10 negatives. We also distribute three new word analogy datasets, for French, Hindi and Polish.\n\n## Training procedure\n\n### Tokenization\n\nWe used the [Stanford word segmenter](https://nlp.stanford.edu/software/segmenter.html) for Chinese, [Mecab](http://taku910.github.io/mecab/) for Japanese and [UETsegmenter](https://github.com/phongnt570/UETsegmenter) for Vietnamese. For languages using the Latin, Cyrillic, Hebrew or Greek scripts, we used the tokenizer from the [Europarl](https://www.statmt.org/europarl/) preprocessing tools. For the remaining languages, we used the ICU tokenizer.\n\nMore information about the training of these models can be found in the article [Learning Word Vectors for 157 Languages](https://arxiv.org/abs/1802.06893).\n\n### License\n\nThe language identification model is distributed under the [*Creative Commons Attribution-NonCommercial 4.0 International Public License*](https://creativecommons.org/licenses/by-nc/4.0/).\n\n### Evaluation datasets\n\nThe analogy evaluation datasets described in the paper are available here: [French](https://dl.fbaipublicfiles.com/fasttext/word-analogies/questions-words-fr.txt), [Hindi](https://dl.fbaipublicfiles.com/fasttext/word-analogies/questions-words-hi.txt), [Polish](https://dl.fbaipublicfiles.com/fasttext/word-analogies/questions-words-pl.txt).\n\n### BibTeX entry and citation info\n\nPlease cite [1] if using this code for learning word representations or [2] if using for text classification.\n\n[1] P. Bojanowski\\*, E. Grave\\*, A. Joulin, T. Mikolov, [*Enriching Word Vectors with Subword Information*](https://arxiv.org/abs/1607.04606)\n\n```markup\n@article{bojanowski2016enriching,\n  title={Enriching Word Vectors with Subword Information},\n  author={Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},\n  journal={arXiv preprint arXiv:1607.04606},\n  year={2016}\n}\n```\n\n[2] A. Joulin, E. Grave, P. Bojanowski, T. Mikolov, [*Bag of Tricks for Efficient Text Classification*](https://arxiv.org/abs/1607.01759)\n\n```markup\n@article{joulin2016bag,\n  title={Bag of Tricks for Efficient Text Classification},\n  author={Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Mikolov, Tomas},\n  journal={arXiv preprint arXiv:1607.01759},\n  year={2016}\n}\n```\n\n[3] A. Joulin, E. Grave, P. Bojanowski, M. Douze, H. J\u00e9gou, T. Mikolov, [*FastText.zip: Compressing text classification models*](https://arxiv.org/abs/1612.03651)\n\n```markup\n@article{joulin2016fasttext,\n  title={FastText.zip: Compressing text classification models},\n  author={Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Douze, Matthijs and J{'e}gou, H{'e}rve and Mikolov, Tomas},\n  journal={arXiv preprint arXiv:1612.03651},\n  year={2016}\n}\n```\n\nIf you use these word vectors, please cite the following paper:\n\n[4] E. Grave\\*, P. Bojanowski\\*, P. Gupta, A. Joulin, T. Mikolov, [*Learning Word Vectors for 157 Languages*](https://arxiv.org/abs/1802.06893)\n\n```markup\n@inproceedings{grave2018learning,\n  title={Learning Word Vectors for 157 Languages},\n  author={Grave, Edouard and Bojanowski, Piotr and Gupta, Prakhar and Joulin, Armand and Mikolov, Tomas},\n  booktitle={Proceedings of the International Conference on Language Resources and Evaluation (LREC 2018)},\n  year={2018}\n}\n```\n\n(\\* These authors contributed equally.)\n\n",
    "card_content": "---\nlicense: cc-by-nc-4.0\nlibrary_name: fasttext\ntags:\n- text-classification\n- language-identification\n---\n\n# fastText (Language Identification)\n\nfastText is an open-source, free, lightweight library that allows users to learn text representations and text classifiers. It works on standard, generic hardware. Models can later be reduced in size to even fit on mobile devices. It was introduced in [this paper](https://arxiv.org/abs/1607.04606). The official website can be found [here](https://fasttext.cc/).\n\nThis LID (Language IDentification) model is used to predict the language of the input text, and the hosted version (`lid218e`) was [released as part of the NLLB project](https://github.com/facebookresearch/fairseq/blob/nllb/README.md#lid-model) and can detect 217 languages. You can find older versions (ones that can identify 157 languages) on the [official fastText website](https://fasttext.cc/docs/en/language-identification.html).\n\n## Model description\n\nfastText is a library for efficient learning of word representations and sentence classification. fastText is designed to be simple to use for developers, domain experts, and students. It's dedicated to text classification and learning word representations, and was designed to allow for quick model iteration and refinement without specialized hardware. fastText models can be trained on more than a billion words on any multicore CPU in less than a few minutes.\n\nIt includes pre-trained models learned on Wikipedia and in over 157 different languages. fastText can be used as a command line, linked to a C++ application, or used as a library for use cases from experimentation and prototyping to production.\n\n## Intended uses & limitations\n\nYou can use pre-trained word vectors for text classification or language identification. See the [tutorials](https://fasttext.cc/docs/en/supervised-tutorial.html) and [resources](https://fasttext.cc/docs/en/english-vectors.html) on its official website to look for tasks that interest you.\n\n### How to use\n\nHere is how to use this model to detect the language of a given text:\n\n```python\n>>> import fasttext\n>>> from huggingface_hub import hf_hub_download\n\n>>> model_path = hf_hub_download(repo_id=\"facebook/fasttext-language-identification\", filename=\"model.bin\")\n>>> model = fasttext.load_model(model_path)\n>>> model.predict(\"Hello, world!\")\n\n(('__label__eng_Latn',), array([0.81148803]))\n\n>>> model.predict(\"Hello, world!\", k=5)\n\n(('__label__eng_Latn', '__label__vie_Latn', '__label__nld_Latn', '__label__pol_Latn', '__label__deu_Latn'), \n array([0.61224753, 0.21323682, 0.09696738, 0.01359863, 0.01319415]))\n```\n\n### Limitations and bias\n\nEven if the training data used for this model could be characterized as fairly neutral, this model can have biased predictions. \n\nCosine similarity can be used to measure the similarity between two different word vectors. If two two vectors are identical, the cosine similarity will be 1. For two completely unrelated vectors, the value will be 0. If two vectors have an opposite relationship, the value will be -1.\n\n```python\n>>> import numpy as np\n\n>>> def cosine_similarity(word1, word2):\n>>>     return np.dot(model[word1], model[word2]) / (np.linalg.norm(model[word1]) * np.linalg.norm(model[word2]))\n\n>>> cosine_similarity(\"man\", \"boy\")\n\n0.061653383\n\n>>> cosine_similarity(\"man\", \"ceo\")\n\n0.11989131\n\n>>> cosine_similarity(\"woman\", \"ceo\")\n\n-0.08834904\n```\n\n## Training data\n\nPre-trained word vectors for 157 languages were trained on [Common Crawl](http://commoncrawl.org/) and [Wikipedia](https://www.wikipedia.org/) using fastText. These models were trained using CBOW with position-weights, in dimension 300, with character n-grams of length 5, a window of size 5 and 10 negatives. We also distribute three new word analogy datasets, for French, Hindi and Polish.\n\n## Training procedure\n\n### Tokenization\n\nWe used the [Stanford word segmenter](https://nlp.stanford.edu/software/segmenter.html) for Chinese, [Mecab](http://taku910.github.io/mecab/) for Japanese and [UETsegmenter](https://github.com/phongnt570/UETsegmenter) for Vietnamese. For languages using the Latin, Cyrillic, Hebrew or Greek scripts, we used the tokenizer from the [Europarl](https://www.statmt.org/europarl/) preprocessing tools. For the remaining languages, we used the ICU tokenizer.\n\nMore information about the training of these models can be found in the article [Learning Word Vectors for 157 Languages](https://arxiv.org/abs/1802.06893).\n\n### License\n\nThe language identification model is distributed under the [*Creative Commons Attribution-NonCommercial 4.0 International Public License*](https://creativecommons.org/licenses/by-nc/4.0/).\n\n### Evaluation datasets\n\nThe analogy evaluation datasets described in the paper are available here: [French](https://dl.fbaipublicfiles.com/fasttext/word-analogies/questions-words-fr.txt), [Hindi](https://dl.fbaipublicfiles.com/fasttext/word-analogies/questions-words-hi.txt), [Polish](https://dl.fbaipublicfiles.com/fasttext/word-analogies/questions-words-pl.txt).\n\n### BibTeX entry and citation info\n\nPlease cite [1] if using this code for learning word representations or [2] if using for text classification.\n\n[1] P. Bojanowski\\*, E. Grave\\*, A. Joulin, T. Mikolov, [*Enriching Word Vectors with Subword Information*](https://arxiv.org/abs/1607.04606)\n\n```markup\n@article{bojanowski2016enriching,\n  title={Enriching Word Vectors with Subword Information},\n  author={Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},\n  journal={arXiv preprint arXiv:1607.04606},\n  year={2016}\n}\n```\n\n[2] A. Joulin, E. Grave, P. Bojanowski, T. Mikolov, [*Bag of Tricks for Efficient Text Classification*](https://arxiv.org/abs/1607.01759)\n\n```markup\n@article{joulin2016bag,\n  title={Bag of Tricks for Efficient Text Classification},\n  author={Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Mikolov, Tomas},\n  journal={arXiv preprint arXiv:1607.01759},\n  year={2016}\n}\n```\n\n[3] A. Joulin, E. Grave, P. Bojanowski, M. Douze, H. J\u00e9gou, T. Mikolov, [*FastText.zip: Compressing text classification models*](https://arxiv.org/abs/1612.03651)\n\n```markup\n@article{joulin2016fasttext,\n  title={FastText.zip: Compressing text classification models},\n  author={Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Douze, Matthijs and J{'e}gou, H{'e}rve and Mikolov, Tomas},\n  journal={arXiv preprint arXiv:1612.03651},\n  year={2016}\n}\n```\n\nIf you use these word vectors, please cite the following paper:\n\n[4] E. Grave\\*, P. Bojanowski\\*, P. Gupta, A. Joulin, T. Mikolov, [*Learning Word Vectors for 157 Languages*](https://arxiv.org/abs/1802.06893)\n\n```markup\n@inproceedings{grave2018learning,\n  title={Learning Word Vectors for 157 Languages},\n  author={Grave, Edouard and Bojanowski, Piotr and Gupta, Prakhar and Joulin, Armand and Mikolov, Tomas},\n  booktitle={Proceedings of the International Conference on Language Resources and Evaluation (LREC 2018)},\n  year={2018}\n}\n```\n\n(\\* These authors contributed equally.)\n\n",
    "library_name": "fasttext"
  },
  {
    "model_id": "Elron/bleurt-tiny-512",
    "model_name": "Elron/bleurt-tiny-512",
    "author": "Elron",
    "downloads": 641849,
    "likes": 4,
    "tags": [
      "transformers",
      "pytorch",
      "bert",
      "text-classification",
      "arxiv:1910.09700",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/Elron/bleurt-tiny-512",
    "dependencies": [
      [
        "transformers",
        "4.30.2"
      ],
      [
        "torch",
        "2.0.1"
      ],
      [
        "numpy",
        "1.24.3"
      ]
    ],
    "analysis_date": "2025-03-26T00:47:58.191480",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "bert",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "tags": [
        "text-classification",
        "bert"
      ]
    },
    "card_text": "\n# Model Card for bleurt-tiny-512 \n \n# Model Details\n \n## Model Description\n \nPytorch version of the original BLEURT models from ACL paper\n \n- **Developed by:** Elron Bandel, Thibault Sellam, Dipanjan Das and Ankur P. Parikh of Google Research\n- **Shared by [Optional]:** Elron Bandel\n- **Model type:** Text Classification \n- **Language(s) (NLP):** More information needed\n- **License:** More information needed \n- **Parent Model:** BERT\n- **Resources for more information:**\n     - [GitHub Repo](https://github.com/google-research/bleurt/tree/master)\n \t  - [Associated Paper](https://aclanthology.org/2020.acl-main.704/)\n    - [Blog Post](https://ai.googleblog.com/2020/05/evaluating-natural-language-generation.html)\n \t\n\n\n# Uses\n \n\n## Direct Use\nThis model can be used for the task of Text Classification \n \n## Downstream Use [Optional]\n \nMore information needed.\n \n## Out-of-Scope Use\n \nThe model should not be used to intentionally create hostile or alienating environments for people. \n \n# Bias, Risks, and Limitations\n \n \nSignificant research has explored bias and fairness issues with language models (see, e.g., [Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)). Predictions generated by the model may include disturbing and harmful stereotypes across protected classes; identity characteristics; and sensitive, social, and occupational groups.\n\n\n\n## Recommendations\n \n \nUsers (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.\n\n# Training Details\n \n## Training Data\nThe model authors note in the [associated paper](https://aclanthology.org/2020.acl-main.704.pdf): \n> We use years 2017 to 2019 of the WMT Metrics Shared Task, to-English language pairs. For each year, we used the of- ficial WMT test set, which include several thou- sand pairs of sentences with human ratings from the news domain. The training sets contain 5,360, 9,492, and 147,691 records for each year. \n \n \n## Training Procedure\n\n \n### Preprocessing\n \nMore information needed \n \n### Speeds, Sizes, Times\nMore information needed \n\n \n# Evaluation\n \n \n## Testing Data, Factors & Metrics\n \n### Testing Data\n \nThe test sets for years 2018 and 2019 [of the WMT Metrics Shared Task, to-English language pairs.]  are noisier,\n \n \n \n### Factors\nMore information needed\n \n### Metrics\n \nMore information needed\n \n \n## Results \n \nMore information needed\n\n \n# Model Examination\n \nMore information needed\n \n# Environmental Impact\n \nCarbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).\n \n- **Hardware Type:** More information needed\n- **Hours used:** More information needed\n- **Cloud Provider:** More information needed\n- **Compute Region:** More information needed\n- **Carbon Emitted:** More information needed\n \n# Technical Specifications [optional]\n \n## Model Architecture and Objective\n\nMore information needed \n \n## Compute Infrastructure\n \nMore information needed \n \n### Hardware\n \n \nMore information needed\n \n### Software\n \nMore information needed.\n \n# Citation\n\n \n**BibTeX:**\n \n \n```bibtex\n@inproceedings{sellam2020bleurt,\n  title = {BLEURT: Learning Robust Metrics for Text Generation},\n  author = {Thibault Sellam and Dipanjan Das and Ankur P Parikh},\n  year = {2020},\n  booktitle = {Proceedings of ACL}\n}\n```\n \n \n \n \n# Glossary [optional]\nMore information needed \n \n# More Information [optional]\nMore information needed \n\n \n# Model Card Authors [optional]\n \n Elron Bandel in collaboration with Ezi Ozoani and the Hugging Face team\n\n\n# Model Card Contact\n \nMore information needed\n \n# How to Get Started with the Model\n \nUse the code below to get started with the model.\n \n<details>\n<summary> Click to expand </summary>\n\n```python\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"Elron/bleurt-tiny-512\")\nmodel = AutoModelForSequenceClassification.from_pretrained(\"Elron/bleurt-tiny-512\")\nmodel.eval()\n\nreferences = [\"hello world\", \"hello world\"]\ncandidates = [\"hi universe\", \"bye world\"]\n\nwith torch.no_grad():\n  scores = model(**tokenizer(references, candidates, return_tensors='pt'))[0].squeeze()\n\nprint(scores) # tensor([-0.9414, -0.5678])\n ```\n\nSee [this notebook](https://colab.research.google.com/drive/1KsCUkFW45d5_ROSv2aHtXgeBa2Z98r03?usp=sharing) for model conversion code. \n</details>\n",
    "card_content": "---\ntags:\n- text-classification\n- bert\n---\n\n# Model Card for bleurt-tiny-512 \n \n# Model Details\n \n## Model Description\n \nPytorch version of the original BLEURT models from ACL paper\n \n- **Developed by:** Elron Bandel, Thibault Sellam, Dipanjan Das and Ankur P. Parikh of Google Research\n- **Shared by [Optional]:** Elron Bandel\n- **Model type:** Text Classification \n- **Language(s) (NLP):** More information needed\n- **License:** More information needed \n- **Parent Model:** BERT\n- **Resources for more information:**\n     - [GitHub Repo](https://github.com/google-research/bleurt/tree/master)\n \t  - [Associated Paper](https://aclanthology.org/2020.acl-main.704/)\n    - [Blog Post](https://ai.googleblog.com/2020/05/evaluating-natural-language-generation.html)\n \t\n\n\n# Uses\n \n\n## Direct Use\nThis model can be used for the task of Text Classification \n \n## Downstream Use [Optional]\n \nMore information needed.\n \n## Out-of-Scope Use\n \nThe model should not be used to intentionally create hostile or alienating environments for people. \n \n# Bias, Risks, and Limitations\n \n \nSignificant research has explored bias and fairness issues with language models (see, e.g., [Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)). Predictions generated by the model may include disturbing and harmful stereotypes across protected classes; identity characteristics; and sensitive, social, and occupational groups.\n\n\n\n## Recommendations\n \n \nUsers (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.\n\n# Training Details\n \n## Training Data\nThe model authors note in the [associated paper](https://aclanthology.org/2020.acl-main.704.pdf): \n> We use years 2017 to 2019 of the WMT Metrics Shared Task, to-English language pairs. For each year, we used the of- ficial WMT test set, which include several thou- sand pairs of sentences with human ratings from the news domain. The training sets contain 5,360, 9,492, and 147,691 records for each year. \n \n \n## Training Procedure\n\n \n### Preprocessing\n \nMore information needed \n \n### Speeds, Sizes, Times\nMore information needed \n\n \n# Evaluation\n \n \n## Testing Data, Factors & Metrics\n \n### Testing Data\n \nThe test sets for years 2018 and 2019 [of the WMT Metrics Shared Task, to-English language pairs.]  are noisier,\n \n \n \n### Factors\nMore information needed\n \n### Metrics\n \nMore information needed\n \n \n## Results \n \nMore information needed\n\n \n# Model Examination\n \nMore information needed\n \n# Environmental Impact\n \nCarbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).\n \n- **Hardware Type:** More information needed\n- **Hours used:** More information needed\n- **Cloud Provider:** More information needed\n- **Compute Region:** More information needed\n- **Carbon Emitted:** More information needed\n \n# Technical Specifications [optional]\n \n## Model Architecture and Objective\n\nMore information needed \n \n## Compute Infrastructure\n \nMore information needed \n \n### Hardware\n \n \nMore information needed\n \n### Software\n \nMore information needed.\n \n# Citation\n\n \n**BibTeX:**\n \n \n```bibtex\n@inproceedings{sellam2020bleurt,\n  title = {BLEURT: Learning Robust Metrics for Text Generation},\n  author = {Thibault Sellam and Dipanjan Das and Ankur P Parikh},\n  year = {2020},\n  booktitle = {Proceedings of ACL}\n}\n```\n \n \n \n \n# Glossary [optional]\nMore information needed \n \n# More Information [optional]\nMore information needed \n\n \n# Model Card Authors [optional]\n \n Elron Bandel in collaboration with Ezi Ozoani and the Hugging Face team\n\n\n# Model Card Contact\n \nMore information needed\n \n# How to Get Started with the Model\n \nUse the code below to get started with the model.\n \n<details>\n<summary> Click to expand </summary>\n\n```python\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"Elron/bleurt-tiny-512\")\nmodel = AutoModelForSequenceClassification.from_pretrained(\"Elron/bleurt-tiny-512\")\nmodel.eval()\n\nreferences = [\"hello world\", \"hello world\"]\ncandidates = [\"hi universe\", \"bye world\"]\n\nwith torch.no_grad():\n  scores = model(**tokenizer(references, candidates, return_tensors='pt'))[0].squeeze()\n\nprint(scores) # tensor([-0.9414, -0.5678])\n ```\n\nSee [this notebook](https://colab.research.google.com/drive/1KsCUkFW45d5_ROSv2aHtXgeBa2Z98r03?usp=sharing) for model conversion code. \n</details>\n",
    "library_name": "transformers"
  },
  {
    "model_id": "cross-encoder/ms-marco-MiniLM-L12-v2",
    "model_name": "cross-encoder/ms-marco-MiniLM-L12-v2",
    "author": "cross-encoder",
    "downloads": 632176,
    "likes": 68,
    "tags": [
      "transformers",
      "pytorch",
      "jax",
      "safetensors",
      "bert",
      "text-classification",
      "license:apache-2.0",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/cross-encoder/ms-marco-MiniLM-L12-v2",
    "dependencies": [
      [
        "sentence_transformers",
        null
      ],
      [
        "transformers",
        null
      ],
      [
        "torch",
        null
      ],
      [
        "sentence-transformers",
        "2.2.2"
      ]
    ],
    "analysis_date": "2025-03-26T00:47:59.410108",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "bert",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "license": "apache-2.0"
    },
    "card_text": "# Cross-Encoder for MS Marco\n\nThis model was trained on the [MS Marco Passage Ranking](https://github.com/microsoft/MSMARCO-Passage-Ranking) task.\n\nThe model can be used for Information Retrieval: Given a query, encode the query will all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. See [SBERT.net Retrieve & Re-rank](https://www.sbert.net/examples/applications/retrieve_rerank/README.html) for more details. The training code is available here: [SBERT.net Training MS Marco](https://github.com/UKPLab/sentence-transformers/tree/master/examples/training/ms_marco)\n\n\n## Usage with SentenceTransformers\n\nThe usage is easy when you have [SentenceTransformers](https://www.sbert.net/) installed. Then you can use the pre-trained models like this:\n```python\nfrom sentence_transformers import CrossEncoder\n\nmodel = CrossEncoder('cross-encoder/ms-marco-MiniLM-L12-v2')\nscores = model.predict([\n    (\"How many people live in Berlin?\", \"Berlin had a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\"),\n    (\"How many people live in Berlin?\", \"Berlin is well known for its museums.\"),\n])\nprint(scores)\n# [ 9.218911  -4.0780287]\n```\n\n\n## Usage with Transformers\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\n\nmodel = AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-MiniLM-L12-v2')\ntokenizer = AutoTokenizer.from_pretrained('cross-encoder/ms-marco-MiniLM-L12-v2')\n\nfeatures = tokenizer(['How many people live in Berlin?', 'How many people live in Berlin?'], ['Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.', 'New York City is famous for the Metropolitan Museum of Art.'],  padding=True, truncation=True, return_tensors=\"pt\")\n\nmodel.eval()\nwith torch.no_grad():\n    scores = model(**features).logits\n    print(scores)\n```\n\n\n\n## Performance\nIn the following table, we provide various pre-trained Cross-Encoders together with their performance on the [TREC Deep Learning 2019](https://microsoft.github.io/TREC-2019-Deep-Learning/) and the [MS Marco Passage Reranking](https://github.com/microsoft/MSMARCO-Passage-Ranking/) dataset. \n\n\n| Model-Name        | NDCG@10 (TREC DL 19) | MRR@10 (MS Marco Dev)  | Docs / Sec |\n| ------------- |:-------------| -----| --- | \n| **Version 2 models** | | | \n| cross-encoder/ms-marco-TinyBERT-L2-v2 | 69.84 | 32.56 | 9000\n| cross-encoder/ms-marco-MiniLM-L2-v2 | 71.01 | 34.85 | 4100\n| cross-encoder/ms-marco-MiniLM-L4-v2 | 73.04 | 37.70 | 2500\n| cross-encoder/ms-marco-MiniLM-L6-v2 | 74.30 | 39.01 | 1800\n| cross-encoder/ms-marco-MiniLM-L12-v2 | 74.31 | 39.02 | 960\n| **Version 1 models** | | | \n| cross-encoder/ms-marco-TinyBERT-L2  | 67.43 | 30.15  | 9000\n| cross-encoder/ms-marco-TinyBERT-L4  | 68.09 | 34.50  | 2900\n| cross-encoder/ms-marco-TinyBERT-L6 |  69.57 | 36.13  | 680\n| cross-encoder/ms-marco-electra-base | 71.99 | 36.41 | 340\n| **Other models** | | | \n| nboost/pt-tinybert-msmarco | 63.63 | 28.80 | 2900 \n| nboost/pt-bert-base-uncased-msmarco | 70.94 | 34.75 | 340 \n| nboost/pt-bert-large-msmarco | 73.36 | 36.48 | 100 \n| Capreolus/electra-base-msmarco | 71.23 | 36.89 | 340 \n| amberoad/bert-multilingual-passage-reranking-msmarco | 68.40 | 35.54 | 330 \n| sebastian-hofstaetter/distilbert-cat-margin_mse-T2-msmarco | 72.82 | 37.88 | 720\n \n Note: Runtime was computed on a V100 GPU.\n",
    "card_content": "---\nlicense: apache-2.0\n---\n# Cross-Encoder for MS Marco\n\nThis model was trained on the [MS Marco Passage Ranking](https://github.com/microsoft/MSMARCO-Passage-Ranking) task.\n\nThe model can be used for Information Retrieval: Given a query, encode the query will all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. See [SBERT.net Retrieve & Re-rank](https://www.sbert.net/examples/applications/retrieve_rerank/README.html) for more details. The training code is available here: [SBERT.net Training MS Marco](https://github.com/UKPLab/sentence-transformers/tree/master/examples/training/ms_marco)\n\n\n## Usage with SentenceTransformers\n\nThe usage is easy when you have [SentenceTransformers](https://www.sbert.net/) installed. Then you can use the pre-trained models like this:\n```python\nfrom sentence_transformers import CrossEncoder\n\nmodel = CrossEncoder('cross-encoder/ms-marco-MiniLM-L12-v2')\nscores = model.predict([\n    (\"How many people live in Berlin?\", \"Berlin had a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\"),\n    (\"How many people live in Berlin?\", \"Berlin is well known for its museums.\"),\n])\nprint(scores)\n# [ 9.218911  -4.0780287]\n```\n\n\n## Usage with Transformers\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\n\nmodel = AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-MiniLM-L12-v2')\ntokenizer = AutoTokenizer.from_pretrained('cross-encoder/ms-marco-MiniLM-L12-v2')\n\nfeatures = tokenizer(['How many people live in Berlin?', 'How many people live in Berlin?'], ['Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.', 'New York City is famous for the Metropolitan Museum of Art.'],  padding=True, truncation=True, return_tensors=\"pt\")\n\nmodel.eval()\nwith torch.no_grad():\n    scores = model(**features).logits\n    print(scores)\n```\n\n\n\n## Performance\nIn the following table, we provide various pre-trained Cross-Encoders together with their performance on the [TREC Deep Learning 2019](https://microsoft.github.io/TREC-2019-Deep-Learning/) and the [MS Marco Passage Reranking](https://github.com/microsoft/MSMARCO-Passage-Ranking/) dataset. \n\n\n| Model-Name        | NDCG@10 (TREC DL 19) | MRR@10 (MS Marco Dev)  | Docs / Sec |\n| ------------- |:-------------| -----| --- | \n| **Version 2 models** | | | \n| cross-encoder/ms-marco-TinyBERT-L2-v2 | 69.84 | 32.56 | 9000\n| cross-encoder/ms-marco-MiniLM-L2-v2 | 71.01 | 34.85 | 4100\n| cross-encoder/ms-marco-MiniLM-L4-v2 | 73.04 | 37.70 | 2500\n| cross-encoder/ms-marco-MiniLM-L6-v2 | 74.30 | 39.01 | 1800\n| cross-encoder/ms-marco-MiniLM-L12-v2 | 74.31 | 39.02 | 960\n| **Version 1 models** | | | \n| cross-encoder/ms-marco-TinyBERT-L2  | 67.43 | 30.15  | 9000\n| cross-encoder/ms-marco-TinyBERT-L4  | 68.09 | 34.50  | 2900\n| cross-encoder/ms-marco-TinyBERT-L6 |  69.57 | 36.13  | 680\n| cross-encoder/ms-marco-electra-base | 71.99 | 36.41 | 340\n| **Other models** | | | \n| nboost/pt-tinybert-msmarco | 63.63 | 28.80 | 2900 \n| nboost/pt-bert-base-uncased-msmarco | 70.94 | 34.75 | 340 \n| nboost/pt-bert-large-msmarco | 73.36 | 36.48 | 100 \n| Capreolus/electra-base-msmarco | 71.23 | 36.89 | 340 \n| amberoad/bert-multilingual-passage-reranking-msmarco | 68.40 | 35.54 | 330 \n| sebastian-hofstaetter/distilbert-cat-margin_mse-T2-msmarco | 72.82 | 37.88 | 720\n \n Note: Runtime was computed on a V100 GPU.\n",
    "library_name": "transformers"
  },
  {
    "model_id": "finiteautomata/beto-sentiment-analysis",
    "model_name": "finiteautomata/beto-sentiment-analysis",
    "author": "finiteautomata",
    "downloads": 617958,
    "likes": 30,
    "tags": [
      "transformers",
      "pytorch",
      "jax",
      "bert",
      "text-classification",
      "sentiment-analysis",
      "es",
      "arxiv:2106.09462",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/finiteautomata/beto-sentiment-analysis",
    "dependencies": [
      [
        "pysentimiento",
        "0.7.2"
      ]
    ],
    "analysis_date": "2025-03-26T00:48:00.719009",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "bert",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": [
        "es"
      ],
      "tags": [
        "sentiment-analysis"
      ]
    },
    "card_text": "\n# Sentiment Analysis in Spanish\n## beto-sentiment-analysis\n\n**NOTE: this model will be removed soon -- use [pysentimiento/robertuito-sentiment-analysis](https://huggingface.co/pysentimiento/robertuito-sentiment-analysis) instead**\n\nRepository: [https://github.com/finiteautomata/pysentimiento/](https://github.com/pysentimiento/pysentimiento/)\n\n\nModel trained with TASS 2020 corpus (around ~5k tweets) of several dialects of Spanish. Base model is [BETO](https://github.com/dccuchile/beto), a BERT model trained in Spanish.\n\nUses `POS`, `NEG`, `NEU` labels.\n\n## License\n\n`pysentimiento` is an open-source library for non-commercial use and scientific research purposes only. Please be aware that models are trained with third-party datasets and are subject to their respective licenses. \n\n1. [TASS Dataset license](http://tass.sepln.org/tass_data/download.php)\n2. [SEMEval 2017 Dataset license]()\n\n## Citation\n\nIf you use this model in your work, please cite the following papers:\n\n```\n@misc{perez2021pysentimiento,\n      title={pysentimiento: A Python Toolkit for Sentiment Analysis and SocialNLP tasks},\n      author={Juan Manuel P\u00e9rez and Juan Carlos Giudici and Franco Luque},\n      year={2021},\n      eprint={2106.09462},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n\n@article{canete2020spanish,\n  title={Spanish pre-trained bert model and evaluation data},\n  author={Ca{\\~n}ete, Jos{\\'e} and Chaperon, Gabriel and Fuentes, Rodrigo and Ho, Jou-Hui and Kang, Hojin and P{\\'e}rez, Jorge},\n  journal={Pml4dc at iclr},\n  volume={2020},\n  number={2020},\n  pages={1--10},\n  year={2020}\n}\n```\n\nEnjoy! \ud83e\udd17\n",
    "card_content": "---\nlanguage:\n- es\ntags:\n- sentiment-analysis\n---\n\n# Sentiment Analysis in Spanish\n## beto-sentiment-analysis\n\n**NOTE: this model will be removed soon -- use [pysentimiento/robertuito-sentiment-analysis](https://huggingface.co/pysentimiento/robertuito-sentiment-analysis) instead**\n\nRepository: [https://github.com/finiteautomata/pysentimiento/](https://github.com/pysentimiento/pysentimiento/)\n\n\nModel trained with TASS 2020 corpus (around ~5k tweets) of several dialects of Spanish. Base model is [BETO](https://github.com/dccuchile/beto), a BERT model trained in Spanish.\n\nUses `POS`, `NEG`, `NEU` labels.\n\n## License\n\n`pysentimiento` is an open-source library for non-commercial use and scientific research purposes only. Please be aware that models are trained with third-party datasets and are subject to their respective licenses. \n\n1. [TASS Dataset license](http://tass.sepln.org/tass_data/download.php)\n2. [SEMEval 2017 Dataset license]()\n\n## Citation\n\nIf you use this model in your work, please cite the following papers:\n\n```\n@misc{perez2021pysentimiento,\n      title={pysentimiento: A Python Toolkit for Sentiment Analysis and SocialNLP tasks},\n      author={Juan Manuel P\u00e9rez and Juan Carlos Giudici and Franco Luque},\n      year={2021},\n      eprint={2106.09462},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n\n@article{canete2020spanish,\n  title={Spanish pre-trained bert model and evaluation data},\n  author={Ca{\\~n}ete, Jos{\\'e} and Chaperon, Gabriel and Fuentes, Rodrigo and Ho, Jou-Hui and Kang, Hojin and P{\\'e}rez, Jorge},\n  journal={Pml4dc at iclr},\n  volume={2020},\n  number={2020},\n  pages={1--10},\n  year={2020}\n}\n```\n\nEnjoy! \ud83e\udd17\n",
    "library_name": "transformers"
  },
  {
    "model_id": "martin-ha/toxic-comment-model",
    "model_name": "martin-ha/toxic-comment-model",
    "author": "martin-ha",
    "downloads": 616316,
    "likes": 61,
    "tags": [
      "transformers",
      "pytorch",
      "distilbert",
      "text-classification",
      "en",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/martin-ha/toxic-comment-model",
    "dependencies": [
      [
        "transformers",
        "4.30.2"
      ],
      [
        "torch",
        "2.0.1"
      ],
      [
        "numpy",
        "1.24.3"
      ],
      [
        "xformers",
        "0.0.20"
      ]
    ],
    "analysis_date": "2025-03-26T00:48:01.919488",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "distilbert",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": "en"
    },
    "card_text": "\n## Model description\nThis model is a fine-tuned version of the [DistilBERT model](https://huggingface.co/transformers/model_doc/distilbert.html) to classify toxic comments. \n\n## How to use\n\nYou can use the model with the following code.\n\n```python\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, TextClassificationPipeline\n\nmodel_path = \"martin-ha/toxic-comment-model\"\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_path)\n\npipeline =  TextClassificationPipeline(model=model, tokenizer=tokenizer)\nprint(pipeline('This is a test text.'))\n```\n\n## Limitations and Bias\n\nThis model is intended to use for classify toxic online classifications. However, one limitation of the model is that it performs poorly for some comments that mention a specific identity subgroup, like Muslim. The following table shows a evaluation score for different identity group. You can learn the specific meaning of this metrics [here](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/overview/evaluation). But basically, those metrics shows how well a model performs for a specific group. The larger the number, the better.\n\n| **subgroup**                  | **subgroup_size** | **subgroup_auc** | **bpsn_auc** | **bnsp_auc** |\n| ----------------------------- | ----------------- | ---------------- | ------------ | ------------ |\n| muslim                        | 108               | 0.689            | 0.811        | 0.88         |\n| jewish                        | 40                | 0.749            | 0.86         | 0.825        |\n| homosexual_gay_or_lesbian     | 56                | 0.795            | 0.706        | 0.972        |\n| black                         | 84                | 0.866            | 0.758        | 0.975        |\n| white                         | 112               | 0.876            | 0.784        | 0.97         |\n| female                        | 306               | 0.898            | 0.887        | 0.948        |\n| christian                     | 231               | 0.904            | 0.917        | 0.93         |\n| male                          | 225               | 0.922            | 0.862        | 0.967        |\n| psychiatric_or_mental_illness | 26                | 0.924            | 0.907        | 0.95         |\n\nThe table above shows that the model performs poorly for the muslim and jewish group. In fact, you pass the sentence \"Muslims are people who follow or practice Islam, an Abrahamic monotheistic religion.\" Into the model, the model will classify it as toxic. Be mindful for this type of potential bias.\n\n## Training data\nThe training data comes this [Kaggle competition](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/data). We use 10% of the `train.csv` data to train the model.\n\n## Training procedure\n\nYou can see [this documentation and codes](https://github.com/MSIA/wenyang_pan_nlp_project_2021) for how we train the model. It takes about 3 hours in a P-100 GPU.\n\n## Evaluation results\n\nThe model achieves 94% accuracy and 0.59 f1-score in a 10000 rows held-out test set.",
    "card_content": "---\nlanguage: en\n---\n\n## Model description\nThis model is a fine-tuned version of the [DistilBERT model](https://huggingface.co/transformers/model_doc/distilbert.html) to classify toxic comments. \n\n## How to use\n\nYou can use the model with the following code.\n\n```python\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, TextClassificationPipeline\n\nmodel_path = \"martin-ha/toxic-comment-model\"\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_path)\n\npipeline =  TextClassificationPipeline(model=model, tokenizer=tokenizer)\nprint(pipeline('This is a test text.'))\n```\n\n## Limitations and Bias\n\nThis model is intended to use for classify toxic online classifications. However, one limitation of the model is that it performs poorly for some comments that mention a specific identity subgroup, like Muslim. The following table shows a evaluation score for different identity group. You can learn the specific meaning of this metrics [here](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/overview/evaluation). But basically, those metrics shows how well a model performs for a specific group. The larger the number, the better.\n\n| **subgroup**                  | **subgroup_size** | **subgroup_auc** | **bpsn_auc** | **bnsp_auc** |\n| ----------------------------- | ----------------- | ---------------- | ------------ | ------------ |\n| muslim                        | 108               | 0.689            | 0.811        | 0.88         |\n| jewish                        | 40                | 0.749            | 0.86         | 0.825        |\n| homosexual_gay_or_lesbian     | 56                | 0.795            | 0.706        | 0.972        |\n| black                         | 84                | 0.866            | 0.758        | 0.975        |\n| white                         | 112               | 0.876            | 0.784        | 0.97         |\n| female                        | 306               | 0.898            | 0.887        | 0.948        |\n| christian                     | 231               | 0.904            | 0.917        | 0.93         |\n| male                          | 225               | 0.922            | 0.862        | 0.967        |\n| psychiatric_or_mental_illness | 26                | 0.924            | 0.907        | 0.95         |\n\nThe table above shows that the model performs poorly for the muslim and jewish group. In fact, you pass the sentence \"Muslims are people who follow or practice Islam, an Abrahamic monotheistic religion.\" Into the model, the model will classify it as toxic. Be mindful for this type of potential bias.\n\n## Training data\nThe training data comes this [Kaggle competition](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/data). We use 10% of the `train.csv` data to train the model.\n\n## Training procedure\n\nYou can see [this documentation and codes](https://github.com/MSIA/wenyang_pan_nlp_project_2021) for how we train the model. It takes about 3 hours in a P-100 GPU.\n\n## Evaluation results\n\nThe model achieves 94% accuracy and 0.59 f1-score in a 10000 rows held-out test set.",
    "library_name": "transformers"
  },
  {
    "model_id": "nbroad/ESG-BERT",
    "model_name": "nbroad/ESG-BERT",
    "author": "nbroad",
    "downloads": 577156,
    "likes": 63,
    "tags": [
      "transformers",
      "pytorch",
      "safetensors",
      "bert",
      "text-classification",
      "en",
      "arxiv:1910.09700",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/nbroad/ESG-BERT",
    "dependencies": [
      [
        "torchserve",
        "0.8.2"
      ],
      [
        "torch-model-archiver",
        "0.8.1"
      ],
      [
        "torchvision",
        "0.14.1"
      ],
      [
        "transformers",
        "4.30.2"
      ],
      [
        "numpy",
        "1.24.3"
      ],
      [
        "xformers",
        "0.0.20"
      ]
    ],
    "analysis_date": "2025-03-26T00:48:03.790293",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "bert",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": [
        "en"
      ],
      "widget": [
        {
          "text": "In fiscal year 2019, we reduced our comprehensive carbon footprint for the fourth consecutive year\u2014down 35 percent compared to 2015, when Apple\u2019s carbon emissions peaked, even as net revenue increased by 11 percent over that same period. In the past year, we avoided over 10 million metric tons from our emissions reduction initiatives\u2014like our Supplier Clean Energy Program, which lowered our footprint by 4.4 million metric tons. ",
          "example_title": "Reduced carbon footprint"
        },
        {
          "text": "We believe it is essential to establish validated conflict-free sources of 3TG within the Democratic Republic of the Congo (the \u201cDRC\u201d) and adjoining countries (together, with the DRC, the \u201cCovered Countries\u201d), so that these minerals can be procured in a way that contributes to economic growth and development in the region. To aid in this effort, we have established a conflict minerals policy and an internal team to implement the policy.",
          "example_title": "Conflict minerals policy"
        }
      ]
    },
    "card_text": "# Model Card for ESG-BERT\nDomain Specific BERT Model for Text Mining in Sustainable Investing\n \n \n \n# Model Details\n \n## Model Description\n \n \n \n- **Developed by:** [Mukut Mukherjee](https://www.linkedin.com/in/mukutm/), [Charan Pothireddi](https://www.linkedin.com/in/sree-charan-pothireddi-6a0a3587/) and [Parabole.ai](https://www.linkedin.com/in/sree-charan-pothireddi-6a0a3587/)\n- **Shared by [Optional]:** HuggingFace\n- **Model type:** Language model\n- **Language(s) (NLP):** en\n- **License:** More information needed\n- **Related Models:** \n  - **Parent Model:** BERT\n- **Resources for more information:** \n - [GitHub Repo](https://github.com/mukut03/ESG-BERT)\n - [Blog Post](https://towardsdatascience.com/nlp-meets-sustainable-investing-d0542b3c264b?source=friends_link&sk=1f7e6641c3378aaff319a81decf387bf)\n \n# Uses\n \n \n## Direct Use\n \nText Mining in Sustainable Investing\n \n## Downstream Use [Optional]\n \nThe applications of ESG-BERT can be expanded way beyond just text classification. It can be fine-tuned to perform various other downstream NLP tasks in the domain of Sustainable Investing.\n \n## Out-of-Scope Use\n \nThe model should not be used to intentionally create hostile or alienating environments for people. \n# Bias, Risks, and Limitations\n \n \nSignificant research has explored bias and fairness issues with language models (see, e.g., [Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)). Predictions generated by the model may include disturbing and harmful stereotypes across protected classes; identity characteristics; and sensitive, social, and occupational groups.\n \n \n## Recommendations\n \n \nUsers (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recomendations.\n \n \n# Training Details\n \n## Training Data\n \nMore information needed\n \n## Training Procedure\n \n<!-- This relates heavily to the Technical Specifications. Content here should link to that section when it is relevant to the training procedure. -->\n \n### Preprocessing\n \nMore information needed\n \n### Speeds, Sizes, Times\n \nMore information needed\n \n# Evaluation\n \n \n \n## Testing Data, Factors & Metrics\n \n### Testing Data\n \nThe fine-tuned model for text classification is also available [here](https://drive.google.com/drive/folders/1Qz4HP3xkjLfJ6DGCFNeJ7GmcPq65_HVe?usp=sharing). It can be used directly to make predictions using just a few steps.  First, download the fine-tuned pytorch_model.bin, config.json, and vocab.txt\n \n### Factors\n \nMore information needed\n \n### Metrics\n \nMore information needed\n \n## Results \n \nESG-BERT was further trained on unstructured text data with accuracies of 100% and 98% for Next Sentence Prediction and Masked Language Modelling tasks. Fine-tuning ESG-BERT for text classification yielded an F-1 score of 0.90. For comparison, the general BERT (BERT-base) model scored 0.79 after fine-tuning, and the sci-kit learn approach scored 0.67.\n \n# Model Examination\n \nMore information needed\n \n# Environmental Impact\n \n \nCarbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).\n \n- **Hardware Type:** More information needed\n- **Hours used:** More information needed\n- **Cloud Provider:**  information needed\n- **Compute Region:** More information needed\n- **Carbon Emitted:** More information needed\n \n# Technical Specifications [optional]\n \n## Model Architecture and Objective\n \nMore information needed\n \n## Compute Infrastructure\n \nMore information needed\n \n### Hardware\n \nMore information needed\n \n### Software\n \nJDK 11 is needed to serve the model\n \n# Citation\n \n<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->\n \n**BibTeX:**\n \nMore information needed\n \n**APA:**\n \nMore information needed\n \n# Glossary [optional]\n \n<!-- If relevant, include terms and calculations in this section that can help readers understand the model or model card. -->\n \nMore information needed\n \n# More Information [optional]\n \nMore information needed\n \n# Model Card Authors [optional]\n[Mukut Mukherjee](https://www.linkedin.com/in/mukutm/), [Charan Pothireddi](https://www.linkedin.com/in/sree-charan-pothireddi-6a0a3587/) and [Parabole.ai](https://www.linkedin.com/in/sree-charan-pothireddi-6a0a3587/), in collaboration with the Ezi Ozoani and the HuggingFace Team\n \n \n# Model Card Contact\n \nMore information needed\n \n# How to Get Started with the Model\n \nUse the code below to get started with the model.\n \n<details>\n <summary> Click to expand </summary>\n \n```\npip install torchserve torch-model-archiver\n \npip install torchvision\n \npip install transformers\n \n```\n \nNext up, we'll set up the handler script. It is a basic handler for text classification that can be improved upon. Save this script as \"handler.py\" in your directory. [1]\n \n```\n \nfrom abc import ABC\n \nimport json\n \nimport logging\n \nimport os\n \nimport torch\n \nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n \nfrom ts.torch_handler.base_handler import BaseHandler\n \nlogger = logging.getLogger(__name__)\n \nclass TransformersClassifierHandler(BaseHandler, ABC):\n \n   \"\"\"\n \n   Transformers text classifier handler class. This handler takes a text (string) and\n \n   as input and returns the classification text based on the serialized transformers checkpoint.\n \n   \"\"\"\n \n   def __init__(self):\n \n       super(TransformersClassifierHandler, self).__init__()\n \n       self.initialized = False\n \ndef initialize(self, ctx):\n \n       self.manifest = ctx.manifest\n \nproperties = ctx.system_properties\n \n       model_dir = properties.get(\"model_dir\")\n \n       self.device = torch.device(\"cuda:\" + str(properties.get(\"gpu_id\")) if torch.cuda.is_available() else \"cpu\")\n \n# Read model serialize/pt file\n \n       self.model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n \n       self.tokenizer = AutoTokenizer.from_pretrained(model_dir)\n \nself.model.to(self.device)\n \n       self.model.eval()\n \nlogger.debug('Transformer model from path {0} loaded successfully'.format(model_dir))\n \n# Read the mapping file, index to object name\n \n       mapping_file_path = os.path.join(model_dir, \"index_to_name.json\")\n \nif os.path.isfile(mapping_file_path):\n \n           with open(mapping_file_path) as f:\n \n               self.mapping = json.load(f)\n \n       else:\n \n           logger.warning('Missing the index_to_name.json file. Inference output will not include class name.')\n \nself.initialized = True\n \ndef preprocess(self, data):\n \n       \"\"\" Very basic preprocessing code - only tokenizes.\n \n           Extend with your own preprocessing steps as needed.\n \n       \"\"\"\n \n       text = data[0].get(\"data\")\n \n       if text is None:\n \n           text = data[0].get(\"body\")\n \n       sentences = text.decode('utf-8')\n \n       logger.info(\"Received text: '%s'\", sentences)\n \ninputs = self.tokenizer.encode_plus(\n \n           sentences,\n \n           add_special_tokens=True,\n \n           return_tensors=\"pt\"\n \n       )\n \n       return inputs\n \ndef inference(self, inputs):\n \n       \"\"\"\n \n       Predict the class of a text using a trained transformer model.\n \n       \"\"\"\n \n       # NOTE: This makes the assumption that your model expects text to be tokenized \n \n       # with \"input_ids\" and \"token_type_ids\" - which is true for some popular transformer models, e.g. bert.\n \n       # If your transformer model expects different tokenization, adapt this code to suit\n \n       # its expected input format.\n \n       prediction = self.model(\n \n           inputs['input_ids'].to(self.device),\n \n           token_type_ids=inputs['token_type_ids'].to(self.device)\n \n       )[0].argmax().item()\n \n       logger.info(\"Model predicted: '%s'\", prediction)\n \nif self.mapping:\n \n           prediction = self.mapping[str(prediction)]\n \nreturn [prediction]\n \ndef postprocess(self, inference_output):\n \n       # TODO: Add any needed post-processing of the model predictions here\n \n       return inference_output\n \n_service = TransformersClassifierHandler()\n \ndef handle(data, context):\n \n   try:\n \n       if not _service.initialized:\n \n           _service.initialize(context)\n \nif data is None:\n \n           return None\n \ndata = _service.preprocess(data)\n \n       data = _service.inference(data)\n \n       data = _service.postprocess(data)\n \nreturn data\n \n   except Exception as e:\n \n       raise e\n \n \n \n```\n \nTorcheServe uses a format called MAR (Model Archive). We can convert our PyTorch model to a .mar file using this command:\n \n```\n \ntorch-model-archiver --model-name \"bert\" --version 1.0 --serialized-file ./bert_model/pytorch_model.bin --extra-files \"./bert_model/config.json,./bert_model/vocab.txt\" --handler \"./handler.py\"\n \n```\n \nMove the .mar file into a new directory: \n \n```\n \nmkdir model_store && mv bert.mar model_store\n \n```\n \nFinally, we can start TorchServe using the command: \n \n```\n \ntorchserve --start --model-store model_store --models bert=bert.mar\n \n```\n \nWe can now query the model from another terminal window using the Inference API. We pass a text file containing text that the model will try to classify. \n \n\n \n \n```\n \ncurl -X POST http://127.0.0.1:8080/predictions/bert -T predict.txt\n \n```\n \nThis returns a label number which correlates to a textual label. This is stored in the label_dict.txt dictionary file. \n \n```\n \n__label__Business_Ethics :  0\n \n__label__Data_Security :  1\n \n__label__Access_And_Affordability :  2\n \n__label__Business_Model_Resilience :  3\n \n__label__Competitive_Behavior :  4\n \n__label__Critical_Incident_Risk_Management :  5\n \n__label__Customer_Welfare :  6\n \n__label__Director_Removal :  7\n \n__label__Employee_Engagement_Inclusion_And_Diversity :  8\n \n__label__Employee_Health_And_Safety :  9\n \n__label__Human_Rights_And_Community_Relations :  10\n \n__label__Labor_Practices :  11\n \n__label__Management_Of_Legal_And_Regulatory_Framework :  12\n \n__label__Physical_Impacts_Of_Climate_Change :  13\n \n__label__Product_Quality_And_Safety :  14\n \n__label__Product_Design_And_Lifecycle_Management :  15\n \n__label__Selling_Practices_And_Product_Labeling :  16\n \n__label__Supply_Chain_Management :  17\n \n__label__Systemic_Risk_Management :  18\n \n__label__Waste_And_Hazardous_Materials_Management :  19\n \n__label__Water_And_Wastewater_Management :  20\n \n__label__Air_Quality :  21\n \n__label__Customer_Privacy :  22\n \n__label__Ecological_Impacts :  23\n \n__label__Energy_Management :  24\n \n__label__GHG_Emissions :  25\n \n```\n\n<\\details>\n",
    "card_content": "---\nlanguage:\n- en\nwidget:\n- text: 'In fiscal year 2019, we reduced our comprehensive carbon footprint for the\n    fourth consecutive year\u2014down 35 percent compared to 2015, when Apple\u2019s carbon\n    emissions peaked, even as net revenue increased by 11 percent over that same period.\n    In the past year, we avoided over 10 million metric tons from our emissions reduction\n    initiatives\u2014like our Supplier Clean Energy Program, which lowered our footprint\n    by 4.4 million metric tons. '\n  example_title: Reduced carbon footprint\n- text: We believe it is essential to establish validated conflict-free sources of\n    3TG within the Democratic Republic of the Congo (the \u201cDRC\u201d) and adjoining countries\n    (together, with the DRC, the \u201cCovered Countries\u201d), so that these minerals can\n    be procured in a way that contributes to economic growth and development in the\n    region. To aid in this effort, we have established a conflict minerals policy\n    and an internal team to implement the policy.\n  example_title: Conflict minerals policy\n---\n# Model Card for ESG-BERT\nDomain Specific BERT Model for Text Mining in Sustainable Investing\n \n \n \n# Model Details\n \n## Model Description\n \n \n \n- **Developed by:** [Mukut Mukherjee](https://www.linkedin.com/in/mukutm/), [Charan Pothireddi](https://www.linkedin.com/in/sree-charan-pothireddi-6a0a3587/) and [Parabole.ai](https://www.linkedin.com/in/sree-charan-pothireddi-6a0a3587/)\n- **Shared by [Optional]:** HuggingFace\n- **Model type:** Language model\n- **Language(s) (NLP):** en\n- **License:** More information needed\n- **Related Models:** \n  - **Parent Model:** BERT\n- **Resources for more information:** \n - [GitHub Repo](https://github.com/mukut03/ESG-BERT)\n - [Blog Post](https://towardsdatascience.com/nlp-meets-sustainable-investing-d0542b3c264b?source=friends_link&sk=1f7e6641c3378aaff319a81decf387bf)\n \n# Uses\n \n \n## Direct Use\n \nText Mining in Sustainable Investing\n \n## Downstream Use [Optional]\n \nThe applications of ESG-BERT can be expanded way beyond just text classification. It can be fine-tuned to perform various other downstream NLP tasks in the domain of Sustainable Investing.\n \n## Out-of-Scope Use\n \nThe model should not be used to intentionally create hostile or alienating environments for people. \n# Bias, Risks, and Limitations\n \n \nSignificant research has explored bias and fairness issues with language models (see, e.g., [Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)). Predictions generated by the model may include disturbing and harmful stereotypes across protected classes; identity characteristics; and sensitive, social, and occupational groups.\n \n \n## Recommendations\n \n \nUsers (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recomendations.\n \n \n# Training Details\n \n## Training Data\n \nMore information needed\n \n## Training Procedure\n \n<!-- This relates heavily to the Technical Specifications. Content here should link to that section when it is relevant to the training procedure. -->\n \n### Preprocessing\n \nMore information needed\n \n### Speeds, Sizes, Times\n \nMore information needed\n \n# Evaluation\n \n \n \n## Testing Data, Factors & Metrics\n \n### Testing Data\n \nThe fine-tuned model for text classification is also available [here](https://drive.google.com/drive/folders/1Qz4HP3xkjLfJ6DGCFNeJ7GmcPq65_HVe?usp=sharing). It can be used directly to make predictions using just a few steps.  First, download the fine-tuned pytorch_model.bin, config.json, and vocab.txt\n \n### Factors\n \nMore information needed\n \n### Metrics\n \nMore information needed\n \n## Results \n \nESG-BERT was further trained on unstructured text data with accuracies of 100% and 98% for Next Sentence Prediction and Masked Language Modelling tasks. Fine-tuning ESG-BERT for text classification yielded an F-1 score of 0.90. For comparison, the general BERT (BERT-base) model scored 0.79 after fine-tuning, and the sci-kit learn approach scored 0.67.\n \n# Model Examination\n \nMore information needed\n \n# Environmental Impact\n \n \nCarbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).\n \n- **Hardware Type:** More information needed\n- **Hours used:** More information needed\n- **Cloud Provider:**  information needed\n- **Compute Region:** More information needed\n- **Carbon Emitted:** More information needed\n \n# Technical Specifications [optional]\n \n## Model Architecture and Objective\n \nMore information needed\n \n## Compute Infrastructure\n \nMore information needed\n \n### Hardware\n \nMore information needed\n \n### Software\n \nJDK 11 is needed to serve the model\n \n# Citation\n \n<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->\n \n**BibTeX:**\n \nMore information needed\n \n**APA:**\n \nMore information needed\n \n# Glossary [optional]\n \n<!-- If relevant, include terms and calculations in this section that can help readers understand the model or model card. -->\n \nMore information needed\n \n# More Information [optional]\n \nMore information needed\n \n# Model Card Authors [optional]\n[Mukut Mukherjee](https://www.linkedin.com/in/mukutm/), [Charan Pothireddi](https://www.linkedin.com/in/sree-charan-pothireddi-6a0a3587/) and [Parabole.ai](https://www.linkedin.com/in/sree-charan-pothireddi-6a0a3587/), in collaboration with the Ezi Ozoani and the HuggingFace Team\n \n \n# Model Card Contact\n \nMore information needed\n \n# How to Get Started with the Model\n \nUse the code below to get started with the model.\n \n<details>\n <summary> Click to expand </summary>\n \n```\npip install torchserve torch-model-archiver\n \npip install torchvision\n \npip install transformers\n \n```\n \nNext up, we'll set up the handler script. It is a basic handler for text classification that can be improved upon. Save this script as \"handler.py\" in your directory. [1]\n \n```\n \nfrom abc import ABC\n \nimport json\n \nimport logging\n \nimport os\n \nimport torch\n \nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n \nfrom ts.torch_handler.base_handler import BaseHandler\n \nlogger = logging.getLogger(__name__)\n \nclass TransformersClassifierHandler(BaseHandler, ABC):\n \n   \"\"\"\n \n   Transformers text classifier handler class. This handler takes a text (string) and\n \n   as input and returns the classification text based on the serialized transformers checkpoint.\n \n   \"\"\"\n \n   def __init__(self):\n \n       super(TransformersClassifierHandler, self).__init__()\n \n       self.initialized = False\n \ndef initialize(self, ctx):\n \n       self.manifest = ctx.manifest\n \nproperties = ctx.system_properties\n \n       model_dir = properties.get(\"model_dir\")\n \n       self.device = torch.device(\"cuda:\" + str(properties.get(\"gpu_id\")) if torch.cuda.is_available() else \"cpu\")\n \n# Read model serialize/pt file\n \n       self.model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n \n       self.tokenizer = AutoTokenizer.from_pretrained(model_dir)\n \nself.model.to(self.device)\n \n       self.model.eval()\n \nlogger.debug('Transformer model from path {0} loaded successfully'.format(model_dir))\n \n# Read the mapping file, index to object name\n \n       mapping_file_path = os.path.join(model_dir, \"index_to_name.json\")\n \nif os.path.isfile(mapping_file_path):\n \n           with open(mapping_file_path) as f:\n \n               self.mapping = json.load(f)\n \n       else:\n \n           logger.warning('Missing the index_to_name.json file. Inference output will not include class name.')\n \nself.initialized = True\n \ndef preprocess(self, data):\n \n       \"\"\" Very basic preprocessing code - only tokenizes.\n \n           Extend with your own preprocessing steps as needed.\n \n       \"\"\"\n \n       text = data[0].get(\"data\")\n \n       if text is None:\n \n           text = data[0].get(\"body\")\n \n       sentences = text.decode('utf-8')\n \n       logger.info(\"Received text: '%s'\", sentences)\n \ninputs = self.tokenizer.encode_plus(\n \n           sentences,\n \n           add_special_tokens=True,\n \n           return_tensors=\"pt\"\n \n       )\n \n       return inputs\n \ndef inference(self, inputs):\n \n       \"\"\"\n \n       Predict the class of a text using a trained transformer model.\n \n       \"\"\"\n \n       # NOTE: This makes the assumption that your model expects text to be tokenized \n \n       # with \"input_ids\" and \"token_type_ids\" - which is true for some popular transformer models, e.g. bert.\n \n       # If your transformer model expects different tokenization, adapt this code to suit\n \n       # its expected input format.\n \n       prediction = self.model(\n \n           inputs['input_ids'].to(self.device),\n \n           token_type_ids=inputs['token_type_ids'].to(self.device)\n \n       )[0].argmax().item()\n \n       logger.info(\"Model predicted: '%s'\", prediction)\n \nif self.mapping:\n \n           prediction = self.mapping[str(prediction)]\n \nreturn [prediction]\n \ndef postprocess(self, inference_output):\n \n       # TODO: Add any needed post-processing of the model predictions here\n \n       return inference_output\n \n_service = TransformersClassifierHandler()\n \ndef handle(data, context):\n \n   try:\n \n       if not _service.initialized:\n \n           _service.initialize(context)\n \nif data is None:\n \n           return None\n \ndata = _service.preprocess(data)\n \n       data = _service.inference(data)\n \n       data = _service.postprocess(data)\n \nreturn data\n \n   except Exception as e:\n \n       raise e\n \n \n \n```\n \nTorcheServe uses a format called MAR (Model Archive). We can convert our PyTorch model to a .mar file using this command:\n \n```\n \ntorch-model-archiver --model-name \"bert\" --version 1.0 --serialized-file ./bert_model/pytorch_model.bin --extra-files \"./bert_model/config.json,./bert_model/vocab.txt\" --handler \"./handler.py\"\n \n```\n \nMove the .mar file into a new directory: \n \n```\n \nmkdir model_store && mv bert.mar model_store\n \n```\n \nFinally, we can start TorchServe using the command: \n \n```\n \ntorchserve --start --model-store model_store --models bert=bert.mar\n \n```\n \nWe can now query the model from another terminal window using the Inference API. We pass a text file containing text that the model will try to classify. \n \n\n \n \n```\n \ncurl -X POST http://127.0.0.1:8080/predictions/bert -T predict.txt\n \n```\n \nThis returns a label number which correlates to a textual label. This is stored in the label_dict.txt dictionary file. \n \n```\n \n__label__Business_Ethics :  0\n \n__label__Data_Security :  1\n \n__label__Access_And_Affordability :  2\n \n__label__Business_Model_Resilience :  3\n \n__label__Competitive_Behavior :  4\n \n__label__Critical_Incident_Risk_Management :  5\n \n__label__Customer_Welfare :  6\n \n__label__Director_Removal :  7\n \n__label__Employee_Engagement_Inclusion_And_Diversity :  8\n \n__label__Employee_Health_And_Safety :  9\n \n__label__Human_Rights_And_Community_Relations :  10\n \n__label__Labor_Practices :  11\n \n__label__Management_Of_Legal_And_Regulatory_Framework :  12\n \n__label__Physical_Impacts_Of_Climate_Change :  13\n \n__label__Product_Quality_And_Safety :  14\n \n__label__Product_Design_And_Lifecycle_Management :  15\n \n__label__Selling_Practices_And_Product_Labeling :  16\n \n__label__Supply_Chain_Management :  17\n \n__label__Systemic_Risk_Management :  18\n \n__label__Waste_And_Hazardous_Materials_Management :  19\n \n__label__Water_And_Wastewater_Management :  20\n \n__label__Air_Quality :  21\n \n__label__Customer_Privacy :  22\n \n__label__Ecological_Impacts :  23\n \n__label__Energy_Management :  24\n \n__label__GHG_Emissions :  25\n \n```\n\n<\\details>\n",
    "library_name": "transformers"
  },
  {
    "model_id": "SamLowe/roberta-base-go_emotions",
    "model_name": "SamLowe/roberta-base-go_emotions",
    "author": "SamLowe",
    "downloads": 559120,
    "likes": 552,
    "tags": [
      "transformers",
      "pytorch",
      "safetensors",
      "roberta",
      "text-classification",
      "emotions",
      "multi-class-classification",
      "multi-label-classification",
      "en",
      "dataset:go_emotions",
      "doi:10.57967/hf/3548",
      "license:mit",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/SamLowe/roberta-base-go_emotions",
    "dependencies": [
      [
        "transformers",
        "4.30.2"
      ],
      [
        "torch",
        "2.0.1"
      ],
      [
        "numpy",
        "1.24.3"
      ],
      [
        "hf_xet",
        "0.1.0"
      ],
      [
        "xformers",
        "0.0.22"
      ]
    ],
    "analysis_date": "2025-03-26T00:48:05.566837",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "roberta",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": "en",
      "license": "mit",
      "tags": [
        "text-classification",
        "pytorch",
        "roberta",
        "emotions",
        "multi-class-classification",
        "multi-label-classification"
      ],
      "datasets": [
        "go_emotions"
      ],
      "widget": [
        {
          "text": "I am not having a great day."
        }
      ]
    },
    "card_text": "\n#### Overview\n\nModel trained from [roberta-base](https://huggingface.co/roberta-base) on the [go_emotions](https://huggingface.co/datasets/go_emotions) dataset for multi-label classification.\n\n##### ONNX version also available\n\nA version of this model in ONNX format (including an INT8 quantized ONNX version) is now available at [https://huggingface.co/SamLowe/roberta-base-go_emotions-onnx](https://huggingface.co/SamLowe/roberta-base-go_emotions-onnx). These are faster for inference, esp for smaller batch sizes, massively reduce the size of the dependencies required for inference, make inference of the model more multi-platform, and in the case of the quantized version reduce the model file/download size by 75% whilst retaining almost all the accuracy if you only need inference.\n\n#### Dataset used for the model\n\n[go_emotions](https://huggingface.co/datasets/go_emotions) is based on Reddit data and has 28 labels. It is a multi-label dataset where one or multiple labels may apply for any given input text, hence this model is a multi-label classification model with 28 'probability' float outputs for any given input text. Typically a threshold of 0.5 is applied to the probabilities for the prediction for each label.\n\n#### How the model was created\n\nThe model was trained using `AutoModelForSequenceClassification.from_pretrained` with `problem_type=\"multi_label_classification\"` for 3 epochs with a learning rate of 2e-5 and weight decay of 0.01.\n\n#### Inference\n\nThere are multiple ways to use this model in Huggingface Transformers. Possibly the simplest is using a pipeline:\n\n```python\nfrom transformers import pipeline\n\nclassifier = pipeline(task=\"text-classification\", model=\"SamLowe/roberta-base-go_emotions\", top_k=None)\n\nsentences = [\"I am not having a great day\"]\n\nmodel_outputs = classifier(sentences)\nprint(model_outputs[0])\n# produces a list of dicts for each of the labels\n```\n\n#### Evaluation / metrics\n\nEvaluation of the model is available at\n\n- https://github.com/samlowe/go_emotions-dataset/blob/main/eval-roberta-base-go_emotions.ipynb\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/samlowe/go_emotions-dataset/blob/main/eval-roberta-base-go_emotions.ipynb)\n\n##### Summary\n\nAs provided in the above notebook, evaluation of the multi-label output (of the 28 dim output via a threshold of 0.5 to binarize each) using the dataset test split gives:\n\n- Accuracy: 0.474\n- Precision: 0.575\n- Recall: 0.396\n- F1: 0.450\n\nBut the metrics are more meaningful when measured per label given the multi-label nature (each label is effectively an independent binary classification) and the fact that there is drastically different representations of the labels in the dataset.\n\nWith a threshold of 0.5 applied to binarize the model outputs, as per the above notebook, the metrics per label are:\n\n|                | accuracy | precision | recall | f1    | mcc   | support | threshold |\n| -------------- | -------- | --------- | ------ | ----- | ----- | ------- | --------- |\n| admiration     | 0.946    | 0.725     | 0.675  | 0.699 | 0.670 | 504     | 0.5       |\n| amusement      | 0.982    | 0.790     | 0.871  | 0.829 | 0.821 | 264     | 0.5       |\n| anger          | 0.970    | 0.652     | 0.379  | 0.479 | 0.483 | 198     | 0.5       |\n| annoyance      | 0.940    | 0.472     | 0.159  | 0.238 | 0.250 | 320     | 0.5       |\n| approval       | 0.942    | 0.609     | 0.302  | 0.404 | 0.403 | 351     | 0.5       |\n| caring         | 0.973    | 0.448     | 0.319  | 0.372 | 0.364 | 135     | 0.5       |\n| confusion      | 0.972    | 0.500     | 0.431  | 0.463 | 0.450 | 153     | 0.5       |\n| curiosity      | 0.950    | 0.537     | 0.356  | 0.428 | 0.412 | 284     | 0.5       |\n| desire         | 0.987    | 0.630     | 0.410  | 0.496 | 0.502 | 83      | 0.5       |\n| disappointment | 0.974    | 0.625     | 0.199  | 0.302 | 0.343 | 151     | 0.5       |\n| disapproval    | 0.950    | 0.494     | 0.307  | 0.379 | 0.365 | 267     | 0.5       |\n| disgust        | 0.982    | 0.707     | 0.333  | 0.453 | 0.478 | 123     | 0.5       |\n| embarrassment  | 0.994    | 0.750     | 0.243  | 0.367 | 0.425 | 37      | 0.5       |\n| excitement     | 0.983    | 0.603     | 0.340  | 0.435 | 0.445 | 103     | 0.5       |\n| fear           | 0.992    | 0.758     | 0.603  | 0.671 | 0.672 | 78      | 0.5       |\n| gratitude      | 0.990    | 0.960     | 0.881  | 0.919 | 0.914 | 352     | 0.5       |\n| grief          | 0.999    | 0.000     | 0.000  | 0.000 | 0.000 | 6       | 0.5       |\n| joy            | 0.978    | 0.647     | 0.559  | 0.600 | 0.590 | 161     | 0.5       |\n| love           | 0.982    | 0.773     | 0.832  | 0.802 | 0.793 | 238     | 0.5       |\n| nervousness    | 0.996    | 0.600     | 0.130  | 0.214 | 0.278 | 23      | 0.5       |\n| optimism       | 0.972    | 0.667     | 0.376  | 0.481 | 0.488 | 186     | 0.5       |\n| pride          | 0.997    | 0.000     | 0.000  | 0.000 | 0.000 | 16      | 0.5       |\n| realization    | 0.974    | 0.541     | 0.138  | 0.220 | 0.264 | 145     | 0.5       |\n| relief         | 0.998    | 0.000     | 0.000  | 0.000 | 0.000 | 11      | 0.5       |\n| remorse        | 0.991    | 0.553     | 0.750  | 0.636 | 0.640 | 56      | 0.5       |\n| sadness        | 0.977    | 0.621     | 0.494  | 0.550 | 0.542 | 156     | 0.5       |\n| surprise       | 0.981    | 0.750     | 0.404  | 0.525 | 0.542 | 141     | 0.5       |\n| neutral        | 0.782    | 0.694     | 0.604  | 0.646 | 0.492 | 1787    | 0.5       |\n\nOptimizing the threshold per label for the one that gives the optimum F1 metrics gives slightly better metrics - sacrificing some precision for a greater gain in recall, hence to the benefit of F1 (how this was done is shown in the above notebook):\n\n|                | accuracy | precision | recall | f1    | mcc   | support | threshold |\n| -------------- | -------- | --------- | ------ | ----- | ----- | ------- | --------- |\n| admiration     | 0.940    | 0.651     | 0.776  | 0.708 | 0.678 | 504     | 0.25      |\n| amusement      | 0.982    | 0.781     | 0.890  | 0.832 | 0.825 | 264     | 0.45      |\n| anger          | 0.959    | 0.454     | 0.601  | 0.517 | 0.502 | 198     | 0.15      |\n| annoyance      | 0.864    | 0.243     | 0.619  | 0.349 | 0.328 | 320     | 0.10      |\n| approval       | 0.926    | 0.432     | 0.442  | 0.437 | 0.397 | 351     | 0.30      |\n| caring         | 0.972    | 0.426     | 0.385  | 0.405 | 0.391 | 135     | 0.40      |\n| confusion      | 0.974    | 0.548     | 0.412  | 0.470 | 0.462 | 153     | 0.55      |\n| curiosity      | 0.943    | 0.473     | 0.711  | 0.568 | 0.552 | 284     | 0.25      |\n| desire         | 0.985    | 0.518     | 0.530  | 0.524 | 0.516 | 83      | 0.25      |\n| disappointment | 0.974    | 0.562     | 0.298  | 0.390 | 0.398 | 151     | 0.40      |\n| disapproval    | 0.941    | 0.414     | 0.468  | 0.439 | 0.409 | 267     | 0.30      |\n| disgust        | 0.978    | 0.523     | 0.463  | 0.491 | 0.481 | 123     | 0.20      |\n| embarrassment  | 0.994    | 0.567     | 0.459  | 0.507 | 0.507 | 37      | 0.10      |\n| excitement     | 0.981    | 0.500     | 0.417  | 0.455 | 0.447 | 103     | 0.35      |\n| fear           | 0.991    | 0.712     | 0.667  | 0.689 | 0.685 | 78      | 0.40      |\n| gratitude      | 0.990    | 0.957     | 0.889  | 0.922 | 0.917 | 352     | 0.45      |\n| grief          | 0.999    | 0.333     | 0.333  | 0.333 | 0.333 | 6       | 0.05      |\n| joy            | 0.978    | 0.623     | 0.646  | 0.634 | 0.623 | 161     | 0.40      |\n| love           | 0.982    | 0.740     | 0.899  | 0.812 | 0.807 | 238     | 0.25      |\n| nervousness    | 0.996    | 0.571     | 0.348  | 0.432 | 0.444 | 23      | 0.25      |\n| optimism       | 0.971    | 0.580     | 0.565  | 0.572 | 0.557 | 186     | 0.20      |\n| pride          | 0.998    | 0.875     | 0.438  | 0.583 | 0.618 | 16      | 0.10      |\n| realization    | 0.961    | 0.270     | 0.262  | 0.266 | 0.246 | 145     | 0.15      |\n| relief         | 0.992    | 0.152     | 0.636  | 0.246 | 0.309 | 11      | 0.05      |\n| remorse        | 0.991    | 0.541     | 0.946  | 0.688 | 0.712 | 56      | 0.10      |\n| sadness        | 0.977    | 0.599     | 0.583  | 0.591 | 0.579 | 156     | 0.40      |\n| surprise       | 0.977    | 0.543     | 0.674  | 0.601 | 0.593 | 141     | 0.15      |\n| neutral        | 0.758    | 0.598     | 0.810  | 0.688 | 0.513 | 1787    | 0.25      |\n\nThis improves the overall metrics:\n\n- Precision: 0.542\n- Recall: 0.577\n- F1: 0.541\n\nOr if calculated weighted by the relative size of the support of each label:\n\n- Precision: 0.572\n- Recall: 0.677\n- F1: 0.611\n\n#### Commentary on the dataset\n\nSome labels (E.g. gratitude) when considered independently perform very strongly with F1 exceeding 0.9, whilst others (E.g. relief) perform very poorly.\n\nThis is a challenging dataset. Labels such as relief do have much fewer examples in the training data (less than 100 out of the 40k+, and only 11 in the test split).\n\nBut there is also some ambiguity and/or labelling errors visible in the training data of go_emotions that is suspected to constrain the performance. Data cleaning on the dataset to reduce some of the mistakes, ambiguity, conflicts and duplication in the labelling would produce a higher performing model.",
    "card_content": "---\nlanguage: en\nlicense: mit\ntags:\n- text-classification\n- pytorch\n- roberta\n- emotions\n- multi-class-classification\n- multi-label-classification\ndatasets:\n- go_emotions\nwidget:\n- text: I am not having a great day.\n---\n\n#### Overview\n\nModel trained from [roberta-base](https://huggingface.co/roberta-base) on the [go_emotions](https://huggingface.co/datasets/go_emotions) dataset for multi-label classification.\n\n##### ONNX version also available\n\nA version of this model in ONNX format (including an INT8 quantized ONNX version) is now available at [https://huggingface.co/SamLowe/roberta-base-go_emotions-onnx](https://huggingface.co/SamLowe/roberta-base-go_emotions-onnx). These are faster for inference, esp for smaller batch sizes, massively reduce the size of the dependencies required for inference, make inference of the model more multi-platform, and in the case of the quantized version reduce the model file/download size by 75% whilst retaining almost all the accuracy if you only need inference.\n\n#### Dataset used for the model\n\n[go_emotions](https://huggingface.co/datasets/go_emotions) is based on Reddit data and has 28 labels. It is a multi-label dataset where one or multiple labels may apply for any given input text, hence this model is a multi-label classification model with 28 'probability' float outputs for any given input text. Typically a threshold of 0.5 is applied to the probabilities for the prediction for each label.\n\n#### How the model was created\n\nThe model was trained using `AutoModelForSequenceClassification.from_pretrained` with `problem_type=\"multi_label_classification\"` for 3 epochs with a learning rate of 2e-5 and weight decay of 0.01.\n\n#### Inference\n\nThere are multiple ways to use this model in Huggingface Transformers. Possibly the simplest is using a pipeline:\n\n```python\nfrom transformers import pipeline\n\nclassifier = pipeline(task=\"text-classification\", model=\"SamLowe/roberta-base-go_emotions\", top_k=None)\n\nsentences = [\"I am not having a great day\"]\n\nmodel_outputs = classifier(sentences)\nprint(model_outputs[0])\n# produces a list of dicts for each of the labels\n```\n\n#### Evaluation / metrics\n\nEvaluation of the model is available at\n\n- https://github.com/samlowe/go_emotions-dataset/blob/main/eval-roberta-base-go_emotions.ipynb\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/samlowe/go_emotions-dataset/blob/main/eval-roberta-base-go_emotions.ipynb)\n\n##### Summary\n\nAs provided in the above notebook, evaluation of the multi-label output (of the 28 dim output via a threshold of 0.5 to binarize each) using the dataset test split gives:\n\n- Accuracy: 0.474\n- Precision: 0.575\n- Recall: 0.396\n- F1: 0.450\n\nBut the metrics are more meaningful when measured per label given the multi-label nature (each label is effectively an independent binary classification) and the fact that there is drastically different representations of the labels in the dataset.\n\nWith a threshold of 0.5 applied to binarize the model outputs, as per the above notebook, the metrics per label are:\n\n|                | accuracy | precision | recall | f1    | mcc   | support | threshold |\n| -------------- | -------- | --------- | ------ | ----- | ----- | ------- | --------- |\n| admiration     | 0.946    | 0.725     | 0.675  | 0.699 | 0.670 | 504     | 0.5       |\n| amusement      | 0.982    | 0.790     | 0.871  | 0.829 | 0.821 | 264     | 0.5       |\n| anger          | 0.970    | 0.652     | 0.379  | 0.479 | 0.483 | 198     | 0.5       |\n| annoyance      | 0.940    | 0.472     | 0.159  | 0.238 | 0.250 | 320     | 0.5       |\n| approval       | 0.942    | 0.609     | 0.302  | 0.404 | 0.403 | 351     | 0.5       |\n| caring         | 0.973    | 0.448     | 0.319  | 0.372 | 0.364 | 135     | 0.5       |\n| confusion      | 0.972    | 0.500     | 0.431  | 0.463 | 0.450 | 153     | 0.5       |\n| curiosity      | 0.950    | 0.537     | 0.356  | 0.428 | 0.412 | 284     | 0.5       |\n| desire         | 0.987    | 0.630     | 0.410  | 0.496 | 0.502 | 83      | 0.5       |\n| disappointment | 0.974    | 0.625     | 0.199  | 0.302 | 0.343 | 151     | 0.5       |\n| disapproval    | 0.950    | 0.494     | 0.307  | 0.379 | 0.365 | 267     | 0.5       |\n| disgust        | 0.982    | 0.707     | 0.333  | 0.453 | 0.478 | 123     | 0.5       |\n| embarrassment  | 0.994    | 0.750     | 0.243  | 0.367 | 0.425 | 37      | 0.5       |\n| excitement     | 0.983    | 0.603     | 0.340  | 0.435 | 0.445 | 103     | 0.5       |\n| fear           | 0.992    | 0.758     | 0.603  | 0.671 | 0.672 | 78      | 0.5       |\n| gratitude      | 0.990    | 0.960     | 0.881  | 0.919 | 0.914 | 352     | 0.5       |\n| grief          | 0.999    | 0.000     | 0.000  | 0.000 | 0.000 | 6       | 0.5       |\n| joy            | 0.978    | 0.647     | 0.559  | 0.600 | 0.590 | 161     | 0.5       |\n| love           | 0.982    | 0.773     | 0.832  | 0.802 | 0.793 | 238     | 0.5       |\n| nervousness    | 0.996    | 0.600     | 0.130  | 0.214 | 0.278 | 23      | 0.5       |\n| optimism       | 0.972    | 0.667     | 0.376  | 0.481 | 0.488 | 186     | 0.5       |\n| pride          | 0.997    | 0.000     | 0.000  | 0.000 | 0.000 | 16      | 0.5       |\n| realization    | 0.974    | 0.541     | 0.138  | 0.220 | 0.264 | 145     | 0.5       |\n| relief         | 0.998    | 0.000     | 0.000  | 0.000 | 0.000 | 11      | 0.5       |\n| remorse        | 0.991    | 0.553     | 0.750  | 0.636 | 0.640 | 56      | 0.5       |\n| sadness        | 0.977    | 0.621     | 0.494  | 0.550 | 0.542 | 156     | 0.5       |\n| surprise       | 0.981    | 0.750     | 0.404  | 0.525 | 0.542 | 141     | 0.5       |\n| neutral        | 0.782    | 0.694     | 0.604  | 0.646 | 0.492 | 1787    | 0.5       |\n\nOptimizing the threshold per label for the one that gives the optimum F1 metrics gives slightly better metrics - sacrificing some precision for a greater gain in recall, hence to the benefit of F1 (how this was done is shown in the above notebook):\n\n|                | accuracy | precision | recall | f1    | mcc   | support | threshold |\n| -------------- | -------- | --------- | ------ | ----- | ----- | ------- | --------- |\n| admiration     | 0.940    | 0.651     | 0.776  | 0.708 | 0.678 | 504     | 0.25      |\n| amusement      | 0.982    | 0.781     | 0.890  | 0.832 | 0.825 | 264     | 0.45      |\n| anger          | 0.959    | 0.454     | 0.601  | 0.517 | 0.502 | 198     | 0.15      |\n| annoyance      | 0.864    | 0.243     | 0.619  | 0.349 | 0.328 | 320     | 0.10      |\n| approval       | 0.926    | 0.432     | 0.442  | 0.437 | 0.397 | 351     | 0.30      |\n| caring         | 0.972    | 0.426     | 0.385  | 0.405 | 0.391 | 135     | 0.40      |\n| confusion      | 0.974    | 0.548     | 0.412  | 0.470 | 0.462 | 153     | 0.55      |\n| curiosity      | 0.943    | 0.473     | 0.711  | 0.568 | 0.552 | 284     | 0.25      |\n| desire         | 0.985    | 0.518     | 0.530  | 0.524 | 0.516 | 83      | 0.25      |\n| disappointment | 0.974    | 0.562     | 0.298  | 0.390 | 0.398 | 151     | 0.40      |\n| disapproval    | 0.941    | 0.414     | 0.468  | 0.439 | 0.409 | 267     | 0.30      |\n| disgust        | 0.978    | 0.523     | 0.463  | 0.491 | 0.481 | 123     | 0.20      |\n| embarrassment  | 0.994    | 0.567     | 0.459  | 0.507 | 0.507 | 37      | 0.10      |\n| excitement     | 0.981    | 0.500     | 0.417  | 0.455 | 0.447 | 103     | 0.35      |\n| fear           | 0.991    | 0.712     | 0.667  | 0.689 | 0.685 | 78      | 0.40      |\n| gratitude      | 0.990    | 0.957     | 0.889  | 0.922 | 0.917 | 352     | 0.45      |\n| grief          | 0.999    | 0.333     | 0.333  | 0.333 | 0.333 | 6       | 0.05      |\n| joy            | 0.978    | 0.623     | 0.646  | 0.634 | 0.623 | 161     | 0.40      |\n| love           | 0.982    | 0.740     | 0.899  | 0.812 | 0.807 | 238     | 0.25      |\n| nervousness    | 0.996    | 0.571     | 0.348  | 0.432 | 0.444 | 23      | 0.25      |\n| optimism       | 0.971    | 0.580     | 0.565  | 0.572 | 0.557 | 186     | 0.20      |\n| pride          | 0.998    | 0.875     | 0.438  | 0.583 | 0.618 | 16      | 0.10      |\n| realization    | 0.961    | 0.270     | 0.262  | 0.266 | 0.246 | 145     | 0.15      |\n| relief         | 0.992    | 0.152     | 0.636  | 0.246 | 0.309 | 11      | 0.05      |\n| remorse        | 0.991    | 0.541     | 0.946  | 0.688 | 0.712 | 56      | 0.10      |\n| sadness        | 0.977    | 0.599     | 0.583  | 0.591 | 0.579 | 156     | 0.40      |\n| surprise       | 0.977    | 0.543     | 0.674  | 0.601 | 0.593 | 141     | 0.15      |\n| neutral        | 0.758    | 0.598     | 0.810  | 0.688 | 0.513 | 1787    | 0.25      |\n\nThis improves the overall metrics:\n\n- Precision: 0.542\n- Recall: 0.577\n- F1: 0.541\n\nOr if calculated weighted by the relative size of the support of each label:\n\n- Precision: 0.572\n- Recall: 0.677\n- F1: 0.611\n\n#### Commentary on the dataset\n\nSome labels (E.g. gratitude) when considered independently perform very strongly with F1 exceeding 0.9, whilst others (E.g. relief) perform very poorly.\n\nThis is a challenging dataset. Labels such as relief do have much fewer examples in the training data (less than 100 out of the 40k+, and only 11 in the test split).\n\nBut there is also some ambiguity and/or labelling errors visible in the training data of go_emotions that is suspected to constrain the performance. Data cleaning on the dataset to reduce some of the mistakes, ambiguity, conflicts and duplication in the labelling would produce a higher performing model.",
    "library_name": "transformers"
  },
  {
    "model_id": "tals/albert-xlarge-vitaminc-mnli",
    "model_name": "tals/albert-xlarge-vitaminc-mnli",
    "author": "tals",
    "downloads": 481732,
    "likes": 6,
    "tags": [
      "transformers",
      "pytorch",
      "tf",
      "safetensors",
      "albert",
      "text-classification",
      "dataset:glue",
      "dataset:multi_nli",
      "dataset:tals/vitaminc",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/tals/albert-xlarge-vitaminc-mnli",
    "dependencies": [
      [
        "datasets",
        "2.14.5"
      ],
      [
        "torch",
        "2.0.1"
      ],
      [
        "transformers",
        "4.30.2"
      ],
      [
        "numpy",
        "1.24.3"
      ],
      [
        "sentencepiece",
        "0.1.99"
      ]
    ],
    "analysis_date": "2025-03-26T00:48:06.659893",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "albert",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "datasets": [
        "glue",
        "multi_nli",
        "tals/vitaminc"
      ]
    },
    "card_text": "# Details\nModel used in [Get Your Vitamin C! Robust Fact Verification with Contrastive Evidence](https://aclanthology.org/2021.naacl-main.52/) (Schuster et al., NAACL 21`).\n\nFor more details see: https://github.com/TalSchuster/VitaminC\n\nWhen using this model, please cite the paper.\n\n# BibTeX entry and citation info\n\n```bibtex\n@inproceedings{schuster-etal-2021-get,\n    title = \"Get Your Vitamin {C}! Robust Fact Verification with Contrastive Evidence\",\n    author = \"Schuster, Tal  and\n      Fisch, Adam  and\n      Barzilay, Regina\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.52\",\n    doi = \"10.18653/v1/2021.naacl-main.52\",\n    pages = \"624--643\",\n    abstract = \"Typical fact verification models use retrieved written evidence to verify claims. Evidence sources, however, often change over time as more information is gathered and revised. In order to adapt, models must be sensitive to subtle differences in supporting evidence. We present VitaminC, a benchmark infused with challenging cases that require fact verification models to discern and adjust to slight factual changes. We collect over 100,000 Wikipedia revisions that modify an underlying fact, and leverage these revisions, together with additional synthetically constructed ones, to create a total of over 400,000 claim-evidence pairs. Unlike previous resources, the examples in VitaminC are contrastive, i.e., they contain evidence pairs that are nearly identical in language and content, with the exception that one supports a given claim while the other does not. We show that training using this design increases robustness{---}improving accuracy by 10{\\%} on adversarial fact verification and 6{\\%} on adversarial natural language inference (NLI). Moreover, the structure of VitaminC leads us to define additional tasks for fact-checking resources: tagging relevant words in the evidence for verifying the claim, identifying factual revisions, and providing automatic edits via factually consistent text generation.\",\n}\n\n```\n",
    "card_content": "---\ndatasets:\n- glue\n- multi_nli\n- tals/vitaminc\n---\n# Details\nModel used in [Get Your Vitamin C! Robust Fact Verification with Contrastive Evidence](https://aclanthology.org/2021.naacl-main.52/) (Schuster et al., NAACL 21`).\n\nFor more details see: https://github.com/TalSchuster/VitaminC\n\nWhen using this model, please cite the paper.\n\n# BibTeX entry and citation info\n\n```bibtex\n@inproceedings{schuster-etal-2021-get,\n    title = \"Get Your Vitamin {C}! Robust Fact Verification with Contrastive Evidence\",\n    author = \"Schuster, Tal  and\n      Fisch, Adam  and\n      Barzilay, Regina\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.52\",\n    doi = \"10.18653/v1/2021.naacl-main.52\",\n    pages = \"624--643\",\n    abstract = \"Typical fact verification models use retrieved written evidence to verify claims. Evidence sources, however, often change over time as more information is gathered and revised. In order to adapt, models must be sensitive to subtle differences in supporting evidence. We present VitaminC, a benchmark infused with challenging cases that require fact verification models to discern and adjust to slight factual changes. We collect over 100,000 Wikipedia revisions that modify an underlying fact, and leverage these revisions, together with additional synthetically constructed ones, to create a total of over 400,000 claim-evidence pairs. Unlike previous resources, the examples in VitaminC are contrastive, i.e., they contain evidence pairs that are nearly identical in language and content, with the exception that one supports a given claim while the other does not. We show that training using this design increases robustness{---}improving accuracy by 10{\\%} on adversarial fact verification and 6{\\%} on adversarial natural language inference (NLI). Moreover, the structure of VitaminC leads us to define additional tasks for fact-checking resources: tagging relevant words in the evidence for verifying the claim, identifying factual revisions, and providing automatic edits via factually consistent text generation.\",\n}\n\n```\n",
    "library_name": "transformers"
  },
  {
    "model_id": "mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis",
    "model_name": "mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis",
    "author": "mrm8488",
    "downloads": 480223,
    "likes": 381,
    "tags": [
      "transformers",
      "pytorch",
      "tensorboard",
      "safetensors",
      "roberta",
      "text-classification",
      "generated_from_trainer",
      "financial",
      "stocks",
      "sentiment",
      "dataset:financial_phrasebank",
      "license:apache-2.0",
      "model-index",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis",
    "dependencies": [
      [
        "transformers",
        "4.10.2"
      ],
      [
        "torch",
        "1.9.0"
      ],
      [
        "datasets",
        "1.12.1"
      ],
      [
        "tokenizers",
        "0.10.3"
      ],
      [
        "numpy",
        "1.24.3"
      ]
    ],
    "analysis_date": "2025-03-26T00:48:09.126206",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "roberta",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "license": "apache-2.0",
      "tags": [
        "generated_from_trainer",
        "financial",
        "stocks",
        "sentiment"
      ],
      "datasets": [
        "financial_phrasebank"
      ],
      "metrics": [
        "accuracy"
      ],
      "thumbnail": "https://huggingface.co/mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis/resolve/main/logo_no_bg.png",
      "widget": [
        {
          "text": "Operating profit totaled EUR 9.4 mn , down from EUR 11.7 mn in 2004 ."
        }
      ],
      "model-index": [
        {
          "name": "distilRoberta-financial-sentiment",
          "results": [
            {
              "task": {
                "type": "text-classification",
                "name": "Text Classification"
              },
              "dataset": {
                "name": "financial_phrasebank",
                "type": "financial_phrasebank",
                "args": "sentences_allagree"
              },
              "metrics": [
                {
                  "type": "accuracy",
                  "value": 0.9823008849557522,
                  "name": "Accuracy"
                }
              ]
            }
          ]
        }
      ]
    },
    "card_text": "\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n\n<div style=\"text-align:center;width:250px;height:250px;\">\n    <img src=\"https://huggingface.co/mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis/resolve/main/logo_no_bg.png\" alt=\"logo\">\n</div>\n\n\n# DistilRoberta-financial-sentiment\n\n\nThis model is a fine-tuned version of [distilroberta-base](https://huggingface.co/distilroberta-base) on the financial_phrasebank dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.1116\n- Accuracy: **0.98**23\n\n## Base Model description\n\nThis model is a distilled version of the [RoBERTa-base model](https://huggingface.co/roberta-base). It follows the same training procedure as [DistilBERT](https://huggingface.co/distilbert-base-uncased).\nThe code for the distillation process can be found [here](https://github.com/huggingface/transformers/tree/master/examples/distillation).\nThis model is case-sensitive: it makes a difference between English and English.\n\nThe model has 6 layers, 768 dimension and 12 heads, totalizing 82M parameters (compared to 125M parameters for RoBERTa-base).\nOn average DistilRoBERTa is twice as fast as Roberta-base.\n\n## Training Data\n\nPolar sentiment dataset of sentences from financial news. The dataset consists of 4840 sentences from English language financial news categorised by sentiment. The dataset is divided by agreement rate of 5-8 annotators.\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 5\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss | Accuracy |\n|:-------------:|:-----:|:----:|:---------------:|:--------:|\n| No log        | 1.0   | 255  | 0.1670          | 0.9646   |\n| 0.209         | 2.0   | 510  | 0.2290          | 0.9558   |\n| 0.209         | 3.0   | 765  | 0.2044          | 0.9558   |\n| 0.0326        | 4.0   | 1020 | 0.1116          | 0.9823   |\n| 0.0326        | 5.0   | 1275 | 0.1127          | 0.9779   |\n\n\n### Framework versions\n\n- Transformers 4.10.2\n- Pytorch 1.9.0+cu102\n- Datasets 1.12.1\n- Tokenizers 0.10.3\n",
    "card_content": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\n- financial\n- stocks\n- sentiment\ndatasets:\n- financial_phrasebank\nmetrics:\n- accuracy\nthumbnail: https://huggingface.co/mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis/resolve/main/logo_no_bg.png\nwidget:\n- text: Operating profit totaled EUR 9.4 mn , down from EUR 11.7 mn in 2004 .\nmodel-index:\n- name: distilRoberta-financial-sentiment\n  results:\n  - task:\n      type: text-classification\n      name: Text Classification\n    dataset:\n      name: financial_phrasebank\n      type: financial_phrasebank\n      args: sentences_allagree\n    metrics:\n    - type: accuracy\n      value: 0.9823008849557522\n      name: Accuracy\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n\n<div style=\"text-align:center;width:250px;height:250px;\">\n    <img src=\"https://huggingface.co/mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis/resolve/main/logo_no_bg.png\" alt=\"logo\">\n</div>\n\n\n# DistilRoberta-financial-sentiment\n\n\nThis model is a fine-tuned version of [distilroberta-base](https://huggingface.co/distilroberta-base) on the financial_phrasebank dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.1116\n- Accuracy: **0.98**23\n\n## Base Model description\n\nThis model is a distilled version of the [RoBERTa-base model](https://huggingface.co/roberta-base). It follows the same training procedure as [DistilBERT](https://huggingface.co/distilbert-base-uncased).\nThe code for the distillation process can be found [here](https://github.com/huggingface/transformers/tree/master/examples/distillation).\nThis model is case-sensitive: it makes a difference between English and English.\n\nThe model has 6 layers, 768 dimension and 12 heads, totalizing 82M parameters (compared to 125M parameters for RoBERTa-base).\nOn average DistilRoBERTa is twice as fast as Roberta-base.\n\n## Training Data\n\nPolar sentiment dataset of sentences from financial news. The dataset consists of 4840 sentences from English language financial news categorised by sentiment. The dataset is divided by agreement rate of 5-8 annotators.\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 5\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss | Accuracy |\n|:-------------:|:-----:|:----:|:---------------:|:--------:|\n| No log        | 1.0   | 255  | 0.1670          | 0.9646   |\n| 0.209         | 2.0   | 510  | 0.2290          | 0.9558   |\n| 0.209         | 3.0   | 765  | 0.2044          | 0.9558   |\n| 0.0326        | 4.0   | 1020 | 0.1116          | 0.9823   |\n| 0.0326        | 5.0   | 1275 | 0.1127          | 0.9779   |\n\n\n### Framework versions\n\n- Transformers 4.10.2\n- Pytorch 1.9.0+cu102\n- Datasets 1.12.1\n- Tokenizers 0.10.3\n",
    "library_name": "transformers"
  },
  {
    "model_id": "cross-encoder/ms-marco-electra-base",
    "model_name": "cross-encoder/ms-marco-electra-base",
    "author": "cross-encoder",
    "downloads": 407643,
    "likes": 5,
    "tags": [
      "transformers",
      "pytorch",
      "safetensors",
      "electra",
      "text-classification",
      "license:apache-2.0",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/cross-encoder/ms-marco-electra-base",
    "dependencies": [
      [
        "sentence_transformers",
        null
      ],
      [
        "transformers",
        null
      ],
      [
        "torch",
        null
      ],
      [
        "sentence-transformers",
        "2.2.2"
      ]
    ],
    "analysis_date": "2025-03-26T00:48:11.255143",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "electra",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "license": "apache-2.0"
    },
    "card_text": "# Cross-Encoder for MS Marco\n\nThis model was trained on the [MS Marco Passage Ranking](https://github.com/microsoft/MSMARCO-Passage-Ranking) task.\n\nThe model can be used for Information Retrieval: Given a query, encode the query will all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. See [SBERT.net Retrieve & Re-rank](https://www.sbert.net/examples/applications/retrieve_rerank/README.html) for more details. The training code is available here: [SBERT.net Training MS Marco](https://github.com/UKPLab/sentence-transformers/tree/master/examples/training/ms_marco)\n\n\n## Usage with SentenceTransformers\n\nThe usage is easy when you have [SentenceTransformers](https://www.sbert.net/) installed. Then you can use the pre-trained models like this:\n```python\nfrom sentence_transformers import CrossEncoder\n\nmodel = CrossEncoder('cross-encoder/ms-marco-electra-base')\nscores = model.predict([\n    (\"How many people live in Berlin?\", \"Berlin had a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\"),\n    (\"How many people live in Berlin?\", \"Berlin is well known for its museums.\"),\n])\nprint(scores)\n# [9.9227107e-01 2.0136760e-05]\n```\n\n## Usage with Transformers\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\n\nmodel = AutoModelForSequenceClassification.from_pretrained('model_name')\ntokenizer = AutoTokenizer.from_pretrained('model_name')\n\nfeatures = tokenizer(['How many people live in Berlin?', 'How many people live in Berlin?'], ['Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.', 'New York City is famous for the Metropolitan Museum of Art.'],  padding=True, truncation=True, return_tensors=\"pt\")\n\nmodel.eval()\nwith torch.no_grad():\n    scores = model(**features).logits\n    print(scores)\n```\n\n\n## Performance\nIn the following table, we provide various pre-trained Cross-Encoders together with their performance on the [TREC Deep Learning 2019](https://microsoft.github.io/TREC-2019-Deep-Learning/) and the [MS Marco Passage Reranking](https://github.com/microsoft/MSMARCO-Passage-Ranking/) dataset. \n\n\n| Model-Name        | NDCG@10 (TREC DL 19) | MRR@10 (MS Marco Dev)  | Docs / Sec |\n| ------------- |:-------------| -----| --- | \n| **Version 2 models** | | | \n| cross-encoder/ms-marco-TinyBERT-L-2-v2 | 69.84 | 32.56 | 9000\n| cross-encoder/ms-marco-MiniLM-L-2-v2 | 71.01 | 34.85 | 4100\n| cross-encoder/ms-marco-MiniLM-L-4-v2 | 73.04 | 37.70 | 2500\n| cross-encoder/ms-marco-MiniLM-L-6-v2 | 74.30 | 39.01 | 1800\n| cross-encoder/ms-marco-MiniLM-L-12-v2 | 74.31 | 39.02 | 960\n| **Version 1 models** | | | \n| cross-encoder/ms-marco-TinyBERT-L-2  | 67.43 | 30.15  | 9000\n| cross-encoder/ms-marco-TinyBERT-L-4  | 68.09 | 34.50  | 2900\n| cross-encoder/ms-marco-TinyBERT-L-6 |  69.57 | 36.13  | 680\n| cross-encoder/ms-marco-electra-base | 71.99 | 36.41 | 340\n| **Other models** | | | \n| nboost/pt-tinybert-msmarco | 63.63 | 28.80 | 2900 \n| nboost/pt-bert-base-uncased-msmarco | 70.94 | 34.75 | 340 \n| nboost/pt-bert-large-msmarco | 73.36 | 36.48 | 100 \n| Capreolus/electra-base-msmarco | 71.23 | 36.89 | 340 \n| amberoad/bert-multilingual-passage-reranking-msmarco | 68.40 | 35.54 | 330 \n| sebastian-hofstaetter/distilbert-cat-margin_mse-T2-msmarco | 72.82 | 37.88 | 720\n \n Note: Runtime was computed on a V100 GPU.\n",
    "card_content": "---\nlicense: apache-2.0\n---\n# Cross-Encoder for MS Marco\n\nThis model was trained on the [MS Marco Passage Ranking](https://github.com/microsoft/MSMARCO-Passage-Ranking) task.\n\nThe model can be used for Information Retrieval: Given a query, encode the query will all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. See [SBERT.net Retrieve & Re-rank](https://www.sbert.net/examples/applications/retrieve_rerank/README.html) for more details. The training code is available here: [SBERT.net Training MS Marco](https://github.com/UKPLab/sentence-transformers/tree/master/examples/training/ms_marco)\n\n\n## Usage with SentenceTransformers\n\nThe usage is easy when you have [SentenceTransformers](https://www.sbert.net/) installed. Then you can use the pre-trained models like this:\n```python\nfrom sentence_transformers import CrossEncoder\n\nmodel = CrossEncoder('cross-encoder/ms-marco-electra-base')\nscores = model.predict([\n    (\"How many people live in Berlin?\", \"Berlin had a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\"),\n    (\"How many people live in Berlin?\", \"Berlin is well known for its museums.\"),\n])\nprint(scores)\n# [9.9227107e-01 2.0136760e-05]\n```\n\n## Usage with Transformers\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\n\nmodel = AutoModelForSequenceClassification.from_pretrained('model_name')\ntokenizer = AutoTokenizer.from_pretrained('model_name')\n\nfeatures = tokenizer(['How many people live in Berlin?', 'How many people live in Berlin?'], ['Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.', 'New York City is famous for the Metropolitan Museum of Art.'],  padding=True, truncation=True, return_tensors=\"pt\")\n\nmodel.eval()\nwith torch.no_grad():\n    scores = model(**features).logits\n    print(scores)\n```\n\n\n## Performance\nIn the following table, we provide various pre-trained Cross-Encoders together with their performance on the [TREC Deep Learning 2019](https://microsoft.github.io/TREC-2019-Deep-Learning/) and the [MS Marco Passage Reranking](https://github.com/microsoft/MSMARCO-Passage-Ranking/) dataset. \n\n\n| Model-Name        | NDCG@10 (TREC DL 19) | MRR@10 (MS Marco Dev)  | Docs / Sec |\n| ------------- |:-------------| -----| --- | \n| **Version 2 models** | | | \n| cross-encoder/ms-marco-TinyBERT-L-2-v2 | 69.84 | 32.56 | 9000\n| cross-encoder/ms-marco-MiniLM-L-2-v2 | 71.01 | 34.85 | 4100\n| cross-encoder/ms-marco-MiniLM-L-4-v2 | 73.04 | 37.70 | 2500\n| cross-encoder/ms-marco-MiniLM-L-6-v2 | 74.30 | 39.01 | 1800\n| cross-encoder/ms-marco-MiniLM-L-12-v2 | 74.31 | 39.02 | 960\n| **Version 1 models** | | | \n| cross-encoder/ms-marco-TinyBERT-L-2  | 67.43 | 30.15  | 9000\n| cross-encoder/ms-marco-TinyBERT-L-4  | 68.09 | 34.50  | 2900\n| cross-encoder/ms-marco-TinyBERT-L-6 |  69.57 | 36.13  | 680\n| cross-encoder/ms-marco-electra-base | 71.99 | 36.41 | 340\n| **Other models** | | | \n| nboost/pt-tinybert-msmarco | 63.63 | 28.80 | 2900 \n| nboost/pt-bert-base-uncased-msmarco | 70.94 | 34.75 | 340 \n| nboost/pt-bert-large-msmarco | 73.36 | 36.48 | 100 \n| Capreolus/electra-base-msmarco | 71.23 | 36.89 | 340 \n| amberoad/bert-multilingual-passage-reranking-msmarco | 68.40 | 35.54 | 330 \n| sebastian-hofstaetter/distilbert-cat-margin_mse-T2-msmarco | 72.82 | 37.88 | 720\n \n Note: Runtime was computed on a V100 GPU.\n",
    "library_name": "transformers"
  },
  {
    "model_id": "jinaai/jina-reranker-v2-base-multilingual",
    "model_name": "jinaai/jina-reranker-v2-base-multilingual",
    "author": "jinaai",
    "downloads": 404063,
    "likes": 253,
    "tags": [
      "transformers",
      "pytorch",
      "onnx",
      "safetensors",
      "text-classification",
      "reranker",
      "cross-encoder",
      "transformers.js",
      "custom_code",
      "multilingual",
      "license:cc-by-nc-4.0",
      "autotrain_compatible",
      "region:eu"
    ],
    "card_url": "https://huggingface.co/jinaai/jina-reranker-v2-base-multilingual",
    "dependencies": [
      [
        "transformers",
        "4.30.2"
      ],
      [
        "einops",
        "0.6.1"
      ],
      [
        "sentence-transformers",
        "2.2.2"
      ]
    ],
    "analysis_date": "2025-03-26T00:48:13.108257",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "unknown",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": [
        "multilingual"
      ],
      "license": "cc-by-nc-4.0",
      "library_name": "transformers",
      "tags": [
        "transformers",
        "reranker",
        "cross-encoder",
        "transformers.js"
      ],
      "pipeline_tag": "text-classification",
      "inference": false
    },
    "card_text": "\n<br><br>\n\n<p align=\"center\">\n<img src=\"https://huggingface.co/datasets/jinaai/documentation-images/resolve/main/logo.webp\" alt=\"Jina AI: Your Search Foundation, Supercharged!\" width=\"150px\">\n</p>\n\n<p align=\"center\">\n<b>Trained by <a href=\"https://jina.ai/\"><b>Jina AI</b></a>.</b>\n</p>\n\n# jina-reranker-v2-base-multilingual\n\n## Intended Usage & Model Info\n\nThe **Jina Reranker v2** (`jina-reranker-v2-base-multilingual`) is a transformer-based model that has been fine-tuned for text reranking task, which is a crucial component in many information retrieval systems. It is a cross-encoder model that takes a query and a document pair as input and outputs a score indicating the relevance of the document to the query. The model is trained on a large dataset of query-document pairs and is capable of reranking documents in multiple languages with high accuracy.\n\nCompared with the state-of-the-art reranker models, including the previous released `jina-reranker-v1-base-en`, the **Jina Reranker v2** model has demonstrated competitiveness across a series of benchmarks targeting for text retrieval, multilingual capability, function-calling-aware and text-to-SQL-aware reranking, and code retrieval tasks.\n\nThe `jina-reranker-v2-base-multilingual` model is capable of handling long texts with a context length of up to `1024` tokens, enabling the processing of extensive inputs. To enable the model to handle long texts that exceed 1024 tokens, the model uses a sliding window approach to chunk the input text into smaller pieces and rerank each chunk separately.\n\nThe model is also equipped with a flash attention mechanism, which significantly improves the model's performance.\n\n\n# Usage\n\n_This model repository is licenced for research and evaluation purposes under CC-BY-NC-4.0. For commercial usage, please refer to Jina AI's APIs, AWS Sagemaker or Azure Marketplace offerings. Please [contact us](https://jina.ai/contact-sales) for any further clarifications._\n1. The easiest way to use `jina-reranker-v2-base-multilingual` is to call Jina AI's [Reranker API](https://jina.ai/reranker/).\n\n```bash\ncurl https://api.jina.ai/v1/rerank \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer YOUR_API_KEY\" \\\n  -d '{\n  \"model\": \"jina-reranker-v2-base-multilingual\",\n  \"query\": \"Organic skincare products for sensitive skin\",\n  \"documents\": [\n    \"Organic skincare for sensitive skin with aloe vera and chamomile.\",\n    \"New makeup trends focus on bold colors and innovative techniques\",\n    \"Bio-Hautpflege f\u00fcr empfindliche Haut mit Aloe Vera und Kamille\",\n    \"Neue Make-up-Trends setzen auf kr\u00e4ftige Farben und innovative Techniken\",\n    \"Cuidado de la piel org\u00e1nico para piel sensible con aloe vera y manzanilla\",\n    \"Las nuevas tendencias de maquillaje se centran en colores vivos y t\u00e9cnicas innovadoras\",\n    \"\u9488\u5bf9\u654f\u611f\u808c\u4e13\u95e8\u8bbe\u8ba1\u7684\u5929\u7136\u6709\u673a\u62a4\u80a4\u4ea7\u54c1\",\n    \"\u65b0\u7684\u5316\u5986\u8d8b\u52bf\u6ce8\u91cd\u9c9c\u8273\u7684\u989c\u8272\u548c\u521b\u65b0\u7684\u6280\u5de7\",\n    \"\u654f\u611f\u808c\u306e\u305f\u3081\u306b\u7279\u5225\u306b\u8a2d\u8a08\u3055\u308c\u305f\u5929\u7136\u6709\u6a5f\u30b9\u30ad\u30f3\u30b1\u30a2\u88fd\u54c1\",\n    \"\u65b0\u3057\u3044\u30e1\u30a4\u30af\u306e\u30c8\u30ec\u30f3\u30c9\u306f\u9bae\u3084\u304b\u306a\u8272\u3068\u9769\u65b0\u7684\u306a\u6280\u8853\u306b\u7126\u70b9\u3092\u5f53\u3066\u3066\u3044\u307e\u3059\"\n  ],\n  \"top_n\": 3\n}'\n```\n\n2. You can also use the `transformers` library to interact with the model programmatically.\n\nBefore you start, install the `transformers` and `einops` libraries:\n\n```bash\npip install transformers einops\n```\n\nAnd then:\n```python\nfrom transformers import AutoModelForSequenceClassification\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    'jinaai/jina-reranker-v2-base-multilingual',\n    torch_dtype=\"auto\",\n    trust_remote_code=True,\n)\n\nmodel.to('cuda') # or 'cpu' if no GPU is available\nmodel.eval()\n\n# Example query and documents\nquery = \"Organic skincare products for sensitive skin\"\ndocuments = [\n    \"Organic skincare for sensitive skin with aloe vera and chamomile.\",\n    \"New makeup trends focus on bold colors and innovative techniques\",\n    \"Bio-Hautpflege f\u00fcr empfindliche Haut mit Aloe Vera und Kamille\",\n    \"Neue Make-up-Trends setzen auf kr\u00e4ftige Farben und innovative Techniken\",\n    \"Cuidado de la piel org\u00e1nico para piel sensible con aloe vera y manzanilla\",\n    \"Las nuevas tendencias de maquillaje se centran en colores vivos y t\u00e9cnicas innovadoras\",\n    \"\u9488\u5bf9\u654f\u611f\u808c\u4e13\u95e8\u8bbe\u8ba1\u7684\u5929\u7136\u6709\u673a\u62a4\u80a4\u4ea7\u54c1\",\n    \"\u65b0\u7684\u5316\u5986\u8d8b\u52bf\u6ce8\u91cd\u9c9c\u8273\u7684\u989c\u8272\u548c\u521b\u65b0\u7684\u6280\u5de7\",\n    \"\u654f\u611f\u808c\u306e\u305f\u3081\u306b\u7279\u5225\u306b\u8a2d\u8a08\u3055\u308c\u305f\u5929\u7136\u6709\u6a5f\u30b9\u30ad\u30f3\u30b1\u30a2\u88fd\u54c1\",\n    \"\u65b0\u3057\u3044\u30e1\u30a4\u30af\u306e\u30c8\u30ec\u30f3\u30c9\u306f\u9bae\u3084\u304b\u306a\u8272\u3068\u9769\u65b0\u7684\u306a\u6280\u8853\u306b\u7126\u70b9\u3092\u5f53\u3066\u3066\u3044\u307e\u3059\",\n]\n\n# construct sentence pairs\nsentence_pairs = [[query, doc] for doc in documents]\n\nscores = model.compute_score(sentence_pairs, max_length=1024)\n```\n\nThe scores will be a list of floats, where each float represents the relevance score of the corresponding document to the query. Higher scores indicate higher relevance.\nFor instance the returning scores in this case will be:\n```bash\n[0.8311430811882019, 0.09401018172502518,\n 0.6334102749824524, 0.08269733935594559,\n 0.7620701193809509, 0.09947021305561066,\n 0.9263036847114563, 0.05834583938121796,\n 0.8418256044387817, 0.11124119907617569]\n```\n\nThe model gives high relevance scores to the documents that are most relevant to the query regardless of the language of the document.\n\nNote that by default, the `jina-reranker-v2-base-multilingual` model uses [flash attention](https://github.com/Dao-AILab/flash-attention), which requires certain types of GPU hardware to run.\nIf you encounter any issues, you can try call `AutoModelForSequenceClassification.from_pretrained()` with `use_flash_attn=False`.\nThis will use the standard attention mechanism instead of flash attention.\n\nIf you want to use flash attention for fast inference, you need to install the following packages:\n```bash\npip install ninja # required for flash attention\npip install flash-attn --no-build-isolation\n```\nEnjoy the 3x-6x speedup with flash attention! \u26a1\ufe0f\u26a1\ufe0f\u26a1\ufe0f\n\n\n3. You can also use the `transformers.js` library to run the model directly in JavaScript (in-browser, Node.js, Deno, etc.)!\n\nIf you haven't already, you can install the [Transformers.js](https://huggingface.co/docs/transformers.js) JavaScript library (v3) using:\n```bash\nnpm i xenova/transformers.js#v3\n```\n\nThen, you can use the following code to interact with the model:\n```js\nimport { AutoTokenizer, XLMRobertaModel } from '@xenova/transformers';\n\nconst model_id = 'jinaai/jina-reranker-v2-base-multilingual';\nconst model = await XLMRobertaModel.from_pretrained(model_id, { dtype: 'fp32' });\nconst tokenizer = await AutoTokenizer.from_pretrained(model_id);\n\n/**\n * Performs ranking with the CrossEncoder on the given query and documents. Returns a sorted list with the document indices and scores.\n * @param {string} query A single query\n * @param {string[]} documents A list of documents\n * @param {Object} options Options for ranking\n * @param {number} [options.top_k=undefined] Return the top-k documents. If undefined, all documents are returned.\n * @param {number} [options.return_documents=false] If true, also returns the documents. If false, only returns the indices and scores.\n */\nasync function rank(query, documents, {\n    top_k = undefined,\n    return_documents = false,\n} = {}) {\n    const inputs = tokenizer(\n        new Array(documents.length).fill(query),\n        { text_pair: documents, padding: true, truncation: true }\n    )\n    const { logits } = await model(inputs);\n    return logits.sigmoid().tolist()\n        .map(([score], i) => ({\n            corpus_id: i,\n            score,\n            ...(return_documents ? { text: documents[i] } : {})\n        })).sort((a, b) => b.score - a.score).slice(0, top_k);\n}\n\n// Example usage:\nconst query = \"Organic skincare products for sensitive skin\"\nconst documents = [\n    \"Organic skincare for sensitive skin with aloe vera and chamomile.\",\n    \"New makeup trends focus on bold colors and innovative techniques\",\n    \"Bio-Hautpflege f\u00fcr empfindliche Haut mit Aloe Vera und Kamille\",\n    \"Neue Make-up-Trends setzen auf kr\u00e4ftige Farben und innovative Techniken\",\n    \"Cuidado de la piel org\u00e1nico para piel sensible con aloe vera y manzanilla\",\n    \"Las nuevas tendencias de maquillaje se centran en colores vivos y t\u00e9cnicas innovadoras\",\n    \"\u9488\u5bf9\u654f\u611f\u808c\u4e13\u95e8\u8bbe\u8ba1\u7684\u5929\u7136\u6709\u673a\u62a4\u80a4\u4ea7\u54c1\",\n    \"\u65b0\u7684\u5316\u5986\u8d8b\u52bf\u6ce8\u91cd\u9c9c\u8273\u7684\u989c\u8272\u548c\u521b\u65b0\u7684\u6280\u5de7\",\n    \"\u654f\u611f\u808c\u306e\u305f\u3081\u306b\u7279\u5225\u306b\u8a2d\u8a08\u3055\u308c\u305f\u5929\u7136\u6709\u6a5f\u30b9\u30ad\u30f3\u30b1\u30a2\u88fd\u54c1\",\n    \"\u65b0\u3057\u3044\u30e1\u30a4\u30af\u306e\u30c8\u30ec\u30f3\u30c9\u306f\u9bae\u3084\u304b\u306a\u8272\u3068\u9769\u65b0\u7684\u306a\u6280\u8853\u306b\u7126\u70b9\u3092\u5f53\u3066\u3066\u3044\u307e\u3059\",\n]\n\nconst results = await rank(query, documents, { return_documents: true, top_k: 3 });\nconsole.log(results);\n```\n\n\nThat's it! You can now use the `jina-reranker-v2-base-multilingual` model in your projects.\n\n\nIn addition to the `compute_score()` function, the `jina-reranker-v2-base-multilingual` model also provides a `model.rerank()` function that can be used to rerank documents based on a query. You can use it as follows:\n\n```python\nresult = model.rerank(\n    query,\n    documents,\n    max_query_length=512,\n    max_length=1024,\n    top_n=3\n)\n```\n\nInside the `result` object, you will find the reranked documents along with their scores. You can use this information to further process the documents as needed.\n\nThe `rerank()` function will automatically chunk the input documents into smaller pieces if they exceed the model's maximum input length. This allows you to rerank long documents without running into memory issues.\nSpecifically, the `rerank()` function will split the documents into chunks of size `max_length` and rerank each chunk separately. The scores from all the chunks are then combined to produce the final reranking results. You can control the query length and document length in each chunk by setting the `max_query_length` and `max_length` parameters. The `rerank()` function also supports the `overlap` parameter (default is `80`) which determines how much overlap there is between adjacent chunks. This can be useful when reranking long documents to ensure that the model has enough context to make accurate predictions.\n\n3. Alternatively, `jina-reranker-v2-base-multilingual` has been integrated with `CrossEncoder` from the `sentence-transformers` library.\n\nBefore you start, install the `sentence-transformers` libraries:\n\n```bash\npip install sentence-transformers\n```\n\nThe [`CrossEncoder`](https://sbert.net/docs/package_reference/cross_encoder/cross_encoder.html) class supports a [`predict`](https://sbert.net/docs/package_reference/cross_encoder/cross_encoder.html#sentence_transformers.cross_encoder.CrossEncoder.predict) method to get query-document relevance scores, and a [`rank`](https://sbert.net/docs/package_reference/cross_encoder/cross_encoder.html#sentence_transformers.cross_encoder.CrossEncoder.rank) method to rank all documents given your query.\n\n```python\nfrom sentence_transformers import CrossEncoder\n\nmodel = CrossEncoder(\n    \"jinaai/jina-reranker-v2-base-multilingual\",\n    automodel_args={\"torch_dtype\": \"auto\"},\n    trust_remote_code=True,\n)\n\n# Example query and documents\nquery = \"Organic skincare products for sensitive skin\"\ndocuments = [\n    \"Organic skincare for sensitive skin with aloe vera and chamomile.\",\n    \"New makeup trends focus on bold colors and innovative techniques\",\n    \"Bio-Hautpflege f\u00fcr empfindliche Haut mit Aloe Vera und Kamille\",\n    \"Neue Make-up-Trends setzen auf kr\u00e4ftige Farben und innovative Techniken\",\n    \"Cuidado de la piel org\u00e1nico para piel sensible con aloe vera y manzanilla\",\n    \"Las nuevas tendencias de maquillaje se centran en colores vivos y t\u00e9cnicas innovadoras\",\n    \"\u9488\u5bf9\u654f\u611f\u808c\u4e13\u95e8\u8bbe\u8ba1\u7684\u5929\u7136\u6709\u673a\u62a4\u80a4\u4ea7\u54c1\",\n    \"\u65b0\u7684\u5316\u5986\u8d8b\u52bf\u6ce8\u91cd\u9c9c\u8273\u7684\u989c\u8272\u548c\u521b\u65b0\u7684\u6280\u5de7\",\n    \"\u654f\u611f\u808c\u306e\u305f\u3081\u306b\u7279\u5225\u306b\u8a2d\u8a08\u3055\u308c\u305f\u5929\u7136\u6709\u6a5f\u30b9\u30ad\u30f3\u30b1\u30a2\u88fd\u54c1\",\n    \"\u65b0\u3057\u3044\u30e1\u30a4\u30af\u306e\u30c8\u30ec\u30f3\u30c9\u306f\u9bae\u3084\u304b\u306a\u8272\u3068\u9769\u65b0\u7684\u306a\u6280\u8853\u306b\u7126\u70b9\u3092\u5f53\u3066\u3066\u3044\u307e\u3059\",\n]\n\n# construct sentence pairs\nsentence_pairs = [[query, doc] for doc in documents]\n\nscores = model.predict(sentence_pairs, convert_to_tensor=True).tolist()\n\"\"\"\n[0.828125, 0.0927734375, 0.6328125, 0.08251953125, 0.76171875, 0.099609375, 0.92578125, 0.058349609375, 0.84375, 0.111328125]\n\"\"\"\n\nrankings = model.rank(query, documents, return_documents=True, convert_to_tensor=True)\nprint(f\"Query: {query}\")\nfor ranking in rankings:\n    print(f\"ID: {ranking['corpus_id']}, Score: {ranking['score']:.4f}, Text: {ranking['text']}\")\n\"\"\"\nQuery: Organic skincare products for sensitive skin\nID: 6, Score: 0.9258, Text: \u9488\u5bf9\u654f\u611f\u808c\u4e13\u95e8\u8bbe\u8ba1\u7684\u5929\u7136\u6709\u673a\u62a4\u80a4\u4ea7\u54c1\nID: 8, Score: 0.8438, Text: \u654f\u611f\u808c\u306e\u305f\u3081\u306b\u7279\u5225\u306b\u8a2d\u8a08\u3055\u308c\u305f\u5929\u7136\u6709\u6a5f\u30b9\u30ad\u30f3\u30b1\u30a2\u88fd\u54c1\nID: 0, Score: 0.8281, Text: Organic skincare for sensitive skin with aloe vera and chamomile.\nID: 4, Score: 0.7617, Text: Cuidado de la piel org\u00e1nico para piel sensible con aloe vera y manzanilla\nID: 2, Score: 0.6328, Text: Bio-Hautpflege f\u00fcr empfindliche Haut mit Aloe Vera und Kamille\nID: 9, Score: 0.1113, Text: \u65b0\u3057\u3044\u30e1\u30a4\u30af\u306e\u30c8\u30ec\u30f3\u30c9\u306f\u9bae\u3084\u304b\u306a\u8272\u3068\u9769\u65b0\u7684\u306a\u6280\u8853\u306b\u7126\u70b9\u3092\u5f53\u3066\u3066\u3044\u307e\u3059\nID: 5, Score: 0.0996, Text: Las nuevas tendencias de maquillaje se centran en colores vivos y t\u00e9cnicas innovadoras\nID: 1, Score: 0.0928, Text: New makeup trends focus on bold colors and innovative techniques\nID: 3, Score: 0.0825, Text: Neue Make-up-Trends setzen auf kr\u00e4ftige Farben und innovative Techniken\nID: 7, Score: 0.0583, Text: \u65b0\u7684\u5316\u5986\u8d8b\u52bf\u6ce8\u91cd\u9c9c\u8273\u7684\u989c\u8272\u548c\u521b\u65b0\u7684\u6280\u5de7\n\"\"\"\n```\n\n# Evaluation\n\nWe evaluated Jina Reranker v2 on multiple benchmarks to ensure top-tier performance and search relevance.\n\n|           Model Name            |   Model Size | MKQA(nDCG@10, 26 langs) \t| BEIR(nDCG@10, 17 datasets) \t| MLDR(recall@10, 13 langs) | CodeSearchNet (MRR@10, 3 tasks) \t| AirBench (nDCG@10, zh/en) \t| ToolBench (recall@3, 3 tasks) \t| TableSearch (recall@3) \t|\n| :-----------------------------: | :----------: | ------------------------- | ---------------------------- | --------------------------- | --------------------------------- | --------------------------- | ------------------------------- | ------------------------ |\n| jina-reranker-v2-multilingual \t|    278M    \t|          54.83          \t|            53.17           \t|           68.95           \t|              71.36              \t|           61.33           \t|             77.75             \t|          93.31         \t|\n|       bge-reranker-v2-m3      \t|    568M    \t|          54.17          \t|            53.65           \t|           59.73           \t|              62.86              \t|           61.28           \t|             78.46             \t|          74.86         \t|\n|  mmarco-mMiniLMv2-L12-H384-v1 \t|    118M    \t|          53.37          \t|            45.40           \t|           28.91           \t|              51.78              \t|           56.46           \t|             58.39             \t|          53.60         \t|\n|    jina-reranker-v1-base-en   \t|    137M    \t|            -            \t|            52.45           \t|             -             \t|                -                \t|             -             \t|             74.13             \t|          72.89         \t|\n\nNote:\n- NDCG@10 and MRR@10 measure ranking quality, with higher scores indicating better search results\n- recall@3 measures the proportion of relevant documents retrieved, with higher scores indicating better search results",
    "card_content": "---\nlanguage:\n- multilingual\nlicense: cc-by-nc-4.0\nlibrary_name: transformers\ntags:\n- transformers\n- reranker\n- cross-encoder\n- transformers.js\npipeline_tag: text-classification\ninference: false\n---\n\n<br><br>\n\n<p align=\"center\">\n<img src=\"https://huggingface.co/datasets/jinaai/documentation-images/resolve/main/logo.webp\" alt=\"Jina AI: Your Search Foundation, Supercharged!\" width=\"150px\">\n</p>\n\n<p align=\"center\">\n<b>Trained by <a href=\"https://jina.ai/\"><b>Jina AI</b></a>.</b>\n</p>\n\n# jina-reranker-v2-base-multilingual\n\n## Intended Usage & Model Info\n\nThe **Jina Reranker v2** (`jina-reranker-v2-base-multilingual`) is a transformer-based model that has been fine-tuned for text reranking task, which is a crucial component in many information retrieval systems. It is a cross-encoder model that takes a query and a document pair as input and outputs a score indicating the relevance of the document to the query. The model is trained on a large dataset of query-document pairs and is capable of reranking documents in multiple languages with high accuracy.\n\nCompared with the state-of-the-art reranker models, including the previous released `jina-reranker-v1-base-en`, the **Jina Reranker v2** model has demonstrated competitiveness across a series of benchmarks targeting for text retrieval, multilingual capability, function-calling-aware and text-to-SQL-aware reranking, and code retrieval tasks.\n\nThe `jina-reranker-v2-base-multilingual` model is capable of handling long texts with a context length of up to `1024` tokens, enabling the processing of extensive inputs. To enable the model to handle long texts that exceed 1024 tokens, the model uses a sliding window approach to chunk the input text into smaller pieces and rerank each chunk separately.\n\nThe model is also equipped with a flash attention mechanism, which significantly improves the model's performance.\n\n\n# Usage\n\n_This model repository is licenced for research and evaluation purposes under CC-BY-NC-4.0. For commercial usage, please refer to Jina AI's APIs, AWS Sagemaker or Azure Marketplace offerings. Please [contact us](https://jina.ai/contact-sales) for any further clarifications._\n1. The easiest way to use `jina-reranker-v2-base-multilingual` is to call Jina AI's [Reranker API](https://jina.ai/reranker/).\n\n```bash\ncurl https://api.jina.ai/v1/rerank \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer YOUR_API_KEY\" \\\n  -d '{\n  \"model\": \"jina-reranker-v2-base-multilingual\",\n  \"query\": \"Organic skincare products for sensitive skin\",\n  \"documents\": [\n    \"Organic skincare for sensitive skin with aloe vera and chamomile.\",\n    \"New makeup trends focus on bold colors and innovative techniques\",\n    \"Bio-Hautpflege f\u00fcr empfindliche Haut mit Aloe Vera und Kamille\",\n    \"Neue Make-up-Trends setzen auf kr\u00e4ftige Farben und innovative Techniken\",\n    \"Cuidado de la piel org\u00e1nico para piel sensible con aloe vera y manzanilla\",\n    \"Las nuevas tendencias de maquillaje se centran en colores vivos y t\u00e9cnicas innovadoras\",\n    \"\u9488\u5bf9\u654f\u611f\u808c\u4e13\u95e8\u8bbe\u8ba1\u7684\u5929\u7136\u6709\u673a\u62a4\u80a4\u4ea7\u54c1\",\n    \"\u65b0\u7684\u5316\u5986\u8d8b\u52bf\u6ce8\u91cd\u9c9c\u8273\u7684\u989c\u8272\u548c\u521b\u65b0\u7684\u6280\u5de7\",\n    \"\u654f\u611f\u808c\u306e\u305f\u3081\u306b\u7279\u5225\u306b\u8a2d\u8a08\u3055\u308c\u305f\u5929\u7136\u6709\u6a5f\u30b9\u30ad\u30f3\u30b1\u30a2\u88fd\u54c1\",\n    \"\u65b0\u3057\u3044\u30e1\u30a4\u30af\u306e\u30c8\u30ec\u30f3\u30c9\u306f\u9bae\u3084\u304b\u306a\u8272\u3068\u9769\u65b0\u7684\u306a\u6280\u8853\u306b\u7126\u70b9\u3092\u5f53\u3066\u3066\u3044\u307e\u3059\"\n  ],\n  \"top_n\": 3\n}'\n```\n\n2. You can also use the `transformers` library to interact with the model programmatically.\n\nBefore you start, install the `transformers` and `einops` libraries:\n\n```bash\npip install transformers einops\n```\n\nAnd then:\n```python\nfrom transformers import AutoModelForSequenceClassification\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    'jinaai/jina-reranker-v2-base-multilingual',\n    torch_dtype=\"auto\",\n    trust_remote_code=True,\n)\n\nmodel.to('cuda') # or 'cpu' if no GPU is available\nmodel.eval()\n\n# Example query and documents\nquery = \"Organic skincare products for sensitive skin\"\ndocuments = [\n    \"Organic skincare for sensitive skin with aloe vera and chamomile.\",\n    \"New makeup trends focus on bold colors and innovative techniques\",\n    \"Bio-Hautpflege f\u00fcr empfindliche Haut mit Aloe Vera und Kamille\",\n    \"Neue Make-up-Trends setzen auf kr\u00e4ftige Farben und innovative Techniken\",\n    \"Cuidado de la piel org\u00e1nico para piel sensible con aloe vera y manzanilla\",\n    \"Las nuevas tendencias de maquillaje se centran en colores vivos y t\u00e9cnicas innovadoras\",\n    \"\u9488\u5bf9\u654f\u611f\u808c\u4e13\u95e8\u8bbe\u8ba1\u7684\u5929\u7136\u6709\u673a\u62a4\u80a4\u4ea7\u54c1\",\n    \"\u65b0\u7684\u5316\u5986\u8d8b\u52bf\u6ce8\u91cd\u9c9c\u8273\u7684\u989c\u8272\u548c\u521b\u65b0\u7684\u6280\u5de7\",\n    \"\u654f\u611f\u808c\u306e\u305f\u3081\u306b\u7279\u5225\u306b\u8a2d\u8a08\u3055\u308c\u305f\u5929\u7136\u6709\u6a5f\u30b9\u30ad\u30f3\u30b1\u30a2\u88fd\u54c1\",\n    \"\u65b0\u3057\u3044\u30e1\u30a4\u30af\u306e\u30c8\u30ec\u30f3\u30c9\u306f\u9bae\u3084\u304b\u306a\u8272\u3068\u9769\u65b0\u7684\u306a\u6280\u8853\u306b\u7126\u70b9\u3092\u5f53\u3066\u3066\u3044\u307e\u3059\",\n]\n\n# construct sentence pairs\nsentence_pairs = [[query, doc] for doc in documents]\n\nscores = model.compute_score(sentence_pairs, max_length=1024)\n```\n\nThe scores will be a list of floats, where each float represents the relevance score of the corresponding document to the query. Higher scores indicate higher relevance.\nFor instance the returning scores in this case will be:\n```bash\n[0.8311430811882019, 0.09401018172502518,\n 0.6334102749824524, 0.08269733935594559,\n 0.7620701193809509, 0.09947021305561066,\n 0.9263036847114563, 0.05834583938121796,\n 0.8418256044387817, 0.11124119907617569]\n```\n\nThe model gives high relevance scores to the documents that are most relevant to the query regardless of the language of the document.\n\nNote that by default, the `jina-reranker-v2-base-multilingual` model uses [flash attention](https://github.com/Dao-AILab/flash-attention), which requires certain types of GPU hardware to run.\nIf you encounter any issues, you can try call `AutoModelForSequenceClassification.from_pretrained()` with `use_flash_attn=False`.\nThis will use the standard attention mechanism instead of flash attention.\n\nIf you want to use flash attention for fast inference, you need to install the following packages:\n```bash\npip install ninja # required for flash attention\npip install flash-attn --no-build-isolation\n```\nEnjoy the 3x-6x speedup with flash attention! \u26a1\ufe0f\u26a1\ufe0f\u26a1\ufe0f\n\n\n3. You can also use the `transformers.js` library to run the model directly in JavaScript (in-browser, Node.js, Deno, etc.)!\n\nIf you haven't already, you can install the [Transformers.js](https://huggingface.co/docs/transformers.js) JavaScript library (v3) using:\n```bash\nnpm i xenova/transformers.js#v3\n```\n\nThen, you can use the following code to interact with the model:\n```js\nimport { AutoTokenizer, XLMRobertaModel } from '@xenova/transformers';\n\nconst model_id = 'jinaai/jina-reranker-v2-base-multilingual';\nconst model = await XLMRobertaModel.from_pretrained(model_id, { dtype: 'fp32' });\nconst tokenizer = await AutoTokenizer.from_pretrained(model_id);\n\n/**\n * Performs ranking with the CrossEncoder on the given query and documents. Returns a sorted list with the document indices and scores.\n * @param {string} query A single query\n * @param {string[]} documents A list of documents\n * @param {Object} options Options for ranking\n * @param {number} [options.top_k=undefined] Return the top-k documents. If undefined, all documents are returned.\n * @param {number} [options.return_documents=false] If true, also returns the documents. If false, only returns the indices and scores.\n */\nasync function rank(query, documents, {\n    top_k = undefined,\n    return_documents = false,\n} = {}) {\n    const inputs = tokenizer(\n        new Array(documents.length).fill(query),\n        { text_pair: documents, padding: true, truncation: true }\n    )\n    const { logits } = await model(inputs);\n    return logits.sigmoid().tolist()\n        .map(([score], i) => ({\n            corpus_id: i,\n            score,\n            ...(return_documents ? { text: documents[i] } : {})\n        })).sort((a, b) => b.score - a.score).slice(0, top_k);\n}\n\n// Example usage:\nconst query = \"Organic skincare products for sensitive skin\"\nconst documents = [\n    \"Organic skincare for sensitive skin with aloe vera and chamomile.\",\n    \"New makeup trends focus on bold colors and innovative techniques\",\n    \"Bio-Hautpflege f\u00fcr empfindliche Haut mit Aloe Vera und Kamille\",\n    \"Neue Make-up-Trends setzen auf kr\u00e4ftige Farben und innovative Techniken\",\n    \"Cuidado de la piel org\u00e1nico para piel sensible con aloe vera y manzanilla\",\n    \"Las nuevas tendencias de maquillaje se centran en colores vivos y t\u00e9cnicas innovadoras\",\n    \"\u9488\u5bf9\u654f\u611f\u808c\u4e13\u95e8\u8bbe\u8ba1\u7684\u5929\u7136\u6709\u673a\u62a4\u80a4\u4ea7\u54c1\",\n    \"\u65b0\u7684\u5316\u5986\u8d8b\u52bf\u6ce8\u91cd\u9c9c\u8273\u7684\u989c\u8272\u548c\u521b\u65b0\u7684\u6280\u5de7\",\n    \"\u654f\u611f\u808c\u306e\u305f\u3081\u306b\u7279\u5225\u306b\u8a2d\u8a08\u3055\u308c\u305f\u5929\u7136\u6709\u6a5f\u30b9\u30ad\u30f3\u30b1\u30a2\u88fd\u54c1\",\n    \"\u65b0\u3057\u3044\u30e1\u30a4\u30af\u306e\u30c8\u30ec\u30f3\u30c9\u306f\u9bae\u3084\u304b\u306a\u8272\u3068\u9769\u65b0\u7684\u306a\u6280\u8853\u306b\u7126\u70b9\u3092\u5f53\u3066\u3066\u3044\u307e\u3059\",\n]\n\nconst results = await rank(query, documents, { return_documents: true, top_k: 3 });\nconsole.log(results);\n```\n\n\nThat's it! You can now use the `jina-reranker-v2-base-multilingual` model in your projects.\n\n\nIn addition to the `compute_score()` function, the `jina-reranker-v2-base-multilingual` model also provides a `model.rerank()` function that can be used to rerank documents based on a query. You can use it as follows:\n\n```python\nresult = model.rerank(\n    query,\n    documents,\n    max_query_length=512,\n    max_length=1024,\n    top_n=3\n)\n```\n\nInside the `result` object, you will find the reranked documents along with their scores. You can use this information to further process the documents as needed.\n\nThe `rerank()` function will automatically chunk the input documents into smaller pieces if they exceed the model's maximum input length. This allows you to rerank long documents without running into memory issues.\nSpecifically, the `rerank()` function will split the documents into chunks of size `max_length` and rerank each chunk separately. The scores from all the chunks are then combined to produce the final reranking results. You can control the query length and document length in each chunk by setting the `max_query_length` and `max_length` parameters. The `rerank()` function also supports the `overlap` parameter (default is `80`) which determines how much overlap there is between adjacent chunks. This can be useful when reranking long documents to ensure that the model has enough context to make accurate predictions.\n\n3. Alternatively, `jina-reranker-v2-base-multilingual` has been integrated with `CrossEncoder` from the `sentence-transformers` library.\n\nBefore you start, install the `sentence-transformers` libraries:\n\n```bash\npip install sentence-transformers\n```\n\nThe [`CrossEncoder`](https://sbert.net/docs/package_reference/cross_encoder/cross_encoder.html) class supports a [`predict`](https://sbert.net/docs/package_reference/cross_encoder/cross_encoder.html#sentence_transformers.cross_encoder.CrossEncoder.predict) method to get query-document relevance scores, and a [`rank`](https://sbert.net/docs/package_reference/cross_encoder/cross_encoder.html#sentence_transformers.cross_encoder.CrossEncoder.rank) method to rank all documents given your query.\n\n```python\nfrom sentence_transformers import CrossEncoder\n\nmodel = CrossEncoder(\n    \"jinaai/jina-reranker-v2-base-multilingual\",\n    automodel_args={\"torch_dtype\": \"auto\"},\n    trust_remote_code=True,\n)\n\n# Example query and documents\nquery = \"Organic skincare products for sensitive skin\"\ndocuments = [\n    \"Organic skincare for sensitive skin with aloe vera and chamomile.\",\n    \"New makeup trends focus on bold colors and innovative techniques\",\n    \"Bio-Hautpflege f\u00fcr empfindliche Haut mit Aloe Vera und Kamille\",\n    \"Neue Make-up-Trends setzen auf kr\u00e4ftige Farben und innovative Techniken\",\n    \"Cuidado de la piel org\u00e1nico para piel sensible con aloe vera y manzanilla\",\n    \"Las nuevas tendencias de maquillaje se centran en colores vivos y t\u00e9cnicas innovadoras\",\n    \"\u9488\u5bf9\u654f\u611f\u808c\u4e13\u95e8\u8bbe\u8ba1\u7684\u5929\u7136\u6709\u673a\u62a4\u80a4\u4ea7\u54c1\",\n    \"\u65b0\u7684\u5316\u5986\u8d8b\u52bf\u6ce8\u91cd\u9c9c\u8273\u7684\u989c\u8272\u548c\u521b\u65b0\u7684\u6280\u5de7\",\n    \"\u654f\u611f\u808c\u306e\u305f\u3081\u306b\u7279\u5225\u306b\u8a2d\u8a08\u3055\u308c\u305f\u5929\u7136\u6709\u6a5f\u30b9\u30ad\u30f3\u30b1\u30a2\u88fd\u54c1\",\n    \"\u65b0\u3057\u3044\u30e1\u30a4\u30af\u306e\u30c8\u30ec\u30f3\u30c9\u306f\u9bae\u3084\u304b\u306a\u8272\u3068\u9769\u65b0\u7684\u306a\u6280\u8853\u306b\u7126\u70b9\u3092\u5f53\u3066\u3066\u3044\u307e\u3059\",\n]\n\n# construct sentence pairs\nsentence_pairs = [[query, doc] for doc in documents]\n\nscores = model.predict(sentence_pairs, convert_to_tensor=True).tolist()\n\"\"\"\n[0.828125, 0.0927734375, 0.6328125, 0.08251953125, 0.76171875, 0.099609375, 0.92578125, 0.058349609375, 0.84375, 0.111328125]\n\"\"\"\n\nrankings = model.rank(query, documents, return_documents=True, convert_to_tensor=True)\nprint(f\"Query: {query}\")\nfor ranking in rankings:\n    print(f\"ID: {ranking['corpus_id']}, Score: {ranking['score']:.4f}, Text: {ranking['text']}\")\n\"\"\"\nQuery: Organic skincare products for sensitive skin\nID: 6, Score: 0.9258, Text: \u9488\u5bf9\u654f\u611f\u808c\u4e13\u95e8\u8bbe\u8ba1\u7684\u5929\u7136\u6709\u673a\u62a4\u80a4\u4ea7\u54c1\nID: 8, Score: 0.8438, Text: \u654f\u611f\u808c\u306e\u305f\u3081\u306b\u7279\u5225\u306b\u8a2d\u8a08\u3055\u308c\u305f\u5929\u7136\u6709\u6a5f\u30b9\u30ad\u30f3\u30b1\u30a2\u88fd\u54c1\nID: 0, Score: 0.8281, Text: Organic skincare for sensitive skin with aloe vera and chamomile.\nID: 4, Score: 0.7617, Text: Cuidado de la piel org\u00e1nico para piel sensible con aloe vera y manzanilla\nID: 2, Score: 0.6328, Text: Bio-Hautpflege f\u00fcr empfindliche Haut mit Aloe Vera und Kamille\nID: 9, Score: 0.1113, Text: \u65b0\u3057\u3044\u30e1\u30a4\u30af\u306e\u30c8\u30ec\u30f3\u30c9\u306f\u9bae\u3084\u304b\u306a\u8272\u3068\u9769\u65b0\u7684\u306a\u6280\u8853\u306b\u7126\u70b9\u3092\u5f53\u3066\u3066\u3044\u307e\u3059\nID: 5, Score: 0.0996, Text: Las nuevas tendencias de maquillaje se centran en colores vivos y t\u00e9cnicas innovadoras\nID: 1, Score: 0.0928, Text: New makeup trends focus on bold colors and innovative techniques\nID: 3, Score: 0.0825, Text: Neue Make-up-Trends setzen auf kr\u00e4ftige Farben und innovative Techniken\nID: 7, Score: 0.0583, Text: \u65b0\u7684\u5316\u5986\u8d8b\u52bf\u6ce8\u91cd\u9c9c\u8273\u7684\u989c\u8272\u548c\u521b\u65b0\u7684\u6280\u5de7\n\"\"\"\n```\n\n# Evaluation\n\nWe evaluated Jina Reranker v2 on multiple benchmarks to ensure top-tier performance and search relevance.\n\n|           Model Name            |   Model Size | MKQA(nDCG@10, 26 langs) \t| BEIR(nDCG@10, 17 datasets) \t| MLDR(recall@10, 13 langs) | CodeSearchNet (MRR@10, 3 tasks) \t| AirBench (nDCG@10, zh/en) \t| ToolBench (recall@3, 3 tasks) \t| TableSearch (recall@3) \t|\n| :-----------------------------: | :----------: | ------------------------- | ---------------------------- | --------------------------- | --------------------------------- | --------------------------- | ------------------------------- | ------------------------ |\n| jina-reranker-v2-multilingual \t|    278M    \t|          54.83          \t|            53.17           \t|           68.95           \t|              71.36              \t|           61.33           \t|             77.75             \t|          93.31         \t|\n|       bge-reranker-v2-m3      \t|    568M    \t|          54.17          \t|            53.65           \t|           59.73           \t|              62.86              \t|           61.28           \t|             78.46             \t|          74.86         \t|\n|  mmarco-mMiniLMv2-L12-H384-v1 \t|    118M    \t|          53.37          \t|            45.40           \t|           28.91           \t|              51.78              \t|           56.46           \t|             58.39             \t|          53.60         \t|\n|    jina-reranker-v1-base-en   \t|    137M    \t|            -            \t|            52.45           \t|             -             \t|                -                \t|             -             \t|             74.13             \t|          72.89         \t|\n\nNote:\n- NDCG@10 and MRR@10 measure ranking quality, with higher scores indicating better search results\n- recall@3 measures the proportion of relevant documents retrieved, with higher scores indicating better search results",
    "library_name": "transformers"
  },
  {
    "model_id": "MoritzLaurer/mDeBERTa-v3-base-mnli-xnli",
    "model_name": "MoritzLaurer/mDeBERTa-v3-base-mnli-xnli",
    "author": "MoritzLaurer",
    "downloads": 398891,
    "likes": 248,
    "tags": [
      "transformers",
      "pytorch",
      "onnx",
      "safetensors",
      "deberta-v2",
      "text-classification",
      "zero-shot-classification",
      "nli",
      "multilingual",
      "en",
      "ar",
      "bg",
      "de",
      "el",
      "es",
      "fr",
      "hi",
      "ru",
      "sw",
      "th",
      "tr",
      "ur",
      "vi",
      "zh",
      "dataset:multi_nli",
      "dataset:xnli",
      "arxiv:2111.09543",
      "arxiv:1809.05053",
      "arxiv:1911.02116",
      "license:mit",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/MoritzLaurer/mDeBERTa-v3-base-mnli-xnli",
    "dependencies": [
      [
        "transformers",
        "4.30.2"
      ],
      [
        "torch",
        "2.0.1"
      ],
      [
        "numpy",
        "1.24.3"
      ]
    ],
    "analysis_date": "2025-03-26T00:48:14.248483",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "deberta-v2",
    "pipeline_tag": "zero-shot-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": [
        "multilingual",
        "en",
        "ar",
        "bg",
        "de",
        "el",
        "es",
        "fr",
        "hi",
        "ru",
        "sw",
        "th",
        "tr",
        "ur",
        "vi",
        "zh"
      ],
      "license": "mit",
      "tags": [
        "zero-shot-classification",
        "text-classification",
        "nli",
        "pytorch"
      ],
      "datasets": [
        "multi_nli",
        "xnli"
      ],
      "metrics": [
        "accuracy"
      ],
      "pipeline_tag": "zero-shot-classification",
      "widget": [
        {
          "text": "Angela Merkel ist eine Politikerin in Deutschland und Vorsitzende der CDU",
          "candidate_labels": "politics, economy, entertainment, environment"
        }
      ]
    },
    "card_text": "# Multilingual mDeBERTa-v3-base-mnli-xnli\n## Model description\nThis multilingual model can perform natural language inference (NLI) on 100 languages and is therefore also suitable for multilingual \nzero-shot classification. The underlying model was pre-trained by Microsoft on the \n[CC100 multilingual dataset](https://huggingface.co/datasets/cc100). It was then fine-tuned on the [XNLI dataset](https://huggingface.co/datasets/xnli), which contains hypothesis-premise pairs from 15 languages, as well as the English [MNLI dataset](https://huggingface.co/datasets/multi_nli).\nAs of December 2021, mDeBERTa-base is the best performing multilingual base-sized transformer model, \nintroduced by Microsoft in [this paper](https://arxiv.org/pdf/2111.09543.pdf). \n\nIf you are looking for a smaller, faster (but less performant) model, you can \ntry [multilingual-MiniLMv2-L6-mnli-xnli](https://huggingface.co/MoritzLaurer/multilingual-MiniLMv2-L6-mnli-xnli).\n\n### How to use the model\n#### Simple zero-shot classification pipeline\n```python\nfrom transformers import pipeline\nclassifier = pipeline(\"zero-shot-classification\", model=\"MoritzLaurer/mDeBERTa-v3-base-mnli-xnli\")\n\nsequence_to_classify = \"Angela Merkel ist eine Politikerin in Deutschland und Vorsitzende der CDU\"\ncandidate_labels = [\"politics\", \"economy\", \"entertainment\", \"environment\"]\noutput = classifier(sequence_to_classify, candidate_labels, multi_label=False)\nprint(output)\n```\n#### NLI use-case\n```python\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n\nmodel_name = \"MoritzLaurer/mDeBERTa-v3-base-mnli-xnli\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name)\n\npremise = \"Angela Merkel ist eine Politikerin in Deutschland und Vorsitzende der CDU\"\nhypothesis = \"Emmanuel Macron is the President of France\"\n\ninput = tokenizer(premise, hypothesis, truncation=True, return_tensors=\"pt\")\noutput = model(input[\"input_ids\"].to(device))  # device = \"cuda:0\" or \"cpu\"\nprediction = torch.softmax(output[\"logits\"][0], -1).tolist()\nlabel_names = [\"entailment\", \"neutral\", \"contradiction\"]\nprediction = {name: round(float(pred) * 100, 1) for pred, name in zip(prediction, label_names)}\nprint(prediction)\n```\n\n### Training data\nThis model was trained on the XNLI development dataset and the MNLI train dataset. The XNLI development set consists of 2490 professionally translated texts from English to 14 other languages (37350 texts in total) (see [this paper](https://arxiv.org/pdf/1809.05053.pdf)). Note that the XNLI contains a training set of 15 machine translated versions of the MNLI dataset for 15 languages, but due to quality issues with these machine translations, this model was only trained on the professional translations from the XNLI development set and the original English MNLI training set (392 702 texts). Not using machine translated texts can avoid overfitting the model to the 15 languages; avoids catastrophic forgetting of the other 85 languages mDeBERTa was pre-trained on; and significantly reduces training costs. \n\n### Training procedure\nmDeBERTa-v3-base-mnli-xnli was trained using the Hugging Face trainer with the following hyperparameters.\n```\ntraining_args = TrainingArguments(\n    num_train_epochs=2,              # total number of training epochs\n    learning_rate=2e-05,\n    per_device_train_batch_size=16,   # batch size per device during training\n    per_device_eval_batch_size=16,    # batch size for evaluation\n    warmup_ratio=0.1,                # number of warmup steps for learning rate scheduler\n    weight_decay=0.06,               # strength of weight decay\n)\n```\n### Eval results\nThe model was evaluated on the XNLI test set on 15 languages (5010 texts per language, 75150 in total). Note that multilingual NLI models are capable of classifying NLI texts without receiving NLI training data in the specific language (cross-lingual transfer). This means that the model is also able of doing NLI on the other 85 languages mDeBERTa was training on, but performance is most likely lower than for those languages available in XNLI.\n\nAlso note that if other multilingual models on the model hub claim performance of around 90% on languages other than English, the authors have most likely made a mistake during testing since non of the latest papers shows a multilingual average performance of more than a few points above 80% on XNLI (see [here](https://arxiv.org/pdf/2111.09543.pdf) or [here](https://arxiv.org/pdf/1911.02116.pdf)). \n\naverage | ar | bg | de | el | en | es | fr | hi | ru | sw | th | tr | ur | vi | zh \n---------|----------|---------|----------|----------|----------|----------|----------|----------|----------|----------|----------|----------|----------|----------|----------\n0.808 | 0.802 | 0.829 | 0.825 | 0.826 |\u00a00.883 | 0.845 |\u00a00.834 |\u00a00.771 |\u00a00.813 |\u00a00.748 |\u00a00.793 |\u00a00.807 |\u00a00.740 |\u00a00.795 |\u00a00.8116\n\n## Limitations and bias\nPlease consult the original DeBERTa-V3 paper and literature on different NLI datasets for potential biases. \n\n## Citation\nIf you use this model, please cite: Laurer, Moritz, Wouter van Atteveldt, Andreu Salleras Casas, and Kasper Welbers. 2022. \u2018Less Annotating, More Classifying \u2013 Addressing the Data Scarcity Issue of Supervised Machine Learning with Deep Transfer Learning and BERT - NLI\u2019. Preprint, June. Open Science Framework. https://osf.io/74b8k.\n\n## Ideas for cooperation or questions?\nIf you have questions or ideas for cooperation, contact me at m{dot}laurer{at}vu{dot}nl or [LinkedIn](https://www.linkedin.com/in/moritz-laurer/)\n\n## Debugging and issues\nNote that DeBERTa-v3 was released in late 2021 and older versions of HF Transformers seem to have issues running the model (e.g. resulting in an issue with the tokenizer). Using Transformers>=4.13 or higher might solve some issues. Note that mDeBERTa currently does not support FP16, see here: https://github.com/microsoft/DeBERTa/issues/77\n",
    "card_content": "---\nlanguage:\n- multilingual\n- en\n- ar\n- bg\n- de\n- el\n- es\n- fr\n- hi\n- ru\n- sw\n- th\n- tr\n- ur\n- vi\n- zh\nlicense: mit\ntags:\n- zero-shot-classification\n- text-classification\n- nli\n- pytorch\ndatasets:\n- multi_nli\n- xnli\nmetrics:\n- accuracy\npipeline_tag: zero-shot-classification\nwidget:\n- text: Angela Merkel ist eine Politikerin in Deutschland und Vorsitzende der CDU\n  candidate_labels: politics, economy, entertainment, environment\n---\n# Multilingual mDeBERTa-v3-base-mnli-xnli\n## Model description\nThis multilingual model can perform natural language inference (NLI) on 100 languages and is therefore also suitable for multilingual \nzero-shot classification. The underlying model was pre-trained by Microsoft on the \n[CC100 multilingual dataset](https://huggingface.co/datasets/cc100). It was then fine-tuned on the [XNLI dataset](https://huggingface.co/datasets/xnli), which contains hypothesis-premise pairs from 15 languages, as well as the English [MNLI dataset](https://huggingface.co/datasets/multi_nli).\nAs of December 2021, mDeBERTa-base is the best performing multilingual base-sized transformer model, \nintroduced by Microsoft in [this paper](https://arxiv.org/pdf/2111.09543.pdf). \n\nIf you are looking for a smaller, faster (but less performant) model, you can \ntry [multilingual-MiniLMv2-L6-mnli-xnli](https://huggingface.co/MoritzLaurer/multilingual-MiniLMv2-L6-mnli-xnli).\n\n### How to use the model\n#### Simple zero-shot classification pipeline\n```python\nfrom transformers import pipeline\nclassifier = pipeline(\"zero-shot-classification\", model=\"MoritzLaurer/mDeBERTa-v3-base-mnli-xnli\")\n\nsequence_to_classify = \"Angela Merkel ist eine Politikerin in Deutschland und Vorsitzende der CDU\"\ncandidate_labels = [\"politics\", \"economy\", \"entertainment\", \"environment\"]\noutput = classifier(sequence_to_classify, candidate_labels, multi_label=False)\nprint(output)\n```\n#### NLI use-case\n```python\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n\nmodel_name = \"MoritzLaurer/mDeBERTa-v3-base-mnli-xnli\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name)\n\npremise = \"Angela Merkel ist eine Politikerin in Deutschland und Vorsitzende der CDU\"\nhypothesis = \"Emmanuel Macron is the President of France\"\n\ninput = tokenizer(premise, hypothesis, truncation=True, return_tensors=\"pt\")\noutput = model(input[\"input_ids\"].to(device))  # device = \"cuda:0\" or \"cpu\"\nprediction = torch.softmax(output[\"logits\"][0], -1).tolist()\nlabel_names = [\"entailment\", \"neutral\", \"contradiction\"]\nprediction = {name: round(float(pred) * 100, 1) for pred, name in zip(prediction, label_names)}\nprint(prediction)\n```\n\n### Training data\nThis model was trained on the XNLI development dataset and the MNLI train dataset. The XNLI development set consists of 2490 professionally translated texts from English to 14 other languages (37350 texts in total) (see [this paper](https://arxiv.org/pdf/1809.05053.pdf)). Note that the XNLI contains a training set of 15 machine translated versions of the MNLI dataset for 15 languages, but due to quality issues with these machine translations, this model was only trained on the professional translations from the XNLI development set and the original English MNLI training set (392 702 texts). Not using machine translated texts can avoid overfitting the model to the 15 languages; avoids catastrophic forgetting of the other 85 languages mDeBERTa was pre-trained on; and significantly reduces training costs. \n\n### Training procedure\nmDeBERTa-v3-base-mnli-xnli was trained using the Hugging Face trainer with the following hyperparameters.\n```\ntraining_args = TrainingArguments(\n    num_train_epochs=2,              # total number of training epochs\n    learning_rate=2e-05,\n    per_device_train_batch_size=16,   # batch size per device during training\n    per_device_eval_batch_size=16,    # batch size for evaluation\n    warmup_ratio=0.1,                # number of warmup steps for learning rate scheduler\n    weight_decay=0.06,               # strength of weight decay\n)\n```\n### Eval results\nThe model was evaluated on the XNLI test set on 15 languages (5010 texts per language, 75150 in total). Note that multilingual NLI models are capable of classifying NLI texts without receiving NLI training data in the specific language (cross-lingual transfer). This means that the model is also able of doing NLI on the other 85 languages mDeBERTa was training on, but performance is most likely lower than for those languages available in XNLI.\n\nAlso note that if other multilingual models on the model hub claim performance of around 90% on languages other than English, the authors have most likely made a mistake during testing since non of the latest papers shows a multilingual average performance of more than a few points above 80% on XNLI (see [here](https://arxiv.org/pdf/2111.09543.pdf) or [here](https://arxiv.org/pdf/1911.02116.pdf)). \n\naverage | ar | bg | de | el | en | es | fr | hi | ru | sw | th | tr | ur | vi | zh \n---------|----------|---------|----------|----------|----------|----------|----------|----------|----------|----------|----------|----------|----------|----------|----------\n0.808 | 0.802 | 0.829 | 0.825 | 0.826 |\u00a00.883 | 0.845 |\u00a00.834 |\u00a00.771 |\u00a00.813 |\u00a00.748 |\u00a00.793 |\u00a00.807 |\u00a00.740 |\u00a00.795 |\u00a00.8116\n\n## Limitations and bias\nPlease consult the original DeBERTa-V3 paper and literature on different NLI datasets for potential biases. \n\n## Citation\nIf you use this model, please cite: Laurer, Moritz, Wouter van Atteveldt, Andreu Salleras Casas, and Kasper Welbers. 2022. \u2018Less Annotating, More Classifying \u2013 Addressing the Data Scarcity Issue of Supervised Machine Learning with Deep Transfer Learning and BERT - NLI\u2019. Preprint, June. Open Science Framework. https://osf.io/74b8k.\n\n## Ideas for cooperation or questions?\nIf you have questions or ideas for cooperation, contact me at m{dot}laurer{at}vu{dot}nl or [LinkedIn](https://www.linkedin.com/in/moritz-laurer/)\n\n## Debugging and issues\nNote that DeBERTa-v3 was released in late 2021 and older versions of HF Transformers seem to have issues running the model (e.g. resulting in an issue with the tokenizer). Using Transformers>=4.13 or higher might solve some issues. Note that mDeBERTa currently does not support FP16, see here: https://github.com/microsoft/DeBERTa/issues/77\n",
    "library_name": "transformers"
  },
  {
    "model_id": "oliverguhr/german-sentiment-bert",
    "model_name": "oliverguhr/german-sentiment-bert",
    "author": "oliverguhr",
    "downloads": 393859,
    "likes": 60,
    "tags": [
      "transformers",
      "pytorch",
      "tf",
      "jax",
      "onnx",
      "safetensors",
      "bert",
      "text-classification",
      "sentiment",
      "de",
      "license:mit",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/oliverguhr/german-sentiment-bert",
    "dependencies": [
      [
        "germansentiment",
        "1.0.6"
      ]
    ],
    "analysis_date": "2025-03-26T00:48:15.382353",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "bert",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": [
        "de"
      ],
      "license": "mit",
      "tags": [
        "sentiment",
        "bert"
      ],
      "metrics": [
        "f1"
      ],
      "widget": [
        {
          "text": "Das ist gar nicht mal so schlecht"
        }
      ]
    },
    "card_text": "\n\n# German Sentiment Classification with Bert\n\nThis model was trained for sentiment classification of German language texts. To achieve the best results all model inputs needs to be preprocessed with the same procedure, that was applied during the training. To simplify the usage of the model, \nwe provide a Python package that bundles the code need for the preprocessing and inferencing. \n\nThe model uses the Googles Bert architecture and was trained on 1.834 million German-language samples. The training data contains texts from various domains like Twitter, Facebook and movie, app and hotel reviews. \nYou can find more information about the dataset and the training process in the [paper](http://www.lrec-conf.org/proceedings/lrec2020/pdf/2020.lrec-1.202.pdf).\n\n## Using the Python package\n\nTo get started install the package from [pypi](https://pypi.org/project/germansentiment/):\n\n```bash\npip install germansentiment\n```\n\n```python\nfrom germansentiment import SentimentModel\n\nmodel = SentimentModel()\n\ntexts = [\n    \"Mit keinem guten Ergebniss\",\"Das ist gar nicht mal so gut\",\n    \"Total awesome!\",\"nicht so schlecht wie erwartet\",\n    \"Der Test verlief positiv.\",\"Sie f\u00e4hrt ein gr\u00fcnes Auto.\"]\n       \nresult = model.predict_sentiment(texts)\nprint(result)\n```\n\nThe code above will output following list:\n\n```python\n[\"negative\",\"negative\",\"positive\",\"positive\",\"neutral\", \"neutral\"]\n```\n\n### Output class probabilities\n\n```python\nfrom germansentiment import SentimentModel\n\nmodel = SentimentModel()\n\nclasses, probabilities = model.predict_sentiment([\"das ist super\"], output_probabilities = True) \nprint(classes, probabilities)\n```\n```python\n['positive'] [[['positive', 0.9761366844177246], ['negative', 0.023540444672107697], ['neutral', 0.00032294404809363186]]]\n```\n\n\n\n## Model and Data\n\nIf you are interested in code and data that was used to train this model please have a look at [this repository](https://github.com/oliverguhr/german-sentiment) and our [paper](http://www.lrec-conf.org/proceedings/lrec2020/pdf/2020.lrec-1.202.pdf). Here is a table of the F1 scores that this model achieves on different datasets. Since we trained this model with a newer version of the transformer library, the results are slightly better than reported in the paper.\n\n| Dataset                                                      | F1 micro Score |\n| :----------------------------------------------------------- | -------------: |\n| [holidaycheck](https://github.com/oliverguhr/german-sentiment) |         0.9568 |\n| [scare](https://www.romanklinger.de/scare/)                  |         0.9418 |\n| [filmstarts](https://github.com/oliverguhr/german-sentiment) |         0.9021 |\n| [germeval](https://sites.google.com/view/germeval2017-absa/home) |         0.7536 |\n| [PotTS](https://www.aclweb.org/anthology/L16-1181/)          |         0.6780 |\n| [emotions](https://github.com/oliverguhr/german-sentiment)  |         0.9649 |\n| [sb10k](https://www.spinningbytes.com/resources/germansentiment/) |         0.7376 |\n| [Leipzig Wikipedia Corpus 2016](https://wortschatz.uni-leipzig.de/de/download/german) |         0.9967 |\n| all                                                          |         0.9639 |\n\n## Cite\n\nFor feedback and questions contact me view mail or Twitter [@oliverguhr](https://twitter.com/oliverguhr). Please cite us if you found this useful:\n\n```\n@InProceedings{guhr-EtAl:2020:LREC,\n  author    = {Guhr, Oliver  and  Schumann, Anne-Kathrin  and  Bahrmann, Frank  and  B\u00f6hme, Hans Joachim},\n  title     = {Training a Broad-Coverage German Sentiment Classification Model for Dialog Systems},\n  booktitle      = {Proceedings of The 12th Language Resources and Evaluation Conference},\n  month          = {May},\n  year           = {2020},\n  address        = {Marseille, France},\n  publisher      = {European Language Resources Association},\n  pages     = {1620--1625},\n  url       = {https://www.aclweb.org/anthology/2020.lrec-1.202}\n}\n```\n\n\n",
    "card_content": "---\nlanguage:\n- de\nlicense: mit\ntags:\n- sentiment\n- bert\nmetrics:\n- f1\nwidget:\n- text: Das ist gar nicht mal so schlecht\n---\n\n\n# German Sentiment Classification with Bert\n\nThis model was trained for sentiment classification of German language texts. To achieve the best results all model inputs needs to be preprocessed with the same procedure, that was applied during the training. To simplify the usage of the model, \nwe provide a Python package that bundles the code need for the preprocessing and inferencing. \n\nThe model uses the Googles Bert architecture and was trained on 1.834 million German-language samples. The training data contains texts from various domains like Twitter, Facebook and movie, app and hotel reviews. \nYou can find more information about the dataset and the training process in the [paper](http://www.lrec-conf.org/proceedings/lrec2020/pdf/2020.lrec-1.202.pdf).\n\n## Using the Python package\n\nTo get started install the package from [pypi](https://pypi.org/project/germansentiment/):\n\n```bash\npip install germansentiment\n```\n\n```python\nfrom germansentiment import SentimentModel\n\nmodel = SentimentModel()\n\ntexts = [\n    \"Mit keinem guten Ergebniss\",\"Das ist gar nicht mal so gut\",\n    \"Total awesome!\",\"nicht so schlecht wie erwartet\",\n    \"Der Test verlief positiv.\",\"Sie f\u00e4hrt ein gr\u00fcnes Auto.\"]\n       \nresult = model.predict_sentiment(texts)\nprint(result)\n```\n\nThe code above will output following list:\n\n```python\n[\"negative\",\"negative\",\"positive\",\"positive\",\"neutral\", \"neutral\"]\n```\n\n### Output class probabilities\n\n```python\nfrom germansentiment import SentimentModel\n\nmodel = SentimentModel()\n\nclasses, probabilities = model.predict_sentiment([\"das ist super\"], output_probabilities = True) \nprint(classes, probabilities)\n```\n```python\n['positive'] [[['positive', 0.9761366844177246], ['negative', 0.023540444672107697], ['neutral', 0.00032294404809363186]]]\n```\n\n\n\n## Model and Data\n\nIf you are interested in code and data that was used to train this model please have a look at [this repository](https://github.com/oliverguhr/german-sentiment) and our [paper](http://www.lrec-conf.org/proceedings/lrec2020/pdf/2020.lrec-1.202.pdf). Here is a table of the F1 scores that this model achieves on different datasets. Since we trained this model with a newer version of the transformer library, the results are slightly better than reported in the paper.\n\n| Dataset                                                      | F1 micro Score |\n| :----------------------------------------------------------- | -------------: |\n| [holidaycheck](https://github.com/oliverguhr/german-sentiment) |         0.9568 |\n| [scare](https://www.romanklinger.de/scare/)                  |         0.9418 |\n| [filmstarts](https://github.com/oliverguhr/german-sentiment) |         0.9021 |\n| [germeval](https://sites.google.com/view/germeval2017-absa/home) |         0.7536 |\n| [PotTS](https://www.aclweb.org/anthology/L16-1181/)          |         0.6780 |\n| [emotions](https://github.com/oliverguhr/german-sentiment)  |         0.9649 |\n| [sb10k](https://www.spinningbytes.com/resources/germansentiment/) |         0.7376 |\n| [Leipzig Wikipedia Corpus 2016](https://wortschatz.uni-leipzig.de/de/download/german) |         0.9967 |\n| all                                                          |         0.9639 |\n\n## Cite\n\nFor feedback and questions contact me view mail or Twitter [@oliverguhr](https://twitter.com/oliverguhr). Please cite us if you found this useful:\n\n```\n@InProceedings{guhr-EtAl:2020:LREC,\n  author    = {Guhr, Oliver  and  Schumann, Anne-Kathrin  and  Bahrmann, Frank  and  B\u00f6hme, Hans Joachim},\n  title     = {Training a Broad-Coverage German Sentiment Classification Model for Dialog Systems},\n  booktitle      = {Proceedings of The 12th Language Resources and Evaluation Conference},\n  month          = {May},\n  year           = {2020},\n  address        = {Marseille, France},\n  publisher      = {European Language Resources Association},\n  pages     = {1620--1625},\n  url       = {https://www.aclweb.org/anthology/2020.lrec-1.202}\n}\n```\n\n\n",
    "library_name": "transformers"
  },
  {
    "model_id": "mixedbread-ai/mxbai-rerank-xsmall-v1",
    "model_name": "mixedbread-ai/mxbai-rerank-xsmall-v1",
    "author": "mixedbread-ai",
    "downloads": 391248,
    "likes": 39,
    "tags": [
      "transformers",
      "onnx",
      "safetensors",
      "deberta-v2",
      "text-classification",
      "reranker",
      "transformers.js",
      "en",
      "license:apache-2.0",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/mixedbread-ai/mxbai-rerank-xsmall-v1",
    "dependencies": null,
    "analysis_date": "2025-03-26T00:48:20.059251",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "deberta-v2",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": [
        "en"
      ],
      "license": "apache-2.0",
      "library_name": "transformers",
      "tags": [
        "reranker",
        "transformers.js"
      ]
    },
    "card_text": "<br><br>\n\n<p align=\"center\">\n<svg xmlns=\"http://www.w3.org/2000/svg\" xml:space=\"preserve\" viewBox=\"0 0 2020 1130\" width=\"150\" height=\"150\" aria-hidden=\"true\"><path fill=\"#e95a0f\" d=\"M398.167 621.992c-1.387-20.362-4.092-40.739-3.851-61.081.355-30.085 6.873-59.139 21.253-85.976 10.487-19.573 24.09-36.822 40.662-51.515 16.394-14.535 34.338-27.046 54.336-36.182 15.224-6.955 31.006-12.609 47.829-14.168 11.809-1.094 23.753-2.514 35.524-1.836 23.033 1.327 45.131 7.255 66.255 16.75 16.24 7.3 31.497 16.165 45.651 26.969 12.997 9.921 24.412 21.37 34.158 34.509 11.733 15.817 20.849 33.037 25.987 52.018 3.468 12.81 6.438 25.928 7.779 39.097 1.722 16.908 1.642 34.003 2.235 51.021.427 12.253.224 24.547 1.117 36.762 1.677 22.93 4.062 45.764 11.8 67.7 5.376 15.239 12.499 29.55 20.846 43.681l-18.282 20.328c-1.536 1.71-2.795 3.665-4.254 5.448l-19.323 23.533c-13.859-5.449-27.446-11.803-41.657-16.086-13.622-4.106-27.793-6.765-41.905-8.775-15.256-2.173-30.701-3.475-46.105-4.049-23.571-.879-47.178-1.056-70.769-1.029-10.858.013-21.723 1.116-32.57 1.926-5.362.4-10.69 1.255-16.464 1.477-2.758-7.675-5.284-14.865-7.367-22.181-3.108-10.92-4.325-22.554-13.16-31.095-2.598-2.512-5.069-5.341-6.883-8.443-6.366-10.884-12.48-21.917-18.571-32.959-4.178-7.573-8.411-14.375-17.016-18.559-10.34-5.028-19.538-12.387-29.311-18.611-3.173-2.021-6.414-4.312-9.952-5.297-5.857-1.63-11.98-2.301-17.991-3.376z\"></path><path fill=\"#ed6d7b\" d=\"M1478.998 758.842c-12.025.042-24.05.085-36.537-.373-.14-8.536.231-16.569.453-24.607.033-1.179-.315-2.986-1.081-3.4-.805-.434-2.376.338-3.518.81-.856.354-1.562 1.069-3.589 2.521-.239-3.308-.664-5.586-.519-7.827.488-7.544 2.212-15.166 1.554-22.589-1.016-11.451 1.397-14.592-12.332-14.419-3.793.048-3.617-2.803-3.332-5.331.499-4.422 1.45-8.803 1.77-13.233.311-4.316.068-8.672.068-12.861-2.554-.464-4.326-.86-6.12-1.098-4.415-.586-6.051-2.251-5.065-7.31 1.224-6.279.848-12.862 1.276-19.306.19-2.86-.971-4.473-3.794-4.753-4.113-.407-8.242-1.057-12.352-.975-4.663.093-5.192-2.272-4.751-6.012.733-6.229 1.252-12.483 1.875-18.726l1.102-10.495c-5.905-.309-11.146-.805-16.385-.778-3.32.017-5.174-1.4-5.566-4.4-1.172-8.968-2.479-17.944-3.001-26.96-.26-4.484-1.936-5.705-6.005-5.774-9.284-.158-18.563-.594-27.843-.953-7.241-.28-10.137-2.764-11.3-9.899-.746-4.576-2.715-7.801-7.777-8.207-7.739-.621-15.511-.992-23.207-1.961-7.327-.923-14.587-2.415-21.853-3.777-5.021-.941-10.003-2.086-15.003-3.14 4.515-22.952 13.122-44.382 26.284-63.587 18.054-26.344 41.439-47.239 69.102-63.294 15.847-9.197 32.541-16.277 50.376-20.599 16.655-4.036 33.617-5.715 50.622-4.385 33.334 2.606 63.836 13.955 92.415 31.15 15.864 9.545 30.241 20.86 42.269 34.758 8.113 9.374 15.201 19.78 21.718 30.359 10.772 17.484 16.846 36.922 20.611 56.991 1.783 9.503 2.815 19.214 3.318 28.876.758 14.578.755 29.196.65 44.311l-51.545 20.013c-7.779 3.059-15.847 5.376-21.753 12.365-4.73 5.598-10.658 10.316-16.547 14.774-9.9 7.496-18.437 15.988-25.083 26.631-3.333 5.337-7.901 10.381-12.999 14.038-11.355 8.144-17.397 18.973-19.615 32.423l-6.988 41.011z\"></path><path fill=\"#ec663e\" d=\"M318.11 923.047c-.702 17.693-.832 35.433-2.255 53.068-1.699 21.052-6.293 41.512-14.793 61.072-9.001 20.711-21.692 38.693-38.496 53.583-16.077 14.245-34.602 24.163-55.333 30.438-21.691 6.565-43.814 8.127-66.013 6.532-22.771-1.636-43.88-9.318-62.74-22.705-20.223-14.355-35.542-32.917-48.075-54.096-9.588-16.203-16.104-33.55-19.201-52.015-2.339-13.944-2.307-28.011-.403-42.182 2.627-19.545 9.021-37.699 17.963-55.067 11.617-22.564 27.317-41.817 48.382-56.118 15.819-10.74 33.452-17.679 52.444-20.455 8.77-1.282 17.696-1.646 26.568-2.055 11.755-.542 23.534-.562 35.289-1.11 8.545-.399 17.067-1.291 26.193-1.675 1.349 1.77 2.24 3.199 2.835 4.742 4.727 12.261 10.575 23.865 18.636 34.358 7.747 10.084 14.83 20.684 22.699 30.666 3.919 4.972 8.37 9.96 13.609 13.352 7.711 4.994 16.238 8.792 24.617 12.668 5.852 2.707 12.037 4.691 18.074 6.998z\"></path><path fill=\"#ea580e\" d=\"M1285.167 162.995c3.796-29.75 13.825-56.841 32.74-80.577 16.339-20.505 36.013-36.502 59.696-47.614 14.666-6.881 29.971-11.669 46.208-12.749 10.068-.669 20.239-1.582 30.255-.863 16.6 1.191 32.646 5.412 47.9 12.273 19.39 8.722 36.44 20.771 50.582 36.655 15.281 17.162 25.313 37.179 31.49 59.286 5.405 19.343 6.31 39.161 4.705 58.825-2.37 29.045-11.836 55.923-30.451 78.885-10.511 12.965-22.483 24.486-37.181 33.649-5.272-5.613-10.008-11.148-14.539-16.846-5.661-7.118-10.958-14.533-16.78-21.513-4.569-5.478-9.548-10.639-14.624-15.658-3.589-3.549-7.411-6.963-11.551-9.827-5.038-3.485-10.565-6.254-15.798-9.468-8.459-5.195-17.011-9.669-26.988-11.898-12.173-2.72-24.838-4.579-35.622-11.834-1.437-.967-3.433-1.192-5.213-1.542-12.871-2.529-25.454-5.639-36.968-12.471-5.21-3.091-11.564-4.195-17.011-6.965-4.808-2.445-8.775-6.605-13.646-8.851-8.859-4.085-18.114-7.311-27.204-10.896z\"></path><path fill=\"#f8ab00\" d=\"M524.963 311.12c-9.461-5.684-19.513-10.592-28.243-17.236-12.877-9.801-24.031-21.578-32.711-35.412-11.272-17.965-19.605-37.147-21.902-58.403-1.291-11.951-2.434-24.073-1.87-36.034.823-17.452 4.909-34.363 11.581-50.703 8.82-21.603 22.25-39.792 39.568-55.065 18.022-15.894 39.162-26.07 62.351-32.332 19.22-5.19 38.842-6.177 58.37-4.674 23.803 1.831 45.56 10.663 65.062 24.496 17.193 12.195 31.688 27.086 42.894 45.622-11.403 8.296-22.633 16.117-34.092 23.586-17.094 11.142-34.262 22.106-48.036 37.528-8.796 9.848-17.201 20.246-27.131 28.837-16.859 14.585-27.745 33.801-41.054 51.019-11.865 15.349-20.663 33.117-30.354 50.08-5.303 9.283-9.654 19.11-14.434 28.692z\"></path><path fill=\"#ea5227\" d=\"M1060.11 1122.049c-7.377 1.649-14.683 4.093-22.147 4.763-11.519 1.033-23.166 1.441-34.723 1.054-19.343-.647-38.002-4.7-55.839-12.65-15.078-6.72-28.606-15.471-40.571-26.836-24.013-22.81-42.053-49.217-49.518-81.936-1.446-6.337-1.958-12.958-2.235-19.477-.591-13.926-.219-27.909-1.237-41.795-.916-12.5-3.16-24.904-4.408-37.805 1.555-1.381 3.134-2.074 3.778-3.27 4.729-8.79 12.141-15.159 19.083-22.03 5.879-5.818 10.688-12.76 16.796-18.293 6.993-6.335 11.86-13.596 14.364-22.612l8.542-29.993c8.015 1.785 15.984 3.821 24.057 5.286 8.145 1.478 16.371 2.59 24.602 3.493 8.453.927 16.956 1.408 25.891 2.609 1.119 16.09 1.569 31.667 2.521 47.214.676 11.045 1.396 22.154 3.234 33.043 2.418 14.329 5.708 28.527 9.075 42.674 3.499 14.705 4.028 29.929 10.415 44.188 10.157 22.674 18.29 46.25 28.281 69.004 7.175 16.341 12.491 32.973 15.078 50.615.645 4.4 3.256 8.511 4.963 12.755z\"></path><path fill=\"#ea5330\" d=\"M1060.512 1122.031c-2.109-4.226-4.72-8.337-5.365-12.737-2.587-17.642-7.904-34.274-15.078-50.615-9.991-22.755-18.124-46.33-28.281-69.004-6.387-14.259-6.916-29.482-10.415-44.188-3.366-14.147-6.656-28.346-9.075-42.674-1.838-10.889-2.558-21.999-3.234-33.043-.951-15.547-1.401-31.124-2.068-47.146 8.568-.18 17.146.487 25.704.286l41.868-1.4c.907 3.746 1.245 7.04 1.881 10.276l8.651 42.704c.903 4.108 2.334 8.422 4.696 11.829 7.165 10.338 14.809 20.351 22.456 30.345 4.218 5.512 8.291 11.304 13.361 15.955 8.641 7.927 18.065 14.995 27.071 22.532 12.011 10.052 24.452 19.302 40.151 22.854-1.656 11.102-2.391 22.44-5.172 33.253-4.792 18.637-12.38 36.209-23.412 52.216-13.053 18.94-29.086 34.662-49.627 45.055-10.757 5.443-22.443 9.048-34.111 13.501z\"></path><path fill=\"#f8aa05\" d=\"M1989.106 883.951c5.198 8.794 11.46 17.148 15.337 26.491 5.325 12.833 9.744 26.207 12.873 39.737 2.95 12.757 3.224 25.908 1.987 39.219-1.391 14.973-4.643 29.268-10.349 43.034-5.775 13.932-13.477 26.707-23.149 38.405-14.141 17.104-31.215 30.458-50.807 40.488-14.361 7.352-29.574 12.797-45.741 14.594-10.297 1.144-20.732 2.361-31.031 1.894-24.275-1.1-47.248-7.445-68.132-20.263-6.096-3.741-11.925-7.917-17.731-12.342 5.319-5.579 10.361-10.852 15.694-15.811l37.072-34.009c.975-.892 2.113-1.606 3.08-2.505 6.936-6.448 14.765-12.2 20.553-19.556 8.88-11.285 20.064-19.639 31.144-28.292 4.306-3.363 9.06-6.353 12.673-10.358 5.868-6.504 10.832-13.814 16.422-20.582 6.826-8.264 13.727-16.481 20.943-24.401 4.065-4.461 8.995-8.121 13.249-12.424 14.802-14.975 28.77-30.825 45.913-43.317z\"></path><path fill=\"#ed6876\" d=\"M1256.099 523.419c5.065.642 10.047 1.787 15.068 2.728 7.267 1.362 14.526 2.854 21.853 3.777 7.696.97 15.468 1.34 23.207 1.961 5.062.406 7.031 3.631 7.777 8.207 1.163 7.135 4.059 9.62 11.3 9.899l27.843.953c4.069.069 5.745 1.291 6.005 5.774.522 9.016 1.829 17.992 3.001 26.96.392 3 2.246 4.417 5.566 4.4 5.239-.026 10.48.469 16.385.778l-1.102 10.495-1.875 18.726c-.44 3.74.088 6.105 4.751 6.012 4.11-.082 8.239.568 12.352.975 2.823.28 3.984 1.892 3.794 4.753-.428 6.444-.052 13.028-1.276 19.306-.986 5.059.651 6.724 5.065 7.31 1.793.238 3.566.634 6.12 1.098 0 4.189.243 8.545-.068 12.861-.319 4.43-1.27 8.811-1.77 13.233-.285 2.528-.461 5.379 3.332 5.331 13.729-.173 11.316 2.968 12.332 14.419.658 7.423-1.066 15.045-1.554 22.589-.145 2.241.28 4.519.519 7.827 2.026-1.452 2.733-2.167 3.589-2.521 1.142-.472 2.713-1.244 3.518-.81.767.414 1.114 2.221 1.081 3.4l-.917 24.539c-11.215.82-22.45.899-33.636 1.674l-43.952 3.436c-1.086-3.01-2.319-5.571-2.296-8.121.084-9.297-4.468-16.583-9.091-24.116-3.872-6.308-8.764-13.052-9.479-19.987-1.071-10.392-5.716-15.936-14.889-18.979-1.097-.364-2.16-.844-3.214-1.327-7.478-3.428-15.548-5.918-19.059-14.735-.904-2.27-3.657-3.775-5.461-5.723-2.437-2.632-4.615-5.525-7.207-7.987-2.648-2.515-5.352-5.346-8.589-6.777-4.799-2.121-10.074-3.185-15.175-4.596l-15.785-4.155c.274-12.896 1.722-25.901.54-38.662-1.647-17.783-3.457-35.526-2.554-53.352.528-10.426 2.539-20.777 3.948-31.574z\"></path><path fill=\"#f6a200\" d=\"M525.146 311.436c4.597-9.898 8.947-19.725 14.251-29.008 9.691-16.963 18.49-34.73 30.354-50.08 13.309-17.218 24.195-36.434 41.054-51.019 9.93-8.591 18.335-18.989 27.131-28.837 13.774-15.422 30.943-26.386 48.036-37.528 11.459-7.469 22.688-15.29 34.243-23.286 11.705 16.744 19.716 35.424 22.534 55.717 2.231 16.066 2.236 32.441 2.753 49.143-4.756 1.62-9.284 2.234-13.259 4.056-6.43 2.948-12.193 7.513-18.774 9.942-19.863 7.331-33.806 22.349-47.926 36.784-7.86 8.035-13.511 18.275-19.886 27.705-4.434 6.558-9.345 13.037-12.358 20.254-4.249 10.177-6.94 21.004-10.296 31.553-12.33.053-24.741 1.027-36.971-.049-20.259-1.783-40.227-5.567-58.755-14.69-.568-.28-1.295-.235-2.132-.658z\"></path><path fill=\"#f7a80d\" d=\"M1989.057 883.598c-17.093 12.845-31.061 28.695-45.863 43.67-4.254 4.304-9.184 7.963-13.249 12.424-7.216 7.92-14.117 16.137-20.943 24.401-5.59 6.768-10.554 14.078-16.422 20.582-3.614 4.005-8.367 6.995-12.673 10.358-11.08 8.653-22.264 17.007-31.144 28.292-5.788 7.356-13.617 13.108-20.553 19.556-.967.899-2.105 1.614-3.08 2.505l-37.072 34.009c-5.333 4.96-10.375 10.232-15.859 15.505-21.401-17.218-37.461-38.439-48.623-63.592 3.503-1.781 7.117-2.604 9.823-4.637 8.696-6.536 20.392-8.406 27.297-17.714.933-1.258 2.646-1.973 4.065-2.828 17.878-10.784 36.338-20.728 53.441-32.624 10.304-7.167 18.637-17.23 27.583-26.261 3.819-3.855 7.436-8.091 10.3-12.681 12.283-19.68 24.43-39.446 40.382-56.471 12.224-13.047 17.258-29.524 22.539-45.927 15.85 4.193 29.819 12.129 42.632 22.08 10.583 8.219 19.782 17.883 27.42 29.351z\"></path><path fill=\"#ef7a72\" d=\"M1479.461 758.907c1.872-13.734 4.268-27.394 6.525-41.076 2.218-13.45 8.26-24.279 19.615-32.423 5.099-3.657 9.667-8.701 12.999-14.038 6.646-10.643 15.183-19.135 25.083-26.631 5.888-4.459 11.817-9.176 16.547-14.774 5.906-6.99 13.974-9.306 21.753-12.365l51.48-19.549c.753 11.848.658 23.787 1.641 35.637 1.771 21.353 4.075 42.672 11.748 62.955.17.449.107.985-.019 2.158-6.945 4.134-13.865 7.337-20.437 11.143-3.935 2.279-7.752 5.096-10.869 8.384-6.011 6.343-11.063 13.624-17.286 19.727-9.096 8.92-12.791 20.684-18.181 31.587-.202.409-.072.984-.096 1.481-8.488-1.72-16.937-3.682-25.476-5.094-9.689-1.602-19.426-3.084-29.201-3.949-15.095-1.335-30.241-2.1-45.828-3.172z\"></path><path fill=\"#e94e3b\" d=\"M957.995 766.838c-20.337-5.467-38.791-14.947-55.703-27.254-8.2-5.967-15.451-13.238-22.958-20.37 2.969-3.504 5.564-6.772 8.598-9.563 7.085-6.518 11.283-14.914 15.8-23.153 4.933-8.996 10.345-17.743 14.966-26.892 2.642-5.231 5.547-11.01 5.691-16.611.12-4.651.194-8.932 2.577-12.742 8.52-13.621 15.483-28.026 18.775-43.704 2.11-10.049 7.888-18.774 7.81-29.825-.064-9.089 4.291-18.215 6.73-27.313 3.212-11.983 7.369-23.797 9.492-35.968 3.202-18.358 5.133-36.945 7.346-55.466l4.879-45.8c6.693.288 13.386.575 20.54 1.365.13 3.458-.41 6.407-.496 9.37l-1.136 42.595c-.597 11.552-2.067 23.058-3.084 34.59l-3.845 44.478c-.939 10.202-1.779 20.432-3.283 30.557-.96 6.464-4.46 12.646-1.136 19.383.348.706-.426 1.894-.448 2.864-.224 9.918-5.99 19.428-2.196 29.646.103.279-.033.657-.092.983l-8.446 46.205c-1.231 6.469-2.936 12.846-4.364 19.279-1.5 6.757-2.602 13.621-4.456 20.277-3.601 12.93-10.657 25.3-5.627 39.47.368 1.036.234 2.352.017 3.476l-5.949 30.123z\"></path><path fill=\"#ea5043\" d=\"M958.343 767.017c1.645-10.218 3.659-20.253 5.602-30.302.217-1.124.351-2.44-.017-3.476-5.03-14.17 2.026-26.539 5.627-39.47 1.854-6.656 2.956-13.52 4.456-20.277 1.428-6.433 3.133-12.81 4.364-19.279l8.446-46.205c.059-.326.196-.705.092-.983-3.794-10.218 1.972-19.728 2.196-29.646.022-.97.796-2.158.448-2.864-3.324-6.737.176-12.919 1.136-19.383 1.504-10.125 2.344-20.355 3.283-30.557l3.845-44.478c1.017-11.532 2.488-23.038 3.084-34.59.733-14.18.722-28.397 1.136-42.595.086-2.963.626-5.912.956-9.301 5.356-.48 10.714-.527 16.536-.081 2.224 15.098 1.855 29.734 1.625 44.408-.157 10.064 1.439 20.142 1.768 30.23.334 10.235-.035 20.49.116 30.733.084 5.713.789 11.418.861 17.13.054 4.289-.469 8.585-.702 12.879-.072 1.323-.138 2.659-.031 3.975l2.534 34.405-1.707 36.293-1.908 48.69c-.182 8.103.993 16.237.811 24.34-.271 12.076-1.275 24.133-1.787 36.207-.102 2.414-.101 5.283 1.06 7.219 4.327 7.22 4.463 15.215 4.736 23.103.365 10.553.088 21.128.086 31.693-11.44 2.602-22.84.688-34.106-.916-11.486-1.635-22.806-4.434-34.546-6.903z\"></path><path fill=\"#eb5d19\" d=\"M398.091 622.45c6.086.617 12.21 1.288 18.067 2.918 3.539.985 6.779 3.277 9.952 5.297 9.773 6.224 18.971 13.583 29.311 18.611 8.606 4.184 12.839 10.986 17.016 18.559l18.571 32.959c1.814 3.102 4.285 5.931 6.883 8.443 8.835 8.542 10.052 20.175 13.16 31.095 2.082 7.317 4.609 14.507 6.946 22.127-29.472 3.021-58.969 5.582-87.584 15.222-1.185-2.302-1.795-4.362-2.769-6.233-4.398-8.449-6.703-18.174-14.942-24.299-2.511-1.866-5.103-3.814-7.047-6.218-8.358-10.332-17.028-20.276-28.772-26.973 4.423-11.478 9.299-22.806 13.151-34.473 4.406-13.348 6.724-27.18 6.998-41.313.098-5.093.643-10.176 1.06-15.722z\"></path><path fill=\"#e94c32\" d=\"M981.557 392.109c-1.172 15.337-2.617 30.625-4.438 45.869-2.213 18.521-4.144 37.108-7.346 55.466-2.123 12.171-6.28 23.985-9.492 35.968-2.439 9.098-6.794 18.224-6.73 27.313.078 11.051-5.7 19.776-7.81 29.825-3.292 15.677-10.255 30.082-18.775 43.704-2.383 3.81-2.458 8.091-2.577 12.742-.144 5.6-3.049 11.38-5.691 16.611-4.621 9.149-10.033 17.896-14.966 26.892-4.517 8.239-8.715 16.635-15.8 23.153-3.034 2.791-5.629 6.06-8.735 9.255-12.197-10.595-21.071-23.644-29.301-37.24-7.608-12.569-13.282-25.962-17.637-40.37 13.303-6.889 25.873-13.878 35.311-25.315.717-.869 1.934-1.312 2.71-2.147 5.025-5.405 10.515-10.481 14.854-16.397 6.141-8.374 10.861-17.813 17.206-26.008 8.22-10.618 13.657-22.643 20.024-34.466 4.448-.626 6.729-3.21 8.114-6.89 1.455-3.866 2.644-7.895 4.609-11.492 4.397-8.05 9.641-15.659 13.708-23.86 3.354-6.761 5.511-14.116 8.203-21.206 5.727-15.082 7.277-31.248 12.521-46.578 3.704-10.828 3.138-23.116 4.478-34.753l7.56-.073z\"></path><path fill=\"#f7a617\" d=\"M1918.661 831.99c-4.937 16.58-9.971 33.057-22.196 46.104-15.952 17.025-28.099 36.791-40.382 56.471-2.864 4.59-6.481 8.825-10.3 12.681-8.947 9.031-17.279 19.094-27.583 26.261-17.103 11.896-35.564 21.84-53.441 32.624-1.419.856-3.132 1.571-4.065 2.828-6.904 9.308-18.6 11.178-27.297 17.714-2.705 2.033-6.319 2.856-9.874 4.281-3.413-9.821-6.916-19.583-9.36-29.602-1.533-6.284-1.474-12.957-1.665-19.913 1.913-.78 3.374-1.057 4.81-1.431 15.822-4.121 31.491-8.029 43.818-20.323 9.452-9.426 20.371-17.372 30.534-26.097 6.146-5.277 13.024-10.052 17.954-16.326 14.812-18.848 28.876-38.285 43.112-57.581 2.624-3.557 5.506-7.264 6.83-11.367 2.681-8.311 4.375-16.94 6.476-25.438 17.89.279 35.333 3.179 52.629 9.113z\"></path><path fill=\"#ea553a\" d=\"M1172.91 977.582c-15.775-3.127-28.215-12.377-40.227-22.43-9.005-7.537-18.43-14.605-27.071-22.532-5.07-4.651-9.143-10.443-13.361-15.955-7.647-9.994-15.291-20.007-22.456-30.345-2.361-3.407-3.792-7.72-4.696-11.829-3.119-14.183-5.848-28.453-8.651-42.704-.636-3.236-.974-6.53-1.452-10.209 15.234-2.19 30.471-3.969 46.408-5.622 2.692 5.705 4.882 11.222 6.63 16.876 2.9 9.381 7.776 17.194 15.035 24.049 7.056 6.662 13.305 14.311 19.146 22.099 9.509 12.677 23.01 19.061 36.907 25.054-1.048 7.441-2.425 14.854-3.066 22.33-.956 11.162-1.393 22.369-2.052 33.557l-1.096 17.661z\"></path><path fill=\"#ea5453\" d=\"M1163.123 704.036c-4.005 5.116-7.685 10.531-12.075 15.293-12.842 13.933-27.653 25.447-44.902 34.538-3.166-5.708-5.656-11.287-8.189-17.251-3.321-12.857-6.259-25.431-9.963-37.775-4.6-15.329-10.6-30.188-11.349-46.562-.314-6.871-1.275-14.287-7.114-19.644-1.047-.961-1.292-3.053-1.465-4.67l-4.092-39.927c-.554-5.245-.383-10.829-2.21-15.623-3.622-9.503-4.546-19.253-4.688-29.163-.088-6.111 1.068-12.256.782-18.344-.67-14.281-1.76-28.546-2.9-42.8-.657-8.222-1.951-16.395-2.564-24.62-.458-6.137-.285-12.322-.104-18.21.959 5.831 1.076 11.525 2.429 16.909 2.007 7.986 5.225 15.664 7.324 23.632 3.222 12.23 1.547 25.219 6.728 37.355 4.311 10.099 6.389 21.136 9.732 31.669 2.228 7.02 6.167 13.722 7.121 20.863 1.119 8.376 6.1 13.974 10.376 20.716l2.026 10.576c1.711 9.216 3.149 18.283 8.494 26.599 6.393 9.946 11.348 20.815 16.943 31.276 4.021 7.519 6.199 16.075 12.925 22.065l24.462 22.26c.556.503 1.507.571 2.274.841z\"></path><path fill=\"#ea5b15\" d=\"M1285.092 163.432c9.165 3.148 18.419 6.374 27.279 10.459 4.871 2.246 8.838 6.406 13.646 8.851 5.446 2.77 11.801 3.874 17.011 6.965 11.514 6.831 24.097 9.942 36.968 12.471 1.78.35 3.777.576 5.213 1.542 10.784 7.255 23.448 9.114 35.622 11.834 9.977 2.23 18.529 6.703 26.988 11.898 5.233 3.214 10.76 5.983 15.798 9.468 4.14 2.864 7.962 6.279 11.551 9.827 5.076 5.02 10.056 10.181 14.624 15.658 5.822 6.98 11.119 14.395 16.78 21.513 4.531 5.698 9.267 11.233 14.222 16.987-10.005 5.806-20.07 12.004-30.719 16.943-7.694 3.569-16.163 5.464-24.688 7.669-2.878-7.088-5.352-13.741-7.833-20.392-.802-2.15-1.244-4.55-2.498-6.396-4.548-6.7-9.712-12.999-14.011-19.847-6.672-10.627-15.34-18.93-26.063-25.376-9.357-5.625-18.367-11.824-27.644-17.587-6.436-3.997-12.902-8.006-19.659-11.405-5.123-2.577-11.107-3.536-16.046-6.37-17.187-9.863-35.13-17.887-54.031-23.767-4.403-1.37-8.953-2.267-13.436-3.382l.926-27.565z\"></path><path fill=\"#ea504b\" d=\"M1098 737l7.789 16.893c-15.04 9.272-31.679 15.004-49.184 17.995-9.464 1.617-19.122 2.097-29.151 3.019-.457-10.636-.18-21.211-.544-31.764-.273-7.888-.409-15.883-4.736-23.103-1.16-1.936-1.162-4.805-1.06-7.219l1.787-36.207c.182-8.103-.993-16.237-.811-24.34.365-16.236 1.253-32.461 1.908-48.69.484-12 .942-24.001 1.98-36.069 5.57 10.19 10.632 20.42 15.528 30.728 1.122 2.362 2.587 5.09 2.339 7.488-1.536 14.819 5.881 26.839 12.962 38.33 10.008 16.241 16.417 33.54 20.331 51.964 2.285 10.756 4.729 21.394 11.958 30.165L1098 737z\"></path><path fill=\"#f6a320\" d=\"M1865.78 822.529c-1.849 8.846-3.544 17.475-6.224 25.786-1.323 4.102-4.206 7.81-6.83 11.367l-43.112 57.581c-4.93 6.273-11.808 11.049-17.954 16.326-10.162 8.725-21.082 16.671-30.534 26.097-12.327 12.294-27.997 16.202-43.818 20.323-1.436.374-2.897.651-4.744.986-1.107-17.032-1.816-34.076-2.079-51.556 1.265-.535 2.183-.428 2.888-.766 10.596-5.072 20.8-11.059 32.586-13.273 1.69-.317 3.307-1.558 4.732-2.662l26.908-21.114c4.992-4.003 11.214-7.393 14.381-12.585 11.286-18.5 22.363-37.263 27.027-58.87l36.046 1.811c3.487.165 6.983.14 10.727.549z\"></path><path fill=\"#ec6333\" d=\"M318.448 922.814c-6.374-2.074-12.56-4.058-18.412-6.765-8.379-3.876-16.906-7.675-24.617-12.668-5.239-3.392-9.69-8.381-13.609-13.352-7.87-9.983-14.953-20.582-22.699-30.666-8.061-10.493-13.909-22.097-18.636-34.358-.595-1.543-1.486-2.972-2.382-4.783 6.84-1.598 13.797-3.023 20.807-4.106 18.852-2.912 36.433-9.493 53.737-17.819.697.888.889 1.555 1.292 2.051l17.921 21.896c4.14 4.939 8.06 10.191 12.862 14.412 5.67 4.984 12.185 9.007 18.334 13.447-8.937 16.282-16.422 33.178-20.696 51.31-1.638 6.951-2.402 14.107-3.903 21.403z\"></path><path fill=\"#f49700\" d=\"M623.467 326.903c2.893-10.618 5.584-21.446 9.833-31.623 3.013-7.217 7.924-13.696 12.358-20.254 6.375-9.43 12.026-19.67 19.886-27.705 14.12-14.434 28.063-29.453 47.926-36.784 6.581-2.429 12.344-6.994 18.774-9.942 3.975-1.822 8.503-2.436 13.186-3.592 1.947 18.557 3.248 37.15 8.307 55.686-15.453 7.931-28.853 18.092-40.46 29.996-10.417 10.683-19.109 23.111-28.013 35.175-3.238 4.388-4.888 9.948-7.262 14.973-17.803-3.987-35.767-6.498-54.535-5.931z\"></path><path fill=\"#ea544c\" d=\"M1097.956 736.615c-2.925-3.218-5.893-6.822-8.862-10.425-7.229-8.771-9.672-19.409-11.958-30.165-3.914-18.424-10.323-35.722-20.331-51.964-7.081-11.491-14.498-23.511-12.962-38.33.249-2.398-1.217-5.126-2.339-7.488l-15.232-31.019-3.103-34.338c-.107-1.316-.041-2.653.031-3.975.233-4.294.756-8.59.702-12.879-.072-5.713-.776-11.417-.861-17.13l-.116-30.733c-.329-10.088-1.926-20.166-1.768-30.23.23-14.674.599-29.31-1.162-44.341 9.369-.803 18.741-1.179 28.558-1.074 1.446 15.814 2.446 31.146 3.446 46.478.108 6.163-.064 12.348.393 18.485.613 8.225 1.907 16.397 2.564 24.62l2.9 42.8c.286 6.088-.869 12.234-.782 18.344.142 9.91 1.066 19.661 4.688 29.163 1.827 4.794 1.657 10.377 2.21 15.623l4.092 39.927c.172 1.617.417 3.71 1.465 4.67 5.839 5.357 6.8 12.773 7.114 19.644.749 16.374 6.749 31.233 11.349 46.562 3.704 12.344 6.642 24.918 9.963 37.775z\"></path><path fill=\"#ec5c61\" d=\"M1204.835 568.008c1.254 25.351-1.675 50.16-10.168 74.61-8.598-4.883-18.177-8.709-24.354-15.59-7.44-8.289-13.929-17.442-21.675-25.711-8.498-9.072-16.731-18.928-21.084-31.113-.54-1.513-1.691-2.807-2.594-4.564-4.605-9.247-7.706-18.544-7.96-29.09-.835-7.149-1.214-13.944-2.609-20.523-2.215-10.454-5.626-20.496-7.101-31.302-2.513-18.419-7.207-36.512-5.347-55.352.24-2.43-.17-4.949-.477-7.402l-4.468-34.792c2.723-.379 5.446-.757 8.585-.667 1.749 8.781 2.952 17.116 4.448 25.399 1.813 10.037 3.64 20.084 5.934 30.017 1.036 4.482 3.953 8.573 4.73 13.064 1.794 10.377 4.73 20.253 9.272 29.771 2.914 6.105 4.761 12.711 7.496 18.912 2.865 6.496 6.264 12.755 9.35 19.156 3.764 7.805 7.667 15.013 16.1 19.441 7.527 3.952 13.713 10.376 20.983 14.924 6.636 4.152 13.932 7.25 20.937 10.813z\"></path><path fill=\"#ed676f\" d=\"M1140.75 379.231c18.38-4.858 36.222-11.21 53.979-18.971 3.222 3.368 5.693 6.744 8.719 9.512 2.333 2.134 5.451 5.07 8.067 4.923 7.623-.429 12.363 2.688 17.309 8.215 5.531 6.18 12.744 10.854 19.224 16.184-5.121 7.193-10.461 14.241-15.323 21.606-13.691 20.739-22.99 43.255-26.782 67.926-.543 3.536-1.281 7.043-2.366 10.925-14.258-6.419-26.411-14.959-32.731-29.803-1.087-2.553-2.596-4.93-3.969-7.355-1.694-2.993-3.569-5.89-5.143-8.943-1.578-3.062-2.922-6.249-4.295-9.413-1.57-3.621-3.505-7.163-4.47-10.946-1.257-4.93-.636-10.572-2.725-15.013-5.831-12.397-7.467-25.628-9.497-38.847z\"></path><path fill=\"#ed656e\" d=\"M1254.103 647.439c5.325.947 10.603 2.272 15.847 3.722 5.101 1.41 10.376 2.475 15.175 4.596 3.237 1.431 5.942 4.262 8.589 6.777 2.592 2.462 4.77 5.355 7.207 7.987 1.804 1.948 4.557 3.453 5.461 5.723 3.51 8.817 11.581 11.307 19.059 14.735 1.053.483 2.116.963 3.214 1.327 9.172 3.043 13.818 8.587 14.889 18.979.715 6.935 5.607 13.679 9.479 19.987 4.623 7.533 9.175 14.819 9.091 24.116-.023 2.55 1.21 5.111 1.874 8.055-19.861 2.555-39.795 4.296-59.597 9.09l-11.596-23.203c-1.107-2.169-2.526-4.353-4.307-5.975-7.349-6.694-14.863-13.209-22.373-19.723l-17.313-14.669c-2.776-2.245-5.935-4.017-8.92-6.003l11.609-38.185c1.508-5.453 1.739-11.258 2.613-17.336z\"></path><path fill=\"#ec6168\" d=\"M1140.315 379.223c2.464 13.227 4.101 26.459 9.931 38.856 2.089 4.441 1.468 10.083 2.725 15.013.965 3.783 2.9 7.325 4.47 10.946 1.372 3.164 2.716 6.351 4.295 9.413 1.574 3.053 3.449 5.95 5.143 8.943 1.372 2.425 2.882 4.803 3.969 7.355 6.319 14.844 18.473 23.384 32.641 30.212.067 5.121-.501 10.201-.435 15.271l.985 38.117c.151 4.586.616 9.162.868 14.201-7.075-3.104-14.371-6.202-21.007-10.354-7.269-4.548-13.456-10.972-20.983-14.924-8.434-4.428-12.337-11.637-16.1-19.441-3.087-6.401-6.485-12.66-9.35-19.156-2.735-6.201-4.583-12.807-7.496-18.912-4.542-9.518-7.477-19.394-9.272-29.771-.777-4.491-3.694-8.581-4.73-13.064-2.294-9.933-4.121-19.98-5.934-30.017-1.496-8.283-2.699-16.618-4.036-25.335 10.349-2.461 20.704-4.511 31.054-6.582.957-.191 1.887-.515 3.264-.769z\"></path><path fill=\"#e94c28\" d=\"M922 537c-6.003 11.784-11.44 23.81-19.66 34.428-6.345 8.196-11.065 17.635-17.206 26.008-4.339 5.916-9.828 10.992-14.854 16.397-.776.835-1.993 1.279-2.71 2.147-9.439 11.437-22.008 18.427-35.357 24.929-4.219-10.885-6.942-22.155-7.205-33.905l-.514-49.542c7.441-2.893 14.452-5.197 21.334-7.841 1.749-.672 3.101-2.401 4.604-3.681 6.749-5.745 12.845-12.627 20.407-16.944 7.719-4.406 14.391-9.101 18.741-16.889.626-1.122 1.689-2.077 2.729-2.877 7.197-5.533 12.583-12.51 16.906-20.439.68-1.247 2.495-1.876 4.105-2.651 2.835 1.408 5.267 2.892 7.884 3.892 3.904 1.491 4.392 3.922 2.833 7.439-1.47 3.318-2.668 6.756-4.069 10.106-1.247 2.981-.435 5.242 2.413 6.544 2.805 1.282 3.125 3.14 1.813 5.601l-6.907 12.799L922 537z\"></path><path fill=\"#eb5659\" d=\"M1124.995 566c.868 1.396 2.018 2.691 2.559 4.203 4.353 12.185 12.586 22.041 21.084 31.113 7.746 8.269 14.235 17.422 21.675 25.711 6.176 6.881 15.756 10.707 24.174 15.932-6.073 22.316-16.675 42.446-31.058 60.937-1.074-.131-2.025-.199-2.581-.702l-24.462-22.26c-6.726-5.99-8.904-14.546-12.925-22.065-5.594-10.461-10.55-21.33-16.943-31.276-5.345-8.315-6.783-17.383-8.494-26.599-.63-3.394-1.348-6.772-1.738-10.848-.371-6.313-1.029-11.934-1.745-18.052l6.34 4.04 1.288-.675-2.143-15.385 9.454 1.208v-8.545L1124.995 566z\"></path><path fill=\"#f5a02d\" d=\"M1818.568 820.096c-4.224 21.679-15.302 40.442-26.587 58.942-3.167 5.192-9.389 8.582-14.381 12.585l-26.908 21.114c-1.425 1.104-3.042 2.345-4.732 2.662-11.786 2.214-21.99 8.201-32.586 13.273-.705.338-1.624.231-2.824.334a824.35 824.35 0 0 1-8.262-42.708c4.646-2.14 9.353-3.139 13.269-5.47 5.582-3.323 11.318-6.942 15.671-11.652 7.949-8.6 14.423-18.572 22.456-27.081 8.539-9.046 13.867-19.641 18.325-30.922l46.559 8.922z\"></path><path fill=\"#eb5a57\" d=\"M1124.96 565.639c-5.086-4.017-10.208-8.395-15.478-12.901v8.545l-9.454-1.208 2.143 15.385-1.288.675-6.34-4.04c.716 6.118 1.375 11.74 1.745 17.633-4.564-6.051-9.544-11.649-10.663-20.025-.954-7.141-4.892-13.843-7.121-20.863-3.344-10.533-5.421-21.57-9.732-31.669-5.181-12.135-3.506-25.125-6.728-37.355-2.099-7.968-5.317-15.646-7.324-23.632-1.353-5.384-1.47-11.078-2.429-16.909l-3.294-46.689a278.63 278.63 0 0 1 27.57-2.084c2.114 12.378 3.647 24.309 5.479 36.195 1.25 8.111 2.832 16.175 4.422 24.23 1.402 7.103 2.991 14.169 4.55 21.241 1.478 6.706.273 14.002 4.6 20.088 5.401 7.597 7.176 16.518 9.467 25.337 1.953 7.515 5.804 14.253 11.917 19.406.254 10.095 3.355 19.392 7.96 28.639z\"></path><path fill=\"#ea541c\" d=\"M911.651 810.999c-2.511 10.165-5.419 20.146-8.2 30.162-2.503 9.015-7.37 16.277-14.364 22.612-6.108 5.533-10.917 12.475-16.796 18.293-6.942 6.871-14.354 13.24-19.083 22.03-.644 1.196-2.222 1.889-3.705 2.857-2.39-7.921-4.101-15.991-6.566-23.823-5.451-17.323-12.404-33.976-23.414-48.835l21.627-21.095c3.182-3.29 5.532-7.382 8.295-11.083l10.663-14.163c9.528 4.78 18.925 9.848 28.625 14.247 7.324 3.321 15.036 5.785 22.917 8.799z\"></path><path fill=\"#eb5d19\" d=\"M1284.092 191.421c4.557.69 9.107 1.587 13.51 2.957 18.901 5.881 36.844 13.904 54.031 23.767 4.938 2.834 10.923 3.792 16.046 6.37 6.757 3.399 13.224 7.408 19.659 11.405l27.644 17.587c10.723 6.446 19.392 14.748 26.063 25.376 4.299 6.848 9.463 13.147 14.011 19.847 1.254 1.847 1.696 4.246 2.498 6.396l7.441 20.332c-11.685 1.754-23.379 3.133-35.533 4.037-.737-2.093-.995-3.716-1.294-5.33-3.157-17.057-14.048-30.161-23.034-44.146-3.027-4.71-7.786-8.529-12.334-11.993-9.346-7.116-19.004-13.834-28.688-20.491-6.653-4.573-13.311-9.251-20.431-13.002-8.048-4.24-16.479-7.85-24.989-11.091-11.722-4.465-23.673-8.328-35.527-12.449l.927-19.572z\"></path><path fill=\"#eb5e24\" d=\"M1283.09 211.415c11.928 3.699 23.88 7.562 35.602 12.027 8.509 3.241 16.941 6.852 24.989 11.091 7.12 3.751 13.778 8.429 20.431 13.002 9.684 6.657 19.342 13.375 28.688 20.491 4.548 3.463 9.307 7.283 12.334 11.993 8.986 13.985 19.877 27.089 23.034 44.146.299 1.615.557 3.237.836 5.263-13.373-.216-26.749-.839-40.564-1.923-2.935-9.681-4.597-18.92-12.286-26.152-15.577-14.651-30.4-30.102-45.564-45.193-.686-.683-1.626-1.156-2.516-1.584l-47.187-22.615 2.203-20.546z\"></path><path fill=\"#e9511f\" d=\"M913 486.001c-1.29.915-3.105 1.543-3.785 2.791-4.323 7.929-9.709 14.906-16.906 20.439-1.04.8-2.103 1.755-2.729 2.877-4.35 7.788-11.022 12.482-18.741 16.889-7.562 4.317-13.658 11.199-20.407 16.944-1.503 1.28-2.856 3.009-4.604 3.681-6.881 2.643-13.893 4.948-21.262 7.377-.128-11.151.202-22.302.378-33.454.03-1.892-.6-3.795-.456-6.12 13.727-1.755 23.588-9.527 33.278-17.663 2.784-2.337 6.074-4.161 8.529-6.784l29.057-31.86c1.545-1.71 3.418-3.401 4.221-5.459 5.665-14.509 11.49-28.977 16.436-43.736 2.817-8.407 4.074-17.338 6.033-26.032 5.039.714 10.078 1.427 15.536 2.629-.909 8.969-2.31 17.438-3.546 25.931-2.41 16.551-5.84 32.839-11.991 48.461L913 486.001z\"></path><path fill=\"#ea5741\" d=\"M1179.451 903.828c-14.224-5.787-27.726-12.171-37.235-24.849-5.841-7.787-12.09-15.436-19.146-22.099-7.259-6.854-12.136-14.667-15.035-24.049-1.748-5.654-3.938-11.171-6.254-17.033 15.099-4.009 30.213-8.629 44.958-15.533l28.367 36.36c6.09 8.015 13.124 14.75 22.72 18.375-7.404 14.472-13.599 29.412-17.48 45.244-.271 1.106-.382 2.25-.895 3.583z\"></path><path fill=\"#ea522a\" d=\"M913.32 486.141c2.693-7.837 5.694-15.539 8.722-23.231 6.151-15.622 9.581-31.91 11.991-48.461l3.963-25.861c7.582.317 15.168 1.031 22.748 1.797 4.171.421 8.333.928 12.877 1.596-.963 11.836-.398 24.125-4.102 34.953-5.244 15.33-6.794 31.496-12.521 46.578-2.692 7.09-4.849 14.445-8.203 21.206-4.068 8.201-9.311 15.81-13.708 23.86-1.965 3.597-3.154 7.627-4.609 11.492-1.385 3.68-3.666 6.265-8.114 6.89-1.994-1.511-3.624-3.059-5.077-4.44l6.907-12.799c1.313-2.461.993-4.318-1.813-5.601-2.849-1.302-3.66-3.563-2.413-6.544 1.401-3.35 2.599-6.788 4.069-10.106 1.558-3.517 1.071-5.948-2.833-7.439-2.617-1-5.049-2.484-7.884-3.892z\"></path><path fill=\"#eb5e24\" d=\"M376.574 714.118c12.053 6.538 20.723 16.481 29.081 26.814 1.945 2.404 4.537 4.352 7.047 6.218 8.24 6.125 10.544 15.85 14.942 24.299.974 1.871 1.584 3.931 2.376 6.29-7.145 3.719-14.633 6.501-21.386 10.517-9.606 5.713-18.673 12.334-28.425 18.399-3.407-3.73-6.231-7.409-9.335-10.834l-30.989-33.862c11.858-11.593 22.368-24.28 31.055-38.431 1.86-3.031 3.553-6.164 5.632-9.409z\"></path><path fill=\"#e95514\" d=\"M859.962 787.636c-3.409 5.037-6.981 9.745-10.516 14.481-2.763 3.701-5.113 7.792-8.295 11.083-6.885 7.118-14.186 13.834-21.65 20.755-13.222-17.677-29.417-31.711-48.178-42.878-.969-.576-2.068-.934-3.27-1.709 6.28-8.159 12.733-15.993 19.16-23.849 1.459-1.783 2.718-3.738 4.254-5.448l18.336-19.969c4.909 5.34 9.619 10.738 14.081 16.333 9.72 12.19 21.813 21.566 34.847 29.867.411.262.725.674 1.231 1.334z\"></path><path fill=\"#eb5f2d\" d=\"M339.582 762.088l31.293 33.733c3.104 3.425 5.928 7.104 9.024 10.979-12.885 11.619-24.548 24.139-33.899 38.704-.872 1.359-1.56 2.837-2.644 4.428-6.459-4.271-12.974-8.294-18.644-13.278-4.802-4.221-8.722-9.473-12.862-14.412l-17.921-21.896c-.403-.496-.595-1.163-.926-2.105 16.738-10.504 32.58-21.87 46.578-36.154z\"></path><path fill=\"#f28d00\" d=\"M678.388 332.912c1.989-5.104 3.638-10.664 6.876-15.051 8.903-12.064 17.596-24.492 28.013-35.175 11.607-11.904 25.007-22.064 40.507-29.592 4.873 11.636 9.419 23.412 13.67 35.592-5.759 4.084-11.517 7.403-16.594 11.553-4.413 3.607-8.124 8.092-12.023 12.301-5.346 5.772-10.82 11.454-15.782 17.547-3.929 4.824-7.17 10.208-10.716 15.344l-33.95-12.518z\"></path><path fill=\"#f08369\" d=\"M1580.181 771.427c-.191-.803-.322-1.377-.119-1.786 5.389-10.903 9.084-22.666 18.181-31.587 6.223-6.103 11.276-13.385 17.286-19.727 3.117-3.289 6.933-6.105 10.869-8.384 6.572-3.806 13.492-7.009 20.461-10.752 1.773 3.23 3.236 6.803 4.951 10.251l12.234 24.993c-1.367 1.966-2.596 3.293-3.935 4.499-7.845 7.07-16.315 13.564-23.407 21.32-6.971 7.623-12.552 16.517-18.743 24.854l-37.777-13.68z\"></path><path fill=\"#f18b5e\" d=\"M1618.142 785.4c6.007-8.63 11.588-17.524 18.559-25.147 7.092-7.755 15.562-14.249 23.407-21.32 1.338-1.206 2.568-2.534 3.997-4.162l28.996 33.733c1.896 2.205 4.424 3.867 6.66 6.394-6.471 7.492-12.967 14.346-19.403 21.255l-18.407 19.953c-12.958-12.409-27.485-22.567-43.809-30.706z\"></path><path fill=\"#f49c3a\" d=\"M1771.617 811.1c-4.066 11.354-9.394 21.949-17.933 30.995-8.032 8.509-14.507 18.481-22.456 27.081-4.353 4.71-10.089 8.329-15.671 11.652-3.915 2.331-8.623 3.331-13.318 5.069-4.298-9.927-8.255-19.998-12.1-30.743 4.741-4.381 9.924-7.582 13.882-11.904 7.345-8.021 14.094-16.603 20.864-25.131 4.897-6.168 9.428-12.626 14.123-18.955l32.61 11.936z\"></path><path fill=\"#f08000\" d=\"M712.601 345.675c3.283-5.381 6.524-10.765 10.453-15.589 4.962-6.093 10.435-11.774 15.782-17.547 3.899-4.21 7.61-8.695 12.023-12.301 5.078-4.15 10.836-7.469 16.636-11.19a934.12 934.12 0 0 1 23.286 35.848c-4.873 6.234-9.676 11.895-14.63 17.421l-25.195 27.801c-11.713-9.615-24.433-17.645-38.355-24.443z\"></path><path fill=\"#ed6e04\" d=\"M751.11 370.42c8.249-9.565 16.693-18.791 25.041-28.103 4.954-5.526 9.757-11.187 14.765-17.106 7.129 6.226 13.892 13.041 21.189 19.225 5.389 4.567 11.475 8.312 17.53 12.92-5.51 7.863-10.622 15.919-17.254 22.427-8.881 8.716-18.938 16.233-28.49 24.264-5.703-6.587-11.146-13.427-17.193-19.682-4.758-4.921-10.261-9.121-15.587-13.944z\"></path><path fill=\"#ea541c\" d=\"M921.823 385.544c-1.739 9.04-2.995 17.971-5.813 26.378-4.946 14.759-10.771 29.227-16.436 43.736-.804 2.058-2.676 3.749-4.221 5.459l-29.057 31.86c-2.455 2.623-5.745 4.447-8.529 6.784-9.69 8.135-19.551 15.908-33.208 17.237-1.773-9.728-3.147-19.457-4.091-29.6l36.13-16.763c.581-.267 1.046-.812 1.525-1.269 8.033-7.688 16.258-15.19 24.011-23.152 4.35-4.467 9.202-9.144 11.588-14.69 6.638-15.425 15.047-30.299 17.274-47.358 3.536.344 7.072.688 10.829 1.377z\"></path><path fill=\"#f3944d\" d=\"M1738.688 798.998c-4.375 6.495-8.906 12.953-13.803 19.121-6.771 8.528-13.519 17.11-20.864 25.131-3.958 4.322-9.141 7.523-13.925 11.54-8.036-13.464-16.465-26.844-27.999-38.387 5.988-6.951 12.094-13.629 18.261-20.25l19.547-20.95 38.783 23.794z\"></path><path fill=\"#ec6168\" d=\"M1239.583 703.142c3.282 1.805 6.441 3.576 9.217 5.821 5.88 4.755 11.599 9.713 17.313 14.669l22.373 19.723c1.781 1.622 3.2 3.806 4.307 5.975 3.843 7.532 7.477 15.171 11.194 23.136-10.764 4.67-21.532 8.973-32.69 12.982l-22.733-27.366c-2.003-2.416-4.096-4.758-6.194-7.093-3.539-3.94-6.927-8.044-10.74-11.701-2.57-2.465-5.762-4.283-8.675-6.39l16.627-29.755z\"></path><path fill=\"#ec663e\" d=\"M1351.006 332.839l-28.499 10.33c-.294.107-.533.367-1.194.264-11.067-19.018-27.026-32.559-44.225-44.855-4.267-3.051-8.753-5.796-13.138-8.682l9.505-24.505c10.055 4.069 19.821 8.227 29.211 13.108 3.998 2.078 7.299 5.565 10.753 8.598 3.077 2.701 5.743 5.891 8.926 8.447 4.116 3.304 9.787 5.345 12.62 9.432 6.083 8.777 10.778 18.517 16.041 27.863z\"></path><path fill=\"#eb5e5b\" d=\"M1222.647 733.051c3.223 1.954 6.415 3.771 8.985 6.237 3.813 3.658 7.201 7.761 10.74 11.701l6.194 7.093 22.384 27.409c-13.056 6.836-25.309 14.613-36.736 24.161l-39.323-44.7 24.494-27.846c1.072-1.224 1.974-2.598 3.264-4.056z\"></path><path fill=\"#ea580e\" d=\"M876.001 376.171c5.874 1.347 11.748 2.694 17.812 4.789-.81 5.265-2.687 9.791-2.639 14.296.124 11.469-4.458 20.383-12.73 27.863-2.075 1.877-3.659 4.286-5.668 6.248l-22.808 21.967c-.442.422-1.212.488-1.813.757l-23.113 10.389-9.875 4.514c-2.305-6.09-4.609-12.181-6.614-18.676 7.64-4.837 15.567-8.54 22.18-13.873 9.697-7.821 18.931-16.361 27.443-25.455 5.613-5.998 12.679-11.331 14.201-20.475.699-4.2 2.384-8.235 3.623-12.345z\"></path><path fill=\"#e95514\" d=\"M815.103 467.384c3.356-1.894 6.641-3.415 9.94-4.903l23.113-10.389c.6-.269 1.371-.335 1.813-.757l22.808-21.967c2.008-1.962 3.593-4.371 5.668-6.248 8.272-7.48 12.854-16.394 12.73-27.863-.049-4.505 1.828-9.031 2.847-13.956 5.427.559 10.836 1.526 16.609 2.68-1.863 17.245-10.272 32.119-16.91 47.544-2.387 5.546-7.239 10.223-11.588 14.69-7.753 7.962-15.978 15.464-24.011 23.152-.478.458-.944 1.002-1.525 1.269l-36.069 16.355c-2.076-6.402-3.783-12.81-5.425-19.607z\"></path><path fill=\"#eb620b\" d=\"M783.944 404.402c9.499-8.388 19.556-15.905 28.437-24.621 6.631-6.508 11.744-14.564 17.575-22.273 9.271 4.016 18.501 8.375 27.893 13.43-4.134 7.07-8.017 13.778-12.833 19.731-5.785 7.15-12.109 13.917-18.666 20.376-7.99 7.869-16.466 15.244-24.731 22.832l-17.674-29.475z\"></path><path fill=\"#ea544c\" d=\"M1197.986 854.686c-9.756-3.309-16.79-10.044-22.88-18.059l-28.001-36.417c8.601-5.939 17.348-11.563 26.758-17.075 1.615 1.026 2.639 1.876 3.505 2.865l26.664 30.44c3.723 4.139 7.995 7.785 12.017 11.656l-18.064 26.591z\"></path><path fill=\"#ec6333\" d=\"M1351.41 332.903c-5.667-9.409-10.361-19.149-16.445-27.926-2.833-4.087-8.504-6.128-12.62-9.432-3.184-2.555-5.849-5.745-8.926-8.447-3.454-3.033-6.756-6.52-10.753-8.598-9.391-4.88-19.157-9.039-29.138-13.499 1.18-5.441 2.727-10.873 4.81-16.607 11.918 4.674 24.209 8.261 34.464 14.962 14.239 9.304 29.011 18.453 39.595 32.464 2.386 3.159 5.121 6.077 7.884 8.923 6.564 6.764 10.148 14.927 11.723 24.093l-20.594 4.067z\"></path><path fill=\"#eb5e5b\" d=\"M1117 536.549c-6.113-4.702-9.965-11.44-11.917-18.955-2.292-8.819-4.066-17.74-9.467-25.337-4.327-6.085-3.122-13.382-4.6-20.088l-4.55-21.241c-1.59-8.054-3.172-16.118-4.422-24.23l-5.037-36.129c6.382-1.43 12.777-2.462 19.582-3.443 1.906 11.646 3.426 23.24 4.878 34.842.307 2.453.717 4.973.477 7.402-1.86 18.84 2.834 36.934 5.347 55.352 1.474 10.806 4.885 20.848 7.101 31.302 1.394 6.579 1.774 13.374 2.609 20.523z\"></path><path fill=\"#ec644b\" d=\"M1263.638 290.071c4.697 2.713 9.183 5.458 13.45 8.509 17.199 12.295 33.158 25.836 43.873 44.907-8.026 4.725-16.095 9.106-24.83 13.372-11.633-15.937-25.648-28.515-41.888-38.689-1.609-1.008-3.555-1.48-5.344-2.2 2.329-3.852 4.766-7.645 6.959-11.573l7.78-14.326z\"></path><path fill=\"#eb5f2d\" d=\"M1372.453 328.903c-2.025-9.233-5.608-17.396-12.172-24.16-2.762-2.846-5.498-5.764-7.884-8.923-10.584-14.01-25.356-23.16-39.595-32.464-10.256-6.701-22.546-10.289-34.284-15.312.325-5.246 1.005-10.444 2.027-15.863l47.529 22.394c.89.428 1.83.901 2.516 1.584l45.564 45.193c7.69 7.233 9.352 16.472 11.849 26.084-5.032.773-10.066 1.154-15.55 1.466z\"></path><path fill=\"#e95a0f\" d=\"M801.776 434.171c8.108-7.882 16.584-15.257 24.573-23.126 6.558-6.459 12.881-13.226 18.666-20.376 4.817-5.953 8.7-12.661 13.011-19.409 5.739 1.338 11.463 3.051 17.581 4.838-.845 4.183-2.53 8.219-3.229 12.418-1.522 9.144-8.588 14.477-14.201 20.475-8.512 9.094-17.745 17.635-27.443 25.455-6.613 5.333-14.54 9.036-22.223 13.51-2.422-4.469-4.499-8.98-6.735-13.786z\"></path><path fill=\"#eb5e5b\" d=\"M1248.533 316.002c2.155.688 4.101 1.159 5.71 2.168 16.24 10.174 30.255 22.752 41.532 38.727-7.166 5.736-14.641 11.319-22.562 16.731-1.16-1.277-1.684-2.585-2.615-3.46l-38.694-36.2 14.203-15.029c.803-.86 1.38-1.93 2.427-2.936z\"></path><path fill=\"#eb5a57\" d=\"M1216.359 827.958c-4.331-3.733-8.603-7.379-12.326-11.518l-26.664-30.44c-.866-.989-1.89-1.839-3.152-2.902 6.483-6.054 13.276-11.959 20.371-18.005l39.315 44.704c-5.648 6.216-11.441 12.12-17.544 18.161z\"></path><path fill=\"#ec6168\" d=\"M1231.598 334.101l38.999 36.066c.931.876 1.456 2.183 2.303 3.608-4.283 4.279-8.7 8.24-13.769 12.091-4.2-3.051-7.512-6.349-11.338-8.867-12.36-8.136-22.893-18.27-32.841-29.093l16.646-13.805z\"></path><path fill=\"#ed656e\" d=\"M1214.597 347.955c10.303 10.775 20.836 20.908 33.196 29.044 3.825 2.518 7.137 5.816 10.992 8.903-3.171 4.397-6.65 8.648-10.432 13.046-6.785-5.184-13.998-9.858-19.529-16.038-4.946-5.527-9.687-8.644-17.309-8.215-2.616.147-5.734-2.788-8.067-4.923-3.026-2.769-5.497-6.144-8.35-9.568 6.286-4.273 12.715-8.237 19.499-12.25z\"></path></svg>\n</p>\n\n<p align=\"center\">\n<b>The crispy rerank family from <a href=\"https://mixedbread.com\"><b>Mixedbread</b></a>.</b>\n</p>\n\n<p align=\"center\">\n<sup> \ud83c\udf5e Looking for a simple end-to-end retrieval solution? Meet Omni, our multimodal and multilingual model. <a href=\"https://mixedbread.com\"><b>Get in touch for access.</a> </sup>\n</p>\n  \n# mxbai-rerank-xsmall-v1\n\nThis is the smallest model in our family of powerful reranker models. You can learn more about the models in our [blog post](https://www.mixedbread.ai/blog/mxbai-rerank-v1).\n\nWe have three models:\n\n- [mxbai-rerank-xsmall-v1](https://huggingface.co/mixedbread-ai/mxbai-rerank-xsmall-v1) (\ud83c\udf5e)\n- [mxbai-rerank-base-v1](https://huggingface.co/mixedbread-ai/mxbai-rerank-base-v1)\n- [mxbai-rerank-large-v1](https://huggingface.co/mixedbread-ai/mxbai-rerank-large-v1)\n\n## Quickstart\n\nCurrently, the best way to use our models is with the most recent version of sentence-transformers.\n\n`pip install -U sentence-transformers`\n\nLet's say you have a query, and you want to rerank a set of documents. You can do that with only one line of code:\n\n```python\nfrom sentence_transformers import CrossEncoder\n\n# Load the model, here we use our base sized model\nmodel = CrossEncoder(\"mixedbread-ai/mxbai-rerank-xsmall-v1\")\n\n\n# Example query and documents\nquery = \"Who wrote 'To Kill a Mockingbird'?\"\ndocuments = [\n    \"'To Kill a Mockingbird' is a novel by Harper Lee published in 1960. It was immediately successful, winning the Pulitzer Prize, and has become a classic of modern American literature.\",\n    \"The novel 'Moby-Dick' was written by Herman Melville and first published in 1851. It is considered a masterpiece of American literature and deals with complex themes of obsession, revenge, and the conflict between good and evil.\",\n    \"Harper Lee, an American novelist widely known for her novel 'To Kill a Mockingbird', was born in 1926 in Monroeville, Alabama. She received the Pulitzer Prize for Fiction in 1961.\",\n    \"Jane Austen was an English novelist known primarily for her six major novels, which interpret, critique and comment upon the British landed gentry at the end of the 18th century.\",\n    \"The 'Harry Potter' series, which consists of seven fantasy novels written by British author J.K. Rowling, is among the most popular and critically acclaimed books of the modern era.\",\n    \"'The Great Gatsby', a novel written by American author F. Scott Fitzgerald, was published in 1925. The story is set in the Jazz Age and follows the life of millionaire Jay Gatsby and his pursuit of Daisy Buchanan.\"\n]\n\n# Lets get the scores\nresults = model.rank(query, documents, return_documents=True, top_k=3)\n```\n\n<details>\n  <summary>JavaScript Example</summary>\n\nInstall [transformers.js](https://github.com/xenova/transformers.js)\n\n`npm i @xenova/transformers`\n\nLet's say you have a query, and you want to rerank a set of documents. In JavaScript, you need to add a function:\n\n```javascript\nimport { AutoTokenizer, AutoModelForSequenceClassification } from '@xenova/transformers';\n\nconst model_id = 'mixedbread-ai/mxbai-rerank-xsmall-v1';\nconst model = await AutoModelForSequenceClassification.from_pretrained(model_id);\nconst tokenizer = await AutoTokenizer.from_pretrained(model_id);\n\n/**\n * Performs ranking with the CrossEncoder on the given query and documents. Returns a sorted list with the document indices and scores.\n * @param {string} query A single query\n * @param {string[]} documents A list of documents\n * @param {Object} options Options for ranking\n * @param {number} [options.top_k=undefined] Return the top-k documents. If undefined, all documents are returned.\n * @param {number} [options.return_documents=false] If true, also returns the documents. If false, only returns the indices and scores.\n */\nasync function rank(query, documents, {\n    top_k = undefined,\n    return_documents = false,\n} = {}) {\n    const inputs = tokenizer(\n        new Array(documents.length).fill(query),\n        {\n            text_pair: documents,\n            padding: true,\n            truncation: true,\n        }\n    )\n    const { logits } = await model(inputs);\n    return logits\n        .sigmoid()\n        .tolist()\n        .map(([score], i) => ({\n            corpus_id: i,\n            score,\n            ...(return_documents ? { text: documents[i] } : {})\n        }))\n        .sort((a, b) => b.score - a.score)\n        .slice(0, top_k);\n}\n\n// Example usage:\nconst query = \"Who wrote 'To Kill a Mockingbird'?\"\nconst documents = [\n    \"'To Kill a Mockingbird' is a novel by Harper Lee published in 1960. It was immediately successful, winning the Pulitzer Prize, and has become a classic of modern American literature.\",\n    \"The novel 'Moby-Dick' was written by Herman Melville and first published in 1851. It is considered a masterpiece of American literature and deals with complex themes of obsession, revenge, and the conflict between good and evil.\",\n    \"Harper Lee, an American novelist widely known for her novel 'To Kill a Mockingbird', was born in 1926 in Monroeville, Alabama. She received the Pulitzer Prize for Fiction in 1961.\",\n    \"Jane Austen was an English novelist known primarily for her six major novels, which interpret, critique and comment upon the British landed gentry at the end of the 18th century.\",\n    \"The 'Harry Potter' series, which consists of seven fantasy novels written by British author J.K. Rowling, is among the most popular and critically acclaimed books of the modern era.\",\n    \"'The Great Gatsby', a novel written by American author F. Scott Fitzgerald, was published in 1925. The story is set in the Jazz Age and follows the life of millionaire Jay Gatsby and his pursuit of Daisy Buchanan.\"\n]\n\nconst results = await rank(query, documents, { return_documents: true, top_k: 3 });\nconsole.log(results);\n```\n</details>\n\n## Using API\n\nYou can use the large model via our API as follows:\n\n```python\nfrom mixedbread_ai.client import MixedbreadAI\n\nmxbai = MixedbreadAI(api_key=\"{MIXEDBREAD_API_KEY}\")\n\nres = mxbai.reranking(\n  model=\"mixedbread-ai/mxbai-rerank-large-v1\",\n  query=\"Who is the author of To Kill a Mockingbird?\",\n  input=[\n    \"To Kill a Mockingbird is a novel by Harper Lee published in 1960. It was immediately successful, winning the Pulitzer Prize, and has become a classic of modern American literature.\",\n    \"The novel Moby-Dick was written by Herman Melville and first published in 1851. It is considered a masterpiece of American literature and deals with complex themes of obsession, revenge, and the conflict between good and evil.\",\n    \"Harper Lee, an American novelist widely known for her novel To Kill a Mockingbird, was born in 1926 in Monroeville, Alabama. She received the Pulitzer Prize for Fiction in 1961.\",\n    \"Jane Austen was an English novelist known primarily for her six major novels, which interpret, critique and comment upon the British landed gentry at the end of the 18th century.\",\n    \"The Harry Potter series, which consists of seven fantasy novels written by British author J.K. Rowling, is among the most popular and critically acclaimed books of the modern era.\",\n    \"The Great Gatsby, a novel written by American author F. Scott Fitzgerald, was published in 1925. The story is set in the Jazz Age and follows the life of millionaire Jay Gatsby and his pursuit of Daisy Buchanan.\"\n  ],\n  top_k=3,\n  return_input=false\n)\n\nprint(res.data)\n```\n\nThe API comes with additional features, such as a continous trained reranker! Check out the [docs](https://www.mixedbread.ai/docs) for more information.\n\n\n## Evaluation\n\nOur reranker models are designed to elevate your search. They work extremely well in combination with keyword search and can even outperform semantic search systems in many cases.\n\n| Model                                                                                 | NDCG@10  | Accuracy@3 |\n| ------------------------------------------------------------------------------------- | -------- | ---------- |\n| Lexical Search (Lucene)                                                               | 38.0     | 66.4       |\n| [BAAI/bge-reranker-base](https://huggingface.co/BAAI/bge-reranker-base)               | 41.6     | 66.9       |\n| [BAAI/bge-reranker-large](https://huggingface.co/BAAI/bge-reranker-large)             | 45.2     | 70.6       |\n| cohere-embed-v3 (semantic search)                                                     | 47.5     | 70.9       |\n| [mxbai-rerank-xsmall-v1](https://huggingface.co/mixedbread-ai/mxbai-rerank-xsmall-v1) | **43.9** | **70.0**   |\n| [mxbai-rerank-base-v1](https://huggingface.co/mixedbread-ai/mxbai-rerank-base-v1)     | **46.9** | **72.3**   |\n| [mxbai-rerank-large-v1](https://huggingface.co/mixedbread-ai/mxbai-rerank-large-v1)   | **48.8** | **74.9**   |\n\nThe reported results are aggregated from 11 datasets of BEIR. We used [Pyserini](https://github.com/castorini/pyserini/) to evaluate the models. Find more in our [blog-post](https://www.mixedbread.ai/blog/mxbai-rerank-v1) and on this [spreadsheet](https://docs.google.com/spreadsheets/d/15ELkSMFv-oHa5TRiIjDvhIstH9dlc3pnZeO-iGz4Ld4/edit?usp=sharing).\n\n## Community\nPlease join our [Discord Community](https://discord.gg/jDfMHzAVfU) and share your feedback and thoughts! We are here to help and also always happy to chat.\n\n## Citation\n\n```bibtex\n@online{rerank2024mxbai,\n  title={Boost Your Search With The Crispy Mixedbread Rerank Models},\n  author={Aamir Shakir and Darius Koenig and Julius Lipp and Sean Lee},\n  year={2024},\n  url={https://www.mixedbread.ai/blog/mxbai-rerank-v1},\n}\n```\n\n## License\nApache 2.0",
    "card_content": "---\nlanguage:\n- en\nlicense: apache-2.0\nlibrary_name: transformers\ntags:\n- reranker\n- transformers.js\n---\n<br><br>\n\n<p align=\"center\">\n<svg xmlns=\"http://www.w3.org/2000/svg\" xml:space=\"preserve\" viewBox=\"0 0 2020 1130\" width=\"150\" height=\"150\" aria-hidden=\"true\"><path fill=\"#e95a0f\" d=\"M398.167 621.992c-1.387-20.362-4.092-40.739-3.851-61.081.355-30.085 6.873-59.139 21.253-85.976 10.487-19.573 24.09-36.822 40.662-51.515 16.394-14.535 34.338-27.046 54.336-36.182 15.224-6.955 31.006-12.609 47.829-14.168 11.809-1.094 23.753-2.514 35.524-1.836 23.033 1.327 45.131 7.255 66.255 16.75 16.24 7.3 31.497 16.165 45.651 26.969 12.997 9.921 24.412 21.37 34.158 34.509 11.733 15.817 20.849 33.037 25.987 52.018 3.468 12.81 6.438 25.928 7.779 39.097 1.722 16.908 1.642 34.003 2.235 51.021.427 12.253.224 24.547 1.117 36.762 1.677 22.93 4.062 45.764 11.8 67.7 5.376 15.239 12.499 29.55 20.846 43.681l-18.282 20.328c-1.536 1.71-2.795 3.665-4.254 5.448l-19.323 23.533c-13.859-5.449-27.446-11.803-41.657-16.086-13.622-4.106-27.793-6.765-41.905-8.775-15.256-2.173-30.701-3.475-46.105-4.049-23.571-.879-47.178-1.056-70.769-1.029-10.858.013-21.723 1.116-32.57 1.926-5.362.4-10.69 1.255-16.464 1.477-2.758-7.675-5.284-14.865-7.367-22.181-3.108-10.92-4.325-22.554-13.16-31.095-2.598-2.512-5.069-5.341-6.883-8.443-6.366-10.884-12.48-21.917-18.571-32.959-4.178-7.573-8.411-14.375-17.016-18.559-10.34-5.028-19.538-12.387-29.311-18.611-3.173-2.021-6.414-4.312-9.952-5.297-5.857-1.63-11.98-2.301-17.991-3.376z\"></path><path fill=\"#ed6d7b\" d=\"M1478.998 758.842c-12.025.042-24.05.085-36.537-.373-.14-8.536.231-16.569.453-24.607.033-1.179-.315-2.986-1.081-3.4-.805-.434-2.376.338-3.518.81-.856.354-1.562 1.069-3.589 2.521-.239-3.308-.664-5.586-.519-7.827.488-7.544 2.212-15.166 1.554-22.589-1.016-11.451 1.397-14.592-12.332-14.419-3.793.048-3.617-2.803-3.332-5.331.499-4.422 1.45-8.803 1.77-13.233.311-4.316.068-8.672.068-12.861-2.554-.464-4.326-.86-6.12-1.098-4.415-.586-6.051-2.251-5.065-7.31 1.224-6.279.848-12.862 1.276-19.306.19-2.86-.971-4.473-3.794-4.753-4.113-.407-8.242-1.057-12.352-.975-4.663.093-5.192-2.272-4.751-6.012.733-6.229 1.252-12.483 1.875-18.726l1.102-10.495c-5.905-.309-11.146-.805-16.385-.778-3.32.017-5.174-1.4-5.566-4.4-1.172-8.968-2.479-17.944-3.001-26.96-.26-4.484-1.936-5.705-6.005-5.774-9.284-.158-18.563-.594-27.843-.953-7.241-.28-10.137-2.764-11.3-9.899-.746-4.576-2.715-7.801-7.777-8.207-7.739-.621-15.511-.992-23.207-1.961-7.327-.923-14.587-2.415-21.853-3.777-5.021-.941-10.003-2.086-15.003-3.14 4.515-22.952 13.122-44.382 26.284-63.587 18.054-26.344 41.439-47.239 69.102-63.294 15.847-9.197 32.541-16.277 50.376-20.599 16.655-4.036 33.617-5.715 50.622-4.385 33.334 2.606 63.836 13.955 92.415 31.15 15.864 9.545 30.241 20.86 42.269 34.758 8.113 9.374 15.201 19.78 21.718 30.359 10.772 17.484 16.846 36.922 20.611 56.991 1.783 9.503 2.815 19.214 3.318 28.876.758 14.578.755 29.196.65 44.311l-51.545 20.013c-7.779 3.059-15.847 5.376-21.753 12.365-4.73 5.598-10.658 10.316-16.547 14.774-9.9 7.496-18.437 15.988-25.083 26.631-3.333 5.337-7.901 10.381-12.999 14.038-11.355 8.144-17.397 18.973-19.615 32.423l-6.988 41.011z\"></path><path fill=\"#ec663e\" d=\"M318.11 923.047c-.702 17.693-.832 35.433-2.255 53.068-1.699 21.052-6.293 41.512-14.793 61.072-9.001 20.711-21.692 38.693-38.496 53.583-16.077 14.245-34.602 24.163-55.333 30.438-21.691 6.565-43.814 8.127-66.013 6.532-22.771-1.636-43.88-9.318-62.74-22.705-20.223-14.355-35.542-32.917-48.075-54.096-9.588-16.203-16.104-33.55-19.201-52.015-2.339-13.944-2.307-28.011-.403-42.182 2.627-19.545 9.021-37.699 17.963-55.067 11.617-22.564 27.317-41.817 48.382-56.118 15.819-10.74 33.452-17.679 52.444-20.455 8.77-1.282 17.696-1.646 26.568-2.055 11.755-.542 23.534-.562 35.289-1.11 8.545-.399 17.067-1.291 26.193-1.675 1.349 1.77 2.24 3.199 2.835 4.742 4.727 12.261 10.575 23.865 18.636 34.358 7.747 10.084 14.83 20.684 22.699 30.666 3.919 4.972 8.37 9.96 13.609 13.352 7.711 4.994 16.238 8.792 24.617 12.668 5.852 2.707 12.037 4.691 18.074 6.998z\"></path><path fill=\"#ea580e\" d=\"M1285.167 162.995c3.796-29.75 13.825-56.841 32.74-80.577 16.339-20.505 36.013-36.502 59.696-47.614 14.666-6.881 29.971-11.669 46.208-12.749 10.068-.669 20.239-1.582 30.255-.863 16.6 1.191 32.646 5.412 47.9 12.273 19.39 8.722 36.44 20.771 50.582 36.655 15.281 17.162 25.313 37.179 31.49 59.286 5.405 19.343 6.31 39.161 4.705 58.825-2.37 29.045-11.836 55.923-30.451 78.885-10.511 12.965-22.483 24.486-37.181 33.649-5.272-5.613-10.008-11.148-14.539-16.846-5.661-7.118-10.958-14.533-16.78-21.513-4.569-5.478-9.548-10.639-14.624-15.658-3.589-3.549-7.411-6.963-11.551-9.827-5.038-3.485-10.565-6.254-15.798-9.468-8.459-5.195-17.011-9.669-26.988-11.898-12.173-2.72-24.838-4.579-35.622-11.834-1.437-.967-3.433-1.192-5.213-1.542-12.871-2.529-25.454-5.639-36.968-12.471-5.21-3.091-11.564-4.195-17.011-6.965-4.808-2.445-8.775-6.605-13.646-8.851-8.859-4.085-18.114-7.311-27.204-10.896z\"></path><path fill=\"#f8ab00\" d=\"M524.963 311.12c-9.461-5.684-19.513-10.592-28.243-17.236-12.877-9.801-24.031-21.578-32.711-35.412-11.272-17.965-19.605-37.147-21.902-58.403-1.291-11.951-2.434-24.073-1.87-36.034.823-17.452 4.909-34.363 11.581-50.703 8.82-21.603 22.25-39.792 39.568-55.065 18.022-15.894 39.162-26.07 62.351-32.332 19.22-5.19 38.842-6.177 58.37-4.674 23.803 1.831 45.56 10.663 65.062 24.496 17.193 12.195 31.688 27.086 42.894 45.622-11.403 8.296-22.633 16.117-34.092 23.586-17.094 11.142-34.262 22.106-48.036 37.528-8.796 9.848-17.201 20.246-27.131 28.837-16.859 14.585-27.745 33.801-41.054 51.019-11.865 15.349-20.663 33.117-30.354 50.08-5.303 9.283-9.654 19.11-14.434 28.692z\"></path><path fill=\"#ea5227\" d=\"M1060.11 1122.049c-7.377 1.649-14.683 4.093-22.147 4.763-11.519 1.033-23.166 1.441-34.723 1.054-19.343-.647-38.002-4.7-55.839-12.65-15.078-6.72-28.606-15.471-40.571-26.836-24.013-22.81-42.053-49.217-49.518-81.936-1.446-6.337-1.958-12.958-2.235-19.477-.591-13.926-.219-27.909-1.237-41.795-.916-12.5-3.16-24.904-4.408-37.805 1.555-1.381 3.134-2.074 3.778-3.27 4.729-8.79 12.141-15.159 19.083-22.03 5.879-5.818 10.688-12.76 16.796-18.293 6.993-6.335 11.86-13.596 14.364-22.612l8.542-29.993c8.015 1.785 15.984 3.821 24.057 5.286 8.145 1.478 16.371 2.59 24.602 3.493 8.453.927 16.956 1.408 25.891 2.609 1.119 16.09 1.569 31.667 2.521 47.214.676 11.045 1.396 22.154 3.234 33.043 2.418 14.329 5.708 28.527 9.075 42.674 3.499 14.705 4.028 29.929 10.415 44.188 10.157 22.674 18.29 46.25 28.281 69.004 7.175 16.341 12.491 32.973 15.078 50.615.645 4.4 3.256 8.511 4.963 12.755z\"></path><path fill=\"#ea5330\" d=\"M1060.512 1122.031c-2.109-4.226-4.72-8.337-5.365-12.737-2.587-17.642-7.904-34.274-15.078-50.615-9.991-22.755-18.124-46.33-28.281-69.004-6.387-14.259-6.916-29.482-10.415-44.188-3.366-14.147-6.656-28.346-9.075-42.674-1.838-10.889-2.558-21.999-3.234-33.043-.951-15.547-1.401-31.124-2.068-47.146 8.568-.18 17.146.487 25.704.286l41.868-1.4c.907 3.746 1.245 7.04 1.881 10.276l8.651 42.704c.903 4.108 2.334 8.422 4.696 11.829 7.165 10.338 14.809 20.351 22.456 30.345 4.218 5.512 8.291 11.304 13.361 15.955 8.641 7.927 18.065 14.995 27.071 22.532 12.011 10.052 24.452 19.302 40.151 22.854-1.656 11.102-2.391 22.44-5.172 33.253-4.792 18.637-12.38 36.209-23.412 52.216-13.053 18.94-29.086 34.662-49.627 45.055-10.757 5.443-22.443 9.048-34.111 13.501z\"></path><path fill=\"#f8aa05\" d=\"M1989.106 883.951c5.198 8.794 11.46 17.148 15.337 26.491 5.325 12.833 9.744 26.207 12.873 39.737 2.95 12.757 3.224 25.908 1.987 39.219-1.391 14.973-4.643 29.268-10.349 43.034-5.775 13.932-13.477 26.707-23.149 38.405-14.141 17.104-31.215 30.458-50.807 40.488-14.361 7.352-29.574 12.797-45.741 14.594-10.297 1.144-20.732 2.361-31.031 1.894-24.275-1.1-47.248-7.445-68.132-20.263-6.096-3.741-11.925-7.917-17.731-12.342 5.319-5.579 10.361-10.852 15.694-15.811l37.072-34.009c.975-.892 2.113-1.606 3.08-2.505 6.936-6.448 14.765-12.2 20.553-19.556 8.88-11.285 20.064-19.639 31.144-28.292 4.306-3.363 9.06-6.353 12.673-10.358 5.868-6.504 10.832-13.814 16.422-20.582 6.826-8.264 13.727-16.481 20.943-24.401 4.065-4.461 8.995-8.121 13.249-12.424 14.802-14.975 28.77-30.825 45.913-43.317z\"></path><path fill=\"#ed6876\" d=\"M1256.099 523.419c5.065.642 10.047 1.787 15.068 2.728 7.267 1.362 14.526 2.854 21.853 3.777 7.696.97 15.468 1.34 23.207 1.961 5.062.406 7.031 3.631 7.777 8.207 1.163 7.135 4.059 9.62 11.3 9.899l27.843.953c4.069.069 5.745 1.291 6.005 5.774.522 9.016 1.829 17.992 3.001 26.96.392 3 2.246 4.417 5.566 4.4 5.239-.026 10.48.469 16.385.778l-1.102 10.495-1.875 18.726c-.44 3.74.088 6.105 4.751 6.012 4.11-.082 8.239.568 12.352.975 2.823.28 3.984 1.892 3.794 4.753-.428 6.444-.052 13.028-1.276 19.306-.986 5.059.651 6.724 5.065 7.31 1.793.238 3.566.634 6.12 1.098 0 4.189.243 8.545-.068 12.861-.319 4.43-1.27 8.811-1.77 13.233-.285 2.528-.461 5.379 3.332 5.331 13.729-.173 11.316 2.968 12.332 14.419.658 7.423-1.066 15.045-1.554 22.589-.145 2.241.28 4.519.519 7.827 2.026-1.452 2.733-2.167 3.589-2.521 1.142-.472 2.713-1.244 3.518-.81.767.414 1.114 2.221 1.081 3.4l-.917 24.539c-11.215.82-22.45.899-33.636 1.674l-43.952 3.436c-1.086-3.01-2.319-5.571-2.296-8.121.084-9.297-4.468-16.583-9.091-24.116-3.872-6.308-8.764-13.052-9.479-19.987-1.071-10.392-5.716-15.936-14.889-18.979-1.097-.364-2.16-.844-3.214-1.327-7.478-3.428-15.548-5.918-19.059-14.735-.904-2.27-3.657-3.775-5.461-5.723-2.437-2.632-4.615-5.525-7.207-7.987-2.648-2.515-5.352-5.346-8.589-6.777-4.799-2.121-10.074-3.185-15.175-4.596l-15.785-4.155c.274-12.896 1.722-25.901.54-38.662-1.647-17.783-3.457-35.526-2.554-53.352.528-10.426 2.539-20.777 3.948-31.574z\"></path><path fill=\"#f6a200\" d=\"M525.146 311.436c4.597-9.898 8.947-19.725 14.251-29.008 9.691-16.963 18.49-34.73 30.354-50.08 13.309-17.218 24.195-36.434 41.054-51.019 9.93-8.591 18.335-18.989 27.131-28.837 13.774-15.422 30.943-26.386 48.036-37.528 11.459-7.469 22.688-15.29 34.243-23.286 11.705 16.744 19.716 35.424 22.534 55.717 2.231 16.066 2.236 32.441 2.753 49.143-4.756 1.62-9.284 2.234-13.259 4.056-6.43 2.948-12.193 7.513-18.774 9.942-19.863 7.331-33.806 22.349-47.926 36.784-7.86 8.035-13.511 18.275-19.886 27.705-4.434 6.558-9.345 13.037-12.358 20.254-4.249 10.177-6.94 21.004-10.296 31.553-12.33.053-24.741 1.027-36.971-.049-20.259-1.783-40.227-5.567-58.755-14.69-.568-.28-1.295-.235-2.132-.658z\"></path><path fill=\"#f7a80d\" d=\"M1989.057 883.598c-17.093 12.845-31.061 28.695-45.863 43.67-4.254 4.304-9.184 7.963-13.249 12.424-7.216 7.92-14.117 16.137-20.943 24.401-5.59 6.768-10.554 14.078-16.422 20.582-3.614 4.005-8.367 6.995-12.673 10.358-11.08 8.653-22.264 17.007-31.144 28.292-5.788 7.356-13.617 13.108-20.553 19.556-.967.899-2.105 1.614-3.08 2.505l-37.072 34.009c-5.333 4.96-10.375 10.232-15.859 15.505-21.401-17.218-37.461-38.439-48.623-63.592 3.503-1.781 7.117-2.604 9.823-4.637 8.696-6.536 20.392-8.406 27.297-17.714.933-1.258 2.646-1.973 4.065-2.828 17.878-10.784 36.338-20.728 53.441-32.624 10.304-7.167 18.637-17.23 27.583-26.261 3.819-3.855 7.436-8.091 10.3-12.681 12.283-19.68 24.43-39.446 40.382-56.471 12.224-13.047 17.258-29.524 22.539-45.927 15.85 4.193 29.819 12.129 42.632 22.08 10.583 8.219 19.782 17.883 27.42 29.351z\"></path><path fill=\"#ef7a72\" d=\"M1479.461 758.907c1.872-13.734 4.268-27.394 6.525-41.076 2.218-13.45 8.26-24.279 19.615-32.423 5.099-3.657 9.667-8.701 12.999-14.038 6.646-10.643 15.183-19.135 25.083-26.631 5.888-4.459 11.817-9.176 16.547-14.774 5.906-6.99 13.974-9.306 21.753-12.365l51.48-19.549c.753 11.848.658 23.787 1.641 35.637 1.771 21.353 4.075 42.672 11.748 62.955.17.449.107.985-.019 2.158-6.945 4.134-13.865 7.337-20.437 11.143-3.935 2.279-7.752 5.096-10.869 8.384-6.011 6.343-11.063 13.624-17.286 19.727-9.096 8.92-12.791 20.684-18.181 31.587-.202.409-.072.984-.096 1.481-8.488-1.72-16.937-3.682-25.476-5.094-9.689-1.602-19.426-3.084-29.201-3.949-15.095-1.335-30.241-2.1-45.828-3.172z\"></path><path fill=\"#e94e3b\" d=\"M957.995 766.838c-20.337-5.467-38.791-14.947-55.703-27.254-8.2-5.967-15.451-13.238-22.958-20.37 2.969-3.504 5.564-6.772 8.598-9.563 7.085-6.518 11.283-14.914 15.8-23.153 4.933-8.996 10.345-17.743 14.966-26.892 2.642-5.231 5.547-11.01 5.691-16.611.12-4.651.194-8.932 2.577-12.742 8.52-13.621 15.483-28.026 18.775-43.704 2.11-10.049 7.888-18.774 7.81-29.825-.064-9.089 4.291-18.215 6.73-27.313 3.212-11.983 7.369-23.797 9.492-35.968 3.202-18.358 5.133-36.945 7.346-55.466l4.879-45.8c6.693.288 13.386.575 20.54 1.365.13 3.458-.41 6.407-.496 9.37l-1.136 42.595c-.597 11.552-2.067 23.058-3.084 34.59l-3.845 44.478c-.939 10.202-1.779 20.432-3.283 30.557-.96 6.464-4.46 12.646-1.136 19.383.348.706-.426 1.894-.448 2.864-.224 9.918-5.99 19.428-2.196 29.646.103.279-.033.657-.092.983l-8.446 46.205c-1.231 6.469-2.936 12.846-4.364 19.279-1.5 6.757-2.602 13.621-4.456 20.277-3.601 12.93-10.657 25.3-5.627 39.47.368 1.036.234 2.352.017 3.476l-5.949 30.123z\"></path><path fill=\"#ea5043\" d=\"M958.343 767.017c1.645-10.218 3.659-20.253 5.602-30.302.217-1.124.351-2.44-.017-3.476-5.03-14.17 2.026-26.539 5.627-39.47 1.854-6.656 2.956-13.52 4.456-20.277 1.428-6.433 3.133-12.81 4.364-19.279l8.446-46.205c.059-.326.196-.705.092-.983-3.794-10.218 1.972-19.728 2.196-29.646.022-.97.796-2.158.448-2.864-3.324-6.737.176-12.919 1.136-19.383 1.504-10.125 2.344-20.355 3.283-30.557l3.845-44.478c1.017-11.532 2.488-23.038 3.084-34.59.733-14.18.722-28.397 1.136-42.595.086-2.963.626-5.912.956-9.301 5.356-.48 10.714-.527 16.536-.081 2.224 15.098 1.855 29.734 1.625 44.408-.157 10.064 1.439 20.142 1.768 30.23.334 10.235-.035 20.49.116 30.733.084 5.713.789 11.418.861 17.13.054 4.289-.469 8.585-.702 12.879-.072 1.323-.138 2.659-.031 3.975l2.534 34.405-1.707 36.293-1.908 48.69c-.182 8.103.993 16.237.811 24.34-.271 12.076-1.275 24.133-1.787 36.207-.102 2.414-.101 5.283 1.06 7.219 4.327 7.22 4.463 15.215 4.736 23.103.365 10.553.088 21.128.086 31.693-11.44 2.602-22.84.688-34.106-.916-11.486-1.635-22.806-4.434-34.546-6.903z\"></path><path fill=\"#eb5d19\" d=\"M398.091 622.45c6.086.617 12.21 1.288 18.067 2.918 3.539.985 6.779 3.277 9.952 5.297 9.773 6.224 18.971 13.583 29.311 18.611 8.606 4.184 12.839 10.986 17.016 18.559l18.571 32.959c1.814 3.102 4.285 5.931 6.883 8.443 8.835 8.542 10.052 20.175 13.16 31.095 2.082 7.317 4.609 14.507 6.946 22.127-29.472 3.021-58.969 5.582-87.584 15.222-1.185-2.302-1.795-4.362-2.769-6.233-4.398-8.449-6.703-18.174-14.942-24.299-2.511-1.866-5.103-3.814-7.047-6.218-8.358-10.332-17.028-20.276-28.772-26.973 4.423-11.478 9.299-22.806 13.151-34.473 4.406-13.348 6.724-27.18 6.998-41.313.098-5.093.643-10.176 1.06-15.722z\"></path><path fill=\"#e94c32\" d=\"M981.557 392.109c-1.172 15.337-2.617 30.625-4.438 45.869-2.213 18.521-4.144 37.108-7.346 55.466-2.123 12.171-6.28 23.985-9.492 35.968-2.439 9.098-6.794 18.224-6.73 27.313.078 11.051-5.7 19.776-7.81 29.825-3.292 15.677-10.255 30.082-18.775 43.704-2.383 3.81-2.458 8.091-2.577 12.742-.144 5.6-3.049 11.38-5.691 16.611-4.621 9.149-10.033 17.896-14.966 26.892-4.517 8.239-8.715 16.635-15.8 23.153-3.034 2.791-5.629 6.06-8.735 9.255-12.197-10.595-21.071-23.644-29.301-37.24-7.608-12.569-13.282-25.962-17.637-40.37 13.303-6.889 25.873-13.878 35.311-25.315.717-.869 1.934-1.312 2.71-2.147 5.025-5.405 10.515-10.481 14.854-16.397 6.141-8.374 10.861-17.813 17.206-26.008 8.22-10.618 13.657-22.643 20.024-34.466 4.448-.626 6.729-3.21 8.114-6.89 1.455-3.866 2.644-7.895 4.609-11.492 4.397-8.05 9.641-15.659 13.708-23.86 3.354-6.761 5.511-14.116 8.203-21.206 5.727-15.082 7.277-31.248 12.521-46.578 3.704-10.828 3.138-23.116 4.478-34.753l7.56-.073z\"></path><path fill=\"#f7a617\" d=\"M1918.661 831.99c-4.937 16.58-9.971 33.057-22.196 46.104-15.952 17.025-28.099 36.791-40.382 56.471-2.864 4.59-6.481 8.825-10.3 12.681-8.947 9.031-17.279 19.094-27.583 26.261-17.103 11.896-35.564 21.84-53.441 32.624-1.419.856-3.132 1.571-4.065 2.828-6.904 9.308-18.6 11.178-27.297 17.714-2.705 2.033-6.319 2.856-9.874 4.281-3.413-9.821-6.916-19.583-9.36-29.602-1.533-6.284-1.474-12.957-1.665-19.913 1.913-.78 3.374-1.057 4.81-1.431 15.822-4.121 31.491-8.029 43.818-20.323 9.452-9.426 20.371-17.372 30.534-26.097 6.146-5.277 13.024-10.052 17.954-16.326 14.812-18.848 28.876-38.285 43.112-57.581 2.624-3.557 5.506-7.264 6.83-11.367 2.681-8.311 4.375-16.94 6.476-25.438 17.89.279 35.333 3.179 52.629 9.113z\"></path><path fill=\"#ea553a\" d=\"M1172.91 977.582c-15.775-3.127-28.215-12.377-40.227-22.43-9.005-7.537-18.43-14.605-27.071-22.532-5.07-4.651-9.143-10.443-13.361-15.955-7.647-9.994-15.291-20.007-22.456-30.345-2.361-3.407-3.792-7.72-4.696-11.829-3.119-14.183-5.848-28.453-8.651-42.704-.636-3.236-.974-6.53-1.452-10.209 15.234-2.19 30.471-3.969 46.408-5.622 2.692 5.705 4.882 11.222 6.63 16.876 2.9 9.381 7.776 17.194 15.035 24.049 7.056 6.662 13.305 14.311 19.146 22.099 9.509 12.677 23.01 19.061 36.907 25.054-1.048 7.441-2.425 14.854-3.066 22.33-.956 11.162-1.393 22.369-2.052 33.557l-1.096 17.661z\"></path><path fill=\"#ea5453\" d=\"M1163.123 704.036c-4.005 5.116-7.685 10.531-12.075 15.293-12.842 13.933-27.653 25.447-44.902 34.538-3.166-5.708-5.656-11.287-8.189-17.251-3.321-12.857-6.259-25.431-9.963-37.775-4.6-15.329-10.6-30.188-11.349-46.562-.314-6.871-1.275-14.287-7.114-19.644-1.047-.961-1.292-3.053-1.465-4.67l-4.092-39.927c-.554-5.245-.383-10.829-2.21-15.623-3.622-9.503-4.546-19.253-4.688-29.163-.088-6.111 1.068-12.256.782-18.344-.67-14.281-1.76-28.546-2.9-42.8-.657-8.222-1.951-16.395-2.564-24.62-.458-6.137-.285-12.322-.104-18.21.959 5.831 1.076 11.525 2.429 16.909 2.007 7.986 5.225 15.664 7.324 23.632 3.222 12.23 1.547 25.219 6.728 37.355 4.311 10.099 6.389 21.136 9.732 31.669 2.228 7.02 6.167 13.722 7.121 20.863 1.119 8.376 6.1 13.974 10.376 20.716l2.026 10.576c1.711 9.216 3.149 18.283 8.494 26.599 6.393 9.946 11.348 20.815 16.943 31.276 4.021 7.519 6.199 16.075 12.925 22.065l24.462 22.26c.556.503 1.507.571 2.274.841z\"></path><path fill=\"#ea5b15\" d=\"M1285.092 163.432c9.165 3.148 18.419 6.374 27.279 10.459 4.871 2.246 8.838 6.406 13.646 8.851 5.446 2.77 11.801 3.874 17.011 6.965 11.514 6.831 24.097 9.942 36.968 12.471 1.78.35 3.777.576 5.213 1.542 10.784 7.255 23.448 9.114 35.622 11.834 9.977 2.23 18.529 6.703 26.988 11.898 5.233 3.214 10.76 5.983 15.798 9.468 4.14 2.864 7.962 6.279 11.551 9.827 5.076 5.02 10.056 10.181 14.624 15.658 5.822 6.98 11.119 14.395 16.78 21.513 4.531 5.698 9.267 11.233 14.222 16.987-10.005 5.806-20.07 12.004-30.719 16.943-7.694 3.569-16.163 5.464-24.688 7.669-2.878-7.088-5.352-13.741-7.833-20.392-.802-2.15-1.244-4.55-2.498-6.396-4.548-6.7-9.712-12.999-14.011-19.847-6.672-10.627-15.34-18.93-26.063-25.376-9.357-5.625-18.367-11.824-27.644-17.587-6.436-3.997-12.902-8.006-19.659-11.405-5.123-2.577-11.107-3.536-16.046-6.37-17.187-9.863-35.13-17.887-54.031-23.767-4.403-1.37-8.953-2.267-13.436-3.382l.926-27.565z\"></path><path fill=\"#ea504b\" d=\"M1098 737l7.789 16.893c-15.04 9.272-31.679 15.004-49.184 17.995-9.464 1.617-19.122 2.097-29.151 3.019-.457-10.636-.18-21.211-.544-31.764-.273-7.888-.409-15.883-4.736-23.103-1.16-1.936-1.162-4.805-1.06-7.219l1.787-36.207c.182-8.103-.993-16.237-.811-24.34.365-16.236 1.253-32.461 1.908-48.69.484-12 .942-24.001 1.98-36.069 5.57 10.19 10.632 20.42 15.528 30.728 1.122 2.362 2.587 5.09 2.339 7.488-1.536 14.819 5.881 26.839 12.962 38.33 10.008 16.241 16.417 33.54 20.331 51.964 2.285 10.756 4.729 21.394 11.958 30.165L1098 737z\"></path><path fill=\"#f6a320\" d=\"M1865.78 822.529c-1.849 8.846-3.544 17.475-6.224 25.786-1.323 4.102-4.206 7.81-6.83 11.367l-43.112 57.581c-4.93 6.273-11.808 11.049-17.954 16.326-10.162 8.725-21.082 16.671-30.534 26.097-12.327 12.294-27.997 16.202-43.818 20.323-1.436.374-2.897.651-4.744.986-1.107-17.032-1.816-34.076-2.079-51.556 1.265-.535 2.183-.428 2.888-.766 10.596-5.072 20.8-11.059 32.586-13.273 1.69-.317 3.307-1.558 4.732-2.662l26.908-21.114c4.992-4.003 11.214-7.393 14.381-12.585 11.286-18.5 22.363-37.263 27.027-58.87l36.046 1.811c3.487.165 6.983.14 10.727.549z\"></path><path fill=\"#ec6333\" d=\"M318.448 922.814c-6.374-2.074-12.56-4.058-18.412-6.765-8.379-3.876-16.906-7.675-24.617-12.668-5.239-3.392-9.69-8.381-13.609-13.352-7.87-9.983-14.953-20.582-22.699-30.666-8.061-10.493-13.909-22.097-18.636-34.358-.595-1.543-1.486-2.972-2.382-4.783 6.84-1.598 13.797-3.023 20.807-4.106 18.852-2.912 36.433-9.493 53.737-17.819.697.888.889 1.555 1.292 2.051l17.921 21.896c4.14 4.939 8.06 10.191 12.862 14.412 5.67 4.984 12.185 9.007 18.334 13.447-8.937 16.282-16.422 33.178-20.696 51.31-1.638 6.951-2.402 14.107-3.903 21.403z\"></path><path fill=\"#f49700\" d=\"M623.467 326.903c2.893-10.618 5.584-21.446 9.833-31.623 3.013-7.217 7.924-13.696 12.358-20.254 6.375-9.43 12.026-19.67 19.886-27.705 14.12-14.434 28.063-29.453 47.926-36.784 6.581-2.429 12.344-6.994 18.774-9.942 3.975-1.822 8.503-2.436 13.186-3.592 1.947 18.557 3.248 37.15 8.307 55.686-15.453 7.931-28.853 18.092-40.46 29.996-10.417 10.683-19.109 23.111-28.013 35.175-3.238 4.388-4.888 9.948-7.262 14.973-17.803-3.987-35.767-6.498-54.535-5.931z\"></path><path fill=\"#ea544c\" d=\"M1097.956 736.615c-2.925-3.218-5.893-6.822-8.862-10.425-7.229-8.771-9.672-19.409-11.958-30.165-3.914-18.424-10.323-35.722-20.331-51.964-7.081-11.491-14.498-23.511-12.962-38.33.249-2.398-1.217-5.126-2.339-7.488l-15.232-31.019-3.103-34.338c-.107-1.316-.041-2.653.031-3.975.233-4.294.756-8.59.702-12.879-.072-5.713-.776-11.417-.861-17.13l-.116-30.733c-.329-10.088-1.926-20.166-1.768-30.23.23-14.674.599-29.31-1.162-44.341 9.369-.803 18.741-1.179 28.558-1.074 1.446 15.814 2.446 31.146 3.446 46.478.108 6.163-.064 12.348.393 18.485.613 8.225 1.907 16.397 2.564 24.62l2.9 42.8c.286 6.088-.869 12.234-.782 18.344.142 9.91 1.066 19.661 4.688 29.163 1.827 4.794 1.657 10.377 2.21 15.623l4.092 39.927c.172 1.617.417 3.71 1.465 4.67 5.839 5.357 6.8 12.773 7.114 19.644.749 16.374 6.749 31.233 11.349 46.562 3.704 12.344 6.642 24.918 9.963 37.775z\"></path><path fill=\"#ec5c61\" d=\"M1204.835 568.008c1.254 25.351-1.675 50.16-10.168 74.61-8.598-4.883-18.177-8.709-24.354-15.59-7.44-8.289-13.929-17.442-21.675-25.711-8.498-9.072-16.731-18.928-21.084-31.113-.54-1.513-1.691-2.807-2.594-4.564-4.605-9.247-7.706-18.544-7.96-29.09-.835-7.149-1.214-13.944-2.609-20.523-2.215-10.454-5.626-20.496-7.101-31.302-2.513-18.419-7.207-36.512-5.347-55.352.24-2.43-.17-4.949-.477-7.402l-4.468-34.792c2.723-.379 5.446-.757 8.585-.667 1.749 8.781 2.952 17.116 4.448 25.399 1.813 10.037 3.64 20.084 5.934 30.017 1.036 4.482 3.953 8.573 4.73 13.064 1.794 10.377 4.73 20.253 9.272 29.771 2.914 6.105 4.761 12.711 7.496 18.912 2.865 6.496 6.264 12.755 9.35 19.156 3.764 7.805 7.667 15.013 16.1 19.441 7.527 3.952 13.713 10.376 20.983 14.924 6.636 4.152 13.932 7.25 20.937 10.813z\"></path><path fill=\"#ed676f\" d=\"M1140.75 379.231c18.38-4.858 36.222-11.21 53.979-18.971 3.222 3.368 5.693 6.744 8.719 9.512 2.333 2.134 5.451 5.07 8.067 4.923 7.623-.429 12.363 2.688 17.309 8.215 5.531 6.18 12.744 10.854 19.224 16.184-5.121 7.193-10.461 14.241-15.323 21.606-13.691 20.739-22.99 43.255-26.782 67.926-.543 3.536-1.281 7.043-2.366 10.925-14.258-6.419-26.411-14.959-32.731-29.803-1.087-2.553-2.596-4.93-3.969-7.355-1.694-2.993-3.569-5.89-5.143-8.943-1.578-3.062-2.922-6.249-4.295-9.413-1.57-3.621-3.505-7.163-4.47-10.946-1.257-4.93-.636-10.572-2.725-15.013-5.831-12.397-7.467-25.628-9.497-38.847z\"></path><path fill=\"#ed656e\" d=\"M1254.103 647.439c5.325.947 10.603 2.272 15.847 3.722 5.101 1.41 10.376 2.475 15.175 4.596 3.237 1.431 5.942 4.262 8.589 6.777 2.592 2.462 4.77 5.355 7.207 7.987 1.804 1.948 4.557 3.453 5.461 5.723 3.51 8.817 11.581 11.307 19.059 14.735 1.053.483 2.116.963 3.214 1.327 9.172 3.043 13.818 8.587 14.889 18.979.715 6.935 5.607 13.679 9.479 19.987 4.623 7.533 9.175 14.819 9.091 24.116-.023 2.55 1.21 5.111 1.874 8.055-19.861 2.555-39.795 4.296-59.597 9.09l-11.596-23.203c-1.107-2.169-2.526-4.353-4.307-5.975-7.349-6.694-14.863-13.209-22.373-19.723l-17.313-14.669c-2.776-2.245-5.935-4.017-8.92-6.003l11.609-38.185c1.508-5.453 1.739-11.258 2.613-17.336z\"></path><path fill=\"#ec6168\" d=\"M1140.315 379.223c2.464 13.227 4.101 26.459 9.931 38.856 2.089 4.441 1.468 10.083 2.725 15.013.965 3.783 2.9 7.325 4.47 10.946 1.372 3.164 2.716 6.351 4.295 9.413 1.574 3.053 3.449 5.95 5.143 8.943 1.372 2.425 2.882 4.803 3.969 7.355 6.319 14.844 18.473 23.384 32.641 30.212.067 5.121-.501 10.201-.435 15.271l.985 38.117c.151 4.586.616 9.162.868 14.201-7.075-3.104-14.371-6.202-21.007-10.354-7.269-4.548-13.456-10.972-20.983-14.924-8.434-4.428-12.337-11.637-16.1-19.441-3.087-6.401-6.485-12.66-9.35-19.156-2.735-6.201-4.583-12.807-7.496-18.912-4.542-9.518-7.477-19.394-9.272-29.771-.777-4.491-3.694-8.581-4.73-13.064-2.294-9.933-4.121-19.98-5.934-30.017-1.496-8.283-2.699-16.618-4.036-25.335 10.349-2.461 20.704-4.511 31.054-6.582.957-.191 1.887-.515 3.264-.769z\"></path><path fill=\"#e94c28\" d=\"M922 537c-6.003 11.784-11.44 23.81-19.66 34.428-6.345 8.196-11.065 17.635-17.206 26.008-4.339 5.916-9.828 10.992-14.854 16.397-.776.835-1.993 1.279-2.71 2.147-9.439 11.437-22.008 18.427-35.357 24.929-4.219-10.885-6.942-22.155-7.205-33.905l-.514-49.542c7.441-2.893 14.452-5.197 21.334-7.841 1.749-.672 3.101-2.401 4.604-3.681 6.749-5.745 12.845-12.627 20.407-16.944 7.719-4.406 14.391-9.101 18.741-16.889.626-1.122 1.689-2.077 2.729-2.877 7.197-5.533 12.583-12.51 16.906-20.439.68-1.247 2.495-1.876 4.105-2.651 2.835 1.408 5.267 2.892 7.884 3.892 3.904 1.491 4.392 3.922 2.833 7.439-1.47 3.318-2.668 6.756-4.069 10.106-1.247 2.981-.435 5.242 2.413 6.544 2.805 1.282 3.125 3.14 1.813 5.601l-6.907 12.799L922 537z\"></path><path fill=\"#eb5659\" d=\"M1124.995 566c.868 1.396 2.018 2.691 2.559 4.203 4.353 12.185 12.586 22.041 21.084 31.113 7.746 8.269 14.235 17.422 21.675 25.711 6.176 6.881 15.756 10.707 24.174 15.932-6.073 22.316-16.675 42.446-31.058 60.937-1.074-.131-2.025-.199-2.581-.702l-24.462-22.26c-6.726-5.99-8.904-14.546-12.925-22.065-5.594-10.461-10.55-21.33-16.943-31.276-5.345-8.315-6.783-17.383-8.494-26.599-.63-3.394-1.348-6.772-1.738-10.848-.371-6.313-1.029-11.934-1.745-18.052l6.34 4.04 1.288-.675-2.143-15.385 9.454 1.208v-8.545L1124.995 566z\"></path><path fill=\"#f5a02d\" d=\"M1818.568 820.096c-4.224 21.679-15.302 40.442-26.587 58.942-3.167 5.192-9.389 8.582-14.381 12.585l-26.908 21.114c-1.425 1.104-3.042 2.345-4.732 2.662-11.786 2.214-21.99 8.201-32.586 13.273-.705.338-1.624.231-2.824.334a824.35 824.35 0 0 1-8.262-42.708c4.646-2.14 9.353-3.139 13.269-5.47 5.582-3.323 11.318-6.942 15.671-11.652 7.949-8.6 14.423-18.572 22.456-27.081 8.539-9.046 13.867-19.641 18.325-30.922l46.559 8.922z\"></path><path fill=\"#eb5a57\" d=\"M1124.96 565.639c-5.086-4.017-10.208-8.395-15.478-12.901v8.545l-9.454-1.208 2.143 15.385-1.288.675-6.34-4.04c.716 6.118 1.375 11.74 1.745 17.633-4.564-6.051-9.544-11.649-10.663-20.025-.954-7.141-4.892-13.843-7.121-20.863-3.344-10.533-5.421-21.57-9.732-31.669-5.181-12.135-3.506-25.125-6.728-37.355-2.099-7.968-5.317-15.646-7.324-23.632-1.353-5.384-1.47-11.078-2.429-16.909l-3.294-46.689a278.63 278.63 0 0 1 27.57-2.084c2.114 12.378 3.647 24.309 5.479 36.195 1.25 8.111 2.832 16.175 4.422 24.23 1.402 7.103 2.991 14.169 4.55 21.241 1.478 6.706.273 14.002 4.6 20.088 5.401 7.597 7.176 16.518 9.467 25.337 1.953 7.515 5.804 14.253 11.917 19.406.254 10.095 3.355 19.392 7.96 28.639z\"></path><path fill=\"#ea541c\" d=\"M911.651 810.999c-2.511 10.165-5.419 20.146-8.2 30.162-2.503 9.015-7.37 16.277-14.364 22.612-6.108 5.533-10.917 12.475-16.796 18.293-6.942 6.871-14.354 13.24-19.083 22.03-.644 1.196-2.222 1.889-3.705 2.857-2.39-7.921-4.101-15.991-6.566-23.823-5.451-17.323-12.404-33.976-23.414-48.835l21.627-21.095c3.182-3.29 5.532-7.382 8.295-11.083l10.663-14.163c9.528 4.78 18.925 9.848 28.625 14.247 7.324 3.321 15.036 5.785 22.917 8.799z\"></path><path fill=\"#eb5d19\" d=\"M1284.092 191.421c4.557.69 9.107 1.587 13.51 2.957 18.901 5.881 36.844 13.904 54.031 23.767 4.938 2.834 10.923 3.792 16.046 6.37 6.757 3.399 13.224 7.408 19.659 11.405l27.644 17.587c10.723 6.446 19.392 14.748 26.063 25.376 4.299 6.848 9.463 13.147 14.011 19.847 1.254 1.847 1.696 4.246 2.498 6.396l7.441 20.332c-11.685 1.754-23.379 3.133-35.533 4.037-.737-2.093-.995-3.716-1.294-5.33-3.157-17.057-14.048-30.161-23.034-44.146-3.027-4.71-7.786-8.529-12.334-11.993-9.346-7.116-19.004-13.834-28.688-20.491-6.653-4.573-13.311-9.251-20.431-13.002-8.048-4.24-16.479-7.85-24.989-11.091-11.722-4.465-23.673-8.328-35.527-12.449l.927-19.572z\"></path><path fill=\"#eb5e24\" d=\"M1283.09 211.415c11.928 3.699 23.88 7.562 35.602 12.027 8.509 3.241 16.941 6.852 24.989 11.091 7.12 3.751 13.778 8.429 20.431 13.002 9.684 6.657 19.342 13.375 28.688 20.491 4.548 3.463 9.307 7.283 12.334 11.993 8.986 13.985 19.877 27.089 23.034 44.146.299 1.615.557 3.237.836 5.263-13.373-.216-26.749-.839-40.564-1.923-2.935-9.681-4.597-18.92-12.286-26.152-15.577-14.651-30.4-30.102-45.564-45.193-.686-.683-1.626-1.156-2.516-1.584l-47.187-22.615 2.203-20.546z\"></path><path fill=\"#e9511f\" d=\"M913 486.001c-1.29.915-3.105 1.543-3.785 2.791-4.323 7.929-9.709 14.906-16.906 20.439-1.04.8-2.103 1.755-2.729 2.877-4.35 7.788-11.022 12.482-18.741 16.889-7.562 4.317-13.658 11.199-20.407 16.944-1.503 1.28-2.856 3.009-4.604 3.681-6.881 2.643-13.893 4.948-21.262 7.377-.128-11.151.202-22.302.378-33.454.03-1.892-.6-3.795-.456-6.12 13.727-1.755 23.588-9.527 33.278-17.663 2.784-2.337 6.074-4.161 8.529-6.784l29.057-31.86c1.545-1.71 3.418-3.401 4.221-5.459 5.665-14.509 11.49-28.977 16.436-43.736 2.817-8.407 4.074-17.338 6.033-26.032 5.039.714 10.078 1.427 15.536 2.629-.909 8.969-2.31 17.438-3.546 25.931-2.41 16.551-5.84 32.839-11.991 48.461L913 486.001z\"></path><path fill=\"#ea5741\" d=\"M1179.451 903.828c-14.224-5.787-27.726-12.171-37.235-24.849-5.841-7.787-12.09-15.436-19.146-22.099-7.259-6.854-12.136-14.667-15.035-24.049-1.748-5.654-3.938-11.171-6.254-17.033 15.099-4.009 30.213-8.629 44.958-15.533l28.367 36.36c6.09 8.015 13.124 14.75 22.72 18.375-7.404 14.472-13.599 29.412-17.48 45.244-.271 1.106-.382 2.25-.895 3.583z\"></path><path fill=\"#ea522a\" d=\"M913.32 486.141c2.693-7.837 5.694-15.539 8.722-23.231 6.151-15.622 9.581-31.91 11.991-48.461l3.963-25.861c7.582.317 15.168 1.031 22.748 1.797 4.171.421 8.333.928 12.877 1.596-.963 11.836-.398 24.125-4.102 34.953-5.244 15.33-6.794 31.496-12.521 46.578-2.692 7.09-4.849 14.445-8.203 21.206-4.068 8.201-9.311 15.81-13.708 23.86-1.965 3.597-3.154 7.627-4.609 11.492-1.385 3.68-3.666 6.265-8.114 6.89-1.994-1.511-3.624-3.059-5.077-4.44l6.907-12.799c1.313-2.461.993-4.318-1.813-5.601-2.849-1.302-3.66-3.563-2.413-6.544 1.401-3.35 2.599-6.788 4.069-10.106 1.558-3.517 1.071-5.948-2.833-7.439-2.617-1-5.049-2.484-7.884-3.892z\"></path><path fill=\"#eb5e24\" d=\"M376.574 714.118c12.053 6.538 20.723 16.481 29.081 26.814 1.945 2.404 4.537 4.352 7.047 6.218 8.24 6.125 10.544 15.85 14.942 24.299.974 1.871 1.584 3.931 2.376 6.29-7.145 3.719-14.633 6.501-21.386 10.517-9.606 5.713-18.673 12.334-28.425 18.399-3.407-3.73-6.231-7.409-9.335-10.834l-30.989-33.862c11.858-11.593 22.368-24.28 31.055-38.431 1.86-3.031 3.553-6.164 5.632-9.409z\"></path><path fill=\"#e95514\" d=\"M859.962 787.636c-3.409 5.037-6.981 9.745-10.516 14.481-2.763 3.701-5.113 7.792-8.295 11.083-6.885 7.118-14.186 13.834-21.65 20.755-13.222-17.677-29.417-31.711-48.178-42.878-.969-.576-2.068-.934-3.27-1.709 6.28-8.159 12.733-15.993 19.16-23.849 1.459-1.783 2.718-3.738 4.254-5.448l18.336-19.969c4.909 5.34 9.619 10.738 14.081 16.333 9.72 12.19 21.813 21.566 34.847 29.867.411.262.725.674 1.231 1.334z\"></path><path fill=\"#eb5f2d\" d=\"M339.582 762.088l31.293 33.733c3.104 3.425 5.928 7.104 9.024 10.979-12.885 11.619-24.548 24.139-33.899 38.704-.872 1.359-1.56 2.837-2.644 4.428-6.459-4.271-12.974-8.294-18.644-13.278-4.802-4.221-8.722-9.473-12.862-14.412l-17.921-21.896c-.403-.496-.595-1.163-.926-2.105 16.738-10.504 32.58-21.87 46.578-36.154z\"></path><path fill=\"#f28d00\" d=\"M678.388 332.912c1.989-5.104 3.638-10.664 6.876-15.051 8.903-12.064 17.596-24.492 28.013-35.175 11.607-11.904 25.007-22.064 40.507-29.592 4.873 11.636 9.419 23.412 13.67 35.592-5.759 4.084-11.517 7.403-16.594 11.553-4.413 3.607-8.124 8.092-12.023 12.301-5.346 5.772-10.82 11.454-15.782 17.547-3.929 4.824-7.17 10.208-10.716 15.344l-33.95-12.518z\"></path><path fill=\"#f08369\" d=\"M1580.181 771.427c-.191-.803-.322-1.377-.119-1.786 5.389-10.903 9.084-22.666 18.181-31.587 6.223-6.103 11.276-13.385 17.286-19.727 3.117-3.289 6.933-6.105 10.869-8.384 6.572-3.806 13.492-7.009 20.461-10.752 1.773 3.23 3.236 6.803 4.951 10.251l12.234 24.993c-1.367 1.966-2.596 3.293-3.935 4.499-7.845 7.07-16.315 13.564-23.407 21.32-6.971 7.623-12.552 16.517-18.743 24.854l-37.777-13.68z\"></path><path fill=\"#f18b5e\" d=\"M1618.142 785.4c6.007-8.63 11.588-17.524 18.559-25.147 7.092-7.755 15.562-14.249 23.407-21.32 1.338-1.206 2.568-2.534 3.997-4.162l28.996 33.733c1.896 2.205 4.424 3.867 6.66 6.394-6.471 7.492-12.967 14.346-19.403 21.255l-18.407 19.953c-12.958-12.409-27.485-22.567-43.809-30.706z\"></path><path fill=\"#f49c3a\" d=\"M1771.617 811.1c-4.066 11.354-9.394 21.949-17.933 30.995-8.032 8.509-14.507 18.481-22.456 27.081-4.353 4.71-10.089 8.329-15.671 11.652-3.915 2.331-8.623 3.331-13.318 5.069-4.298-9.927-8.255-19.998-12.1-30.743 4.741-4.381 9.924-7.582 13.882-11.904 7.345-8.021 14.094-16.603 20.864-25.131 4.897-6.168 9.428-12.626 14.123-18.955l32.61 11.936z\"></path><path fill=\"#f08000\" d=\"M712.601 345.675c3.283-5.381 6.524-10.765 10.453-15.589 4.962-6.093 10.435-11.774 15.782-17.547 3.899-4.21 7.61-8.695 12.023-12.301 5.078-4.15 10.836-7.469 16.636-11.19a934.12 934.12 0 0 1 23.286 35.848c-4.873 6.234-9.676 11.895-14.63 17.421l-25.195 27.801c-11.713-9.615-24.433-17.645-38.355-24.443z\"></path><path fill=\"#ed6e04\" d=\"M751.11 370.42c8.249-9.565 16.693-18.791 25.041-28.103 4.954-5.526 9.757-11.187 14.765-17.106 7.129 6.226 13.892 13.041 21.189 19.225 5.389 4.567 11.475 8.312 17.53 12.92-5.51 7.863-10.622 15.919-17.254 22.427-8.881 8.716-18.938 16.233-28.49 24.264-5.703-6.587-11.146-13.427-17.193-19.682-4.758-4.921-10.261-9.121-15.587-13.944z\"></path><path fill=\"#ea541c\" d=\"M921.823 385.544c-1.739 9.04-2.995 17.971-5.813 26.378-4.946 14.759-10.771 29.227-16.436 43.736-.804 2.058-2.676 3.749-4.221 5.459l-29.057 31.86c-2.455 2.623-5.745 4.447-8.529 6.784-9.69 8.135-19.551 15.908-33.208 17.237-1.773-9.728-3.147-19.457-4.091-29.6l36.13-16.763c.581-.267 1.046-.812 1.525-1.269 8.033-7.688 16.258-15.19 24.011-23.152 4.35-4.467 9.202-9.144 11.588-14.69 6.638-15.425 15.047-30.299 17.274-47.358 3.536.344 7.072.688 10.829 1.377z\"></path><path fill=\"#f3944d\" d=\"M1738.688 798.998c-4.375 6.495-8.906 12.953-13.803 19.121-6.771 8.528-13.519 17.11-20.864 25.131-3.958 4.322-9.141 7.523-13.925 11.54-8.036-13.464-16.465-26.844-27.999-38.387 5.988-6.951 12.094-13.629 18.261-20.25l19.547-20.95 38.783 23.794z\"></path><path fill=\"#ec6168\" d=\"M1239.583 703.142c3.282 1.805 6.441 3.576 9.217 5.821 5.88 4.755 11.599 9.713 17.313 14.669l22.373 19.723c1.781 1.622 3.2 3.806 4.307 5.975 3.843 7.532 7.477 15.171 11.194 23.136-10.764 4.67-21.532 8.973-32.69 12.982l-22.733-27.366c-2.003-2.416-4.096-4.758-6.194-7.093-3.539-3.94-6.927-8.044-10.74-11.701-2.57-2.465-5.762-4.283-8.675-6.39l16.627-29.755z\"></path><path fill=\"#ec663e\" d=\"M1351.006 332.839l-28.499 10.33c-.294.107-.533.367-1.194.264-11.067-19.018-27.026-32.559-44.225-44.855-4.267-3.051-8.753-5.796-13.138-8.682l9.505-24.505c10.055 4.069 19.821 8.227 29.211 13.108 3.998 2.078 7.299 5.565 10.753 8.598 3.077 2.701 5.743 5.891 8.926 8.447 4.116 3.304 9.787 5.345 12.62 9.432 6.083 8.777 10.778 18.517 16.041 27.863z\"></path><path fill=\"#eb5e5b\" d=\"M1222.647 733.051c3.223 1.954 6.415 3.771 8.985 6.237 3.813 3.658 7.201 7.761 10.74 11.701l6.194 7.093 22.384 27.409c-13.056 6.836-25.309 14.613-36.736 24.161l-39.323-44.7 24.494-27.846c1.072-1.224 1.974-2.598 3.264-4.056z\"></path><path fill=\"#ea580e\" d=\"M876.001 376.171c5.874 1.347 11.748 2.694 17.812 4.789-.81 5.265-2.687 9.791-2.639 14.296.124 11.469-4.458 20.383-12.73 27.863-2.075 1.877-3.659 4.286-5.668 6.248l-22.808 21.967c-.442.422-1.212.488-1.813.757l-23.113 10.389-9.875 4.514c-2.305-6.09-4.609-12.181-6.614-18.676 7.64-4.837 15.567-8.54 22.18-13.873 9.697-7.821 18.931-16.361 27.443-25.455 5.613-5.998 12.679-11.331 14.201-20.475.699-4.2 2.384-8.235 3.623-12.345z\"></path><path fill=\"#e95514\" d=\"M815.103 467.384c3.356-1.894 6.641-3.415 9.94-4.903l23.113-10.389c.6-.269 1.371-.335 1.813-.757l22.808-21.967c2.008-1.962 3.593-4.371 5.668-6.248 8.272-7.48 12.854-16.394 12.73-27.863-.049-4.505 1.828-9.031 2.847-13.956 5.427.559 10.836 1.526 16.609 2.68-1.863 17.245-10.272 32.119-16.91 47.544-2.387 5.546-7.239 10.223-11.588 14.69-7.753 7.962-15.978 15.464-24.011 23.152-.478.458-.944 1.002-1.525 1.269l-36.069 16.355c-2.076-6.402-3.783-12.81-5.425-19.607z\"></path><path fill=\"#eb620b\" d=\"M783.944 404.402c9.499-8.388 19.556-15.905 28.437-24.621 6.631-6.508 11.744-14.564 17.575-22.273 9.271 4.016 18.501 8.375 27.893 13.43-4.134 7.07-8.017 13.778-12.833 19.731-5.785 7.15-12.109 13.917-18.666 20.376-7.99 7.869-16.466 15.244-24.731 22.832l-17.674-29.475z\"></path><path fill=\"#ea544c\" d=\"M1197.986 854.686c-9.756-3.309-16.79-10.044-22.88-18.059l-28.001-36.417c8.601-5.939 17.348-11.563 26.758-17.075 1.615 1.026 2.639 1.876 3.505 2.865l26.664 30.44c3.723 4.139 7.995 7.785 12.017 11.656l-18.064 26.591z\"></path><path fill=\"#ec6333\" d=\"M1351.41 332.903c-5.667-9.409-10.361-19.149-16.445-27.926-2.833-4.087-8.504-6.128-12.62-9.432-3.184-2.555-5.849-5.745-8.926-8.447-3.454-3.033-6.756-6.52-10.753-8.598-9.391-4.88-19.157-9.039-29.138-13.499 1.18-5.441 2.727-10.873 4.81-16.607 11.918 4.674 24.209 8.261 34.464 14.962 14.239 9.304 29.011 18.453 39.595 32.464 2.386 3.159 5.121 6.077 7.884 8.923 6.564 6.764 10.148 14.927 11.723 24.093l-20.594 4.067z\"></path><path fill=\"#eb5e5b\" d=\"M1117 536.549c-6.113-4.702-9.965-11.44-11.917-18.955-2.292-8.819-4.066-17.74-9.467-25.337-4.327-6.085-3.122-13.382-4.6-20.088l-4.55-21.241c-1.59-8.054-3.172-16.118-4.422-24.23l-5.037-36.129c6.382-1.43 12.777-2.462 19.582-3.443 1.906 11.646 3.426 23.24 4.878 34.842.307 2.453.717 4.973.477 7.402-1.86 18.84 2.834 36.934 5.347 55.352 1.474 10.806 4.885 20.848 7.101 31.302 1.394 6.579 1.774 13.374 2.609 20.523z\"></path><path fill=\"#ec644b\" d=\"M1263.638 290.071c4.697 2.713 9.183 5.458 13.45 8.509 17.199 12.295 33.158 25.836 43.873 44.907-8.026 4.725-16.095 9.106-24.83 13.372-11.633-15.937-25.648-28.515-41.888-38.689-1.609-1.008-3.555-1.48-5.344-2.2 2.329-3.852 4.766-7.645 6.959-11.573l7.78-14.326z\"></path><path fill=\"#eb5f2d\" d=\"M1372.453 328.903c-2.025-9.233-5.608-17.396-12.172-24.16-2.762-2.846-5.498-5.764-7.884-8.923-10.584-14.01-25.356-23.16-39.595-32.464-10.256-6.701-22.546-10.289-34.284-15.312.325-5.246 1.005-10.444 2.027-15.863l47.529 22.394c.89.428 1.83.901 2.516 1.584l45.564 45.193c7.69 7.233 9.352 16.472 11.849 26.084-5.032.773-10.066 1.154-15.55 1.466z\"></path><path fill=\"#e95a0f\" d=\"M801.776 434.171c8.108-7.882 16.584-15.257 24.573-23.126 6.558-6.459 12.881-13.226 18.666-20.376 4.817-5.953 8.7-12.661 13.011-19.409 5.739 1.338 11.463 3.051 17.581 4.838-.845 4.183-2.53 8.219-3.229 12.418-1.522 9.144-8.588 14.477-14.201 20.475-8.512 9.094-17.745 17.635-27.443 25.455-6.613 5.333-14.54 9.036-22.223 13.51-2.422-4.469-4.499-8.98-6.735-13.786z\"></path><path fill=\"#eb5e5b\" d=\"M1248.533 316.002c2.155.688 4.101 1.159 5.71 2.168 16.24 10.174 30.255 22.752 41.532 38.727-7.166 5.736-14.641 11.319-22.562 16.731-1.16-1.277-1.684-2.585-2.615-3.46l-38.694-36.2 14.203-15.029c.803-.86 1.38-1.93 2.427-2.936z\"></path><path fill=\"#eb5a57\" d=\"M1216.359 827.958c-4.331-3.733-8.603-7.379-12.326-11.518l-26.664-30.44c-.866-.989-1.89-1.839-3.152-2.902 6.483-6.054 13.276-11.959 20.371-18.005l39.315 44.704c-5.648 6.216-11.441 12.12-17.544 18.161z\"></path><path fill=\"#ec6168\" d=\"M1231.598 334.101l38.999 36.066c.931.876 1.456 2.183 2.303 3.608-4.283 4.279-8.7 8.24-13.769 12.091-4.2-3.051-7.512-6.349-11.338-8.867-12.36-8.136-22.893-18.27-32.841-29.093l16.646-13.805z\"></path><path fill=\"#ed656e\" d=\"M1214.597 347.955c10.303 10.775 20.836 20.908 33.196 29.044 3.825 2.518 7.137 5.816 10.992 8.903-3.171 4.397-6.65 8.648-10.432 13.046-6.785-5.184-13.998-9.858-19.529-16.038-4.946-5.527-9.687-8.644-17.309-8.215-2.616.147-5.734-2.788-8.067-4.923-3.026-2.769-5.497-6.144-8.35-9.568 6.286-4.273 12.715-8.237 19.499-12.25z\"></path></svg>\n</p>\n\n<p align=\"center\">\n<b>The crispy rerank family from <a href=\"https://mixedbread.com\"><b>Mixedbread</b></a>.</b>\n</p>\n\n<p align=\"center\">\n<sup> \ud83c\udf5e Looking for a simple end-to-end retrieval solution? Meet Omni, our multimodal and multilingual model. <a href=\"https://mixedbread.com\"><b>Get in touch for access.</a> </sup>\n</p>\n  \n# mxbai-rerank-xsmall-v1\n\nThis is the smallest model in our family of powerful reranker models. You can learn more about the models in our [blog post](https://www.mixedbread.ai/blog/mxbai-rerank-v1).\n\nWe have three models:\n\n- [mxbai-rerank-xsmall-v1](https://huggingface.co/mixedbread-ai/mxbai-rerank-xsmall-v1) (\ud83c\udf5e)\n- [mxbai-rerank-base-v1](https://huggingface.co/mixedbread-ai/mxbai-rerank-base-v1)\n- [mxbai-rerank-large-v1](https://huggingface.co/mixedbread-ai/mxbai-rerank-large-v1)\n\n## Quickstart\n\nCurrently, the best way to use our models is with the most recent version of sentence-transformers.\n\n`pip install -U sentence-transformers`\n\nLet's say you have a query, and you want to rerank a set of documents. You can do that with only one line of code:\n\n```python\nfrom sentence_transformers import CrossEncoder\n\n# Load the model, here we use our base sized model\nmodel = CrossEncoder(\"mixedbread-ai/mxbai-rerank-xsmall-v1\")\n\n\n# Example query and documents\nquery = \"Who wrote 'To Kill a Mockingbird'?\"\ndocuments = [\n    \"'To Kill a Mockingbird' is a novel by Harper Lee published in 1960. It was immediately successful, winning the Pulitzer Prize, and has become a classic of modern American literature.\",\n    \"The novel 'Moby-Dick' was written by Herman Melville and first published in 1851. It is considered a masterpiece of American literature and deals with complex themes of obsession, revenge, and the conflict between good and evil.\",\n    \"Harper Lee, an American novelist widely known for her novel 'To Kill a Mockingbird', was born in 1926 in Monroeville, Alabama. She received the Pulitzer Prize for Fiction in 1961.\",\n    \"Jane Austen was an English novelist known primarily for her six major novels, which interpret, critique and comment upon the British landed gentry at the end of the 18th century.\",\n    \"The 'Harry Potter' series, which consists of seven fantasy novels written by British author J.K. Rowling, is among the most popular and critically acclaimed books of the modern era.\",\n    \"'The Great Gatsby', a novel written by American author F. Scott Fitzgerald, was published in 1925. The story is set in the Jazz Age and follows the life of millionaire Jay Gatsby and his pursuit of Daisy Buchanan.\"\n]\n\n# Lets get the scores\nresults = model.rank(query, documents, return_documents=True, top_k=3)\n```\n\n<details>\n  <summary>JavaScript Example</summary>\n\nInstall [transformers.js](https://github.com/xenova/transformers.js)\n\n`npm i @xenova/transformers`\n\nLet's say you have a query, and you want to rerank a set of documents. In JavaScript, you need to add a function:\n\n```javascript\nimport { AutoTokenizer, AutoModelForSequenceClassification } from '@xenova/transformers';\n\nconst model_id = 'mixedbread-ai/mxbai-rerank-xsmall-v1';\nconst model = await AutoModelForSequenceClassification.from_pretrained(model_id);\nconst tokenizer = await AutoTokenizer.from_pretrained(model_id);\n\n/**\n * Performs ranking with the CrossEncoder on the given query and documents. Returns a sorted list with the document indices and scores.\n * @param {string} query A single query\n * @param {string[]} documents A list of documents\n * @param {Object} options Options for ranking\n * @param {number} [options.top_k=undefined] Return the top-k documents. If undefined, all documents are returned.\n * @param {number} [options.return_documents=false] If true, also returns the documents. If false, only returns the indices and scores.\n */\nasync function rank(query, documents, {\n    top_k = undefined,\n    return_documents = false,\n} = {}) {\n    const inputs = tokenizer(\n        new Array(documents.length).fill(query),\n        {\n            text_pair: documents,\n            padding: true,\n            truncation: true,\n        }\n    )\n    const { logits } = await model(inputs);\n    return logits\n        .sigmoid()\n        .tolist()\n        .map(([score], i) => ({\n            corpus_id: i,\n            score,\n            ...(return_documents ? { text: documents[i] } : {})\n        }))\n        .sort((a, b) => b.score - a.score)\n        .slice(0, top_k);\n}\n\n// Example usage:\nconst query = \"Who wrote 'To Kill a Mockingbird'?\"\nconst documents = [\n    \"'To Kill a Mockingbird' is a novel by Harper Lee published in 1960. It was immediately successful, winning the Pulitzer Prize, and has become a classic of modern American literature.\",\n    \"The novel 'Moby-Dick' was written by Herman Melville and first published in 1851. It is considered a masterpiece of American literature and deals with complex themes of obsession, revenge, and the conflict between good and evil.\",\n    \"Harper Lee, an American novelist widely known for her novel 'To Kill a Mockingbird', was born in 1926 in Monroeville, Alabama. She received the Pulitzer Prize for Fiction in 1961.\",\n    \"Jane Austen was an English novelist known primarily for her six major novels, which interpret, critique and comment upon the British landed gentry at the end of the 18th century.\",\n    \"The 'Harry Potter' series, which consists of seven fantasy novels written by British author J.K. Rowling, is among the most popular and critically acclaimed books of the modern era.\",\n    \"'The Great Gatsby', a novel written by American author F. Scott Fitzgerald, was published in 1925. The story is set in the Jazz Age and follows the life of millionaire Jay Gatsby and his pursuit of Daisy Buchanan.\"\n]\n\nconst results = await rank(query, documents, { return_documents: true, top_k: 3 });\nconsole.log(results);\n```\n</details>\n\n## Using API\n\nYou can use the large model via our API as follows:\n\n```python\nfrom mixedbread_ai.client import MixedbreadAI\n\nmxbai = MixedbreadAI(api_key=\"{MIXEDBREAD_API_KEY}\")\n\nres = mxbai.reranking(\n  model=\"mixedbread-ai/mxbai-rerank-large-v1\",\n  query=\"Who is the author of To Kill a Mockingbird?\",\n  input=[\n    \"To Kill a Mockingbird is a novel by Harper Lee published in 1960. It was immediately successful, winning the Pulitzer Prize, and has become a classic of modern American literature.\",\n    \"The novel Moby-Dick was written by Herman Melville and first published in 1851. It is considered a masterpiece of American literature and deals with complex themes of obsession, revenge, and the conflict between good and evil.\",\n    \"Harper Lee, an American novelist widely known for her novel To Kill a Mockingbird, was born in 1926 in Monroeville, Alabama. She received the Pulitzer Prize for Fiction in 1961.\",\n    \"Jane Austen was an English novelist known primarily for her six major novels, which interpret, critique and comment upon the British landed gentry at the end of the 18th century.\",\n    \"The Harry Potter series, which consists of seven fantasy novels written by British author J.K. Rowling, is among the most popular and critically acclaimed books of the modern era.\",\n    \"The Great Gatsby, a novel written by American author F. Scott Fitzgerald, was published in 1925. The story is set in the Jazz Age and follows the life of millionaire Jay Gatsby and his pursuit of Daisy Buchanan.\"\n  ],\n  top_k=3,\n  return_input=false\n)\n\nprint(res.data)\n```\n\nThe API comes with additional features, such as a continous trained reranker! Check out the [docs](https://www.mixedbread.ai/docs) for more information.\n\n\n## Evaluation\n\nOur reranker models are designed to elevate your search. They work extremely well in combination with keyword search and can even outperform semantic search systems in many cases.\n\n| Model                                                                                 | NDCG@10  | Accuracy@3 |\n| ------------------------------------------------------------------------------------- | -------- | ---------- |\n| Lexical Search (Lucene)                                                               | 38.0     | 66.4       |\n| [BAAI/bge-reranker-base](https://huggingface.co/BAAI/bge-reranker-base)               | 41.6     | 66.9       |\n| [BAAI/bge-reranker-large](https://huggingface.co/BAAI/bge-reranker-large)             | 45.2     | 70.6       |\n| cohere-embed-v3 (semantic search)                                                     | 47.5     | 70.9       |\n| [mxbai-rerank-xsmall-v1](https://huggingface.co/mixedbread-ai/mxbai-rerank-xsmall-v1) | **43.9** | **70.0**   |\n| [mxbai-rerank-base-v1](https://huggingface.co/mixedbread-ai/mxbai-rerank-base-v1)     | **46.9** | **72.3**   |\n| [mxbai-rerank-large-v1](https://huggingface.co/mixedbread-ai/mxbai-rerank-large-v1)   | **48.8** | **74.9**   |\n\nThe reported results are aggregated from 11 datasets of BEIR. We used [Pyserini](https://github.com/castorini/pyserini/) to evaluate the models. Find more in our [blog-post](https://www.mixedbread.ai/blog/mxbai-rerank-v1) and on this [spreadsheet](https://docs.google.com/spreadsheets/d/15ELkSMFv-oHa5TRiIjDvhIstH9dlc3pnZeO-iGz4Ld4/edit?usp=sharing).\n\n## Community\nPlease join our [Discord Community](https://discord.gg/jDfMHzAVfU) and share your feedback and thoughts! We are here to help and also always happy to chat.\n\n## Citation\n\n```bibtex\n@online{rerank2024mxbai,\n  title={Boost Your Search With The Crispy Mixedbread Rerank Models},\n  author={Aamir Shakir and Darius Koenig and Julius Lipp and Sean Lee},\n  year={2024},\n  url={https://www.mixedbread.ai/blog/mxbai-rerank-v1},\n}\n```\n\n## License\nApache 2.0",
    "library_name": "transformers"
  },
  {
    "model_id": "madhurjindal/autonlp-Gibberish-Detector-492513457",
    "model_name": "madhurjindal/autonlp-Gibberish-Detector-492513457",
    "author": "madhurjindal",
    "downloads": 368627,
    "likes": 57,
    "tags": [
      "transformers",
      "pytorch",
      "onnx",
      "safetensors",
      "distilbert",
      "text-classification",
      "autonlp",
      "en",
      "dataset:madhurjindal/autonlp-data-Gibberish-Detector",
      "doi:10.57967/hf/2664",
      "license:mit",
      "co2_eq_emissions",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/madhurjindal/autonlp-Gibberish-Detector-492513457",
    "dependencies": [
      [
        "torch",
        "2.0.1"
      ],
      [
        "transformers",
        "4.30.2"
      ],
      [
        "numpy",
        "1.24.3"
      ],
      [
        "xformers",
        "0.0.22"
      ]
    ],
    "analysis_date": "2025-03-26T00:48:21.432934",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "distilbert",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": "en",
      "license": "mit",
      "tags": [
        "autonlp"
      ],
      "datasets": [
        "madhurjindal/autonlp-data-Gibberish-Detector"
      ],
      "widget": [
        {
          "text": "I love Machine Learning!"
        }
      ],
      "co2_eq_emissions": 5.527544460835904
    },
    "card_text": "\n# Problem Description\nThe ability to process and understand user input is crucial for various applications, such as chatbots or downstream tasks. However, a common challenge faced in such systems is the presence of gibberish or nonsensical input. To address this problem, we present a project focused on developing a gibberish detector for the English language.\nThe primary goal of this project is to classify user input as either **gibberish** or **non-gibberish**, enabling more accurate and meaningful interactions with the system. We also aim to enhance the overall performance and user experience of chatbots and other systems that rely on user input.\n\n>## What is Gibberish?\nGibberish refers to **nonsensical or meaningless language or text** that lacks coherence or any discernible meaning. It can be characterized by a combination of random words, nonsensical phrases, grammatical errors, or syntactical abnormalities that prevent the communication from conveying a clear and understandable message. Gibberish can vary in intensity, ranging from simple noise with no meaningful words to sentences that may appear superficially correct but lack coherence or logical structure when examined closely. Detecting and identifying gibberish is essential in various contexts, such as **natural language processing**, **chatbot systems**, **spam filtering**, and **language-based security measures**, to ensure effective communication and accurate processing of user inputs.\n\n## Label Description\nThus, we break down the problem into 4 categories:\n\n1. **Noise:** Gibberish at the zero level where even the different constituents of the input phrase (words) do not hold any meaning independently.  \n   *For example: `dfdfer fgerfow2e0d qsqskdsd djksdnfkff swq.`*\n   \n2. **Word Salad:** Gibberish at level 1 where words make sense independently, but when looked at the bigger picture (the phrase) any meaning is not depicted.  \n   *For example: `22 madhur old punjab pickle chennai`*\n\n3. **Mild gibberish:** Gibberish at level 2 where there is a part of the sentence that has grammatical errors, word sense errors, or any syntactical abnormalities, which leads the sentence to miss out on a coherent meaning.  \n   *For example: `Madhur study in a teacher`*\n\n4. **Clean:** This category represents a set of words that form a complete and meaningful sentence on its own.  \n   *For example: `I love this website`*\n\n> **Tip:** To facilitate gibberish detection, you can combine the labels based on the desired level of detection. For instance, if you need to detect gibberish at level 1, you can group Noise and Word Salad together as \"Gibberish,\" while considering Mild gibberish and Clean separately as \"NotGibberish.\" This approach allows for flexibility in detecting and categorizing different levels of gibberish based on specific requirements.\n\n\n# Model Trained Using AutoNLP\n\n- Problem type: Multi-class Classification\n- Model ID: 492513457\n- CO2 Emissions (in grams): 5.527544460835904\n\n## Validation Metrics\n\n- Loss: 0.07609463483095169\n- Accuracy: 0.9735624586913417\n- Macro F1: 0.9736173135739408\n- Micro F1: 0.9735624586913417\n- Weighted F1: 0.9736173135739408\n- Macro Precision: 0.9737771415197378\n- Micro Precision: 0.9735624586913417\n- Weighted Precision: 0.9737771415197378\n- Macro Recall: 0.9735624586913417\n- Micro Recall: 0.9735624586913417\n- Weighted Recall: 0.9735624586913417\n\n\n## Usage\n\nYou can use cURL to access this model:\n\n```\n$ curl -X POST -H \"Authorization: Bearer YOUR_API_KEY\" -H \"Content-Type: application/json\" -d '{\"inputs\": \"I love Machine Learning!\"}' https://api-inference.huggingface.co/models/madhurjindal/autonlp-Gibberish-Detector-492513457\n```\n\nOr Python API:\n\n```\nimport torch\nimport torch.nn.functional as F\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\"madhurjindal/autonlp-Gibberish-Detector-492513457\", use_auth_token=True)\n\ntokenizer = AutoTokenizer.from_pretrained(\"madhurjindal/autonlp-Gibberish-Detector-492513457\", use_auth_token=True)\n\ninputs = tokenizer(\"I love Machine Learning!\", return_tensors=\"pt\")\n\noutputs = model(**inputs)\n\nprobs = F.softmax(outputs.logits, dim=-1)\n\npredicted_index = torch.argmax(probs, dim=1).item()\n\npredicted_prob = probs[0][predicted_index].item()\n\nlabels = model.config.id2label\n\npredicted_label = labels[predicted_index]\n\nfor i, prob in enumerate(probs[0]):\n    print(f\"Class: {labels[i]}, Probability: {prob:.4f}\")\n```\n\nAnother simplifed solution with transformers pipline:\n\n```\nfrom transformers import pipeline\nselected_model = \"madhurjindal/autonlp-Gibberish-Detector-492513457\"\nclassifier = pipeline(\"text-classification\", model=selected_model)\nclassifier(\"I love Machine Learning!\")\n```",
    "card_content": "---\nlanguage: en\nlicense: mit\ntags:\n- autonlp\ndatasets:\n- madhurjindal/autonlp-data-Gibberish-Detector\nwidget:\n- text: I love Machine Learning!\nco2_eq_emissions: 5.527544460835904\n---\n\n# Problem Description\nThe ability to process and understand user input is crucial for various applications, such as chatbots or downstream tasks. However, a common challenge faced in such systems is the presence of gibberish or nonsensical input. To address this problem, we present a project focused on developing a gibberish detector for the English language.\nThe primary goal of this project is to classify user input as either **gibberish** or **non-gibberish**, enabling more accurate and meaningful interactions with the system. We also aim to enhance the overall performance and user experience of chatbots and other systems that rely on user input.\n\n>## What is Gibberish?\nGibberish refers to **nonsensical or meaningless language or text** that lacks coherence or any discernible meaning. It can be characterized by a combination of random words, nonsensical phrases, grammatical errors, or syntactical abnormalities that prevent the communication from conveying a clear and understandable message. Gibberish can vary in intensity, ranging from simple noise with no meaningful words to sentences that may appear superficially correct but lack coherence or logical structure when examined closely. Detecting and identifying gibberish is essential in various contexts, such as **natural language processing**, **chatbot systems**, **spam filtering**, and **language-based security measures**, to ensure effective communication and accurate processing of user inputs.\n\n## Label Description\nThus, we break down the problem into 4 categories:\n\n1. **Noise:** Gibberish at the zero level where even the different constituents of the input phrase (words) do not hold any meaning independently.  \n   *For example: `dfdfer fgerfow2e0d qsqskdsd djksdnfkff swq.`*\n   \n2. **Word Salad:** Gibberish at level 1 where words make sense independently, but when looked at the bigger picture (the phrase) any meaning is not depicted.  \n   *For example: `22 madhur old punjab pickle chennai`*\n\n3. **Mild gibberish:** Gibberish at level 2 where there is a part of the sentence that has grammatical errors, word sense errors, or any syntactical abnormalities, which leads the sentence to miss out on a coherent meaning.  \n   *For example: `Madhur study in a teacher`*\n\n4. **Clean:** This category represents a set of words that form a complete and meaningful sentence on its own.  \n   *For example: `I love this website`*\n\n> **Tip:** To facilitate gibberish detection, you can combine the labels based on the desired level of detection. For instance, if you need to detect gibberish at level 1, you can group Noise and Word Salad together as \"Gibberish,\" while considering Mild gibberish and Clean separately as \"NotGibberish.\" This approach allows for flexibility in detecting and categorizing different levels of gibberish based on specific requirements.\n\n\n# Model Trained Using AutoNLP\n\n- Problem type: Multi-class Classification\n- Model ID: 492513457\n- CO2 Emissions (in grams): 5.527544460835904\n\n## Validation Metrics\n\n- Loss: 0.07609463483095169\n- Accuracy: 0.9735624586913417\n- Macro F1: 0.9736173135739408\n- Micro F1: 0.9735624586913417\n- Weighted F1: 0.9736173135739408\n- Macro Precision: 0.9737771415197378\n- Micro Precision: 0.9735624586913417\n- Weighted Precision: 0.9737771415197378\n- Macro Recall: 0.9735624586913417\n- Micro Recall: 0.9735624586913417\n- Weighted Recall: 0.9735624586913417\n\n\n## Usage\n\nYou can use cURL to access this model:\n\n```\n$ curl -X POST -H \"Authorization: Bearer YOUR_API_KEY\" -H \"Content-Type: application/json\" -d '{\"inputs\": \"I love Machine Learning!\"}' https://api-inference.huggingface.co/models/madhurjindal/autonlp-Gibberish-Detector-492513457\n```\n\nOr Python API:\n\n```\nimport torch\nimport torch.nn.functional as F\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\"madhurjindal/autonlp-Gibberish-Detector-492513457\", use_auth_token=True)\n\ntokenizer = AutoTokenizer.from_pretrained(\"madhurjindal/autonlp-Gibberish-Detector-492513457\", use_auth_token=True)\n\ninputs = tokenizer(\"I love Machine Learning!\", return_tensors=\"pt\")\n\noutputs = model(**inputs)\n\nprobs = F.softmax(outputs.logits, dim=-1)\n\npredicted_index = torch.argmax(probs, dim=1).item()\n\npredicted_prob = probs[0][predicted_index].item()\n\nlabels = model.config.id2label\n\npredicted_label = labels[predicted_index]\n\nfor i, prob in enumerate(probs[0]):\n    print(f\"Class: {labels[i]}, Probability: {prob:.4f}\")\n```\n\nAnother simplifed solution with transformers pipline:\n\n```\nfrom transformers import pipeline\nselected_model = \"madhurjindal/autonlp-Gibberish-Detector-492513457\"\nclassifier = pipeline(\"text-classification\", model=selected_model)\nclassifier(\"I love Machine Learning!\")\n```",
    "library_name": "transformers"
  },
  {
    "model_id": "WebOrganizer/TopicClassifier-NoURL",
    "model_name": "WebOrganizer/TopicClassifier-NoURL",
    "author": "WebOrganizer",
    "downloads": 357640,
    "likes": 4,
    "tags": [
      "transformers",
      "safetensors",
      "new",
      "text-classification",
      "custom_code",
      "dataset:WebOrganizer/TopicAnnotations-Llama-3.1-8B",
      "dataset:WebOrganizer/TopicAnnotations-Llama-3.1-405B-FP8",
      "arxiv:2502.10341",
      "base_model:Alibaba-NLP/gte-base-en-v1.5",
      "base_model:finetune:Alibaba-NLP/gte-base-en-v1.5",
      "autotrain_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/WebOrganizer/TopicClassifier-NoURL",
    "dependencies": [
      [
        "transformers",
        "4.30.2"
      ],
      [
        "xformers",
        "0.0.20"
      ],
      [
        "numpy",
        "1.24.3"
      ]
    ],
    "analysis_date": "2025-03-26T00:48:22.866511",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "new",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "library_name": "transformers",
      "datasets": [
        "WebOrganizer/TopicAnnotations-Llama-3.1-8B",
        "WebOrganizer/TopicAnnotations-Llama-3.1-405B-FP8"
      ],
      "base_model": [
        "Alibaba-NLP/gte-base-en-v1.5"
      ]
    },
    "card_text": "# WebOrganizer/TopicClassifier-NoURL\n\n[[Paper](https://arxiv.org/abs/2502.10341)] [[Website](https://weborganizer.allenai.org)] [[GitHub](https://github.com/CodeCreator/WebOrganizer)]\n\nThe TopicClassifier-NoURL organizes web content into 17 categories based on the text contents of web pages (without using URL information).\nThe model is a [gte-base-en-v1.5](https://huggingface.co/Alibaba-NLP/gte-base-en-v1.5) with 140M parameters fine-tuned on the following training data:\n1. [WebOrganizer/TopicAnnotations-Llama-3.1-8B](https://huggingface.co/datasets/WebOrganizer/TopicAnnotations-Llama-3.1-8B): 1M documents annotated by Llama-3.1-8B (first-stage training)\n2. [WebOrganizer/TopicAnnotations-Llama-3.1-405B-FP8](https://huggingface.co/datasets/WebOrganizer/TopicAnnotations-Llama-3.1-405B-FP8): 100K documents annotated by Llama-3.1-405B-FP8 (second-stage training)\n\n#### All Domain Classifiers\n- [WebOrganizer/FormatClassifier](https://huggingface.co/WebOrganizer/FormatClassifier)\n- [WebOrganizer/FormatClassifier-NoURL](https://huggingface.co/WebOrganizer/FormatClassifier-NoURL)\n- [WebOrganizer/TopicClassifier](https://huggingface.co/WebOrganizer/TopicClassifier)\n- [WebOrganizer/TopicClassifier-NoURL](https://huggingface.co/WebOrganizer/TopicClassifier-NoURL) *\u2190 you are here!*\n\n## Usage\n\nThis classifier expects input in the following format:\n```\n{text}\n```\n\nExample:\n```python\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\ntokenizer = AutoTokenizer.from_pretrained(\"WebOrganizer/TopicClassifier-NoURL\")\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"WebOrganizer/TopicClassifier-NoURL\",\n    trust_remote_code=True,\n    use_memory_efficient_attention=False)\n\nweb_page = \"\"\"How to build a computer from scratch? Here are the components you need...\"\"\"\n\ninputs = tokenizer([web_page], return_tensors=\"pt\")\noutputs = model(**inputs)\n\nprobs = outputs.logits.softmax(dim=-1)\nprint(probs.argmax(dim=-1))\n# -> 5 (\"Hardware\" topic)\n```\n\nYou can convert the `logits` of the model with a softmax to obtain a probability distribution over the following 24 categories (in order of labels, also see `id2label` and `label2id` in the model config):\n1. Adult\n2. Art & Design\n3. Software Dev.\n4. Crime & Law\n5. Education & Jobs\n6. Hardware\n7. Entertainment\n8. Social Life\n9. Fashion & Beauty\n10. Finance & Business\n11. Food & Dining\n12. Games\n13. Health\n14. History\n15. Home & Hobbies\n16. Industrial\n17. Literature\n18. Politics\n19. Religion\n20. Science & Tech.\n21. Software\n22. Sports & Fitness\n23. Transportation\n24. Travel\n\nThe full definitions of the categories can be found in the [taxonomy config](https://github.com/CodeCreator/WebOrganizer/blob/main/define_domains/taxonomies/topics.yaml).\n\n#### Efficient Inference\nWe recommend that you use the efficient gte-base-en-v1.5 implementation by enabling unpadding and memory efficient attention. This __requires installing `xformers`__ (see more [here](https://huggingface.co/Alibaba-NLP/new-impl#recommendation-enable-unpadding-and-acceleration-with-xformers)) and loading the model like:\n```python\nAutoModelForSequenceClassification.from_pretrained(\n    \"WebOrganizer/TopicClassifier-NoURL\",\n    trust_remote_code=True,\n    unpad_inputs=True,\n    use_memory_efficient_attention=True,\n    torch_dtype=torch.bfloat16\n)\n```\n\n## Citation\n```bibtex\n@article{wettig2025organize,\n  title={Organize the Web: Constructing Domains Enhances Pre-Training Data Curation},\n  author={Alexander Wettig and Kyle Lo and Sewon Min and Hannaneh Hajishirzi and Danqi Chen and Luca Soldaini},\n  journal={arXiv preprint arXiv:2502.10341},\n  year={2025}\n}\n```",
    "card_content": "---\nlibrary_name: transformers\ndatasets:\n- WebOrganizer/TopicAnnotations-Llama-3.1-8B\n- WebOrganizer/TopicAnnotations-Llama-3.1-405B-FP8\nbase_model:\n- Alibaba-NLP/gte-base-en-v1.5\n---\n# WebOrganizer/TopicClassifier-NoURL\n\n[[Paper](https://arxiv.org/abs/2502.10341)] [[Website](https://weborganizer.allenai.org)] [[GitHub](https://github.com/CodeCreator/WebOrganizer)]\n\nThe TopicClassifier-NoURL organizes web content into 17 categories based on the text contents of web pages (without using URL information).\nThe model is a [gte-base-en-v1.5](https://huggingface.co/Alibaba-NLP/gte-base-en-v1.5) with 140M parameters fine-tuned on the following training data:\n1. [WebOrganizer/TopicAnnotations-Llama-3.1-8B](https://huggingface.co/datasets/WebOrganizer/TopicAnnotations-Llama-3.1-8B): 1M documents annotated by Llama-3.1-8B (first-stage training)\n2. [WebOrganizer/TopicAnnotations-Llama-3.1-405B-FP8](https://huggingface.co/datasets/WebOrganizer/TopicAnnotations-Llama-3.1-405B-FP8): 100K documents annotated by Llama-3.1-405B-FP8 (second-stage training)\n\n#### All Domain Classifiers\n- [WebOrganizer/FormatClassifier](https://huggingface.co/WebOrganizer/FormatClassifier)\n- [WebOrganizer/FormatClassifier-NoURL](https://huggingface.co/WebOrganizer/FormatClassifier-NoURL)\n- [WebOrganizer/TopicClassifier](https://huggingface.co/WebOrganizer/TopicClassifier)\n- [WebOrganizer/TopicClassifier-NoURL](https://huggingface.co/WebOrganizer/TopicClassifier-NoURL) *\u2190 you are here!*\n\n## Usage\n\nThis classifier expects input in the following format:\n```\n{text}\n```\n\nExample:\n```python\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\ntokenizer = AutoTokenizer.from_pretrained(\"WebOrganizer/TopicClassifier-NoURL\")\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"WebOrganizer/TopicClassifier-NoURL\",\n    trust_remote_code=True,\n    use_memory_efficient_attention=False)\n\nweb_page = \"\"\"How to build a computer from scratch? Here are the components you need...\"\"\"\n\ninputs = tokenizer([web_page], return_tensors=\"pt\")\noutputs = model(**inputs)\n\nprobs = outputs.logits.softmax(dim=-1)\nprint(probs.argmax(dim=-1))\n# -> 5 (\"Hardware\" topic)\n```\n\nYou can convert the `logits` of the model with a softmax to obtain a probability distribution over the following 24 categories (in order of labels, also see `id2label` and `label2id` in the model config):\n1. Adult\n2. Art & Design\n3. Software Dev.\n4. Crime & Law\n5. Education & Jobs\n6. Hardware\n7. Entertainment\n8. Social Life\n9. Fashion & Beauty\n10. Finance & Business\n11. Food & Dining\n12. Games\n13. Health\n14. History\n15. Home & Hobbies\n16. Industrial\n17. Literature\n18. Politics\n19. Religion\n20. Science & Tech.\n21. Software\n22. Sports & Fitness\n23. Transportation\n24. Travel\n\nThe full definitions of the categories can be found in the [taxonomy config](https://github.com/CodeCreator/WebOrganizer/blob/main/define_domains/taxonomies/topics.yaml).\n\n#### Efficient Inference\nWe recommend that you use the efficient gte-base-en-v1.5 implementation by enabling unpadding and memory efficient attention. This __requires installing `xformers`__ (see more [here](https://huggingface.co/Alibaba-NLP/new-impl#recommendation-enable-unpadding-and-acceleration-with-xformers)) and loading the model like:\n```python\nAutoModelForSequenceClassification.from_pretrained(\n    \"WebOrganizer/TopicClassifier-NoURL\",\n    trust_remote_code=True,\n    unpad_inputs=True,\n    use_memory_efficient_attention=True,\n    torch_dtype=torch.bfloat16\n)\n```\n\n## Citation\n```bibtex\n@article{wettig2025organize,\n  title={Organize the Web: Constructing Domains Enhances Pre-Training Data Curation},\n  author={Alexander Wettig and Kyle Lo and Sewon Min and Hannaneh Hajishirzi and Danqi Chen and Luca Soldaini},\n  journal={arXiv preprint arXiv:2502.10341},\n  year={2025}\n}\n```",
    "library_name": "transformers"
  },
  {
    "model_id": "cross-encoder/ms-marco-MiniLM-L2-v2",
    "model_name": "cross-encoder/ms-marco-MiniLM-L2-v2",
    "author": "cross-encoder",
    "downloads": 355176,
    "likes": 10,
    "tags": [
      "transformers",
      "pytorch",
      "jax",
      "safetensors",
      "bert",
      "text-classification",
      "license:apache-2.0",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/cross-encoder/ms-marco-MiniLM-L2-v2",
    "dependencies": [
      [
        "sentence_transformers",
        null
      ],
      [
        "torch",
        null
      ],
      [
        "transformers",
        null
      ],
      [
        "sentence-transformers",
        "2.2.2"
      ]
    ],
    "analysis_date": "2025-03-26T00:48:24.160507",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "bert",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "license": "apache-2.0"
    },
    "card_text": "# Cross-Encoder for MS Marco\n\nThis model was trained on the [MS Marco Passage Ranking](https://github.com/microsoft/MSMARCO-Passage-Ranking) task.\n\nThe model can be used for Information Retrieval: Given a query, encode the query will all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. See [SBERT.net Retrieve & Re-rank](https://www.sbert.net/examples/applications/retrieve_rerank/README.html) for more details. The training code is available here: [SBERT.net Training MS Marco](https://github.com/UKPLab/sentence-transformers/tree/master/examples/training/ms_marco)\n\n\n## Usage with SentenceTransformers\n\nThe usage is easy when you have [SentenceTransformers](https://www.sbert.net/) installed. Then you can use the pre-trained models like this:\n```python\nfrom sentence_transformers import CrossEncoder\n\nmodel = CrossEncoder('cross-encoder/ms-marco-MiniLM-L2-v2')\nscores = model.predict([\n    (\"How many people live in Berlin?\", \"Berlin had a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\"),\n    (\"How many people live in Berlin?\", \"Berlin is well known for its museums.\"),\n])\nprint(scores)\n# [ 8.510401 -4.860082]\n```\n\n\n## Usage with Transformers\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\n\nmodel = AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-MiniLM-L2-v2')\ntokenizer = AutoTokenizer.from_pretrained('cross-encoder/ms-marco-MiniLM-L2-v2')\n\nfeatures = tokenizer(['How many people live in Berlin?', 'How many people live in Berlin?'], ['Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.', 'New York City is famous for the Metropolitan Museum of Art.'],  padding=True, truncation=True, return_tensors=\"pt\")\n\nmodel.eval()\nwith torch.no_grad():\n    scores = model(**features).logits\n    print(scores)\n```\n\n\n## Performance\nIn the following table, we provide various pre-trained Cross-Encoders together with their performance on the [TREC Deep Learning 2019](https://microsoft.github.io/TREC-2019-Deep-Learning/) and the [MS Marco Passage Reranking](https://github.com/microsoft/MSMARCO-Passage-Ranking/) dataset. \n\n\n| Model-Name        | NDCG@10 (TREC DL 19) | MRR@10 (MS Marco Dev)  | Docs / Sec |\n| ------------- |:-------------| -----| --- | \n| **Version 2 models** | | | \n| cross-encoder/ms-marco-TinyBERT-L2-v2 | 69.84 | 32.56 | 9000\n| cross-encoder/ms-marco-MiniLM-L2-v2 | 71.01 | 34.85 | 4100\n| cross-encoder/ms-marco-MiniLM-L4-v2 | 73.04 | 37.70 | 2500\n| cross-encoder/ms-marco-MiniLM-L6-v2 | 74.30 | 39.01 | 1800\n| cross-encoder/ms-marco-MiniLM-L12-v2 | 74.31 | 39.02 | 960\n| **Version 1 models** | | | \n| cross-encoder/ms-marco-TinyBERT-L2  | 67.43 | 30.15  | 9000\n| cross-encoder/ms-marco-TinyBERT-L4  | 68.09 | 34.50  | 2900\n| cross-encoder/ms-marco-TinyBERT-L6 |  69.57 | 36.13  | 680\n| cross-encoder/ms-marco-electra-base | 71.99 | 36.41 | 340\n| **Other models** | | | \n| nboost/pt-tinybert-msmarco | 63.63 | 28.80 | 2900 \n| nboost/pt-bert-base-uncased-msmarco | 70.94 | 34.75 | 340 \n| nboost/pt-bert-large-msmarco | 73.36 | 36.48 | 100 \n| Capreolus/electra-base-msmarco | 71.23 | 36.89 | 340 \n| amberoad/bert-multilingual-passage-reranking-msmarco | 68.40 | 35.54 | 330 \n| sebastian-hofstaetter/distilbert-cat-margin_mse-T2-msmarco | 72.82 | 37.88 | 720\n \n Note: Runtime was computed on a V100 GPU.\n",
    "card_content": "---\nlicense: apache-2.0\n---\n# Cross-Encoder for MS Marco\n\nThis model was trained on the [MS Marco Passage Ranking](https://github.com/microsoft/MSMARCO-Passage-Ranking) task.\n\nThe model can be used for Information Retrieval: Given a query, encode the query will all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. See [SBERT.net Retrieve & Re-rank](https://www.sbert.net/examples/applications/retrieve_rerank/README.html) for more details. The training code is available here: [SBERT.net Training MS Marco](https://github.com/UKPLab/sentence-transformers/tree/master/examples/training/ms_marco)\n\n\n## Usage with SentenceTransformers\n\nThe usage is easy when you have [SentenceTransformers](https://www.sbert.net/) installed. Then you can use the pre-trained models like this:\n```python\nfrom sentence_transformers import CrossEncoder\n\nmodel = CrossEncoder('cross-encoder/ms-marco-MiniLM-L2-v2')\nscores = model.predict([\n    (\"How many people live in Berlin?\", \"Berlin had a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\"),\n    (\"How many people live in Berlin?\", \"Berlin is well known for its museums.\"),\n])\nprint(scores)\n# [ 8.510401 -4.860082]\n```\n\n\n## Usage with Transformers\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\n\nmodel = AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-MiniLM-L2-v2')\ntokenizer = AutoTokenizer.from_pretrained('cross-encoder/ms-marco-MiniLM-L2-v2')\n\nfeatures = tokenizer(['How many people live in Berlin?', 'How many people live in Berlin?'], ['Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.', 'New York City is famous for the Metropolitan Museum of Art.'],  padding=True, truncation=True, return_tensors=\"pt\")\n\nmodel.eval()\nwith torch.no_grad():\n    scores = model(**features).logits\n    print(scores)\n```\n\n\n## Performance\nIn the following table, we provide various pre-trained Cross-Encoders together with their performance on the [TREC Deep Learning 2019](https://microsoft.github.io/TREC-2019-Deep-Learning/) and the [MS Marco Passage Reranking](https://github.com/microsoft/MSMARCO-Passage-Ranking/) dataset. \n\n\n| Model-Name        | NDCG@10 (TREC DL 19) | MRR@10 (MS Marco Dev)  | Docs / Sec |\n| ------------- |:-------------| -----| --- | \n| **Version 2 models** | | | \n| cross-encoder/ms-marco-TinyBERT-L2-v2 | 69.84 | 32.56 | 9000\n| cross-encoder/ms-marco-MiniLM-L2-v2 | 71.01 | 34.85 | 4100\n| cross-encoder/ms-marco-MiniLM-L4-v2 | 73.04 | 37.70 | 2500\n| cross-encoder/ms-marco-MiniLM-L6-v2 | 74.30 | 39.01 | 1800\n| cross-encoder/ms-marco-MiniLM-L12-v2 | 74.31 | 39.02 | 960\n| **Version 1 models** | | | \n| cross-encoder/ms-marco-TinyBERT-L2  | 67.43 | 30.15  | 9000\n| cross-encoder/ms-marco-TinyBERT-L4  | 68.09 | 34.50  | 2900\n| cross-encoder/ms-marco-TinyBERT-L6 |  69.57 | 36.13  | 680\n| cross-encoder/ms-marco-electra-base | 71.99 | 36.41 | 340\n| **Other models** | | | \n| nboost/pt-tinybert-msmarco | 63.63 | 28.80 | 2900 \n| nboost/pt-bert-base-uncased-msmarco | 70.94 | 34.75 | 340 \n| nboost/pt-bert-large-msmarco | 73.36 | 36.48 | 100 \n| Capreolus/electra-base-msmarco | 71.23 | 36.89 | 340 \n| amberoad/bert-multilingual-passage-reranking-msmarco | 68.40 | 35.54 | 330 \n| sebastian-hofstaetter/distilbert-cat-margin_mse-T2-msmarco | 72.82 | 37.88 | 720\n \n Note: Runtime was computed on a V100 GPU.\n",
    "library_name": "transformers"
  },
  {
    "model_id": "sadickam/sdgBERT",
    "model_name": "sadickam/sdgBERT",
    "author": "sadickam",
    "downloads": 349358,
    "likes": 11,
    "tags": [
      "transformers",
      "safetensors",
      "bert",
      "text-classification",
      "en",
      "license:mit",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/sadickam/sdgBERT",
    "dependencies": [
      [
        "transformers",
        "4.30.2"
      ],
      [
        "torch",
        "2.0.1"
      ],
      [
        "numpy",
        "1.24.3"
      ],
      [
        "xformers",
        "0.0.20"
      ]
    ],
    "analysis_date": "2025-03-26T00:48:26.369747",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "bert",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": [
        "en"
      ],
      "license": "mit",
      "metrics": [
        "accuracy",
        "matthews_correlation"
      ],
      "widget": [
        {
          "text": "Highway work zones create potential risks for both traffic and workers in addition to traffic congestion and delays that result in increased road user delay."
        },
        {
          "text": "A circular economy is a way of achieving sustainable consumption and production, as well as nature positive outcomes."
        }
      ]
    },
    "card_text": "\n# sadickam/sdgBERT (previously - sadickam/sdg-classification-bert)\n\n<!-- Provide a quick summary of what the model is/does. -->\n\nsgdBERT (previously named \"sdg-classification-bert\"), is an NLP model for classifying text with respect to the United Nations sustainable development goals (SDG).\n\n![image](https://user-images.githubusercontent.com/73560591/216751462-ced482ba-5d8e-48aa-9a48-5557979a35f1.png)\nSource:https://www.un.org/development/desa/disabilities/about-us/sustainable-development-goals-sdgs-and-disability.html\n\n\n## Model Details\n\n### Model Description\n\n<!-- Provide a longer summary of what this model is. -->\n\nThis text classification model was developed by fine-tuning the bert-base-uncased pre-trained model. The training data for this fine-tuned model was sourced from the publicly available OSDG Community Dataset (OSDG-CD) at https://zenodo.org/record/5550238#.ZBulfcJByF4.\nThis model was made as part of academic research at Deakin University. The goal was to make a transformer-based SDG text classification model that anyone could use. Only the first 16 UN SDGs supported. The primary model details are highlighted below:\n\n- **Model type:** Text classification\n- **Language(s) (NLP):** English\n- **License:** mit\n- **Finetuned from model [optional]:** bert-base-uncased\n\n### Model Sources\n<!-- Provide the basic links for the model. -->\n- **Repository:** https://huggingface.co/sadickam/sdg-classification-bert  \n- **Demo:** option 1 (copy/past text and csv): https://sadickam-sdg-text-classifier.hf.space/; option 2 (PDF documents): https://sadickam-document-sdg-app-cpu.hf.space\n\n\n### Direct Use\n\n<!-- This section is for the model use without fine-tuning or plugging into a larger ecosystem/app. -->\n\nThis is a fine-tuned model and therefore requires no further training.\n\n\n## How to Get Started with the Model\n\nUse the code below to get started with the model.\n```python\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\ntokenizer = AutoTokenizer.from_pretrained(\"sadickam/sdg-classification-bert\")\nmodel = AutoModelForSequenceClassification.from_pretrained(\"sadickam/sdg-classification-bert\")\n```\n\n\n## Training Data\n\n<!-- This should link to a Data Card, perhaps with a short stub of information on what the training data is all about as well as documentation related to data pre-processing or additional filtering. -->\nThe training data includes text from a wide range of industries and academic research fields. Hence, this fine-tuned model is not for a specific industry. \n\nSee training here: https://zenodo.org/record/5550238#.ZBulfcJByF4\n\n\n## Training Hyperparameters\n\n- Num_epoch = 3\n- Learning rate = 5e-5\n- Batch size = 16\n\n\n## Evaluation\n\n#### Metrics\n\n<!-- These are the evaluation metrics being used, ideally with a description of why. -->\n- Accuracy = 0.90\n- Matthews correlation = 0.89\n\n\n## Citation\nWill be provided soon\n<!-- Sadick, A.M. (2023). SDG classification with BERT. https://huggingface.co/sadickam/sdg-classification-bert -->\n\n<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->\n\n\n## Model Card Contact\ns.sadick@deakin.edu.au",
    "card_content": "---\nlanguage:\n- en\nlicense: mit\nmetrics:\n- accuracy\n- matthews_correlation\nwidget:\n- text: Highway work zones create potential risks for both traffic and workers in\n    addition to traffic congestion and delays that result in increased road user delay.\n- text: A circular economy is a way of achieving sustainable consumption and production,\n    as well as nature positive outcomes.\n---\n\n# sadickam/sdgBERT (previously - sadickam/sdg-classification-bert)\n\n<!-- Provide a quick summary of what the model is/does. -->\n\nsgdBERT (previously named \"sdg-classification-bert\"), is an NLP model for classifying text with respect to the United Nations sustainable development goals (SDG).\n\n![image](https://user-images.githubusercontent.com/73560591/216751462-ced482ba-5d8e-48aa-9a48-5557979a35f1.png)\nSource:https://www.un.org/development/desa/disabilities/about-us/sustainable-development-goals-sdgs-and-disability.html\n\n\n## Model Details\n\n### Model Description\n\n<!-- Provide a longer summary of what this model is. -->\n\nThis text classification model was developed by fine-tuning the bert-base-uncased pre-trained model. The training data for this fine-tuned model was sourced from the publicly available OSDG Community Dataset (OSDG-CD) at https://zenodo.org/record/5550238#.ZBulfcJByF4.\nThis model was made as part of academic research at Deakin University. The goal was to make a transformer-based SDG text classification model that anyone could use. Only the first 16 UN SDGs supported. The primary model details are highlighted below:\n\n- **Model type:** Text classification\n- **Language(s) (NLP):** English\n- **License:** mit\n- **Finetuned from model [optional]:** bert-base-uncased\n\n### Model Sources\n<!-- Provide the basic links for the model. -->\n- **Repository:** https://huggingface.co/sadickam/sdg-classification-bert  \n- **Demo:** option 1 (copy/past text and csv): https://sadickam-sdg-text-classifier.hf.space/; option 2 (PDF documents): https://sadickam-document-sdg-app-cpu.hf.space\n\n\n### Direct Use\n\n<!-- This section is for the model use without fine-tuning or plugging into a larger ecosystem/app. -->\n\nThis is a fine-tuned model and therefore requires no further training.\n\n\n## How to Get Started with the Model\n\nUse the code below to get started with the model.\n```python\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\ntokenizer = AutoTokenizer.from_pretrained(\"sadickam/sdg-classification-bert\")\nmodel = AutoModelForSequenceClassification.from_pretrained(\"sadickam/sdg-classification-bert\")\n```\n\n\n## Training Data\n\n<!-- This should link to a Data Card, perhaps with a short stub of information on what the training data is all about as well as documentation related to data pre-processing or additional filtering. -->\nThe training data includes text from a wide range of industries and academic research fields. Hence, this fine-tuned model is not for a specific industry. \n\nSee training here: https://zenodo.org/record/5550238#.ZBulfcJByF4\n\n\n## Training Hyperparameters\n\n- Num_epoch = 3\n- Learning rate = 5e-5\n- Batch size = 16\n\n\n## Evaluation\n\n#### Metrics\n\n<!-- These are the evaluation metrics being used, ideally with a description of why. -->\n- Accuracy = 0.90\n- Matthews correlation = 0.89\n\n\n## Citation\nWill be provided soon\n<!-- Sadick, A.M. (2023). SDG classification with BERT. https://huggingface.co/sadickam/sdg-classification-bert -->\n\n<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->\n\n\n## Model Card Contact\ns.sadick@deakin.edu.au",
    "library_name": "transformers"
  },
  {
    "model_id": "finiteautomata/bertweet-base-sentiment-analysis",
    "model_name": "finiteautomata/bertweet-base-sentiment-analysis",
    "author": "finiteautomata",
    "downloads": 307148,
    "likes": 169,
    "tags": [
      "transformers",
      "pytorch",
      "tf",
      "roberta",
      "text-classification",
      "sentiment-analysis",
      "en",
      "arxiv:2106.09462",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/finiteautomata/bertweet-base-sentiment-analysis",
    "dependencies": [
      [
        "pysentimiento",
        "0.7.2"
      ]
    ],
    "analysis_date": "2025-03-26T00:48:27.218160",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "roberta",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": [
        "en"
      ],
      "tags": [
        "sentiment-analysis"
      ]
    },
    "card_text": "# Sentiment Analysis in English\n## bertweet-sentiment-analysis\n\nRepository: [https://github.com/finiteautomata/pysentimiento/](https://github.com/finiteautomata/pysentimiento/)\n\n\nModel trained with SemEval 2017 corpus (around ~40k tweets). Base model is [BERTweet](https://github.com/VinAIResearch/BERTweet), a RoBERTa model trained on English tweets.\n\nUses `POS`, `NEG`, `NEU` labels.\n\n## License\n\n`pysentimiento` is an open-source library for non-commercial use and scientific research purposes only. Please be aware that models are trained with third-party datasets and are subject to their respective licenses. \n\n1. [TASS Dataset license](http://tass.sepln.org/tass_data/download.php)\n2. [SEMEval 2017 Dataset license]()\n\n## Citation\n\nIf you use `pysentimiento` in your work, please cite [this paper](https://arxiv.org/abs/2106.09462)\n\n```\n@misc{perez2021pysentimiento,\n      title={pysentimiento: A Python Toolkit for Sentiment Analysis and SocialNLP tasks},\n      author={Juan Manuel P\u00e9rez and Juan Carlos Giudici and Franco Luque},\n      year={2021},\n      eprint={2106.09462},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\nEnjoy! \ud83e\udd17\n",
    "card_content": "---\nlanguage:\n- en\ntags:\n- sentiment-analysis\n---\n# Sentiment Analysis in English\n## bertweet-sentiment-analysis\n\nRepository: [https://github.com/finiteautomata/pysentimiento/](https://github.com/finiteautomata/pysentimiento/)\n\n\nModel trained with SemEval 2017 corpus (around ~40k tweets). Base model is [BERTweet](https://github.com/VinAIResearch/BERTweet), a RoBERTa model trained on English tweets.\n\nUses `POS`, `NEG`, `NEU` labels.\n\n## License\n\n`pysentimiento` is an open-source library for non-commercial use and scientific research purposes only. Please be aware that models are trained with third-party datasets and are subject to their respective licenses. \n\n1. [TASS Dataset license](http://tass.sepln.org/tass_data/download.php)\n2. [SEMEval 2017 Dataset license]()\n\n## Citation\n\nIf you use `pysentimiento` in your work, please cite [this paper](https://arxiv.org/abs/2106.09462)\n\n```\n@misc{perez2021pysentimiento,\n      title={pysentimiento: A Python Toolkit for Sentiment Analysis and SocialNLP tasks},\n      author={Juan Manuel P\u00e9rez and Juan Carlos Giudici and Franco Luque},\n      year={2021},\n      eprint={2106.09462},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\nEnjoy! \ud83e\udd17\n",
    "library_name": "transformers"
  },
  {
    "model_id": "Alibaba-NLP/gte-multilingual-reranker-base",
    "model_name": "Alibaba-NLP/gte-multilingual-reranker-base",
    "author": "Alibaba-NLP",
    "downloads": 296920,
    "likes": 111,
    "tags": [
      "sentence-transformers",
      "safetensors",
      "new",
      "text-classification",
      "transformers",
      "text-embeddings-inference",
      "custom_code",
      "af",
      "ar",
      "az",
      "be",
      "bg",
      "bn",
      "ca",
      "ceb",
      "cs",
      "cy",
      "da",
      "de",
      "el",
      "en",
      "es",
      "et",
      "eu",
      "fa",
      "fi",
      "fr",
      "gl",
      "gu",
      "he",
      "hi",
      "hr",
      "ht",
      "hu",
      "hy",
      "id",
      "is",
      "it",
      "ja",
      "jv",
      "ka",
      "kk",
      "km",
      "kn",
      "ko",
      "ky",
      "lo",
      "lt",
      "lv",
      "mk",
      "ml",
      "mn",
      "mr",
      "ms",
      "my",
      "ne",
      "nl",
      "no",
      "pa",
      "pl",
      "pt",
      "qu",
      "ro",
      "ru",
      "si",
      "sk",
      "sl",
      "so",
      "sq",
      "sr",
      "sv",
      "sw",
      "ta",
      "te",
      "th",
      "tl",
      "tr",
      "uk",
      "ur",
      "vi",
      "yo",
      "zh",
      "arxiv:2407.19669",
      "license:apache-2.0",
      "region:us"
    ],
    "card_url": "https://huggingface.co/Alibaba-NLP/gte-multilingual-reranker-base",
    "dependencies": [
      [
        "transformers",
        "4.36.0"
      ],
      [
        "torch",
        "2.0.1"
      ],
      [
        "xformers",
        "0.0.22"
      ],
      [
        "numpy",
        "1.24.3"
      ]
    ],
    "analysis_date": "2025-03-26T00:48:28.464256",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "new",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": [
        "af",
        "ar",
        "az",
        "be",
        "bg",
        "bn",
        "ca",
        "ceb",
        "cs",
        "cy",
        "da",
        "de",
        "el",
        "en",
        "es",
        "et",
        "eu",
        "fa",
        "fi",
        "fr",
        "gl",
        "gu",
        "he",
        "hi",
        "hr",
        "ht",
        "hu",
        "hy",
        "id",
        "is",
        "it",
        "ja",
        "jv",
        "ka",
        "kk",
        "km",
        "kn",
        "ko",
        "ky",
        "lo",
        "lt",
        "lv",
        "mk",
        "ml",
        "mn",
        "mr",
        "ms",
        "my",
        "ne",
        "nl",
        "no",
        "pa",
        "pl",
        "pt",
        "qu",
        "ro",
        "ru",
        "si",
        "sk",
        "sl",
        "so",
        "sq",
        "sr",
        "sv",
        "sw",
        "ta",
        "te",
        "th",
        "tl",
        "tr",
        "uk",
        "ur",
        "vi",
        "yo",
        "zh"
      ],
      "license": "apache-2.0",
      "tags": [
        "transformers",
        "sentence-transformers",
        "text-embeddings-inference"
      ],
      "pipeline_tag": "text-classification"
    },
    "card_text": "\n## gte-multilingual-reranker-base\n\nThe **gte-multilingual-reranker-base** model is the first reranker model in the [GTE](https://huggingface.co/collections/Alibaba-NLP/gte-models-6680f0b13f885cb431e6d469) family of models, featuring several key attributes:\n- **High Performance**: Achieves state-of-the-art (SOTA) results in multilingual retrieval tasks and multi-task representation model evaluations when compared to reranker models of similar size.\n- **Training Architecture**: Trained using an encoder-only transformers architecture, resulting in a smaller model size. Unlike previous models based on decode-only LLM architecture (e.g., gte-qwen2-1.5b-instruct), this model has lower hardware requirements for inference, offering a 10x increase in inference speed.\n- **Long Context**: Supports text lengths up to **8192** tokens.\n- **Multilingual Capability**: Supports over **70** languages.\n\n\n## Model Information\n- Model Size: 306M\n- Max Input Tokens: 8192\n\n\n### Usage\n- **It is recommended to install xformers and enable unpadding for acceleration,\nrefer to [enable-unpadding-and-xformers](https://huggingface.co/Alibaba-NLP/new-impl#recommendation-enable-unpadding-and-acceleration-with-xformers).**\n- **How to use it offline: [new-impl/discussions/2](https://huggingface.co/Alibaba-NLP/new-impl/discussions/2#662b08d04d8c3d0a09c88fa3)**\n\n\nUsing Huggingface transformers (transformers>=4.36.0)\n```\nimport torch\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\nmodel_name_or_path = \"Alibaba-NLP/gte-multilingual-reranker-base\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_name_or_path, trust_remote_code=True,\n    torch_dtype=torch.float16\n)\nmodel.eval()\n\npairs = [[\"\u4e2d\u56fd\u7684\u9996\u90fd\u5728\u54ea\u513f\",\"\u5317\u4eac\"], [\"what is the capital of China?\", \"\u5317\u4eac\"], [\"how to implement quick sort in python?\",\"Introduction of quick sort\"]]\nwith torch.no_grad():\n    inputs = tokenizer(pairs, padding=True, truncation=True, return_tensors='pt', max_length=512)\n    scores = model(**inputs, return_dict=True).logits.view(-1, ).float()\n    print(scores)\n\n# tensor([1.2315, 0.5923, 0.3041])\n```\n\nUsage with infinity:\n\n[Infinity](https://github.com/michaelfeil/infinity), a MIT Licensed Inference RestAPI Server.\n```\ndocker run --gpus all -v $PWD/data:/app/.cache -p \"7997\":\"7997\" \\\nmichaelf34/infinity:0.0.68 \\\nv2 --model-id Alibaba-NLP/gte-multilingual-reranker-base --revision \"main\" --dtype bfloat16 --batch-size 32 --device cuda --engine torch --port 7997\n```\n\n## Evaluation\n\nResults of reranking based on multiple text retreival datasets\n\n![image](./images/mgte-reranker.png)\n\n**More detailed experimental results can be found in the [paper](https://arxiv.org/pdf/2407.19669)**.\n\n## Cloud API Services\n\nIn addition to the open-source [GTE](https://huggingface.co/collections/Alibaba-NLP/gte-models-6680f0b13f885cb431e6d469) series models, GTE series models are also available as commercial API services on Alibaba Cloud.\n\n- [Embedding Models](https://help.aliyun.com/zh/model-studio/developer-reference/general-text-embedding/): Three versions of the text embedding models are available: text-embedding-v1/v2/v3, with v3 being the latest API service.\n- [ReRank Models](https://help.aliyun.com/zh/model-studio/developer-reference/general-text-sorting-model/): The gte-rerank model service is available.\n\nNote that the models behind the commercial APIs are not entirely identical to the open-source models.\n\n\n## Citation\n\nIf you find our paper or models helpful, please consider cite:\n\n```\n@inproceedings{zhang2024mgte,\n  title={mGTE: Generalized Long-Context Text Representation and Reranking Models for Multilingual Text Retrieval},\n  author={Zhang, Xin and Zhang, Yanzhao and Long, Dingkun and Xie, Wen and Dai, Ziqi and Tang, Jialong and Lin, Huan and Yang, Baosong and Xie, Pengjun and Huang, Fei and others},\n  booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track},\n  pages={1393--1412},\n  year={2024}\n}\n```",
    "card_content": "---\nlanguage:\n- af\n- ar\n- az\n- be\n- bg\n- bn\n- ca\n- ceb\n- cs\n- cy\n- da\n- de\n- el\n- en\n- es\n- et\n- eu\n- fa\n- fi\n- fr\n- gl\n- gu\n- he\n- hi\n- hr\n- ht\n- hu\n- hy\n- id\n- is\n- it\n- ja\n- jv\n- ka\n- kk\n- km\n- kn\n- ko\n- ky\n- lo\n- lt\n- lv\n- mk\n- ml\n- mn\n- mr\n- ms\n- my\n- ne\n- nl\n- 'no'\n- pa\n- pl\n- pt\n- qu\n- ro\n- ru\n- si\n- sk\n- sl\n- so\n- sq\n- sr\n- sv\n- sw\n- ta\n- te\n- th\n- tl\n- tr\n- uk\n- ur\n- vi\n- yo\n- zh\nlicense: apache-2.0\ntags:\n- transformers\n- sentence-transformers\n- text-embeddings-inference\npipeline_tag: text-classification\n---\n\n## gte-multilingual-reranker-base\n\nThe **gte-multilingual-reranker-base** model is the first reranker model in the [GTE](https://huggingface.co/collections/Alibaba-NLP/gte-models-6680f0b13f885cb431e6d469) family of models, featuring several key attributes:\n- **High Performance**: Achieves state-of-the-art (SOTA) results in multilingual retrieval tasks and multi-task representation model evaluations when compared to reranker models of similar size.\n- **Training Architecture**: Trained using an encoder-only transformers architecture, resulting in a smaller model size. Unlike previous models based on decode-only LLM architecture (e.g., gte-qwen2-1.5b-instruct), this model has lower hardware requirements for inference, offering a 10x increase in inference speed.\n- **Long Context**: Supports text lengths up to **8192** tokens.\n- **Multilingual Capability**: Supports over **70** languages.\n\n\n## Model Information\n- Model Size: 306M\n- Max Input Tokens: 8192\n\n\n### Usage\n- **It is recommended to install xformers and enable unpadding for acceleration,\nrefer to [enable-unpadding-and-xformers](https://huggingface.co/Alibaba-NLP/new-impl#recommendation-enable-unpadding-and-acceleration-with-xformers).**\n- **How to use it offline: [new-impl/discussions/2](https://huggingface.co/Alibaba-NLP/new-impl/discussions/2#662b08d04d8c3d0a09c88fa3)**\n\n\nUsing Huggingface transformers (transformers>=4.36.0)\n```\nimport torch\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\nmodel_name_or_path = \"Alibaba-NLP/gte-multilingual-reranker-base\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_name_or_path, trust_remote_code=True,\n    torch_dtype=torch.float16\n)\nmodel.eval()\n\npairs = [[\"\u4e2d\u56fd\u7684\u9996\u90fd\u5728\u54ea\u513f\",\"\u5317\u4eac\"], [\"what is the capital of China?\", \"\u5317\u4eac\"], [\"how to implement quick sort in python?\",\"Introduction of quick sort\"]]\nwith torch.no_grad():\n    inputs = tokenizer(pairs, padding=True, truncation=True, return_tensors='pt', max_length=512)\n    scores = model(**inputs, return_dict=True).logits.view(-1, ).float()\n    print(scores)\n\n# tensor([1.2315, 0.5923, 0.3041])\n```\n\nUsage with infinity:\n\n[Infinity](https://github.com/michaelfeil/infinity), a MIT Licensed Inference RestAPI Server.\n```\ndocker run --gpus all -v $PWD/data:/app/.cache -p \"7997\":\"7997\" \\\nmichaelf34/infinity:0.0.68 \\\nv2 --model-id Alibaba-NLP/gte-multilingual-reranker-base --revision \"main\" --dtype bfloat16 --batch-size 32 --device cuda --engine torch --port 7997\n```\n\n## Evaluation\n\nResults of reranking based on multiple text retreival datasets\n\n![image](./images/mgte-reranker.png)\n\n**More detailed experimental results can be found in the [paper](https://arxiv.org/pdf/2407.19669)**.\n\n## Cloud API Services\n\nIn addition to the open-source [GTE](https://huggingface.co/collections/Alibaba-NLP/gte-models-6680f0b13f885cb431e6d469) series models, GTE series models are also available as commercial API services on Alibaba Cloud.\n\n- [Embedding Models](https://help.aliyun.com/zh/model-studio/developer-reference/general-text-embedding/): Three versions of the text embedding models are available: text-embedding-v1/v2/v3, with v3 being the latest API service.\n- [ReRank Models](https://help.aliyun.com/zh/model-studio/developer-reference/general-text-sorting-model/): The gte-rerank model service is available.\n\nNote that the models behind the commercial APIs are not entirely identical to the open-source models.\n\n\n## Citation\n\nIf you find our paper or models helpful, please consider cite:\n\n```\n@inproceedings{zhang2024mgte,\n  title={mGTE: Generalized Long-Context Text Representation and Reranking Models for Multilingual Text Retrieval},\n  author={Zhang, Xin and Zhang, Yanzhao and Long, Dingkun and Xie, Wen and Dai, Ziqi and Tang, Jialong and Lin, Huan and Yang, Baosong and Xie, Pengjun and Huang, Fei and others},\n  booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track},\n  pages={1393--1412},\n  year={2024}\n}\n```",
    "library_name": "sentence-transformers"
  },
  {
    "model_id": "cross-encoder/mmarco-mMiniLMv2-L12-H384-v1",
    "model_name": "cross-encoder/mmarco-mMiniLMv2-L12-H384-v1",
    "author": "cross-encoder",
    "downloads": 268873,
    "likes": 49,
    "tags": [
      "transformers",
      "pytorch",
      "safetensors",
      "xlm-roberta",
      "text-classification",
      "en",
      "ar",
      "zh",
      "nl",
      "fr",
      "de",
      "hi",
      "in",
      "it",
      "ja",
      "pt",
      "ru",
      "es",
      "vi",
      "multilingual",
      "dataset:unicamp-dl/mmarco",
      "license:apache-2.0",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/cross-encoder/mmarco-mMiniLMv2-L12-H384-v1",
    "dependencies": [
      [
        "sentence_transformers",
        null
      ],
      [
        "transformers",
        null
      ],
      [
        "torch",
        null
      ],
      [
        "sentence-transformers",
        "2.2.2"
      ]
    ],
    "analysis_date": "2025-03-26T00:48:29.626058",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "xlm-roberta",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": [
        "en",
        "ar",
        "zh",
        "nl",
        "fr",
        "de",
        "hi",
        "in",
        "it",
        "ja",
        "pt",
        "ru",
        "es",
        "vi",
        "multilingual"
      ],
      "license": "apache-2.0",
      "datasets": [
        "unicamp-dl/mmarco"
      ]
    },
    "card_text": "# Cross-Encoder for multilingual MS Marco\n\nThis model was trained on the [MMARCO](https://hf.co/unicamp-dl/mmarco) dataset. It is a machine translated version of MS MARCO using Google Translate. It was translated to 14 languages. In our experiments, we observed that it performs also well for other languages.\n\nAs a base model, we used the [multilingual MiniLMv2](https://huggingface.co/nreimers/mMiniLMv2-L12-H384-distilled-from-XLMR-Large) model.\n\nThe model can be used for Information Retrieval: Given a query, encode the query will all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. See [SBERT.net Retrieve & Re-rank](https://www.sbert.net/examples/applications/retrieve_rerank/README.html) for more details. The training code is available here: [SBERT.net Training MS Marco](https://github.com/UKPLab/sentence-transformers/tree/master/examples/training/ms_marco)\n\n## Usage with SentenceTransformers\n\nThe usage becomes easy when you have [SentenceTransformers](https://www.sbert.net/) installed. Then, you can use the pre-trained models like this:\n```python\nfrom sentence_transformers import CrossEncoder\nmodel = CrossEncoder('model_name')\nscores = model.predict([('Query', 'Paragraph1'), ('Query', 'Paragraph2') , ('Query', 'Paragraph3')])\n```\n\n\n\n\n## Usage with Transformers\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\n\nmodel = AutoModelForSequenceClassification.from_pretrained('model_name')\ntokenizer = AutoTokenizer.from_pretrained('model_name')\n\nfeatures = tokenizer(['How many people live in Berlin?', 'How many people live in Berlin?'], ['Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.', 'New York City is famous for the Metropolitan Museum of Art.'],  padding=True, truncation=True, return_tensors=\"pt\")\n\nmodel.eval()\nwith torch.no_grad():\n    scores = model(**features).logits\n    print(scores)\n```\n\n\n\n",
    "card_content": "---\nlanguage:\n- en\n- ar\n- zh\n- nl\n- fr\n- de\n- hi\n- in\n- it\n- ja\n- pt\n- ru\n- es\n- vi\n- multilingual\nlicense: apache-2.0\ndatasets:\n- unicamp-dl/mmarco\n---\n# Cross-Encoder for multilingual MS Marco\n\nThis model was trained on the [MMARCO](https://hf.co/unicamp-dl/mmarco) dataset. It is a machine translated version of MS MARCO using Google Translate. It was translated to 14 languages. In our experiments, we observed that it performs also well for other languages.\n\nAs a base model, we used the [multilingual MiniLMv2](https://huggingface.co/nreimers/mMiniLMv2-L12-H384-distilled-from-XLMR-Large) model.\n\nThe model can be used for Information Retrieval: Given a query, encode the query will all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. See [SBERT.net Retrieve & Re-rank](https://www.sbert.net/examples/applications/retrieve_rerank/README.html) for more details. The training code is available here: [SBERT.net Training MS Marco](https://github.com/UKPLab/sentence-transformers/tree/master/examples/training/ms_marco)\n\n## Usage with SentenceTransformers\n\nThe usage becomes easy when you have [SentenceTransformers](https://www.sbert.net/) installed. Then, you can use the pre-trained models like this:\n```python\nfrom sentence_transformers import CrossEncoder\nmodel = CrossEncoder('model_name')\nscores = model.predict([('Query', 'Paragraph1'), ('Query', 'Paragraph2') , ('Query', 'Paragraph3')])\n```\n\n\n\n\n## Usage with Transformers\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\n\nmodel = AutoModelForSequenceClassification.from_pretrained('model_name')\ntokenizer = AutoTokenizer.from_pretrained('model_name')\n\nfeatures = tokenizer(['How many people live in Berlin?', 'How many people live in Berlin?'], ['Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.', 'New York City is famous for the Metropolitan Museum of Art.'],  padding=True, truncation=True, return_tensors=\"pt\")\n\nmodel.eval()\nwith torch.no_grad():\n    scores = model(**features).logits\n    print(scores)\n```\n\n\n\n",
    "library_name": "transformers"
  },
  {
    "model_id": "tabularisai/multilingual-sentiment-analysis",
    "model_name": "tabularisai/multilingual-sentiment-analysis",
    "author": "tabularisai",
    "downloads": 265995,
    "likes": 102,
    "tags": [
      "transformers",
      "safetensors",
      "distilbert",
      "text-classification",
      "sentiment-analysis",
      "sentiment",
      "synthetic data",
      "multi-class",
      "social-media-analysis",
      "customer-feedback",
      "product-reviews",
      "brand-monitoring",
      "multilingual",
      "en",
      "zh",
      "es",
      "hi",
      "ar",
      "bn",
      "pt",
      "ru",
      "ja",
      "de",
      "ms",
      "te",
      "vi",
      "ko",
      "fr",
      "tr",
      "it",
      "pl",
      "uk",
      "tl",
      "nl",
      "gsw",
      "base_model:distilbert/distilbert-base-multilingual-cased",
      "base_model:finetune:distilbert/distilbert-base-multilingual-cased",
      "license:cc-by-nc-4.0",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/tabularisai/multilingual-sentiment-analysis",
    "dependencies": [
      [
        "transformers",
        "4.30.2"
      ],
      [
        "torch",
        "2.0.1"
      ],
      [
        "numpy",
        "1.24.3"
      ],
      [
        "xformers",
        "0.0.20"
      ]
    ],
    "analysis_date": "2025-03-26T00:48:30.949029",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "distilbert",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": [
        "en",
        "zh",
        "es",
        "hi",
        "ar",
        "bn",
        "pt",
        "ru",
        "ja",
        "de",
        "ms",
        "te",
        "vi",
        "ko",
        "fr",
        "tr",
        "it",
        "pl",
        "uk",
        "tl",
        "nl",
        "gsw"
      ],
      "license": "cc-by-nc-4.0",
      "library_name": "transformers",
      "tags": [
        "text-classification",
        "sentiment-analysis",
        "sentiment",
        "synthetic data",
        "multi-class",
        "social-media-analysis",
        "customer-feedback",
        "product-reviews",
        "brand-monitoring",
        "multilingual"
      ],
      "base_model": "distilbert/distilbert-base-multilingual-cased",
      "pipeline_tag": "text-classification"
    },
    "card_text": "\n\n# \ud83d\ude80 distilbert-based Multilingual Sentiment Classification Model\n\n<!-- TRY IT HERE: `coming soon`\n -->\n[<img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/Discord%20button.png\" width=\"200\"/>](https://discord.gg/sznxwdqBXj)\n\n\n# NEWS!\n\n- 2024/12: We are excited to introduce a multilingual sentiment model! Now you can analyze sentiment across multiple languages, enhancing your global reach.\n\n## Model Details\n- `Model Name:` tabularisai/multilingual-sentiment-analysis\n- `Base Model:` distilbert/distilbert-base-multilingual-cased\n- `Task:` Text Classification (Sentiment Analysis)\n- `Languages:` Supports English plus Chinese (\u4e2d\u6587), Spanish (Espa\u00f1ol), Hindi (\u0939\u093f\u0928\u094d\u0926\u0940), Arabic (\u0627\u0644\u0639\u0631\u0628\u064a\u0629), Bengali (\u09ac\u09be\u0982\u09b2\u09be), Portuguese (Portugu\u00eas), Russian (\u0420\u0443\u0441\u0441\u043a\u0438\u0439), Japanese (\u65e5\u672c\u8a9e), German (Deutsch), Malay (Bahasa Melayu), Telugu (\u0c24\u0c46\u0c32\u0c41\u0c17\u0c41), Vietnamese (Ti\u1ebfng Vi\u1ec7t), Korean (\ud55c\uad6d\uc5b4), French (Fran\u00e7ais), Turkish (T\u00fcrk\u00e7e), Italian (Italiano), Polish (Polski), Ukrainian (\u0423\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u0430), Tagalog, Dutch (Nederlands), Swiss German (Schweizerdeutsch).\n- `Number of Classes:` 5 (*Very Negative, Negative, Neutral, Positive, Very Positive*)\n- `Usage:`\n  - Social media analysis\n  - Customer feedback analysis\n  - Product reviews classification\n  - Brand monitoring\n  - Market research\n  - Customer service optimization\n  - Competitive intelligence\n\n## Model Description\n\nThis model is a fine-tuned version of `distilbert/distilbert-base-multilingual-cased` for multilingual sentiment analysis. It leverages synthetic data from multiple sources to achieve robust performance across different languages and cultural contexts.\n\n### Training Data\n\nTrained exclusively on synthetic multilingual data generated by advanced LLMs, ensuring wide coverage of sentiment expressions from various languages.\n\n### Training Procedure\n\n- Fine-tuned for 3.5 epochs.\n- Achieved a train_acc_off_by_one of approximately 0.93 on the validation dataset.\n\n## Intended Use\n\nIdeal for:\n- Multilingual social media monitoring\n- International customer feedback analysis\n- Global product review sentiment classification\n- Worldwide brand sentiment tracking\n\n## How to Use\n\nUsing pipelines, it takes only 4 lines:\n\n```python\nfrom transformers import pipeline\n\n# Load the classification pipeline with the specified model\npipe = pipeline(\"text-classification\", model=\"tabularisai/multilingual-sentiment-analysis\")\n\n# Classify a new sentence\nsentence = \"I love this product! It's amazing and works perfectly.\"\nresult = pipe(sentence)\n\n# Print the result\nprint(result)\n```\n\nBelow is a Python example on how to use the multilingual sentiment model without pipelines:\n\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\n\nmodel_name = \"tabularisai/multilingual-sentiment-analysis\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name)\n\ndef predict_sentiment(texts):\n    inputs = tokenizer(texts, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n    with torch.no_grad():\n        outputs = model(**inputs)\n    probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n    sentiment_map = {0: \"Very Negative\", 1: \"Negative\", 2: \"Neutral\", 3: \"Positive\", 4: \"Very Positive\"}\n    return [sentiment_map[p] for p in torch.argmax(probabilities, dim=-1).tolist()]\n\ntexts = [\n    # English\n    \"I absolutely love the new design of this app!\", \"The customer service was disappointing.\", \"The weather is fine, nothing special.\",\n    # Chinese\n    \"\u8fd9\u5bb6\u9910\u5385\u7684\u83dc\u5473\u9053\u975e\u5e38\u68d2\uff01\", \"\u6211\u5bf9\u4ed6\u7684\u56de\u7b54\u5f88\u5931\u671b\u3002\", \"\u5929\u6c14\u4eca\u5929\u4e00\u822c\u3002\",\n    # Spanish\n    \"\u00a1Me encanta c\u00f3mo qued\u00f3 la decoraci\u00f3n!\", \"El servicio fue terrible y muy lento.\", \"El libro estuvo m\u00e1s o menos.\",\n    # Arabic\n    \"\u0627\u0644\u062e\u062f\u0645\u0629 \u0641\u064a \u0647\u0630\u0627 \u0627\u0644\u0641\u0646\u062f\u0642 \u0631\u0627\u0626\u0639\u0629 \u062c\u062f\u064b\u0627!\", \"\u0644\u0645 \u064a\u0639\u062c\u0628\u0646\u064a \u0627\u0644\u0637\u0639\u0627\u0645 \u0641\u064a \u0647\u0630\u0627 \u0627\u0644\u0645\u0637\u0639\u0645.\", \"\u0643\u0627\u0646\u062a \u0627\u0644\u0631\u062d\u0644\u0629 \u0639\u0627\u062f\u064a\u0629\u3002\",\n    # Ukrainian\n    \"\u041c\u0435\u043d\u0456 \u0434\u0443\u0436\u0435 \u0441\u043f\u043e\u0434\u043e\u0431\u0430\u043b\u0430\u0441\u044f \u0446\u044f \u0432\u0438\u0441\u0442\u0430\u0432\u0430!\", \"\u041e\u0431\u0441\u043b\u0443\u0433\u043e\u0432\u0443\u0432\u0430\u043d\u043d\u044f \u0431\u0443\u043b\u043e \u0436\u0430\u0445\u043b\u0438\u0432\u0438\u043c.\", \"\u041a\u043d\u0438\u0433\u0430 \u0431\u0443\u043b\u0430 \u043f\u043e\u0441\u0435\u0440\u0435\u0434\u043d\u044c\u043e\u044e\u3002\",\n    # Hindi\n    \"\u092f\u0939 \u091c\u0917\u0939 \u0938\u091a \u092e\u0947\u0902 \u0905\u0926\u094d\u092d\u0941\u0924 \u0939\u0948!\", \"\u092f\u0939 \u0905\u0928\u0941\u092d\u0935 \u092c\u0939\u0941\u0924 \u0916\u0930\u093e\u092c \u0925\u093e\u0964\", \"\u092b\u093f\u0932\u094d\u092e \u0920\u0940\u0915-\u0920\u093e\u0915 \u0925\u0940\u0964\",\n    # Bengali\n    \"\u098f\u0996\u09be\u09a8\u0995\u09be\u09b0 \u09aa\u09b0\u09bf\u09ac\u09c7\u09b6 \u0985\u09b8\u09be\u09a7\u09be\u09b0\u09a3!\", \"\u09b8\u09c7\u09ac\u09be\u09b0 \u09ae\u09be\u09a8 \u098f\u0995\u09c7\u09ac\u09be\u09b0\u09c7\u0987 \u0996\u09be\u09b0\u09be\u09aa\u0964\", \"\u0996\u09be\u09ac\u09be\u09b0\u099f\u09be \u09ae\u09cb\u099f\u09be\u09ae\u09c1\u099f\u09bf \u099b\u09bf\u09b2\u0964\",\n    # Portuguese\n    \"Este livro \u00e9 fant\u00e1stico! Eu aprendi muitas coisas novas e inspiradoras.\", \n    \"N\u00e3o gostei do produto, veio quebrado.\", \"O filme foi ok, nada de especial.\",\n    # Japanese\n    \"\u3053\u306e\u30ec\u30b9\u30c8\u30e9\u30f3\u306e\u6599\u7406\u306f\u672c\u5f53\u306b\u7f8e\u5473\u3057\u3044\u3067\u3059\uff01\", \"\u3053\u306e\u30db\u30c6\u30eb\u306e\u30b5\u30fc\u30d3\u30b9\u306f\u304c\u3063\u304b\u308a\u3057\u307e\u3057\u305f\u3002\", \"\u5929\u6c17\u306f\u307e\u3042\u307e\u3042\u3067\u3059\u3002\",\n    # Russian\n    \"\u042f \u0432 \u0432\u043e\u0441\u0442\u043e\u0440\u0433\u0435 \u043e\u0442 \u044d\u0442\u043e\u0433\u043e \u043d\u043e\u0432\u043e\u0433\u043e \u0433\u0430\u0434\u0436\u0435\u0442\u0430!\", \"\u042d\u0442\u043e\u0442 \u0441\u0435\u0440\u0432\u0438\u0441 \u043e\u0441\u0442\u0430\u0432\u0438\u043b \u0443 \u043c\u0435\u043d\u044f \u0442\u043e\u043b\u044c\u043a\u043e \u0440\u0430\u0437\u043e\u0447\u0430\u0440\u043e\u0432\u0430\u043d\u0438\u0435.\", \"\u0412\u0441\u0442\u0440\u0435\u0447\u0430 \u0431\u044b\u043b\u0430 \u043e\u0431\u044b\u0447\u043d\u043e\u0439, \u043d\u0438\u0447\u0435\u0433\u043e \u043e\u0441\u043e\u0431\u0435\u043d\u043d\u043e\u0433\u043e.\",\n    # French\n    \"J'adore ce restaurant, c'est excellent !\", \"L'attente \u00e9tait trop longue et frustrante.\", \"Le film \u00e9tait moyen, sans plus.\",\n    # Turkish\n    \"Bu otelin manzaras\u0131na bay\u0131ld\u0131m!\", \"\u00dcr\u00fcn tam bir hayal k\u0131r\u0131kl\u0131\u011f\u0131yd\u0131.\", \"Konser fena de\u011fildi, ortalamayd\u0131.\",\n    # Italian\n    \"Adoro questo posto, \u00e8 fantastico!\", \"Il servizio clienti \u00e8 stato pessimo.\", \"La cena era nella media.\",\n    # Polish\n    \"Uwielbiam t\u0119 restauracj\u0119, jedzenie jest \u015bwietne!\", \"Obs\u0142uga klienta by\u0142a rozczarowuj\u0105ca.\", \"Pogoda jest w porz\u0105dku, nic szczeg\u00f3lnego.\",\n    # Tagalog\n    \"Ang ganda ng lugar na ito, sobrang aliwalas!\", \"Hindi maganda ang serbisyo nila dito.\", \"Maayos lang ang palabas, walang espesyal.\",\n    # Dutch\n    \"Ik ben echt blij met mijn nieuwe aankoop!\", \"De klantenservice was echt slecht.\", \"De presentatie was gewoon ok\u00e9, niet bijzonder.\",\n    # Malay\n    \"Saya suka makanan di sini, sangat sedap!\", \"Pengalaman ini sangat mengecewakan.\", \"Hari ini cuacanya biasa sahaja.\",\n    # Korean\n    \"\uc774 \uac00\uac8c\uc758 \ucf00\uc774\ud06c\ub294 \uc815\ub9d0 \ub9db\uc788\uc5b4\uc694!\", \"\uc11c\ube44\uc2a4\uac00 \ub108\ubb34 \ubcc4\ub85c\uc600\uc5b4\uc694.\", \"\ub0a0\uc528\uac00 \uadf8\uc800 \uadf8\ub807\ub124\uc694.\",\n    # Swiss German\n    \"Ich find d\u00e4 Service i de Beiz mega guet!\", \"D\u00e4s Es\u00e4 het mir n\u00f6d gfalle.\", \"D W\u00e4tter h\u00fct isch so naja.\"\n]\n\nfor text, sentiment in zip(texts, predict_sentiment(texts)):\n    print(f\"Text: {text}\\nSentiment: {sentiment}\\n\")\n```\n\n## Ethical Considerations\n\nSynthetic data reduces bias, but validation in real-world scenarios is advised.\n\n## Citation\n```\nWill be included.\n```\n\n## Contact\n\nFor inquiries, data, private APIs, better models, contact info@tabularis.ai\n\ntabularis.ai",
    "card_content": "---\nlanguage:\n- en\n- zh\n- es\n- hi\n- ar\n- bn\n- pt\n- ru\n- ja\n- de\n- ms\n- te\n- vi\n- ko\n- fr\n- tr\n- it\n- pl\n- uk\n- tl\n- nl\n- gsw\nlicense: cc-by-nc-4.0\nlibrary_name: transformers\ntags:\n- text-classification\n- sentiment-analysis\n- sentiment\n- synthetic data\n- multi-class\n- social-media-analysis\n- customer-feedback\n- product-reviews\n- brand-monitoring\n- multilingual\nbase_model: distilbert/distilbert-base-multilingual-cased\npipeline_tag: text-classification\n---\n\n\n# \ud83d\ude80 distilbert-based Multilingual Sentiment Classification Model\n\n<!-- TRY IT HERE: `coming soon`\n -->\n[<img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/Discord%20button.png\" width=\"200\"/>](https://discord.gg/sznxwdqBXj)\n\n\n# NEWS!\n\n- 2024/12: We are excited to introduce a multilingual sentiment model! Now you can analyze sentiment across multiple languages, enhancing your global reach.\n\n## Model Details\n- `Model Name:` tabularisai/multilingual-sentiment-analysis\n- `Base Model:` distilbert/distilbert-base-multilingual-cased\n- `Task:` Text Classification (Sentiment Analysis)\n- `Languages:` Supports English plus Chinese (\u4e2d\u6587), Spanish (Espa\u00f1ol), Hindi (\u0939\u093f\u0928\u094d\u0926\u0940), Arabic (\u0627\u0644\u0639\u0631\u0628\u064a\u0629), Bengali (\u09ac\u09be\u0982\u09b2\u09be), Portuguese (Portugu\u00eas), Russian (\u0420\u0443\u0441\u0441\u043a\u0438\u0439), Japanese (\u65e5\u672c\u8a9e), German (Deutsch), Malay (Bahasa Melayu), Telugu (\u0c24\u0c46\u0c32\u0c41\u0c17\u0c41), Vietnamese (Ti\u1ebfng Vi\u1ec7t), Korean (\ud55c\uad6d\uc5b4), French (Fran\u00e7ais), Turkish (T\u00fcrk\u00e7e), Italian (Italiano), Polish (Polski), Ukrainian (\u0423\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u0430), Tagalog, Dutch (Nederlands), Swiss German (Schweizerdeutsch).\n- `Number of Classes:` 5 (*Very Negative, Negative, Neutral, Positive, Very Positive*)\n- `Usage:`\n  - Social media analysis\n  - Customer feedback analysis\n  - Product reviews classification\n  - Brand monitoring\n  - Market research\n  - Customer service optimization\n  - Competitive intelligence\n\n## Model Description\n\nThis model is a fine-tuned version of `distilbert/distilbert-base-multilingual-cased` for multilingual sentiment analysis. It leverages synthetic data from multiple sources to achieve robust performance across different languages and cultural contexts.\n\n### Training Data\n\nTrained exclusively on synthetic multilingual data generated by advanced LLMs, ensuring wide coverage of sentiment expressions from various languages.\n\n### Training Procedure\n\n- Fine-tuned for 3.5 epochs.\n- Achieved a train_acc_off_by_one of approximately 0.93 on the validation dataset.\n\n## Intended Use\n\nIdeal for:\n- Multilingual social media monitoring\n- International customer feedback analysis\n- Global product review sentiment classification\n- Worldwide brand sentiment tracking\n\n## How to Use\n\nUsing pipelines, it takes only 4 lines:\n\n```python\nfrom transformers import pipeline\n\n# Load the classification pipeline with the specified model\npipe = pipeline(\"text-classification\", model=\"tabularisai/multilingual-sentiment-analysis\")\n\n# Classify a new sentence\nsentence = \"I love this product! It's amazing and works perfectly.\"\nresult = pipe(sentence)\n\n# Print the result\nprint(result)\n```\n\nBelow is a Python example on how to use the multilingual sentiment model without pipelines:\n\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\n\nmodel_name = \"tabularisai/multilingual-sentiment-analysis\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name)\n\ndef predict_sentiment(texts):\n    inputs = tokenizer(texts, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n    with torch.no_grad():\n        outputs = model(**inputs)\n    probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n    sentiment_map = {0: \"Very Negative\", 1: \"Negative\", 2: \"Neutral\", 3: \"Positive\", 4: \"Very Positive\"}\n    return [sentiment_map[p] for p in torch.argmax(probabilities, dim=-1).tolist()]\n\ntexts = [\n    # English\n    \"I absolutely love the new design of this app!\", \"The customer service was disappointing.\", \"The weather is fine, nothing special.\",\n    # Chinese\n    \"\u8fd9\u5bb6\u9910\u5385\u7684\u83dc\u5473\u9053\u975e\u5e38\u68d2\uff01\", \"\u6211\u5bf9\u4ed6\u7684\u56de\u7b54\u5f88\u5931\u671b\u3002\", \"\u5929\u6c14\u4eca\u5929\u4e00\u822c\u3002\",\n    # Spanish\n    \"\u00a1Me encanta c\u00f3mo qued\u00f3 la decoraci\u00f3n!\", \"El servicio fue terrible y muy lento.\", \"El libro estuvo m\u00e1s o menos.\",\n    # Arabic\n    \"\u0627\u0644\u062e\u062f\u0645\u0629 \u0641\u064a \u0647\u0630\u0627 \u0627\u0644\u0641\u0646\u062f\u0642 \u0631\u0627\u0626\u0639\u0629 \u062c\u062f\u064b\u0627!\", \"\u0644\u0645 \u064a\u0639\u062c\u0628\u0646\u064a \u0627\u0644\u0637\u0639\u0627\u0645 \u0641\u064a \u0647\u0630\u0627 \u0627\u0644\u0645\u0637\u0639\u0645.\", \"\u0643\u0627\u0646\u062a \u0627\u0644\u0631\u062d\u0644\u0629 \u0639\u0627\u062f\u064a\u0629\u3002\",\n    # Ukrainian\n    \"\u041c\u0435\u043d\u0456 \u0434\u0443\u0436\u0435 \u0441\u043f\u043e\u0434\u043e\u0431\u0430\u043b\u0430\u0441\u044f \u0446\u044f \u0432\u0438\u0441\u0442\u0430\u0432\u0430!\", \"\u041e\u0431\u0441\u043b\u0443\u0433\u043e\u0432\u0443\u0432\u0430\u043d\u043d\u044f \u0431\u0443\u043b\u043e \u0436\u0430\u0445\u043b\u0438\u0432\u0438\u043c.\", \"\u041a\u043d\u0438\u0433\u0430 \u0431\u0443\u043b\u0430 \u043f\u043e\u0441\u0435\u0440\u0435\u0434\u043d\u044c\u043e\u044e\u3002\",\n    # Hindi\n    \"\u092f\u0939 \u091c\u0917\u0939 \u0938\u091a \u092e\u0947\u0902 \u0905\u0926\u094d\u092d\u0941\u0924 \u0939\u0948!\", \"\u092f\u0939 \u0905\u0928\u0941\u092d\u0935 \u092c\u0939\u0941\u0924 \u0916\u0930\u093e\u092c \u0925\u093e\u0964\", \"\u092b\u093f\u0932\u094d\u092e \u0920\u0940\u0915-\u0920\u093e\u0915 \u0925\u0940\u0964\",\n    # Bengali\n    \"\u098f\u0996\u09be\u09a8\u0995\u09be\u09b0 \u09aa\u09b0\u09bf\u09ac\u09c7\u09b6 \u0985\u09b8\u09be\u09a7\u09be\u09b0\u09a3!\", \"\u09b8\u09c7\u09ac\u09be\u09b0 \u09ae\u09be\u09a8 \u098f\u0995\u09c7\u09ac\u09be\u09b0\u09c7\u0987 \u0996\u09be\u09b0\u09be\u09aa\u0964\", \"\u0996\u09be\u09ac\u09be\u09b0\u099f\u09be \u09ae\u09cb\u099f\u09be\u09ae\u09c1\u099f\u09bf \u099b\u09bf\u09b2\u0964\",\n    # Portuguese\n    \"Este livro \u00e9 fant\u00e1stico! Eu aprendi muitas coisas novas e inspiradoras.\", \n    \"N\u00e3o gostei do produto, veio quebrado.\", \"O filme foi ok, nada de especial.\",\n    # Japanese\n    \"\u3053\u306e\u30ec\u30b9\u30c8\u30e9\u30f3\u306e\u6599\u7406\u306f\u672c\u5f53\u306b\u7f8e\u5473\u3057\u3044\u3067\u3059\uff01\", \"\u3053\u306e\u30db\u30c6\u30eb\u306e\u30b5\u30fc\u30d3\u30b9\u306f\u304c\u3063\u304b\u308a\u3057\u307e\u3057\u305f\u3002\", \"\u5929\u6c17\u306f\u307e\u3042\u307e\u3042\u3067\u3059\u3002\",\n    # Russian\n    \"\u042f \u0432 \u0432\u043e\u0441\u0442\u043e\u0440\u0433\u0435 \u043e\u0442 \u044d\u0442\u043e\u0433\u043e \u043d\u043e\u0432\u043e\u0433\u043e \u0433\u0430\u0434\u0436\u0435\u0442\u0430!\", \"\u042d\u0442\u043e\u0442 \u0441\u0435\u0440\u0432\u0438\u0441 \u043e\u0441\u0442\u0430\u0432\u0438\u043b \u0443 \u043c\u0435\u043d\u044f \u0442\u043e\u043b\u044c\u043a\u043e \u0440\u0430\u0437\u043e\u0447\u0430\u0440\u043e\u0432\u0430\u043d\u0438\u0435.\", \"\u0412\u0441\u0442\u0440\u0435\u0447\u0430 \u0431\u044b\u043b\u0430 \u043e\u0431\u044b\u0447\u043d\u043e\u0439, \u043d\u0438\u0447\u0435\u0433\u043e \u043e\u0441\u043e\u0431\u0435\u043d\u043d\u043e\u0433\u043e.\",\n    # French\n    \"J'adore ce restaurant, c'est excellent !\", \"L'attente \u00e9tait trop longue et frustrante.\", \"Le film \u00e9tait moyen, sans plus.\",\n    # Turkish\n    \"Bu otelin manzaras\u0131na bay\u0131ld\u0131m!\", \"\u00dcr\u00fcn tam bir hayal k\u0131r\u0131kl\u0131\u011f\u0131yd\u0131.\", \"Konser fena de\u011fildi, ortalamayd\u0131.\",\n    # Italian\n    \"Adoro questo posto, \u00e8 fantastico!\", \"Il servizio clienti \u00e8 stato pessimo.\", \"La cena era nella media.\",\n    # Polish\n    \"Uwielbiam t\u0119 restauracj\u0119, jedzenie jest \u015bwietne!\", \"Obs\u0142uga klienta by\u0142a rozczarowuj\u0105ca.\", \"Pogoda jest w porz\u0105dku, nic szczeg\u00f3lnego.\",\n    # Tagalog\n    \"Ang ganda ng lugar na ito, sobrang aliwalas!\", \"Hindi maganda ang serbisyo nila dito.\", \"Maayos lang ang palabas, walang espesyal.\",\n    # Dutch\n    \"Ik ben echt blij met mijn nieuwe aankoop!\", \"De klantenservice was echt slecht.\", \"De presentatie was gewoon ok\u00e9, niet bijzonder.\",\n    # Malay\n    \"Saya suka makanan di sini, sangat sedap!\", \"Pengalaman ini sangat mengecewakan.\", \"Hari ini cuacanya biasa sahaja.\",\n    # Korean\n    \"\uc774 \uac00\uac8c\uc758 \ucf00\uc774\ud06c\ub294 \uc815\ub9d0 \ub9db\uc788\uc5b4\uc694!\", \"\uc11c\ube44\uc2a4\uac00 \ub108\ubb34 \ubcc4\ub85c\uc600\uc5b4\uc694.\", \"\ub0a0\uc528\uac00 \uadf8\uc800 \uadf8\ub807\ub124\uc694.\",\n    # Swiss German\n    \"Ich find d\u00e4 Service i de Beiz mega guet!\", \"D\u00e4s Es\u00e4 het mir n\u00f6d gfalle.\", \"D W\u00e4tter h\u00fct isch so naja.\"\n]\n\nfor text, sentiment in zip(texts, predict_sentiment(texts)):\n    print(f\"Text: {text}\\nSentiment: {sentiment}\\n\")\n```\n\n## Ethical Considerations\n\nSynthetic data reduces bias, but validation in real-world scenarios is advised.\n\n## Citation\n```\nWill be included.\n```\n\n## Contact\n\nFor inquiries, data, private APIs, better models, contact info@tabularis.ai\n\ntabularis.ai",
    "library_name": "transformers"
  },
  {
    "model_id": "jpwahle/longformer-base-plagiarism-detection",
    "model_name": "jpwahle/longformer-base-plagiarism-detection",
    "author": "jpwahle",
    "downloads": 265816,
    "likes": 12,
    "tags": [
      "transformers",
      "pytorch",
      "safetensors",
      "longformer",
      "text-classification",
      "array",
      "of",
      "tags",
      "en",
      "dataset:jpwahle/machine-paraphrase-dataset",
      "arxiv:2004.05150",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/jpwahle/longformer-base-plagiarism-detection",
    "dependencies": [
      [
        "transformers",
        "4.30.2"
      ],
      [
        "torch",
        "2.0.1"
      ],
      [
        "numpy",
        "1.24.3"
      ],
      [
        "xformers",
        "0.0.22"
      ]
    ],
    "analysis_date": "2025-03-26T00:48:32.542910",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "longformer",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": "en",
      "tags": [
        "array",
        "of",
        "tags"
      ],
      "datasets": [
        "jpwahle/machine-paraphrase-dataset"
      ],
      "thumbnail": "url to a thumbnail used in social sharing",
      "widget": [
        {
          "text": "Plagiarism is the representation of another author's writing, thoughts, ideas, or expressions as one's own work."
        }
      ]
    },
    "card_text": "\n# Longformer-base for Machine-Paraphrase Detection\n\nIf you are using this model in your research work, please cite\n\n```\n@InProceedings{10.1007/978-3-030-96957-8_34,\n    author=\"Wahle, Jan Philip and Ruas, Terry and Folt{\\'y}nek, Tom{\\'a}{\\v{s}} and Meuschke, Norman and Gipp, Bela\",\n    title=\"Identifying Machine-Paraphrased Plagiarism\",\n    booktitle=\"Information for a Better World: Shaping the Global Future\",\n    year=\"2022\",\n    publisher=\"Springer International Publishing\",\n    address=\"Cham\",\n    pages=\"393--413\",\n    abstract=\"Employing paraphrasing tools to conceal plagiarized text is a severe threat to academic integrity. To enable the detection of machine-paraphrased text, we     evaluate the effectiveness of five pre-trained word embedding models combined with machine learning classifiers and state-of-the-art neural language models. We analyze preprints of research papers, graduation theses, and Wikipedia articles, which we paraphrased using different configurations of the tools SpinBot and SpinnerChief. The best performing technique, Longformer, achieved an average F1 score of 80.99{\\%} (F1=99.68{\\%} for SpinBot and F1=71.64{\\%} for SpinnerChief cases), while human evaluators achieved F1=78.4{\\%} for SpinBot and F1=65.6{\\%} for SpinnerChief cases. We show that the automated classification alleviates shortcomings of widely-used text-matching systems, such as Turnitin and PlagScan.\",\n    isbn=\"978-3-030-96957-8\"\n}\n```\n\nThis is the checkpoint for Longformer-base after being trained on the [Machine-Paraphrased Plagiarism Dataset](https://doi.org/10.5281/zenodo.3608000)\n\nAdditional information about this model:\n\n* [The longformer-base-4096 model page](https://huggingface.co/allenai/longformer-base-4096)\n* [Longformer: The Long-Document Transformer](https://arxiv.org/pdf/2004.05150.pdf)\n* [Official implementation by AllenAI](https://github.com/allenai/longformer)\n\nThe model can be loaded to perform Plagiarism like so:\n\n```py\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\nAutoModelForSequenceClassification(\"jpelhaw/longformer-base-plagiarism-detection\")\nAutoTokenizer.from_pretrained(\"jpelhaw/longformer-base-plagiarism-detection\")\n\ninput = \"Plagiarism is the representation of another author's writing, \\\nthoughts, ideas, or expressions as one's own work.\"\n\n\nexample = tokenizer.tokenize(input, add_special_tokens=True)\n\nanswer = model(**example)\n                                \n# \"plagiarised\"\n```",
    "card_content": "---\nlanguage: en\ntags:\n- array\n- of\n- tags\ndatasets:\n- jpwahle/machine-paraphrase-dataset\nthumbnail: url to a thumbnail used in social sharing\nwidget:\n- text: Plagiarism is the representation of another author's writing, thoughts, ideas,\n    or expressions as one's own work.\n---\n\n# Longformer-base for Machine-Paraphrase Detection\n\nIf you are using this model in your research work, please cite\n\n```\n@InProceedings{10.1007/978-3-030-96957-8_34,\n    author=\"Wahle, Jan Philip and Ruas, Terry and Folt{\\'y}nek, Tom{\\'a}{\\v{s}} and Meuschke, Norman and Gipp, Bela\",\n    title=\"Identifying Machine-Paraphrased Plagiarism\",\n    booktitle=\"Information for a Better World: Shaping the Global Future\",\n    year=\"2022\",\n    publisher=\"Springer International Publishing\",\n    address=\"Cham\",\n    pages=\"393--413\",\n    abstract=\"Employing paraphrasing tools to conceal plagiarized text is a severe threat to academic integrity. To enable the detection of machine-paraphrased text, we     evaluate the effectiveness of five pre-trained word embedding models combined with machine learning classifiers and state-of-the-art neural language models. We analyze preprints of research papers, graduation theses, and Wikipedia articles, which we paraphrased using different configurations of the tools SpinBot and SpinnerChief. The best performing technique, Longformer, achieved an average F1 score of 80.99{\\%} (F1=99.68{\\%} for SpinBot and F1=71.64{\\%} for SpinnerChief cases), while human evaluators achieved F1=78.4{\\%} for SpinBot and F1=65.6{\\%} for SpinnerChief cases. We show that the automated classification alleviates shortcomings of widely-used text-matching systems, such as Turnitin and PlagScan.\",\n    isbn=\"978-3-030-96957-8\"\n}\n```\n\nThis is the checkpoint for Longformer-base after being trained on the [Machine-Paraphrased Plagiarism Dataset](https://doi.org/10.5281/zenodo.3608000)\n\nAdditional information about this model:\n\n* [The longformer-base-4096 model page](https://huggingface.co/allenai/longformer-base-4096)\n* [Longformer: The Long-Document Transformer](https://arxiv.org/pdf/2004.05150.pdf)\n* [Official implementation by AllenAI](https://github.com/allenai/longformer)\n\nThe model can be loaded to perform Plagiarism like so:\n\n```py\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\nAutoModelForSequenceClassification(\"jpelhaw/longformer-base-plagiarism-detection\")\nAutoTokenizer.from_pretrained(\"jpelhaw/longformer-base-plagiarism-detection\")\n\ninput = \"Plagiarism is the representation of another author's writing, \\\nthoughts, ideas, or expressions as one's own work.\"\n\n\nexample = tokenizer.tokenize(input, add_special_tokens=True)\n\nanswer = model(**example)\n                                \n# \"plagiarised\"\n```",
    "library_name": "transformers"
  }
]