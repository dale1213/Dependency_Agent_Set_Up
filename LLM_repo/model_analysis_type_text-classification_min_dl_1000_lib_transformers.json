[
  {
    "model_id": "cross-encoder/ms-marco-MiniLM-L6-v2",
    "model_name": "cross-encoder/ms-marco-MiniLM-L6-v2",
    "author": "cross-encoder",
    "downloads": 10940744,
    "downloads_all_time": null,
    "likes": 74,
    "tags": [
      "transformers",
      "pytorch",
      "jax",
      "safetensors",
      "bert",
      "text-classification",
      "license:apache-2.0",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/cross-encoder/ms-marco-MiniLM-L6-v2",
    "dependencies": [
      [
        "sentence_transformers",
        null
      ],
      [
        "transformers",
        null
      ],
      [
        "torch",
        null
      ]
    ],
    "last_modified": "2025-03-07T14:57:52+00:00",
    "created_at": "2022-03-02T23:29:05+00:00",
    "analysis_date": "2025-03-22T00:54:45.736122",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "bert",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "license": "apache-2.0"
    },
    "card_text": "# Cross-Encoder for MS Marco\n\nThis model was trained on the [MS Marco Passage Ranking](https://github.com/microsoft/MSMARCO-Passage-Ranking) task.\n\nThe model can be used for Information Retrieval: Given a query, encode the query will all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. See [SBERT.net Retrieve & Re-rank](https://www.sbert.net/examples/applications/retrieve_rerank/README.html) for more details. The training code is available here: [SBERT.net Training MS Marco](https://github.com/UKPLab/sentence-transformers/tree/master/examples/training/ms_marco)\n\n\n## Usage with SentenceTransformers\n\nThe usage is easy when you have [SentenceTransformers](https://www.sbert.net/) installed. Then you can use the pre-trained models like this:\n```python\nfrom sentence_transformers import CrossEncoder\n\nmodel = CrossEncoder('cross-encoder/ms-marco-MiniLM-L6-v2')\nscores = model.predict([\n    (\"How many people live in Berlin?\", \"Berlin had a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\"),\n    (\"How many people live in Berlin?\", \"Berlin is well known for its museums.\"),\n])\nprint(scores)\n# [ 8.607138 -4.320078]\n```\n\n\n## Usage with Transformers\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\n\nmodel = AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-MiniLM-L6-v2')\ntokenizer = AutoTokenizer.from_pretrained('cross-encoder/ms-marco-MiniLM-L6-v2')\n\nfeatures = tokenizer(['How many people live in Berlin?', 'How many people live in Berlin?'], ['Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.', 'New York City is famous for the Metropolitan Museum of Art.'],  padding=True, truncation=True, return_tensors=\"pt\")\n\nmodel.eval()\nwith torch.no_grad():\n    scores = model(**features).logits\n    print(scores)\n```\n\n\n## Performance\nIn the following table, we provide various pre-trained Cross-Encoders together with their performance on the [TREC Deep Learning 2019](https://microsoft.github.io/TREC-2019-Deep-Learning/) and the [MS Marco Passage Reranking](https://github.com/microsoft/MSMARCO-Passage-Ranking/) dataset. \n\n\n| Model-Name        | NDCG@10 (TREC DL 19) | MRR@10 (MS Marco Dev)  | Docs / Sec |\n| ------------- |:-------------| -----| --- | \n| **Version 2 models** | | | \n| cross-encoder/ms-marco-TinyBERT-L2-v2 | 69.84 | 32.56 | 9000\n| cross-encoder/ms-marco-MiniLM-L2-v2 | 71.01 | 34.85 | 4100\n| cross-encoder/ms-marco-MiniLM-L4-v2 | 73.04 | 37.70 | 2500\n| cross-encoder/ms-marco-MiniLM-L6-v2 | 74.30 | 39.01 | 1800\n| cross-encoder/ms-marco-MiniLM-L12-v2 | 74.31 | 39.02 | 960\n| **Version 1 models** | | | \n| cross-encoder/ms-marco-TinyBERT-L2  | 67.43 | 30.15  | 9000\n| cross-encoder/ms-marco-TinyBERT-L4  | 68.09 | 34.50  | 2900\n| cross-encoder/ms-marco-TinyBERT-L6 |  69.57 | 36.13  | 680\n| cross-encoder/ms-marco-electra-base | 71.99 | 36.41 | 340\n| **Other models** | | | \n| nboost/pt-tinybert-msmarco | 63.63 | 28.80 | 2900 \n| nboost/pt-bert-base-uncased-msmarco | 70.94 | 34.75 | 340 \n| nboost/pt-bert-large-msmarco | 73.36 | 36.48 | 100 \n| Capreolus/electra-base-msmarco | 71.23 | 36.89 | 340 \n| amberoad/bert-multilingual-passage-reranking-msmarco | 68.40 | 35.54 | 330 \n| sebastian-hofstaetter/distilbert-cat-margin_mse-T2-msmarco | 72.82 | 37.88 | 720\n \n Note: Runtime was computed on a V100 GPU.\n",
    "card_content": "---\nlicense: apache-2.0\n---\n# Cross-Encoder for MS Marco\n\nThis model was trained on the [MS Marco Passage Ranking](https://github.com/microsoft/MSMARCO-Passage-Ranking) task.\n\nThe model can be used for Information Retrieval: Given a query, encode the query will all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. See [SBERT.net Retrieve & Re-rank](https://www.sbert.net/examples/applications/retrieve_rerank/README.html) for more details. The training code is available here: [SBERT.net Training MS Marco](https://github.com/UKPLab/sentence-transformers/tree/master/examples/training/ms_marco)\n\n\n## Usage with SentenceTransformers\n\nThe usage is easy when you have [SentenceTransformers](https://www.sbert.net/) installed. Then you can use the pre-trained models like this:\n```python\nfrom sentence_transformers import CrossEncoder\n\nmodel = CrossEncoder('cross-encoder/ms-marco-MiniLM-L6-v2')\nscores = model.predict([\n    (\"How many people live in Berlin?\", \"Berlin had a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\"),\n    (\"How many people live in Berlin?\", \"Berlin is well known for its museums.\"),\n])\nprint(scores)\n# [ 8.607138 -4.320078]\n```\n\n\n## Usage with Transformers\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\n\nmodel = AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-MiniLM-L6-v2')\ntokenizer = AutoTokenizer.from_pretrained('cross-encoder/ms-marco-MiniLM-L6-v2')\n\nfeatures = tokenizer(['How many people live in Berlin?', 'How many people live in Berlin?'], ['Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.', 'New York City is famous for the Metropolitan Museum of Art.'],  padding=True, truncation=True, return_tensors=\"pt\")\n\nmodel.eval()\nwith torch.no_grad():\n    scores = model(**features).logits\n    print(scores)\n```\n\n\n## Performance\nIn the following table, we provide various pre-trained Cross-Encoders together with their performance on the [TREC Deep Learning 2019](https://microsoft.github.io/TREC-2019-Deep-Learning/) and the [MS Marco Passage Reranking](https://github.com/microsoft/MSMARCO-Passage-Ranking/) dataset. \n\n\n| Model-Name        | NDCG@10 (TREC DL 19) | MRR@10 (MS Marco Dev)  | Docs / Sec |\n| ------------- |:-------------| -----| --- | \n| **Version 2 models** | | | \n| cross-encoder/ms-marco-TinyBERT-L2-v2 | 69.84 | 32.56 | 9000\n| cross-encoder/ms-marco-MiniLM-L2-v2 | 71.01 | 34.85 | 4100\n| cross-encoder/ms-marco-MiniLM-L4-v2 | 73.04 | 37.70 | 2500\n| cross-encoder/ms-marco-MiniLM-L6-v2 | 74.30 | 39.01 | 1800\n| cross-encoder/ms-marco-MiniLM-L12-v2 | 74.31 | 39.02 | 960\n| **Version 1 models** | | | \n| cross-encoder/ms-marco-TinyBERT-L2  | 67.43 | 30.15  | 9000\n| cross-encoder/ms-marco-TinyBERT-L4  | 68.09 | 34.50  | 2900\n| cross-encoder/ms-marco-TinyBERT-L6 |  69.57 | 36.13  | 680\n| cross-encoder/ms-marco-electra-base | 71.99 | 36.41 | 340\n| **Other models** | | | \n| nboost/pt-tinybert-msmarco | 63.63 | 28.80 | 2900 \n| nboost/pt-bert-base-uncased-msmarco | 70.94 | 34.75 | 340 \n| nboost/pt-bert-large-msmarco | 73.36 | 36.48 | 100 \n| Capreolus/electra-base-msmarco | 71.23 | 36.89 | 340 \n| amberoad/bert-multilingual-passage-reranking-msmarco | 68.40 | 35.54 | 330 \n| sebastian-hofstaetter/distilbert-cat-margin_mse-T2-msmarco | 72.82 | 37.88 | 720\n \n Note: Runtime was computed on a V100 GPU.\n",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModelForSequenceClassification",
      "custom_class": null,
      "pipeline_tag": "text-classification",
      "processor": "AutoTokenizer"
    },
    "safetensors": {
      "parameters": {
        "I64": 512,
        "F32": 22713601
      },
      "total": 22714113
    },
    "model_index": null,
    "trending_score": null
  },
  {
    "model_id": "distilbert/distilbert-base-uncased-finetuned-sst-2-english",
    "model_name": "distilbert/distilbert-base-uncased-finetuned-sst-2-english",
    "author": "distilbert",
    "downloads": 6766852,
    "downloads_all_time": null,
    "likes": 715,
    "tags": [
      "transformers",
      "pytorch",
      "tf",
      "rust",
      "onnx",
      "safetensors",
      "distilbert",
      "text-classification",
      "en",
      "dataset:sst2",
      "dataset:glue",
      "arxiv:1910.01108",
      "doi:10.57967/hf/0181",
      "license:apache-2.0",
      "model-index",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english",
    "dependencies": [
      [
        "torch",
        null
      ],
      [
        "transformers",
        null
      ]
    ],
    "last_modified": "2023-12-19T16:29:37+00:00",
    "created_at": "2022-03-02T23:29:04+00:00",
    "analysis_date": "2025-03-22T00:54:47.325592",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "distilbert",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": "en",
      "license": "apache-2.0",
      "datasets": [
        "sst2",
        "glue"
      ],
      "model-index": [
        {
          "name": "distilbert-base-uncased-finetuned-sst-2-english",
          "results": [
            {
              "task": {
                "type": "text-classification",
                "name": "Text Classification"
              },
              "dataset": {
                "name": "glue",
                "type": "glue",
                "config": "sst2",
                "split": "validation"
              },
              "metrics": [
                {
                  "type": "accuracy",
                  "value": 0.9105504587155964,
                  "name": "Accuracy",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiN2YyOGMxYjY2Y2JhMjkxNjIzN2FmMjNiNmM2ZWViNGY3MTNmNWI2YzhiYjYxZTY0ZGUyN2M1NGIxZjRiMjQwZiIsInZlcnNpb24iOjF9.uui0srxV5ZHRhxbYN6082EZdwpnBgubPJ5R2-Wk8HTWqmxYE3QHidevR9LLAhidqGw6Ih93fK0goAXncld_gBg"
                },
                {
                  "type": "precision",
                  "value": 0.8978260869565218,
                  "name": "Precision",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMzgwYTYwYjA2MmM0ZTYwNDk0M2NmNTBkZmM2NGNhYzQ1OGEyN2NkNDQ3Mzc2NTQyMmZiNDJiNzBhNGVhZGUyOSIsInZlcnNpb24iOjF9.eHjLmw3K02OU69R2Au8eyuSqT3aBDHgZCn8jSzE3_urD6EUSSsLxUpiAYR4BGLD_U6-ZKcdxVo_A2rdXqvUJDA"
                },
                {
                  "type": "recall",
                  "value": 0.9301801801801802,
                  "name": "Recall",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMGIzM2E3MTI2Mzc2MDYwNmU3ZTVjYmZmZDBkNjY4ZTc5MGY0Y2FkNDU3NjY1MmVkNmE3Y2QzMzAwZDZhOWY1NiIsInZlcnNpb24iOjF9.PUZlqmct13-rJWBXdHm5tdkXgETL9F82GNbbSR4hI8MB-v39KrK59cqzFC2Ac7kJe_DtOeUyosj34O_mFt_1DQ"
                },
                {
                  "type": "auc",
                  "value": 0.9716626673402374,
                  "name": "AUC",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMDM0YWIwZmQ4YjUwOGZmMWU2MjI1YjIxZGQ2MzNjMzRmZmYxMzZkNGFjODhlMDcyZDM1Y2RkMWZlOWQ0MWYwNSIsInZlcnNpb24iOjF9.E7GRlAXmmpEkTHlXheVkuL1W4WNjv4JO3qY_WCVsTVKiO7bUu0UVjPIyQ6g-J1OxsfqZmW3Leli1wY8vPBNNCQ"
                },
                {
                  "type": "f1",
                  "value": 0.9137168141592922,
                  "name": "F1",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMGU4MjNmOGYwZjZjMDQ1ZTkyZTA4YTc1MWYwOTM0NDM4ZWY1ZGVkNDY5MzNhYTQyZGFlNzIyZmUwMDg3NDU0NyIsInZlcnNpb24iOjF9.mW5ftkq50Se58M-jm6a2Pu93QeKa3MfV7xcBwvG3PSB_KNJxZWTCpfMQp-Cmx_EMlmI2siKOyd8akYjJUrzJCA"
                },
                {
                  "type": "loss",
                  "value": 0.39013850688934326,
                  "name": "loss",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMTZiNzAyZDc0MzUzMmE1MGJiN2JlYzFiODE5ZTNlNGE4MmI4YzRiMTc2ODEzMTUwZmEzOTgxNzc4YjJjZTRmNiIsInZlcnNpb24iOjF9.VqIC7uYC-ZZ8ss9zQOlRV39YVOOLc5R36sIzCcVz8lolh61ux_5djm2XjpP6ARc6KqEnXC4ZtfNXsX2HZfrtCQ"
                }
              ]
            },
            {
              "task": {
                "type": "text-classification",
                "name": "Text Classification"
              },
              "dataset": {
                "name": "sst2",
                "type": "sst2",
                "config": "default",
                "split": "train"
              },
              "metrics": [
                {
                  "type": "accuracy",
                  "value": 0.9885521685548412,
                  "name": "Accuracy",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiY2I3NzU3YzhmMDkxZTViY2M3OTY1NmI0ZTdmMDQxNjNjYzJiZmQxNzczM2E4YmExYTY5ODY0NDBkY2I4ZjNkOCIsInZlcnNpb24iOjF9.4Gtk3FeVc9sPWSqZIaeUXJ9oVlPzm-NmujnWpK2y5s1Vhp1l6Y1pK5_78wW0-NxSvQqV6qd5KQf_OAEpVAkQDA"
                },
                {
                  "type": "precision",
                  "value": 0.9881965062029833,
                  "name": "Precision Macro",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZDdlZDMzY2I3MTAwYTljNmM4MGMyMzU2YjAzZDg1NDYwN2ZmM2Y5OWZhMjUyMGJiNjY1YmZiMzFhMDI2ODFhNyIsInZlcnNpb24iOjF9.cqmv6yBxu4St2mykRWrZ07tDsiSLdtLTz2hbqQ7Gm1rMzq9tdlkZ8MyJRxtME_Y8UaOG9rs68pV-gKVUs8wABw"
                },
                {
                  "type": "precision",
                  "value": 0.9885521685548412,
                  "name": "Precision Micro",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZjFlYzAzNmE1YjljNjUwNzBjZjEzZDY0ZDQyMmY5ZWM2OTBhNzNjYjYzYTk1YWE1NjU3YTMxZDQwOTE1Y2FkNyIsInZlcnNpb24iOjF9.jnCHOkUHuAOZZ_ZMVOnetx__OVJCS6LOno4caWECAmfrUaIPnPNV9iJ6izRO3sqkHRmxYpWBb-27GJ4N3LU-BQ"
                },
                {
                  "type": "precision",
                  "value": 0.9885639626373408,
                  "name": "Precision Weighted",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZGUyODFjNjBlNTE2MTY3ZDAxOGU1N2U0YjUyY2NiZjhkOGVmYThjYjBkNGU3NTRkYzkzNDQ2MmMwMjkwMWNiMyIsInZlcnNpb24iOjF9.zTNabMwApiZyXdr76QUn7WgGB7D7lP-iqS3bn35piqVTNsv3wnKjZOaKFVLIUvtBXq4gKw7N2oWxvWc4OcSNDg"
                },
                {
                  "type": "recall",
                  "value": 0.9886145346602994,
                  "name": "Recall Macro",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiNTU1YjlhODU3YTkyNTdiZDcwZGFlZDBiYjY0N2NjMGM2NTRiNjQ3MDNjNGMxOWY2ZGQ4NWU1YmMzY2UwZTI3YSIsInZlcnNpb24iOjF9.xaLPY7U-wHsJ3DDui1yyyM-xWjL0Jz5puRThy7fczal9x05eKEQ9s0a_WD-iLmapvJs0caXpV70hDe2NLcs-DA"
                },
                {
                  "type": "recall",
                  "value": 0.9885521685548412,
                  "name": "Recall Micro",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiODE0YTU0MDBlOGY4YzU0MjY5MzA3OTk2OGNhOGVkMmU5OGRjZmFiZWI2ZjY5ODEzZTQzMTI0N2NiOTVkNDliYiIsInZlcnNpb24iOjF9.SOt1baTBbuZRrsvGcak2sUwoTrQzmNCbyV2m1_yjGsU48SBH0NcKXicidNBSnJ6ihM5jf_Lv_B5_eOBkLfNWDQ"
                },
                {
                  "type": "recall",
                  "value": 0.9885521685548412,
                  "name": "Recall Weighted",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZWNkNmM0ZGRlNmYxYzIwNDk4OTI5MzIwZWU1NzZjZDVhMDcyNDFlMjBhNDQxODU5OWMwMWNhNGEzNjY3ZGUyOSIsInZlcnNpb24iOjF9.b15Fh70GwtlG3cSqPW-8VEZT2oy0CtgvgEOtWiYonOovjkIQ4RSLFVzVG-YfslaIyfg9RzMWzjhLnMY7Bpn2Aw"
                },
                {
                  "type": "f1",
                  "value": 0.9884019815052447,
                  "name": "F1 Macro",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiYmM4NjQ5Yjk5ODRhYTU1MTY3MmRhZDBmODM1NTg3OTFiNWM4NDRmYjI0MzZkNmQ1MzE3MzcxODZlYzBkYTMyYSIsInZlcnNpb24iOjF9.74RaDK8nBVuGRl2Se_-hwQvP6c4lvVxGHpcCWB4uZUCf2_HoC9NT9u7P3pMJfH_tK2cpV7U3VWGgSDhQDi-UBQ"
                },
                {
                  "type": "f1",
                  "value": 0.9885521685548412,
                  "name": "F1 Micro",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZDRmYWRmMmQ0YjViZmQxMzhhYTUyOTE1MTc0ZDU1ZjQyZjFhMDYzYzMzZDE0NzZlYzQyOTBhMTBhNmM5NTlkMiIsInZlcnNpb24iOjF9.VMn_psdAHIZTlW6GbjERZDe8MHhwzJ0rbjV_VJyuMrsdOh5QDmko-wEvaBWNEdT0cEKsbggm-6jd3Gh81PfHAQ"
                },
                {
                  "type": "f1",
                  "value": 0.9885546181087554,
                  "name": "F1 Weighted",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMjUyZWFhZDZhMGQ3MzBmYmRiNDVmN2FkZDBjMjk3ODk0OTAxNGZkMWE0NzU5ZjI0NzE0NGZiNzM0N2Y2NDYyOSIsInZlcnNpb24iOjF9.YsXBhnzEEFEW6jw3mQlFUuIrW7Gabad2Ils-iunYJr-myg0heF8NEnEWABKFE1SnvCWt-69jkLza6SupeyLVCA"
                },
                {
                  "type": "loss",
                  "value": 0.040652573108673096,
                  "name": "loss",
                  "verified": true,
                  "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZTc3YjU3MjdjMzkxODA5MjU5NGUyY2NkMGVhZDg3ZWEzMmU1YWVjMmI0NmU2OWEyZTkzMTVjNDZiYTc0YjIyNCIsInZlcnNpb24iOjF9.lA90qXZVYiILHMFlr6t6H81Oe8a-4KmeX-vyCC1BDia2ofudegv6Vb46-4RzmbtuKeV6yy6YNNXxXxqVak1pAg"
                }
              ]
            }
          ]
        }
      ]
    },
    "card_text": "\n# DistilBERT base uncased finetuned SST-2\n\n## Table of Contents\n- [Model Details](#model-details)\n- [How to Get Started With the Model](#how-to-get-started-with-the-model)\n- [Uses](#uses)\n- [Risks, Limitations and Biases](#risks-limitations-and-biases)\n- [Training](#training)\n\n## Model Details\n**Model Description:** This model is a fine-tune checkpoint of [DistilBERT-base-uncased](https://huggingface.co/distilbert-base-uncased), fine-tuned on SST-2.\nThis model reaches an accuracy of 91.3 on the dev set (for comparison, Bert bert-base-uncased version reaches an accuracy of 92.7).\n- **Developed by:** Hugging Face\n- **Model Type:** Text Classification\n- **Language(s):** English\n- **License:** Apache-2.0\n- **Parent Model:** For more details about DistilBERT, we encourage users to check out [this model card](https://huggingface.co/distilbert-base-uncased).\n- **Resources for more information:**\n    - [Model Documentation](https://huggingface.co/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertForSequenceClassification)\n    - [DistilBERT paper](https://arxiv.org/abs/1910.01108)\n\n## How to Get Started With the Model\n\nExample of single-label classification:\n​​\n```python\nimport torch\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n\ntokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\nmodel = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n\ninputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\nwith torch.no_grad():\n    logits = model(**inputs).logits\n\npredicted_class_id = logits.argmax().item()\nmodel.config.id2label[predicted_class_id]\n\n```\n\n## Uses\n\n#### Direct Use\n\nThis model can be used for  topic classification. You can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to be fine-tuned on a downstream task. See the model hub to look for fine-tuned versions on a task that interests you.\n\n#### Misuse and Out-of-scope Use\nThe model should not be used to intentionally create hostile or alienating environments for people. In addition, the model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model.\n\n\n## Risks, Limitations and Biases\n\nBased on a few experimentations, we observed that this model could produce biased predictions that target underrepresented populations.\n\nFor instance, for sentences like `This film was filmed in COUNTRY`, this binary classification model will give radically different probabilities for the positive label depending on the country (0.89 if the country is France, but 0.08 if the country is Afghanistan) when nothing in the input indicates such a strong semantic shift. In this [colab](https://colab.research.google.com/gist/ageron/fb2f64fb145b4bc7c49efc97e5f114d3/biasmap.ipynb), [Aurélien Géron](https://twitter.com/aureliengeron) made an interesting map plotting these probabilities for each country.\n\n<img src=\"https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english/resolve/main/map.jpeg\" alt=\"Map of positive probabilities per country.\" width=\"500\"/>\n\nWe strongly advise users to thoroughly probe these aspects on their use-cases in order to evaluate the risks of this model. We recommend looking at the following bias evaluation datasets as a place to start: [WinoBias](https://huggingface.co/datasets/wino_bias), [WinoGender](https://huggingface.co/datasets/super_glue), [Stereoset](https://huggingface.co/datasets/stereoset).\n\n\n\n# Training\n\n\n#### Training Data\n\n\nThe authors use the following Stanford Sentiment Treebank([sst2](https://huggingface.co/datasets/sst2)) corpora for the model.\n\n#### Training Procedure\n\n###### Fine-tuning hyper-parameters\n\n\n- learning_rate = 1e-5\n- batch_size = 32\n- warmup = 600\n- max_seq_length = 128\n- num_train_epochs = 3.0\n\n\n",
    "card_content": "---\nlanguage: en\nlicense: apache-2.0\ndatasets:\n- sst2\n- glue\nmodel-index:\n- name: distilbert-base-uncased-finetuned-sst-2-english\n  results:\n  - task:\n      type: text-classification\n      name: Text Classification\n    dataset:\n      name: glue\n      type: glue\n      config: sst2\n      split: validation\n    metrics:\n    - type: accuracy\n      value: 0.9105504587155964\n      name: Accuracy\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiN2YyOGMxYjY2Y2JhMjkxNjIzN2FmMjNiNmM2ZWViNGY3MTNmNWI2YzhiYjYxZTY0ZGUyN2M1NGIxZjRiMjQwZiIsInZlcnNpb24iOjF9.uui0srxV5ZHRhxbYN6082EZdwpnBgubPJ5R2-Wk8HTWqmxYE3QHidevR9LLAhidqGw6Ih93fK0goAXncld_gBg\n    - type: precision\n      value: 0.8978260869565218\n      name: Precision\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMzgwYTYwYjA2MmM0ZTYwNDk0M2NmNTBkZmM2NGNhYzQ1OGEyN2NkNDQ3Mzc2NTQyMmZiNDJiNzBhNGVhZGUyOSIsInZlcnNpb24iOjF9.eHjLmw3K02OU69R2Au8eyuSqT3aBDHgZCn8jSzE3_urD6EUSSsLxUpiAYR4BGLD_U6-ZKcdxVo_A2rdXqvUJDA\n    - type: recall\n      value: 0.9301801801801802\n      name: Recall\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMGIzM2E3MTI2Mzc2MDYwNmU3ZTVjYmZmZDBkNjY4ZTc5MGY0Y2FkNDU3NjY1MmVkNmE3Y2QzMzAwZDZhOWY1NiIsInZlcnNpb24iOjF9.PUZlqmct13-rJWBXdHm5tdkXgETL9F82GNbbSR4hI8MB-v39KrK59cqzFC2Ac7kJe_DtOeUyosj34O_mFt_1DQ\n    - type: auc\n      value: 0.9716626673402374\n      name: AUC\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMDM0YWIwZmQ4YjUwOGZmMWU2MjI1YjIxZGQ2MzNjMzRmZmYxMzZkNGFjODhlMDcyZDM1Y2RkMWZlOWQ0MWYwNSIsInZlcnNpb24iOjF9.E7GRlAXmmpEkTHlXheVkuL1W4WNjv4JO3qY_WCVsTVKiO7bUu0UVjPIyQ6g-J1OxsfqZmW3Leli1wY8vPBNNCQ\n    - type: f1\n      value: 0.9137168141592922\n      name: F1\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMGU4MjNmOGYwZjZjMDQ1ZTkyZTA4YTc1MWYwOTM0NDM4ZWY1ZGVkNDY5MzNhYTQyZGFlNzIyZmUwMDg3NDU0NyIsInZlcnNpb24iOjF9.mW5ftkq50Se58M-jm6a2Pu93QeKa3MfV7xcBwvG3PSB_KNJxZWTCpfMQp-Cmx_EMlmI2siKOyd8akYjJUrzJCA\n    - type: loss\n      value: 0.39013850688934326\n      name: loss\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMTZiNzAyZDc0MzUzMmE1MGJiN2JlYzFiODE5ZTNlNGE4MmI4YzRiMTc2ODEzMTUwZmEzOTgxNzc4YjJjZTRmNiIsInZlcnNpb24iOjF9.VqIC7uYC-ZZ8ss9zQOlRV39YVOOLc5R36sIzCcVz8lolh61ux_5djm2XjpP6ARc6KqEnXC4ZtfNXsX2HZfrtCQ\n  - task:\n      type: text-classification\n      name: Text Classification\n    dataset:\n      name: sst2\n      type: sst2\n      config: default\n      split: train\n    metrics:\n    - type: accuracy\n      value: 0.9885521685548412\n      name: Accuracy\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiY2I3NzU3YzhmMDkxZTViY2M3OTY1NmI0ZTdmMDQxNjNjYzJiZmQxNzczM2E4YmExYTY5ODY0NDBkY2I4ZjNkOCIsInZlcnNpb24iOjF9.4Gtk3FeVc9sPWSqZIaeUXJ9oVlPzm-NmujnWpK2y5s1Vhp1l6Y1pK5_78wW0-NxSvQqV6qd5KQf_OAEpVAkQDA\n    - type: precision\n      value: 0.9881965062029833\n      name: Precision Macro\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZDdlZDMzY2I3MTAwYTljNmM4MGMyMzU2YjAzZDg1NDYwN2ZmM2Y5OWZhMjUyMGJiNjY1YmZiMzFhMDI2ODFhNyIsInZlcnNpb24iOjF9.cqmv6yBxu4St2mykRWrZ07tDsiSLdtLTz2hbqQ7Gm1rMzq9tdlkZ8MyJRxtME_Y8UaOG9rs68pV-gKVUs8wABw\n    - type: precision\n      value: 0.9885521685548412\n      name: Precision Micro\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZjFlYzAzNmE1YjljNjUwNzBjZjEzZDY0ZDQyMmY5ZWM2OTBhNzNjYjYzYTk1YWE1NjU3YTMxZDQwOTE1Y2FkNyIsInZlcnNpb24iOjF9.jnCHOkUHuAOZZ_ZMVOnetx__OVJCS6LOno4caWECAmfrUaIPnPNV9iJ6izRO3sqkHRmxYpWBb-27GJ4N3LU-BQ\n    - type: precision\n      value: 0.9885639626373408\n      name: Precision Weighted\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZGUyODFjNjBlNTE2MTY3ZDAxOGU1N2U0YjUyY2NiZjhkOGVmYThjYjBkNGU3NTRkYzkzNDQ2MmMwMjkwMWNiMyIsInZlcnNpb24iOjF9.zTNabMwApiZyXdr76QUn7WgGB7D7lP-iqS3bn35piqVTNsv3wnKjZOaKFVLIUvtBXq4gKw7N2oWxvWc4OcSNDg\n    - type: recall\n      value: 0.9886145346602994\n      name: Recall Macro\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiNTU1YjlhODU3YTkyNTdiZDcwZGFlZDBiYjY0N2NjMGM2NTRiNjQ3MDNjNGMxOWY2ZGQ4NWU1YmMzY2UwZTI3YSIsInZlcnNpb24iOjF9.xaLPY7U-wHsJ3DDui1yyyM-xWjL0Jz5puRThy7fczal9x05eKEQ9s0a_WD-iLmapvJs0caXpV70hDe2NLcs-DA\n    - type: recall\n      value: 0.9885521685548412\n      name: Recall Micro\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiODE0YTU0MDBlOGY4YzU0MjY5MzA3OTk2OGNhOGVkMmU5OGRjZmFiZWI2ZjY5ODEzZTQzMTI0N2NiOTVkNDliYiIsInZlcnNpb24iOjF9.SOt1baTBbuZRrsvGcak2sUwoTrQzmNCbyV2m1_yjGsU48SBH0NcKXicidNBSnJ6ihM5jf_Lv_B5_eOBkLfNWDQ\n    - type: recall\n      value: 0.9885521685548412\n      name: Recall Weighted\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZWNkNmM0ZGRlNmYxYzIwNDk4OTI5MzIwZWU1NzZjZDVhMDcyNDFlMjBhNDQxODU5OWMwMWNhNGEzNjY3ZGUyOSIsInZlcnNpb24iOjF9.b15Fh70GwtlG3cSqPW-8VEZT2oy0CtgvgEOtWiYonOovjkIQ4RSLFVzVG-YfslaIyfg9RzMWzjhLnMY7Bpn2Aw\n    - type: f1\n      value: 0.9884019815052447\n      name: F1 Macro\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiYmM4NjQ5Yjk5ODRhYTU1MTY3MmRhZDBmODM1NTg3OTFiNWM4NDRmYjI0MzZkNmQ1MzE3MzcxODZlYzBkYTMyYSIsInZlcnNpb24iOjF9.74RaDK8nBVuGRl2Se_-hwQvP6c4lvVxGHpcCWB4uZUCf2_HoC9NT9u7P3pMJfH_tK2cpV7U3VWGgSDhQDi-UBQ\n    - type: f1\n      value: 0.9885521685548412\n      name: F1 Micro\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZDRmYWRmMmQ0YjViZmQxMzhhYTUyOTE1MTc0ZDU1ZjQyZjFhMDYzYzMzZDE0NzZlYzQyOTBhMTBhNmM5NTlkMiIsInZlcnNpb24iOjF9.VMn_psdAHIZTlW6GbjERZDe8MHhwzJ0rbjV_VJyuMrsdOh5QDmko-wEvaBWNEdT0cEKsbggm-6jd3Gh81PfHAQ\n    - type: f1\n      value: 0.9885546181087554\n      name: F1 Weighted\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMjUyZWFhZDZhMGQ3MzBmYmRiNDVmN2FkZDBjMjk3ODk0OTAxNGZkMWE0NzU5ZjI0NzE0NGZiNzM0N2Y2NDYyOSIsInZlcnNpb24iOjF9.YsXBhnzEEFEW6jw3mQlFUuIrW7Gabad2Ils-iunYJr-myg0heF8NEnEWABKFE1SnvCWt-69jkLza6SupeyLVCA\n    - type: loss\n      value: 0.040652573108673096\n      name: loss\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZTc3YjU3MjdjMzkxODA5MjU5NGUyY2NkMGVhZDg3ZWEzMmU1YWVjMmI0NmU2OWEyZTkzMTVjNDZiYTc0YjIyNCIsInZlcnNpb24iOjF9.lA90qXZVYiILHMFlr6t6H81Oe8a-4KmeX-vyCC1BDia2ofudegv6Vb46-4RzmbtuKeV6yy6YNNXxXxqVak1pAg\n---\n\n# DistilBERT base uncased finetuned SST-2\n\n## Table of Contents\n- [Model Details](#model-details)\n- [How to Get Started With the Model](#how-to-get-started-with-the-model)\n- [Uses](#uses)\n- [Risks, Limitations and Biases](#risks-limitations-and-biases)\n- [Training](#training)\n\n## Model Details\n**Model Description:** This model is a fine-tune checkpoint of [DistilBERT-base-uncased](https://huggingface.co/distilbert-base-uncased), fine-tuned on SST-2.\nThis model reaches an accuracy of 91.3 on the dev set (for comparison, Bert bert-base-uncased version reaches an accuracy of 92.7).\n- **Developed by:** Hugging Face\n- **Model Type:** Text Classification\n- **Language(s):** English\n- **License:** Apache-2.0\n- **Parent Model:** For more details about DistilBERT, we encourage users to check out [this model card](https://huggingface.co/distilbert-base-uncased).\n- **Resources for more information:**\n    - [Model Documentation](https://huggingface.co/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertForSequenceClassification)\n    - [DistilBERT paper](https://arxiv.org/abs/1910.01108)\n\n## How to Get Started With the Model\n\nExample of single-label classification:\n​​\n```python\nimport torch\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n\ntokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\nmodel = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n\ninputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\nwith torch.no_grad():\n    logits = model(**inputs).logits\n\npredicted_class_id = logits.argmax().item()\nmodel.config.id2label[predicted_class_id]\n\n```\n\n## Uses\n\n#### Direct Use\n\nThis model can be used for  topic classification. You can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to be fine-tuned on a downstream task. See the model hub to look for fine-tuned versions on a task that interests you.\n\n#### Misuse and Out-of-scope Use\nThe model should not be used to intentionally create hostile or alienating environments for people. In addition, the model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model.\n\n\n## Risks, Limitations and Biases\n\nBased on a few experimentations, we observed that this model could produce biased predictions that target underrepresented populations.\n\nFor instance, for sentences like `This film was filmed in COUNTRY`, this binary classification model will give radically different probabilities for the positive label depending on the country (0.89 if the country is France, but 0.08 if the country is Afghanistan) when nothing in the input indicates such a strong semantic shift. In this [colab](https://colab.research.google.com/gist/ageron/fb2f64fb145b4bc7c49efc97e5f114d3/biasmap.ipynb), [Aurélien Géron](https://twitter.com/aureliengeron) made an interesting map plotting these probabilities for each country.\n\n<img src=\"https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english/resolve/main/map.jpeg\" alt=\"Map of positive probabilities per country.\" width=\"500\"/>\n\nWe strongly advise users to thoroughly probe these aspects on their use-cases in order to evaluate the risks of this model. We recommend looking at the following bias evaluation datasets as a place to start: [WinoBias](https://huggingface.co/datasets/wino_bias), [WinoGender](https://huggingface.co/datasets/super_glue), [Stereoset](https://huggingface.co/datasets/stereoset).\n\n\n\n# Training\n\n\n#### Training Data\n\n\nThe authors use the following Stanford Sentiment Treebank([sst2](https://huggingface.co/datasets/sst2)) corpora for the model.\n\n#### Training Procedure\n\n###### Fine-tuning hyper-parameters\n\n\n- learning_rate = 1e-5\n- batch_size = 32\n- warmup = 600\n- max_seq_length = 128\n- num_train_epochs = 3.0\n\n\n",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModelForSequenceClassification",
      "custom_class": null,
      "pipeline_tag": "text-classification",
      "processor": "AutoTokenizer"
    },
    "safetensors": {
      "parameters": {
        "F32": 66955010
      },
      "total": 66955010
    },
    "model_index": [
      {
        "name": "distilbert-base-uncased-finetuned-sst-2-english",
        "results": [
          {
            "task": {
              "type": "text-classification",
              "name": "Text Classification"
            },
            "dataset": {
              "name": "glue",
              "type": "glue",
              "config": "sst2",
              "split": "validation"
            },
            "metrics": [
              {
                "type": "accuracy",
                "value": 0.9105504587155964,
                "name": "Accuracy",
                "verified": true,
                "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiN2YyOGMxYjY2Y2JhMjkxNjIzN2FmMjNiNmM2ZWViNGY3MTNmNWI2YzhiYjYxZTY0ZGUyN2M1NGIxZjRiMjQwZiIsInZlcnNpb24iOjF9.uui0srxV5ZHRhxbYN6082EZdwpnBgubPJ5R2-Wk8HTWqmxYE3QHidevR9LLAhidqGw6Ih93fK0goAXncld_gBg"
              },
              {
                "type": "precision",
                "value": 0.8978260869565218,
                "name": "Precision",
                "verified": true,
                "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMzgwYTYwYjA2MmM0ZTYwNDk0M2NmNTBkZmM2NGNhYzQ1OGEyN2NkNDQ3Mzc2NTQyMmZiNDJiNzBhNGVhZGUyOSIsInZlcnNpb24iOjF9.eHjLmw3K02OU69R2Au8eyuSqT3aBDHgZCn8jSzE3_urD6EUSSsLxUpiAYR4BGLD_U6-ZKcdxVo_A2rdXqvUJDA"
              },
              {
                "type": "recall",
                "value": 0.9301801801801802,
                "name": "Recall",
                "verified": true,
                "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMGIzM2E3MTI2Mzc2MDYwNmU3ZTVjYmZmZDBkNjY4ZTc5MGY0Y2FkNDU3NjY1MmVkNmE3Y2QzMzAwZDZhOWY1NiIsInZlcnNpb24iOjF9.PUZlqmct13-rJWBXdHm5tdkXgETL9F82GNbbSR4hI8MB-v39KrK59cqzFC2Ac7kJe_DtOeUyosj34O_mFt_1DQ"
              },
              {
                "type": "auc",
                "value": 0.9716626673402374,
                "name": "AUC",
                "verified": true,
                "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMDM0YWIwZmQ4YjUwOGZmMWU2MjI1YjIxZGQ2MzNjMzRmZmYxMzZkNGFjODhlMDcyZDM1Y2RkMWZlOWQ0MWYwNSIsInZlcnNpb24iOjF9.E7GRlAXmmpEkTHlXheVkuL1W4WNjv4JO3qY_WCVsTVKiO7bUu0UVjPIyQ6g-J1OxsfqZmW3Leli1wY8vPBNNCQ"
              },
              {
                "type": "f1",
                "value": 0.9137168141592922,
                "name": "F1",
                "verified": true,
                "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMGU4MjNmOGYwZjZjMDQ1ZTkyZTA4YTc1MWYwOTM0NDM4ZWY1ZGVkNDY5MzNhYTQyZGFlNzIyZmUwMDg3NDU0NyIsInZlcnNpb24iOjF9.mW5ftkq50Se58M-jm6a2Pu93QeKa3MfV7xcBwvG3PSB_KNJxZWTCpfMQp-Cmx_EMlmI2siKOyd8akYjJUrzJCA"
              },
              {
                "type": "loss",
                "value": 0.39013850688934326,
                "name": "loss",
                "verified": true,
                "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMTZiNzAyZDc0MzUzMmE1MGJiN2JlYzFiODE5ZTNlNGE4MmI4YzRiMTc2ODEzMTUwZmEzOTgxNzc4YjJjZTRmNiIsInZlcnNpb24iOjF9.VqIC7uYC-ZZ8ss9zQOlRV39YVOOLc5R36sIzCcVz8lolh61ux_5djm2XjpP6ARc6KqEnXC4ZtfNXsX2HZfrtCQ"
              }
            ]
          },
          {
            "task": {
              "type": "text-classification",
              "name": "Text Classification"
            },
            "dataset": {
              "name": "sst2",
              "type": "sst2",
              "config": "default",
              "split": "train"
            },
            "metrics": [
              {
                "type": "accuracy",
                "value": 0.9885521685548412,
                "name": "Accuracy",
                "verified": true,
                "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiY2I3NzU3YzhmMDkxZTViY2M3OTY1NmI0ZTdmMDQxNjNjYzJiZmQxNzczM2E4YmExYTY5ODY0NDBkY2I4ZjNkOCIsInZlcnNpb24iOjF9.4Gtk3FeVc9sPWSqZIaeUXJ9oVlPzm-NmujnWpK2y5s1Vhp1l6Y1pK5_78wW0-NxSvQqV6qd5KQf_OAEpVAkQDA"
              },
              {
                "type": "precision",
                "value": 0.9881965062029833,
                "name": "Precision Macro",
                "verified": true,
                "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZDdlZDMzY2I3MTAwYTljNmM4MGMyMzU2YjAzZDg1NDYwN2ZmM2Y5OWZhMjUyMGJiNjY1YmZiMzFhMDI2ODFhNyIsInZlcnNpb24iOjF9.cqmv6yBxu4St2mykRWrZ07tDsiSLdtLTz2hbqQ7Gm1rMzq9tdlkZ8MyJRxtME_Y8UaOG9rs68pV-gKVUs8wABw"
              },
              {
                "type": "precision",
                "value": 0.9885521685548412,
                "name": "Precision Micro",
                "verified": true,
                "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZjFlYzAzNmE1YjljNjUwNzBjZjEzZDY0ZDQyMmY5ZWM2OTBhNzNjYjYzYTk1YWE1NjU3YTMxZDQwOTE1Y2FkNyIsInZlcnNpb24iOjF9.jnCHOkUHuAOZZ_ZMVOnetx__OVJCS6LOno4caWECAmfrUaIPnPNV9iJ6izRO3sqkHRmxYpWBb-27GJ4N3LU-BQ"
              },
              {
                "type": "precision",
                "value": 0.9885639626373408,
                "name": "Precision Weighted",
                "verified": true,
                "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZGUyODFjNjBlNTE2MTY3ZDAxOGU1N2U0YjUyY2NiZjhkOGVmYThjYjBkNGU3NTRkYzkzNDQ2MmMwMjkwMWNiMyIsInZlcnNpb24iOjF9.zTNabMwApiZyXdr76QUn7WgGB7D7lP-iqS3bn35piqVTNsv3wnKjZOaKFVLIUvtBXq4gKw7N2oWxvWc4OcSNDg"
              },
              {
                "type": "recall",
                "value": 0.9886145346602994,
                "name": "Recall Macro",
                "verified": true,
                "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiNTU1YjlhODU3YTkyNTdiZDcwZGFlZDBiYjY0N2NjMGM2NTRiNjQ3MDNjNGMxOWY2ZGQ4NWU1YmMzY2UwZTI3YSIsInZlcnNpb24iOjF9.xaLPY7U-wHsJ3DDui1yyyM-xWjL0Jz5puRThy7fczal9x05eKEQ9s0a_WD-iLmapvJs0caXpV70hDe2NLcs-DA"
              },
              {
                "type": "recall",
                "value": 0.9885521685548412,
                "name": "Recall Micro",
                "verified": true,
                "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiODE0YTU0MDBlOGY4YzU0MjY5MzA3OTk2OGNhOGVkMmU5OGRjZmFiZWI2ZjY5ODEzZTQzMTI0N2NiOTVkNDliYiIsInZlcnNpb24iOjF9.SOt1baTBbuZRrsvGcak2sUwoTrQzmNCbyV2m1_yjGsU48SBH0NcKXicidNBSnJ6ihM5jf_Lv_B5_eOBkLfNWDQ"
              },
              {
                "type": "recall",
                "value": 0.9885521685548412,
                "name": "Recall Weighted",
                "verified": true,
                "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZWNkNmM0ZGRlNmYxYzIwNDk4OTI5MzIwZWU1NzZjZDVhMDcyNDFlMjBhNDQxODU5OWMwMWNhNGEzNjY3ZGUyOSIsInZlcnNpb24iOjF9.b15Fh70GwtlG3cSqPW-8VEZT2oy0CtgvgEOtWiYonOovjkIQ4RSLFVzVG-YfslaIyfg9RzMWzjhLnMY7Bpn2Aw"
              },
              {
                "type": "f1",
                "value": 0.9884019815052447,
                "name": "F1 Macro",
                "verified": true,
                "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiYmM4NjQ5Yjk5ODRhYTU1MTY3MmRhZDBmODM1NTg3OTFiNWM4NDRmYjI0MzZkNmQ1MzE3MzcxODZlYzBkYTMyYSIsInZlcnNpb24iOjF9.74RaDK8nBVuGRl2Se_-hwQvP6c4lvVxGHpcCWB4uZUCf2_HoC9NT9u7P3pMJfH_tK2cpV7U3VWGgSDhQDi-UBQ"
              },
              {
                "type": "f1",
                "value": 0.9885521685548412,
                "name": "F1 Micro",
                "verified": true,
                "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZDRmYWRmMmQ0YjViZmQxMzhhYTUyOTE1MTc0ZDU1ZjQyZjFhMDYzYzMzZDE0NzZlYzQyOTBhMTBhNmM5NTlkMiIsInZlcnNpb24iOjF9.VMn_psdAHIZTlW6GbjERZDe8MHhwzJ0rbjV_VJyuMrsdOh5QDmko-wEvaBWNEdT0cEKsbggm-6jd3Gh81PfHAQ"
              },
              {
                "type": "f1",
                "value": 0.9885546181087554,
                "name": "F1 Weighted",
                "verified": true,
                "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMjUyZWFhZDZhMGQ3MzBmYmRiNDVmN2FkZDBjMjk3ODk0OTAxNGZkMWE0NzU5ZjI0NzE0NGZiNzM0N2Y2NDYyOSIsInZlcnNpb24iOjF9.YsXBhnzEEFEW6jw3mQlFUuIrW7Gabad2Ils-iunYJr-myg0heF8NEnEWABKFE1SnvCWt-69jkLza6SupeyLVCA"
              },
              {
                "type": "loss",
                "value": 0.040652573108673096,
                "name": "loss",
                "verified": true,
                "verifyToken": "eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZTc3YjU3MjdjMzkxODA5MjU5NGUyY2NkMGVhZDg3ZWEzMmU1YWVjMmI0NmU2OWEyZTkzMTVjNDZiYTc0YjIyNCIsInZlcnNpb24iOjF9.lA90qXZVYiILHMFlr6t6H81Oe8a-4KmeX-vyCC1BDia2ofudegv6Vb46-4RzmbtuKeV6yy6YNNXxXxqVak1pAg"
              }
            ]
          }
        ]
      }
    ],
    "trending_score": null
  },
  {
    "model_id": "papluca/xlm-roberta-base-language-detection",
    "model_name": "papluca/xlm-roberta-base-language-detection",
    "author": "papluca",
    "downloads": 4434573,
    "downloads_all_time": null,
    "likes": 324,
    "tags": [
      "transformers",
      "pytorch",
      "tf",
      "safetensors",
      "xlm-roberta",
      "text-classification",
      "generated_from_trainer",
      "multilingual",
      "ar",
      "bg",
      "de",
      "el",
      "en",
      "es",
      "fr",
      "hi",
      "it",
      "ja",
      "nl",
      "pl",
      "pt",
      "ru",
      "sw",
      "th",
      "tr",
      "ur",
      "vi",
      "zh",
      "dataset:papluca/language-identification",
      "arxiv:1911.02116",
      "base_model:FacebookAI/xlm-roberta-base",
      "base_model:finetune:FacebookAI/xlm-roberta-base",
      "doi:10.57967/hf/2064",
      "license:mit",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/papluca/xlm-roberta-base-language-detection",
    "dependencies": [
      [
        "transformers",
        "4.12.5"
      ],
      [
        "pytorch",
        "1.10.0+cu111"
      ],
      [
        "datasets",
        "1.15.1"
      ],
      [
        "tokenizers",
        "0.10.3"
      ]
    ],
    "last_modified": "2023-12-28T13:54:18+00:00",
    "created_at": "2022-03-02T23:29:05+00:00",
    "analysis_date": "2025-03-22T00:54:49.084194",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "xlm-roberta",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": [
        "multilingual",
        "ar",
        "bg",
        "de",
        "el",
        "en",
        "es",
        "fr",
        "hi",
        "it",
        "ja",
        "nl",
        "pl",
        "pt",
        "ru",
        "sw",
        "th",
        "tr",
        "ur",
        "vi",
        "zh"
      ],
      "license": "mit",
      "tags": [
        "generated_from_trainer"
      ],
      "datasets": "papluca/language-identification",
      "metrics": [
        "accuracy",
        "f1"
      ],
      "base_model": "xlm-roberta-base",
      "model-index": [
        {
          "name": "xlm-roberta-base-language-detection",
          "results": []
        }
      ]
    },
    "card_text": "\n# xlm-roberta-base-language-detection\n\nThis model is a fine-tuned version of [xlm-roberta-base](https://huggingface.co/xlm-roberta-base) on the [Language Identification](https://huggingface.co/datasets/papluca/language-identification#additional-information) dataset.\n\n## Model description\n\nThis model is an XLM-RoBERTa transformer model with a classification head on top (i.e. a linear layer on top of the pooled output). \nFor additional information please refer to the [xlm-roberta-base](https://huggingface.co/xlm-roberta-base) model card or to the paper [Unsupervised Cross-lingual Representation Learning at Scale](https://arxiv.org/abs/1911.02116) by Conneau et al.\n\n## Intended uses & limitations\n\nYou can directly use this model as a language detector, i.e. for sequence classification tasks. Currently, it supports the following 20 languages: \n\n`arabic (ar), bulgarian (bg), german (de), modern greek (el), english (en), spanish (es), french (fr), hindi (hi), italian (it), japanese (ja), dutch (nl), polish (pl), portuguese (pt), russian (ru), swahili (sw), thai (th), turkish (tr), urdu (ur), vietnamese (vi), and chinese (zh)`\n\n## Training and evaluation data\n\nThe model was fine-tuned on the [Language Identification](https://huggingface.co/datasets/papluca/language-identification#additional-information) dataset, which consists of text sequences in 20 languages. The training set contains 70k samples, while the validation and test sets 10k each. The average accuracy on the test set is **99.6%** (this matches the average macro/weighted F1-score being the test set perfectly balanced). A more detailed evaluation is provided by the following table.\n\n| Language | Precision | Recall | F1-score | support |\n|:--------:|:---------:|:------:|:--------:|:-------:|\n|ar        |0.998      |0.996   |0.997     |500      |\n|bg        |0.998      |0.964   |0.981     |500      |\n|de        |0.998      |0.996   |0.997     |500      |\n|el        |0.996      |1.000   |0.998     |500      |\n|en        |1.000      |1.000   |1.000     |500      |\n|es        |0.967      |1.000   |0.983     |500      |\n|fr        |1.000      |1.000   |1.000     |500      |\n|hi        |0.994      |0.992   |0.993     |500      |\n|it        |1.000      |0.992   |0.996     |500      |\n|ja        |0.996      |0.996   |0.996     |500      |\n|nl        |1.000      |1.000   |1.000     |500      |\n|pl        |1.000      |1.000   |1.000     |500      |\n|pt        |0.988      |1.000   |0.994     |500      |\n|ru        |1.000      |0.994   |0.997     |500      |\n|sw        |1.000      |1.000   |1.000     |500      |\n|th        |1.000      |0.998   |0.999     |500      |\n|tr        |0.994      |0.992   |0.993     |500      |\n|ur        |1.000      |1.000   |1.000     |500      |\n|vi        |0.992      |1.000   |0.996     |500      |\n|zh        |1.000      |1.000   |1.000     |500      |\n\n### Benchmarks\n\nAs a baseline to compare `xlm-roberta-base-language-detection` against, we have used the Python [langid](https://github.com/saffsd/langid.py) library. Since it comes pre-trained on 97 languages, we have used its `.set_languages()` method to constrain the language set to our 20 languages. The average accuracy of langid on the test set is **98.5%**. More details are provided by the table below.\n\n| Language | Precision | Recall | F1-score | support |\n|:--------:|:---------:|:------:|:--------:|:-------:|\n|ar        |0.990      |0.970   |0.980     |500      |\n|bg        |0.998      |0.964   |0.981     |500      |\n|de        |0.992      |0.944   |0.967     |500      |\n|el        |1.000      |0.998   |0.999     |500      |\n|en        |1.000      |1.000   |1.000     |500      |\n|es        |1.000      |0.968   |0.984     |500      |\n|fr        |0.996      |1.000   |0.998     |500      |\n|hi        |0.949      |0.976   |0.963     |500      |\n|it        |0.990      |0.980   |0.985     |500      |\n|ja        |0.927      |0.988   |0.956     |500      |\n|nl        |0.980      |1.000   |0.990     |500      |\n|pl        |0.986      |0.996   |0.991     |500      |\n|pt        |0.950      |0.996   |0.973     |500      |\n|ru        |0.996      |0.974   |0.985     |500      |\n|sw        |1.000      |1.000   |1.000     |500      |\n|th        |1.000      |0.996   |0.998     |500      |\n|tr        |0.990      |0.968   |0.979     |500      |\n|ur        |0.998      |0.996   |0.997     |500      |\n|vi        |0.971      |0.990   |0.980     |500      |\n|zh        |1.000      |1.000   |1.000     |500      |\n\n## How to get started with the model\n\nThe easiest way to use the model is via the high-level `pipeline` API:\n\n```python\nfrom transformers import pipeline\n\ntext = [\n    \"Brevity is the soul of wit.\",\n    \"Amor, ch'a nullo amato amar perdona.\"\n]\n\nmodel_ckpt = \"papluca/xlm-roberta-base-language-detection\"\npipe = pipeline(\"text-classification\", model=model_ckpt)\npipe(text, top_k=1, truncation=True)\n```\n\nOr one can proceed with the tokenizer and model separately:\n\n```python\nimport torch\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\ntext = [\n    \"Brevity is the soul of wit.\",\n    \"Amor, ch'a nullo amato amar perdona.\"\n]\n\nmodel_ckpt = \"papluca/xlm-roberta-base-language-detection\"\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_ckpt)\n\ninputs = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n\nwith torch.no_grad():\n    logits = model(**inputs).logits\n\npreds = torch.softmax(logits, dim=-1)\n\n# Map raw predictions to languages\nid2lang = model.config.id2label\nvals, idxs = torch.max(preds, dim=1)\n{id2lang[k.item()]: v.item() for k, v in zip(idxs, vals)}\n```\n\n## Training procedure\n\nFine-tuning was done via the `Trainer` API. Here is the [Colab notebook](https://colab.research.google.com/drive/15LJTckS6gU3RQOmjLqxVNBmbsBdnUEvl?usp=sharing) with the training code.\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 64\n- eval_batch_size: 128\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 2\n- mixed_precision_training: Native AMP\n\n### Training results\n\nThe validation results on the `valid` split of the Language Identification dataset are summarised here below.\n\n| Training Loss | Epoch | Step | Validation Loss | Accuracy | F1     |\n|:-------------:|:-----:|:----:|:---------------:|:--------:|:------:|\n| 0.2492        | 1.0   | 1094 | 0.0149          | 0.9969   | 0.9969 |\n| 0.0101        | 2.0   | 2188 | 0.0103          | 0.9977   | 0.9977 |\n\nIn short, it achieves the following results on the validation set:\n- Loss: 0.0101\n- Accuracy: 0.9977\n- F1: 0.9977\n\n### Framework versions\n\n- Transformers 4.12.5\n- Pytorch 1.10.0+cu111\n- Datasets 1.15.1\n- Tokenizers 0.10.3\n",
    "card_content": "---\nlanguage:\n- multilingual\n- ar\n- bg\n- de\n- el\n- en\n- es\n- fr\n- hi\n- it\n- ja\n- nl\n- pl\n- pt\n- ru\n- sw\n- th\n- tr\n- ur\n- vi\n- zh\nlicense: mit\ntags:\n- generated_from_trainer\ndatasets: papluca/language-identification\nmetrics:\n- accuracy\n- f1\nbase_model: xlm-roberta-base\nmodel-index:\n- name: xlm-roberta-base-language-detection\n  results: []\n---\n\n# xlm-roberta-base-language-detection\n\nThis model is a fine-tuned version of [xlm-roberta-base](https://huggingface.co/xlm-roberta-base) on the [Language Identification](https://huggingface.co/datasets/papluca/language-identification#additional-information) dataset.\n\n## Model description\n\nThis model is an XLM-RoBERTa transformer model with a classification head on top (i.e. a linear layer on top of the pooled output). \nFor additional information please refer to the [xlm-roberta-base](https://huggingface.co/xlm-roberta-base) model card or to the paper [Unsupervised Cross-lingual Representation Learning at Scale](https://arxiv.org/abs/1911.02116) by Conneau et al.\n\n## Intended uses & limitations\n\nYou can directly use this model as a language detector, i.e. for sequence classification tasks. Currently, it supports the following 20 languages: \n\n`arabic (ar), bulgarian (bg), german (de), modern greek (el), english (en), spanish (es), french (fr), hindi (hi), italian (it), japanese (ja), dutch (nl), polish (pl), portuguese (pt), russian (ru), swahili (sw), thai (th), turkish (tr), urdu (ur), vietnamese (vi), and chinese (zh)`\n\n## Training and evaluation data\n\nThe model was fine-tuned on the [Language Identification](https://huggingface.co/datasets/papluca/language-identification#additional-information) dataset, which consists of text sequences in 20 languages. The training set contains 70k samples, while the validation and test sets 10k each. The average accuracy on the test set is **99.6%** (this matches the average macro/weighted F1-score being the test set perfectly balanced). A more detailed evaluation is provided by the following table.\n\n| Language | Precision | Recall | F1-score | support |\n|:--------:|:---------:|:------:|:--------:|:-------:|\n|ar        |0.998      |0.996   |0.997     |500      |\n|bg        |0.998      |0.964   |0.981     |500      |\n|de        |0.998      |0.996   |0.997     |500      |\n|el        |0.996      |1.000   |0.998     |500      |\n|en        |1.000      |1.000   |1.000     |500      |\n|es        |0.967      |1.000   |0.983     |500      |\n|fr        |1.000      |1.000   |1.000     |500      |\n|hi        |0.994      |0.992   |0.993     |500      |\n|it        |1.000      |0.992   |0.996     |500      |\n|ja        |0.996      |0.996   |0.996     |500      |\n|nl        |1.000      |1.000   |1.000     |500      |\n|pl        |1.000      |1.000   |1.000     |500      |\n|pt        |0.988      |1.000   |0.994     |500      |\n|ru        |1.000      |0.994   |0.997     |500      |\n|sw        |1.000      |1.000   |1.000     |500      |\n|th        |1.000      |0.998   |0.999     |500      |\n|tr        |0.994      |0.992   |0.993     |500      |\n|ur        |1.000      |1.000   |1.000     |500      |\n|vi        |0.992      |1.000   |0.996     |500      |\n|zh        |1.000      |1.000   |1.000     |500      |\n\n### Benchmarks\n\nAs a baseline to compare `xlm-roberta-base-language-detection` against, we have used the Python [langid](https://github.com/saffsd/langid.py) library. Since it comes pre-trained on 97 languages, we have used its `.set_languages()` method to constrain the language set to our 20 languages. The average accuracy of langid on the test set is **98.5%**. More details are provided by the table below.\n\n| Language | Precision | Recall | F1-score | support |\n|:--------:|:---------:|:------:|:--------:|:-------:|\n|ar        |0.990      |0.970   |0.980     |500      |\n|bg        |0.998      |0.964   |0.981     |500      |\n|de        |0.992      |0.944   |0.967     |500      |\n|el        |1.000      |0.998   |0.999     |500      |\n|en        |1.000      |1.000   |1.000     |500      |\n|es        |1.000      |0.968   |0.984     |500      |\n|fr        |0.996      |1.000   |0.998     |500      |\n|hi        |0.949      |0.976   |0.963     |500      |\n|it        |0.990      |0.980   |0.985     |500      |\n|ja        |0.927      |0.988   |0.956     |500      |\n|nl        |0.980      |1.000   |0.990     |500      |\n|pl        |0.986      |0.996   |0.991     |500      |\n|pt        |0.950      |0.996   |0.973     |500      |\n|ru        |0.996      |0.974   |0.985     |500      |\n|sw        |1.000      |1.000   |1.000     |500      |\n|th        |1.000      |0.996   |0.998     |500      |\n|tr        |0.990      |0.968   |0.979     |500      |\n|ur        |0.998      |0.996   |0.997     |500      |\n|vi        |0.971      |0.990   |0.980     |500      |\n|zh        |1.000      |1.000   |1.000     |500      |\n\n## How to get started with the model\n\nThe easiest way to use the model is via the high-level `pipeline` API:\n\n```python\nfrom transformers import pipeline\n\ntext = [\n    \"Brevity is the soul of wit.\",\n    \"Amor, ch'a nullo amato amar perdona.\"\n]\n\nmodel_ckpt = \"papluca/xlm-roberta-base-language-detection\"\npipe = pipeline(\"text-classification\", model=model_ckpt)\npipe(text, top_k=1, truncation=True)\n```\n\nOr one can proceed with the tokenizer and model separately:\n\n```python\nimport torch\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\ntext = [\n    \"Brevity is the soul of wit.\",\n    \"Amor, ch'a nullo amato amar perdona.\"\n]\n\nmodel_ckpt = \"papluca/xlm-roberta-base-language-detection\"\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_ckpt)\n\ninputs = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n\nwith torch.no_grad():\n    logits = model(**inputs).logits\n\npreds = torch.softmax(logits, dim=-1)\n\n# Map raw predictions to languages\nid2lang = model.config.id2label\nvals, idxs = torch.max(preds, dim=1)\n{id2lang[k.item()]: v.item() for k, v in zip(idxs, vals)}\n```\n\n## Training procedure\n\nFine-tuning was done via the `Trainer` API. Here is the [Colab notebook](https://colab.research.google.com/drive/15LJTckS6gU3RQOmjLqxVNBmbsBdnUEvl?usp=sharing) with the training code.\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 64\n- eval_batch_size: 128\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 2\n- mixed_precision_training: Native AMP\n\n### Training results\n\nThe validation results on the `valid` split of the Language Identification dataset are summarised here below.\n\n| Training Loss | Epoch | Step | Validation Loss | Accuracy | F1     |\n|:-------------:|:-----:|:----:|:---------------:|:--------:|:------:|\n| 0.2492        | 1.0   | 1094 | 0.0149          | 0.9969   | 0.9969 |\n| 0.0101        | 2.0   | 2188 | 0.0103          | 0.9977   | 0.9977 |\n\nIn short, it achieves the following results on the validation set:\n- Loss: 0.0101\n- Accuracy: 0.9977\n- F1: 0.9977\n\n### Framework versions\n\n- Transformers 4.12.5\n- Pytorch 1.10.0+cu111\n- Datasets 1.15.1\n- Tokenizers 0.10.3\n",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModelForSequenceClassification",
      "custom_class": null,
      "pipeline_tag": "text-classification",
      "processor": "AutoTokenizer"
    },
    "safetensors": {
      "parameters": {
        "I64": 514,
        "F32": 278059028
      },
      "total": 278059542
    },
    "model_index": [
      {
        "name": "xlm-roberta-base-language-detection",
        "results": []
      }
    ],
    "trending_score": null
  },
  {
    "model_id": "ProsusAI/finbert",
    "model_name": "ProsusAI/finbert",
    "author": "ProsusAI",
    "downloads": 2641745,
    "downloads_all_time": null,
    "likes": 823,
    "tags": [
      "transformers",
      "pytorch",
      "tf",
      "jax",
      "bert",
      "text-classification",
      "financial-sentiment-analysis",
      "sentiment-analysis",
      "en",
      "arxiv:1908.10063",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/ProsusAI/finbert",
    "dependencies": null,
    "last_modified": "2023-05-23T12:43:35+00:00",
    "created_at": "2022-03-02T23:29:04+00:00",
    "analysis_date": "2025-03-22T00:54:49.876760",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "bert",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": "en",
      "tags": [
        "financial-sentiment-analysis",
        "sentiment-analysis"
      ],
      "widget": [
        {
          "text": "Stocks rallied and the British pound gained."
        }
      ]
    },
    "card_text": "\nFinBERT is a pre-trained NLP model to analyze sentiment of financial text. It is built by further training the BERT language model in the finance domain, using a large financial corpus and thereby fine-tuning it for financial sentiment classification. [Financial PhraseBank](https://www.researchgate.net/publication/251231107_Good_Debt_or_Bad_Debt_Detecting_Semantic_Orientations_in_Economic_Texts) by Malo et al. (2014) is used for fine-tuning. For more details, please see the paper [FinBERT: Financial Sentiment Analysis with Pre-trained Language Models](https://arxiv.org/abs/1908.10063) and our related [blog post](https://medium.com/prosus-ai-tech-blog/finbert-financial-sentiment-analysis-with-bert-b277a3607101) on Medium.\n\nThe model will give softmax outputs for three labels: positive, negative or neutral.\n\n---\n\nAbout Prosus\n\nProsus is a global consumer internet group and one of the largest technology investors in the world. Operating and investing globally in markets with long-term growth potential, Prosus builds leading consumer internet companies that empower people and enrich communities. For more information, please visit www.prosus.com.\n\nContact information\n\nPlease contact Dogu Araci dogu.araci[at]prosus[dot]com and Zulkuf Genc zulkuf.genc[at]prosus[dot]com about any FinBERT related issues and questions.\n",
    "card_content": "---\nlanguage: en\ntags:\n- financial-sentiment-analysis\n- sentiment-analysis\nwidget:\n- text: Stocks rallied and the British pound gained.\n---\n\nFinBERT is a pre-trained NLP model to analyze sentiment of financial text. It is built by further training the BERT language model in the finance domain, using a large financial corpus and thereby fine-tuning it for financial sentiment classification. [Financial PhraseBank](https://www.researchgate.net/publication/251231107_Good_Debt_or_Bad_Debt_Detecting_Semantic_Orientations_in_Economic_Texts) by Malo et al. (2014) is used for fine-tuning. For more details, please see the paper [FinBERT: Financial Sentiment Analysis with Pre-trained Language Models](https://arxiv.org/abs/1908.10063) and our related [blog post](https://medium.com/prosus-ai-tech-blog/finbert-financial-sentiment-analysis-with-bert-b277a3607101) on Medium.\n\nThe model will give softmax outputs for three labels: positive, negative or neutral.\n\n---\n\nAbout Prosus\n\nProsus is a global consumer internet group and one of the largest technology investors in the world. Operating and investing globally in markets with long-term growth potential, Prosus builds leading consumer internet companies that empower people and enrich communities. For more information, please visit www.prosus.com.\n\nContact information\n\nPlease contact Dogu Araci dogu.araci[at]prosus[dot]com and Zulkuf Genc zulkuf.genc[at]prosus[dot]com about any FinBERT related issues and questions.\n",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModelForSequenceClassification",
      "custom_class": null,
      "pipeline_tag": "text-classification",
      "processor": "AutoTokenizer"
    },
    "safetensors": null,
    "model_index": null,
    "trending_score": null
  },
  {
    "model_id": "microsoft/deberta-xlarge-mnli",
    "model_name": "microsoft/deberta-xlarge-mnli",
    "author": "microsoft",
    "downloads": 2322144,
    "downloads_all_time": null,
    "likes": 17,
    "tags": [
      "transformers",
      "pytorch",
      "tf",
      "deberta",
      "text-classification",
      "deberta-v1",
      "deberta-mnli",
      "en",
      "arxiv:2006.03654",
      "license:mit",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/microsoft/deberta-xlarge-mnli",
    "dependencies": [
      [
        "torch",
        null
      ],
      [
        "transformers",
        null
      ]
    ],
    "last_modified": "2022-06-27T15:47:33+00:00",
    "created_at": "2022-03-02T23:29:05+00:00",
    "analysis_date": "2025-03-22T00:54:51.151940",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "deberta",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": "en",
      "tags": [
        "deberta-v1",
        "deberta-mnli"
      ],
      "tasks": "mnli",
      "thumbnail": "https://huggingface.co/front/thumbnails/microsoft.png",
      "license": "mit",
      "widget": [
        {
          "text": "[CLS] I love you. [SEP] I like you. [SEP]"
        }
      ]
    },
    "card_text": "\n## DeBERTa: Decoding-enhanced BERT with Disentangled Attention\n\n[DeBERTa](https://arxiv.org/abs/2006.03654) improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder. It outperforms BERT and RoBERTa on  majority of NLU tasks with 80GB training data. \n\nPlease check the [official repository](https://github.com/microsoft/DeBERTa) for more details and updates.\n\nThis the DeBERTa xlarge model(750M) fine-tuned with mnli task.\n\n### Fine-tuning on NLU tasks\n\nWe present the dev results on SQuAD 1.1/2.0 and several GLUE benchmark tasks.\n\n| Model                     | SQuAD 1.1 | SQuAD 2.0 | MNLI-m/mm   | SST-2 | QNLI | CoLA | RTE    | MRPC  | QQP   |STS-B |\n|---------------------------|-----------|-----------|-------------|-------|------|------|--------|-------|-------|------|\n|                           | F1/EM     | F1/EM     | Acc         | Acc   | Acc  | MCC  | Acc    |Acc/F1 |Acc/F1 |P/S   |\n| BERT-Large                | 90.9/84.1 | 81.8/79.0 | 86.6/-      | 93.2  | 92.3 | 60.6 | 70.4   | 88.0/-       | 91.3/- |90.0/- |\n| RoBERTa-Large             | 94.6/88.9 | 89.4/86.5 | 90.2/-      | 96.4  | 93.9 | 68.0 | 86.6   | 90.9/-       | 92.2/- |92.4/- |\n| XLNet-Large               | 95.1/89.7 | 90.6/87.9 | 90.8/-      | 97.0  | 94.9 | 69.0 | 85.9   | 90.8/-       | 92.3/- |92.5/- |\n| [DeBERTa-Large](https://huggingface.co/microsoft/deberta-large)<sup>1</sup> | 95.5/90.1 | 90.7/88.0 | 91.3/91.1| 96.5|95.3| 69.5| 91.0| 92.6/94.6| 92.3/- |92.8/92.5 |\n| [DeBERTa-XLarge](https://huggingface.co/microsoft/deberta-xlarge)<sup>1</sup> | -/-  | -/-  | 91.5/91.2| 97.0 | - | -    | 93.1   | 92.1/94.3    | -    |92.9/92.7|\n| [DeBERTa-V2-XLarge](https://huggingface.co/microsoft/deberta-v2-xlarge)<sup>1</sup>|95.8/90.8| 91.4/88.9|91.7/91.6| **97.5**| 95.8|71.1|**93.9**|92.0/94.2|92.3/89.8|92.9/92.9|\n|**[DeBERTa-V2-XXLarge](https://huggingface.co/microsoft/deberta-v2-xxlarge)<sup>1,2</sup>**|**96.1/91.4**|**92.2/89.7**|**91.7/91.9**|97.2|**96.0**|**72.0**| 93.5| **93.1/94.9**|**92.7/90.3** |**93.2/93.1** |\n--------\n#### Notes.\n - <sup>1</sup> Following RoBERTa, for RTE, MRPC, STS-B, we fine-tune the tasks based on [DeBERTa-Large-MNLI](https://huggingface.co/microsoft/deberta-large-mnli), [DeBERTa-XLarge-MNLI](https://huggingface.co/microsoft/deberta-xlarge-mnli), [DeBERTa-V2-XLarge-MNLI](https://huggingface.co/microsoft/deberta-v2-xlarge-mnli), [DeBERTa-V2-XXLarge-MNLI](https://huggingface.co/microsoft/deberta-v2-xxlarge-mnli). The results of SST-2/QQP/QNLI/SQuADv2 will also be slightly improved when start from MNLI fine-tuned models, however, we only report the numbers fine-tuned from pretrained base models for those 4 tasks.\n - <sup>2</sup> To try the **XXLarge** model with **[HF transformers](https://huggingface.co/transformers/main_classes/trainer.html)**, you need to specify **--sharded_ddp**\n \n```bash  \ncd transformers/examples/text-classification/\nexport TASK_NAME=mrpc\npython -m torch.distributed.launch --nproc_per_node=8 run_glue.py   --model_name_or_path microsoft/deberta-v2-xxlarge   \\\\\n--task_name $TASK_NAME   --do_train   --do_eval   --max_seq_length 128   --per_device_train_batch_size 4   \\\\\n--learning_rate 3e-6   --num_train_epochs 3   --output_dir /tmp/$TASK_NAME/ --overwrite_output_dir --sharded_ddp --fp16\n```\n\n### Citation\n\nIf you find DeBERTa useful for your work, please cite the following paper:\n\n``` latex\n@inproceedings{\nhe2021deberta,\ntitle={DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION},\nauthor={Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=XPZIaotutsD}\n}\n```\n",
    "card_content": "---\nlanguage: en\ntags:\n- deberta-v1\n- deberta-mnli\ntasks: mnli\nthumbnail: https://huggingface.co/front/thumbnails/microsoft.png\nlicense: mit\nwidget:\n- text: '[CLS] I love you. [SEP] I like you. [SEP]'\n---\n\n## DeBERTa: Decoding-enhanced BERT with Disentangled Attention\n\n[DeBERTa](https://arxiv.org/abs/2006.03654) improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder. It outperforms BERT and RoBERTa on  majority of NLU tasks with 80GB training data. \n\nPlease check the [official repository](https://github.com/microsoft/DeBERTa) for more details and updates.\n\nThis the DeBERTa xlarge model(750M) fine-tuned with mnli task.\n\n### Fine-tuning on NLU tasks\n\nWe present the dev results on SQuAD 1.1/2.0 and several GLUE benchmark tasks.\n\n| Model                     | SQuAD 1.1 | SQuAD 2.0 | MNLI-m/mm   | SST-2 | QNLI | CoLA | RTE    | MRPC  | QQP   |STS-B |\n|---------------------------|-----------|-----------|-------------|-------|------|------|--------|-------|-------|------|\n|                           | F1/EM     | F1/EM     | Acc         | Acc   | Acc  | MCC  | Acc    |Acc/F1 |Acc/F1 |P/S   |\n| BERT-Large                | 90.9/84.1 | 81.8/79.0 | 86.6/-      | 93.2  | 92.3 | 60.6 | 70.4   | 88.0/-       | 91.3/- |90.0/- |\n| RoBERTa-Large             | 94.6/88.9 | 89.4/86.5 | 90.2/-      | 96.4  | 93.9 | 68.0 | 86.6   | 90.9/-       | 92.2/- |92.4/- |\n| XLNet-Large               | 95.1/89.7 | 90.6/87.9 | 90.8/-      | 97.0  | 94.9 | 69.0 | 85.9   | 90.8/-       | 92.3/- |92.5/- |\n| [DeBERTa-Large](https://huggingface.co/microsoft/deberta-large)<sup>1</sup> | 95.5/90.1 | 90.7/88.0 | 91.3/91.1| 96.5|95.3| 69.5| 91.0| 92.6/94.6| 92.3/- |92.8/92.5 |\n| [DeBERTa-XLarge](https://huggingface.co/microsoft/deberta-xlarge)<sup>1</sup> | -/-  | -/-  | 91.5/91.2| 97.0 | - | -    | 93.1   | 92.1/94.3    | -    |92.9/92.7|\n| [DeBERTa-V2-XLarge](https://huggingface.co/microsoft/deberta-v2-xlarge)<sup>1</sup>|95.8/90.8| 91.4/88.9|91.7/91.6| **97.5**| 95.8|71.1|**93.9**|92.0/94.2|92.3/89.8|92.9/92.9|\n|**[DeBERTa-V2-XXLarge](https://huggingface.co/microsoft/deberta-v2-xxlarge)<sup>1,2</sup>**|**96.1/91.4**|**92.2/89.7**|**91.7/91.9**|97.2|**96.0**|**72.0**| 93.5| **93.1/94.9**|**92.7/90.3** |**93.2/93.1** |\n--------\n#### Notes.\n - <sup>1</sup> Following RoBERTa, for RTE, MRPC, STS-B, we fine-tune the tasks based on [DeBERTa-Large-MNLI](https://huggingface.co/microsoft/deberta-large-mnli), [DeBERTa-XLarge-MNLI](https://huggingface.co/microsoft/deberta-xlarge-mnli), [DeBERTa-V2-XLarge-MNLI](https://huggingface.co/microsoft/deberta-v2-xlarge-mnli), [DeBERTa-V2-XXLarge-MNLI](https://huggingface.co/microsoft/deberta-v2-xxlarge-mnli). The results of SST-2/QQP/QNLI/SQuADv2 will also be slightly improved when start from MNLI fine-tuned models, however, we only report the numbers fine-tuned from pretrained base models for those 4 tasks.\n - <sup>2</sup> To try the **XXLarge** model with **[HF transformers](https://huggingface.co/transformers/main_classes/trainer.html)**, you need to specify **--sharded_ddp**\n \n```bash  \ncd transformers/examples/text-classification/\nexport TASK_NAME=mrpc\npython -m torch.distributed.launch --nproc_per_node=8 run_glue.py   --model_name_or_path microsoft/deberta-v2-xxlarge   \\\\\n--task_name $TASK_NAME   --do_train   --do_eval   --max_seq_length 128   --per_device_train_batch_size 4   \\\\\n--learning_rate 3e-6   --num_train_epochs 3   --output_dir /tmp/$TASK_NAME/ --overwrite_output_dir --sharded_ddp --fp16\n```\n\n### Citation\n\nIf you find DeBERTa useful for your work, please cite the following paper:\n\n``` latex\n@inproceedings{\nhe2021deberta,\ntitle={DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION},\nauthor={Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=XPZIaotutsD}\n}\n```\n",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModelForSequenceClassification",
      "custom_class": null,
      "pipeline_tag": "text-classification",
      "processor": "AutoTokenizer"
    },
    "safetensors": null,
    "model_index": null,
    "trending_score": null
  },
  {
    "model_id": "cardiffnlp/twitter-roberta-base-sentiment-latest",
    "model_name": "cardiffnlp/twitter-roberta-base-sentiment-latest",
    "author": "cardiffnlp",
    "downloads": 2222298,
    "downloads_all_time": null,
    "likes": 651,
    "tags": [
      "transformers",
      "pytorch",
      "tf",
      "roberta",
      "text-classification",
      "en",
      "dataset:tweet_eval",
      "arxiv:2202.03829",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest",
    "dependencies": [
      [
        "transformers",
        null
      ],
      [
        "numpy",
        null
      ],
      [
        "scipy",
        null
      ]
    ],
    "last_modified": "2023-05-28T05:45:10+00:00",
    "created_at": "2022-03-15T01:21:58+00:00",
    "analysis_date": "2025-03-22T00:54:52.571023",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "roberta",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": "en",
      "widget": [
        {
          "text": "Covid cases are increasing fast!"
        }
      ],
      "datasets": [
        "tweet_eval"
      ]
    },
    "card_text": "\n\n# Twitter-roBERTa-base for Sentiment Analysis - UPDATED (2022)\n\nThis is a RoBERTa-base model trained on ~124M tweets from January 2018 to December 2021, and finetuned for sentiment analysis with the TweetEval benchmark. \nThe original Twitter-based RoBERTa model can be found [here](https://huggingface.co/cardiffnlp/twitter-roberta-base-2021-124m) and the original reference paper is [TweetEval](https://github.com/cardiffnlp/tweeteval). This model is suitable for English. \n\n- Reference Paper: [TimeLMs paper](https://arxiv.org/abs/2202.03829). \n- Git Repo: [TimeLMs official repository](https://github.com/cardiffnlp/timelms).\n\n<b>Labels</b>: \n0 -> Negative;\n1 -> Neutral;\n2 -> Positive\n\nThis sentiment analysis model has been integrated into [TweetNLP](https://github.com/cardiffnlp/tweetnlp). You can access the demo [here](https://tweetnlp.org).\n\n## Example Pipeline\n```python\nfrom transformers import pipeline\nsentiment_task = pipeline(\"sentiment-analysis\", model=model_path, tokenizer=model_path)\nsentiment_task(\"Covid cases are increasing fast!\")\n```\n```\n[{'label': 'Negative', 'score': 0.7236}]\n```\n\n## Full classification example\n\n```python\nfrom transformers import AutoModelForSequenceClassification\nfrom transformers import TFAutoModelForSequenceClassification\nfrom transformers import AutoTokenizer, AutoConfig\nimport numpy as np\nfrom scipy.special import softmax\n# Preprocess text (username and link placeholders)\ndef preprocess(text):\n    new_text = []\n    for t in text.split(\" \"):\n        t = '@user' if t.startswith('@') and len(t) > 1 else t\n        t = 'http' if t.startswith('http') else t\n        new_text.append(t)\n    return \" \".join(new_text)\nMODEL = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\ntokenizer = AutoTokenizer.from_pretrained(MODEL)\nconfig = AutoConfig.from_pretrained(MODEL)\n# PT\nmodel = AutoModelForSequenceClassification.from_pretrained(MODEL)\n#model.save_pretrained(MODEL)\ntext = \"Covid cases are increasing fast!\"\ntext = preprocess(text)\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\nscores = output[0][0].detach().numpy()\nscores = softmax(scores)\n# # TF\n# model = TFAutoModelForSequenceClassification.from_pretrained(MODEL)\n# model.save_pretrained(MODEL)\n# text = \"Covid cases are increasing fast!\"\n# encoded_input = tokenizer(text, return_tensors='tf')\n# output = model(encoded_input)\n# scores = output[0][0].numpy()\n# scores = softmax(scores)\n# Print labels and scores\nranking = np.argsort(scores)\nranking = ranking[::-1]\nfor i in range(scores.shape[0]):\n    l = config.id2label[ranking[i]]\n    s = scores[ranking[i]]\n    print(f\"{i+1}) {l} {np.round(float(s), 4)}\")\n```\n\nOutput: \n\n```\n1) Negative 0.7236\n2) Neutral 0.2287\n3) Positive 0.0477\n```\n\n\n### References \n```\n@inproceedings{camacho-collados-etal-2022-tweetnlp,\n    title = \"{T}weet{NLP}: Cutting-Edge Natural Language Processing for Social Media\",\n    author = \"Camacho-collados, Jose  and\n      Rezaee, Kiamehr  and\n      Riahi, Talayeh  and\n      Ushio, Asahi  and\n      Loureiro, Daniel  and\n      Antypas, Dimosthenis  and\n      Boisson, Joanne  and\n      Espinosa Anke, Luis  and\n      Liu, Fangyu  and\n      Mart{\\'\\i}nez C{\\'a}mara, Eugenio\" and others,\n    booktitle = \"Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: System Demonstrations\",\n    month = dec,\n    year = \"2022\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.emnlp-demos.5\",\n    pages = \"38--49\"\n}\n\n```\n\n```\n@inproceedings{loureiro-etal-2022-timelms,\n    title = \"{T}ime{LM}s: Diachronic Language Models from {T}witter\",\n    author = \"Loureiro, Daniel  and\n      Barbieri, Francesco  and\n      Neves, Leonardo  and\n      Espinosa Anke, Luis  and\n      Camacho-collados, Jose\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-demo.25\",\n    doi = \"10.18653/v1/2022.acl-demo.25\",\n    pages = \"251--260\"\n}\n\n```\n",
    "card_content": "---\nlanguage: en\nwidget:\n- text: Covid cases are increasing fast!\ndatasets:\n- tweet_eval\n---\n\n\n# Twitter-roBERTa-base for Sentiment Analysis - UPDATED (2022)\n\nThis is a RoBERTa-base model trained on ~124M tweets from January 2018 to December 2021, and finetuned for sentiment analysis with the TweetEval benchmark. \nThe original Twitter-based RoBERTa model can be found [here](https://huggingface.co/cardiffnlp/twitter-roberta-base-2021-124m) and the original reference paper is [TweetEval](https://github.com/cardiffnlp/tweeteval). This model is suitable for English. \n\n- Reference Paper: [TimeLMs paper](https://arxiv.org/abs/2202.03829). \n- Git Repo: [TimeLMs official repository](https://github.com/cardiffnlp/timelms).\n\n<b>Labels</b>: \n0 -> Negative;\n1 -> Neutral;\n2 -> Positive\n\nThis sentiment analysis model has been integrated into [TweetNLP](https://github.com/cardiffnlp/tweetnlp). You can access the demo [here](https://tweetnlp.org).\n\n## Example Pipeline\n```python\nfrom transformers import pipeline\nsentiment_task = pipeline(\"sentiment-analysis\", model=model_path, tokenizer=model_path)\nsentiment_task(\"Covid cases are increasing fast!\")\n```\n```\n[{'label': 'Negative', 'score': 0.7236}]\n```\n\n## Full classification example\n\n```python\nfrom transformers import AutoModelForSequenceClassification\nfrom transformers import TFAutoModelForSequenceClassification\nfrom transformers import AutoTokenizer, AutoConfig\nimport numpy as np\nfrom scipy.special import softmax\n# Preprocess text (username and link placeholders)\ndef preprocess(text):\n    new_text = []\n    for t in text.split(\" \"):\n        t = '@user' if t.startswith('@') and len(t) > 1 else t\n        t = 'http' if t.startswith('http') else t\n        new_text.append(t)\n    return \" \".join(new_text)\nMODEL = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\ntokenizer = AutoTokenizer.from_pretrained(MODEL)\nconfig = AutoConfig.from_pretrained(MODEL)\n# PT\nmodel = AutoModelForSequenceClassification.from_pretrained(MODEL)\n#model.save_pretrained(MODEL)\ntext = \"Covid cases are increasing fast!\"\ntext = preprocess(text)\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\nscores = output[0][0].detach().numpy()\nscores = softmax(scores)\n# # TF\n# model = TFAutoModelForSequenceClassification.from_pretrained(MODEL)\n# model.save_pretrained(MODEL)\n# text = \"Covid cases are increasing fast!\"\n# encoded_input = tokenizer(text, return_tensors='tf')\n# output = model(encoded_input)\n# scores = output[0][0].numpy()\n# scores = softmax(scores)\n# Print labels and scores\nranking = np.argsort(scores)\nranking = ranking[::-1]\nfor i in range(scores.shape[0]):\n    l = config.id2label[ranking[i]]\n    s = scores[ranking[i]]\n    print(f\"{i+1}) {l} {np.round(float(s), 4)}\")\n```\n\nOutput: \n\n```\n1) Negative 0.7236\n2) Neutral 0.2287\n3) Positive 0.0477\n```\n\n\n### References \n```\n@inproceedings{camacho-collados-etal-2022-tweetnlp,\n    title = \"{T}weet{NLP}: Cutting-Edge Natural Language Processing for Social Media\",\n    author = \"Camacho-collados, Jose  and\n      Rezaee, Kiamehr  and\n      Riahi, Talayeh  and\n      Ushio, Asahi  and\n      Loureiro, Daniel  and\n      Antypas, Dimosthenis  and\n      Boisson, Joanne  and\n      Espinosa Anke, Luis  and\n      Liu, Fangyu  and\n      Mart{\\'\\i}nez C{\\'a}mara, Eugenio\" and others,\n    booktitle = \"Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: System Demonstrations\",\n    month = dec,\n    year = \"2022\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.emnlp-demos.5\",\n    pages = \"38--49\"\n}\n\n```\n\n```\n@inproceedings{loureiro-etal-2022-timelms,\n    title = \"{T}ime{LM}s: Diachronic Language Models from {T}witter\",\n    author = \"Loureiro, Daniel  and\n      Barbieri, Francesco  and\n      Neves, Leonardo  and\n      Espinosa Anke, Luis  and\n      Camacho-collados, Jose\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-demo.25\",\n    doi = \"10.18653/v1/2022.acl-demo.25\",\n    pages = \"251--260\"\n}\n\n```\n",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModelForSequenceClassification",
      "custom_class": null,
      "pipeline_tag": "text-classification",
      "processor": "AutoTokenizer"
    },
    "safetensors": null,
    "model_index": null,
    "trending_score": null
  },
  {
    "model_id": "cardiffnlp/twitter-roberta-base-sentiment",
    "model_name": "cardiffnlp/twitter-roberta-base-sentiment",
    "author": "cardiffnlp",
    "downloads": 2149980,
    "downloads_all_time": null,
    "likes": 293,
    "tags": [
      "transformers",
      "pytorch",
      "tf",
      "jax",
      "roberta",
      "text-classification",
      "en",
      "dataset:tweet_eval",
      "arxiv:2010.12421",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment",
    "dependencies": [
      [
        "transformers",
        null
      ],
      [
        "numpy",
        null
      ],
      [
        "scipy",
        null
      ],
      [
        "csv",
        null
      ],
      [
        "urllib",
        null
      ]
    ],
    "last_modified": "2023-01-20T09:52:13+00:00",
    "created_at": "2022-03-02T23:29:05+00:00",
    "analysis_date": "2025-03-22T00:54:53.846786",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "roberta",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "datasets": [
        "tweet_eval"
      ],
      "language": [
        "en"
      ]
    },
    "card_text": "# Twitter-roBERTa-base for Sentiment Analysis\n\nThis is a roBERTa-base model trained on ~58M tweets and finetuned for sentiment analysis with the TweetEval benchmark. This model is suitable for English (for a similar multilingual model, see [XLM-T](https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment)).\n\n- Reference Paper: [_TweetEval_ (Findings of EMNLP 2020)](https://arxiv.org/pdf/2010.12421.pdf). \n- Git Repo: [Tweeteval official repository](https://github.com/cardiffnlp/tweeteval).\n\n<b>Labels</b>: \n0 -> Negative;\n1 -> Neutral;\n2 -> Positive\n\n<b>New!</b> We just released a new sentiment analysis model trained on more recent and a larger quantity of tweets. \nSee [twitter-roberta-base-sentiment-latest](https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest) and [TweetNLP](https://tweetnlp.org) for more details.\n\n## Example of classification\n\n```python\nfrom transformers import AutoModelForSequenceClassification\nfrom transformers import TFAutoModelForSequenceClassification\nfrom transformers import AutoTokenizer\nimport numpy as np\nfrom scipy.special import softmax\nimport csv\nimport urllib.request\n\n# Preprocess text (username and link placeholders)\ndef preprocess(text):\n    new_text = []\n \n \n    for t in text.split(\" \"):\n        t = '@user' if t.startswith('@') and len(t) > 1 else t\n        t = 'http' if t.startswith('http') else t\n        new_text.append(t)\n    return \" \".join(new_text)\n\n# Tasks:\n# emoji, emotion, hate, irony, offensive, sentiment\n# stance/abortion, stance/atheism, stance/climate, stance/feminist, stance/hillary\n\ntask='sentiment'\nMODEL = f\"cardiffnlp/twitter-roberta-base-{task}\"\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL)\n\n# download label mapping\nlabels=[]\nmapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt\"\nwith urllib.request.urlopen(mapping_link) as f:\n    html = f.read().decode('utf-8').split(\"\\n\")\n    csvreader = csv.reader(html, delimiter='\\t')\nlabels = [row[1] for row in csvreader if len(row) > 1]\n\n# PT\nmodel = AutoModelForSequenceClassification.from_pretrained(MODEL)\nmodel.save_pretrained(MODEL)\n\ntext = \"Good night 😊\"\ntext = preprocess(text)\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\nscores = output[0][0].detach().numpy()\nscores = softmax(scores)\n\n# # TF\n# model = TFAutoModelForSequenceClassification.from_pretrained(MODEL)\n# model.save_pretrained(MODEL)\n\n# text = \"Good night 😊\"\n# encoded_input = tokenizer(text, return_tensors='tf')\n# output = model(encoded_input)\n# scores = output[0][0].numpy()\n# scores = softmax(scores)\n\nranking = np.argsort(scores)\nranking = ranking[::-1]\nfor i in range(scores.shape[0]):\n    l = labels[ranking[i]]\n    s = scores[ranking[i]]\n    print(f\"{i+1}) {l} {np.round(float(s), 4)}\")\n\n```\n\nOutput: \n\n```\n1) positive 0.8466\n2) neutral 0.1458\n3) negative 0.0076\n```\n\n### BibTeX entry and citation info\n\nPlease cite the [reference paper](https://aclanthology.org/2020.findings-emnlp.148/) if you use this model.\n\n```bibtex\n@inproceedings{barbieri-etal-2020-tweeteval,\n    title = \"{T}weet{E}val: Unified Benchmark and Comparative Evaluation for Tweet Classification\",\n    author = \"Barbieri, Francesco  and\n      Camacho-Collados, Jose  and\n      Espinosa Anke, Luis  and\n      Neves, Leonardo\",\n    booktitle = \"Findings of the Association for Computational Linguistics: EMNLP 2020\",\n    month = nov,\n    year = \"2020\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2020.findings-emnlp.148\",\n    doi = \"10.18653/v1/2020.findings-emnlp.148\",\n    pages = \"1644--1650\"\n}\n```",
    "card_content": "---\ndatasets:\n- tweet_eval\nlanguage:\n- en\n---\n# Twitter-roBERTa-base for Sentiment Analysis\n\nThis is a roBERTa-base model trained on ~58M tweets and finetuned for sentiment analysis with the TweetEval benchmark. This model is suitable for English (for a similar multilingual model, see [XLM-T](https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment)).\n\n- Reference Paper: [_TweetEval_ (Findings of EMNLP 2020)](https://arxiv.org/pdf/2010.12421.pdf). \n- Git Repo: [Tweeteval official repository](https://github.com/cardiffnlp/tweeteval).\n\n<b>Labels</b>: \n0 -> Negative;\n1 -> Neutral;\n2 -> Positive\n\n<b>New!</b> We just released a new sentiment analysis model trained on more recent and a larger quantity of tweets. \nSee [twitter-roberta-base-sentiment-latest](https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest) and [TweetNLP](https://tweetnlp.org) for more details.\n\n## Example of classification\n\n```python\nfrom transformers import AutoModelForSequenceClassification\nfrom transformers import TFAutoModelForSequenceClassification\nfrom transformers import AutoTokenizer\nimport numpy as np\nfrom scipy.special import softmax\nimport csv\nimport urllib.request\n\n# Preprocess text (username and link placeholders)\ndef preprocess(text):\n    new_text = []\n \n \n    for t in text.split(\" \"):\n        t = '@user' if t.startswith('@') and len(t) > 1 else t\n        t = 'http' if t.startswith('http') else t\n        new_text.append(t)\n    return \" \".join(new_text)\n\n# Tasks:\n# emoji, emotion, hate, irony, offensive, sentiment\n# stance/abortion, stance/atheism, stance/climate, stance/feminist, stance/hillary\n\ntask='sentiment'\nMODEL = f\"cardiffnlp/twitter-roberta-base-{task}\"\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL)\n\n# download label mapping\nlabels=[]\nmapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt\"\nwith urllib.request.urlopen(mapping_link) as f:\n    html = f.read().decode('utf-8').split(\"\\n\")\n    csvreader = csv.reader(html, delimiter='\\t')\nlabels = [row[1] for row in csvreader if len(row) > 1]\n\n# PT\nmodel = AutoModelForSequenceClassification.from_pretrained(MODEL)\nmodel.save_pretrained(MODEL)\n\ntext = \"Good night 😊\"\ntext = preprocess(text)\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\nscores = output[0][0].detach().numpy()\nscores = softmax(scores)\n\n# # TF\n# model = TFAutoModelForSequenceClassification.from_pretrained(MODEL)\n# model.save_pretrained(MODEL)\n\n# text = \"Good night 😊\"\n# encoded_input = tokenizer(text, return_tensors='tf')\n# output = model(encoded_input)\n# scores = output[0][0].numpy()\n# scores = softmax(scores)\n\nranking = np.argsort(scores)\nranking = ranking[::-1]\nfor i in range(scores.shape[0]):\n    l = labels[ranking[i]]\n    s = scores[ranking[i]]\n    print(f\"{i+1}) {l} {np.round(float(s), 4)}\")\n\n```\n\nOutput: \n\n```\n1) positive 0.8466\n2) neutral 0.1458\n3) negative 0.0076\n```\n\n### BibTeX entry and citation info\n\nPlease cite the [reference paper](https://aclanthology.org/2020.findings-emnlp.148/) if you use this model.\n\n```bibtex\n@inproceedings{barbieri-etal-2020-tweeteval,\n    title = \"{T}weet{E}val: Unified Benchmark and Comparative Evaluation for Tweet Classification\",\n    author = \"Barbieri, Francesco  and\n      Camacho-Collados, Jose  and\n      Espinosa Anke, Luis  and\n      Neves, Leonardo\",\n    booktitle = \"Findings of the Association for Computational Linguistics: EMNLP 2020\",\n    month = nov,\n    year = \"2020\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2020.findings-emnlp.148\",\n    doi = \"10.18653/v1/2020.findings-emnlp.148\",\n    pages = \"1644--1650\"\n}\n```",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModelForSequenceClassification",
      "custom_class": null,
      "pipeline_tag": "text-classification",
      "processor": "AutoTokenizer"
    },
    "safetensors": null,
    "model_index": null,
    "trending_score": null
  },
  {
    "model_id": "cross-encoder/ms-marco-MiniLM-L4-v2",
    "model_name": "cross-encoder/ms-marco-MiniLM-L4-v2",
    "author": "cross-encoder",
    "downloads": 1558144,
    "downloads_all_time": null,
    "likes": 9,
    "tags": [
      "transformers",
      "pytorch",
      "jax",
      "safetensors",
      "bert",
      "text-classification",
      "license:apache-2.0",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/cross-encoder/ms-marco-MiniLM-L4-v2",
    "dependencies": [
      [
        "sentence_transformers",
        null
      ],
      [
        "transformers",
        null
      ],
      [
        "torch",
        null
      ]
    ],
    "last_modified": "2025-03-07T14:57:14+00:00",
    "created_at": "2022-03-02T23:29:05+00:00",
    "analysis_date": "2025-03-22T00:54:55.257388",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "bert",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "license": "apache-2.0"
    },
    "card_text": "# Cross-Encoder for MS Marco\n\nThis model was trained on the [MS Marco Passage Ranking](https://github.com/microsoft/MSMARCO-Passage-Ranking) task.\n\nThe model can be used for Information Retrieval: Given a query, encode the query will all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. See [SBERT.net Retrieve & Re-rank](https://www.sbert.net/examples/applications/retrieve_rerank/README.html) for more details. The training code is available here: [SBERT.net Training MS Marco](https://github.com/UKPLab/sentence-transformers/tree/master/examples/training/ms_marco)\n\n\n## Usage with SentenceTransformers\n\nThe usage is easy when you have [SentenceTransformers](https://www.sbert.net/) installed. Then you can use the pre-trained models like this:\n```python\nfrom sentence_transformers import CrossEncoder\n\nmodel = CrossEncoder('cross-encoder/ms-marco-MiniLM-L4-v2')\nscores = model.predict([\n    (\"How many people live in Berlin?\", \"Berlin had a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\"),\n    (\"How many people live in Berlin?\", \"Berlin is well known for its museums.\"),\n])\nprint(scores)\n# [ 9.1273365 -4.569759 ]\n```\n\n\n## Usage with Transformers\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\n\nmodel = AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-MiniLM-L4-v2')\ntokenizer = AutoTokenizer.from_pretrained('cross-encoder/ms-marco-MiniLM-L4-v2')\n\nfeatures = tokenizer(['How many people live in Berlin?', 'How many people live in Berlin?'], ['Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.', 'New York City is famous for the Metropolitan Museum of Art.'],  padding=True, truncation=True, return_tensors=\"pt\")\n\nmodel.eval()\nwith torch.no_grad():\n    scores = model(**features).logits\n    print(scores)\n```\n\n\n## Performance\nIn the following table, we provide various pre-trained Cross-Encoders together with their performance on the [TREC Deep Learning 2019](https://microsoft.github.io/TREC-2019-Deep-Learning/) and the [MS Marco Passage Reranking](https://github.com/microsoft/MSMARCO-Passage-Ranking/) dataset. \n\n\n| Model-Name        | NDCG@10 (TREC DL 19) | MRR@10 (MS Marco Dev)  | Docs / Sec |\n| ------------- |:-------------| -----| --- | \n| **Version 2 models** | | | \n| cross-encoder/ms-marco-TinyBERT-L2-v2 | 69.84 | 32.56 | 9000\n| cross-encoder/ms-marco-MiniLM-L2-v2 | 71.01 | 34.85 | 4100\n| cross-encoder/ms-marco-MiniLM-L4-v2 | 73.04 | 37.70 | 2500\n| cross-encoder/ms-marco-MiniLM-L6-v2 | 74.30 | 39.01 | 1800\n| cross-encoder/ms-marco-MiniLM-L12-v2 | 74.31 | 39.02 | 960\n| **Version 1 models** | | | \n| cross-encoder/ms-marco-TinyBERT-L2  | 67.43 | 30.15  | 9000\n| cross-encoder/ms-marco-TinyBERT-L4  | 68.09 | 34.50  | 2900\n| cross-encoder/ms-marco-TinyBERT-L6 |  69.57 | 36.13  | 680\n| cross-encoder/ms-marco-electra-base | 71.99 | 36.41 | 340\n| **Other models** | | | \n| nboost/pt-tinybert-msmarco | 63.63 | 28.80 | 2900 \n| nboost/pt-bert-base-uncased-msmarco | 70.94 | 34.75 | 340 \n| nboost/pt-bert-large-msmarco | 73.36 | 36.48 | 100 \n| Capreolus/electra-base-msmarco | 71.23 | 36.89 | 340 \n| amberoad/bert-multilingual-passage-reranking-msmarco | 68.40 | 35.54 | 330 \n| sebastian-hofstaetter/distilbert-cat-margin_mse-T2-msmarco | 72.82 | 37.88 | 720\n \n Note: Runtime was computed on a V100 GPU.\n",
    "card_content": "---\nlicense: apache-2.0\n---\n# Cross-Encoder for MS Marco\n\nThis model was trained on the [MS Marco Passage Ranking](https://github.com/microsoft/MSMARCO-Passage-Ranking) task.\n\nThe model can be used for Information Retrieval: Given a query, encode the query will all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. See [SBERT.net Retrieve & Re-rank](https://www.sbert.net/examples/applications/retrieve_rerank/README.html) for more details. The training code is available here: [SBERT.net Training MS Marco](https://github.com/UKPLab/sentence-transformers/tree/master/examples/training/ms_marco)\n\n\n## Usage with SentenceTransformers\n\nThe usage is easy when you have [SentenceTransformers](https://www.sbert.net/) installed. Then you can use the pre-trained models like this:\n```python\nfrom sentence_transformers import CrossEncoder\n\nmodel = CrossEncoder('cross-encoder/ms-marco-MiniLM-L4-v2')\nscores = model.predict([\n    (\"How many people live in Berlin?\", \"Berlin had a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\"),\n    (\"How many people live in Berlin?\", \"Berlin is well known for its museums.\"),\n])\nprint(scores)\n# [ 9.1273365 -4.569759 ]\n```\n\n\n## Usage with Transformers\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\n\nmodel = AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-MiniLM-L4-v2')\ntokenizer = AutoTokenizer.from_pretrained('cross-encoder/ms-marco-MiniLM-L4-v2')\n\nfeatures = tokenizer(['How many people live in Berlin?', 'How many people live in Berlin?'], ['Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.', 'New York City is famous for the Metropolitan Museum of Art.'],  padding=True, truncation=True, return_tensors=\"pt\")\n\nmodel.eval()\nwith torch.no_grad():\n    scores = model(**features).logits\n    print(scores)\n```\n\n\n## Performance\nIn the following table, we provide various pre-trained Cross-Encoders together with their performance on the [TREC Deep Learning 2019](https://microsoft.github.io/TREC-2019-Deep-Learning/) and the [MS Marco Passage Reranking](https://github.com/microsoft/MSMARCO-Passage-Ranking/) dataset. \n\n\n| Model-Name        | NDCG@10 (TREC DL 19) | MRR@10 (MS Marco Dev)  | Docs / Sec |\n| ------------- |:-------------| -----| --- | \n| **Version 2 models** | | | \n| cross-encoder/ms-marco-TinyBERT-L2-v2 | 69.84 | 32.56 | 9000\n| cross-encoder/ms-marco-MiniLM-L2-v2 | 71.01 | 34.85 | 4100\n| cross-encoder/ms-marco-MiniLM-L4-v2 | 73.04 | 37.70 | 2500\n| cross-encoder/ms-marco-MiniLM-L6-v2 | 74.30 | 39.01 | 1800\n| cross-encoder/ms-marco-MiniLM-L12-v2 | 74.31 | 39.02 | 960\n| **Version 1 models** | | | \n| cross-encoder/ms-marco-TinyBERT-L2  | 67.43 | 30.15  | 9000\n| cross-encoder/ms-marco-TinyBERT-L4  | 68.09 | 34.50  | 2900\n| cross-encoder/ms-marco-TinyBERT-L6 |  69.57 | 36.13  | 680\n| cross-encoder/ms-marco-electra-base | 71.99 | 36.41 | 340\n| **Other models** | | | \n| nboost/pt-tinybert-msmarco | 63.63 | 28.80 | 2900 \n| nboost/pt-bert-base-uncased-msmarco | 70.94 | 34.75 | 340 \n| nboost/pt-bert-large-msmarco | 73.36 | 36.48 | 100 \n| Capreolus/electra-base-msmarco | 71.23 | 36.89 | 340 \n| amberoad/bert-multilingual-passage-reranking-msmarco | 68.40 | 35.54 | 330 \n| sebastian-hofstaetter/distilbert-cat-margin_mse-T2-msmarco | 72.82 | 37.88 | 720\n \n Note: Runtime was computed on a V100 GPU.\n",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModelForSequenceClassification",
      "custom_class": null,
      "pipeline_tag": "text-classification",
      "processor": "AutoTokenizer"
    },
    "safetensors": {
      "parameters": {
        "I64": 512,
        "F32": 19164673
      },
      "total": 19165185
    },
    "model_index": null,
    "trending_score": null
  },
  {
    "model_id": "cardiffnlp/twitter-xlm-roberta-base-sentiment",
    "model_name": "cardiffnlp/twitter-xlm-roberta-base-sentiment",
    "author": "cardiffnlp",
    "downloads": 1424416,
    "downloads_all_time": null,
    "likes": 210,
    "tags": [
      "transformers",
      "pytorch",
      "tf",
      "xlm-roberta",
      "text-classification",
      "multilingual",
      "arxiv:2104.12250",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment",
    "dependencies": [
      [
        "transformers",
        null
      ],
      [
        "numpy",
        null
      ],
      [
        "scipy",
        null
      ]
    ],
    "last_modified": "2023-07-19T20:41:38+00:00",
    "created_at": "2022-03-02T23:29:05+00:00",
    "analysis_date": "2025-03-22T00:54:56.346047",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "xlm-roberta",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": "multilingual",
      "widget": [
        {
          "text": "🤗"
        },
        {
          "text": "T'estimo! ❤️"
        },
        {
          "text": "I love you!"
        },
        {
          "text": "I hate you 🤮"
        },
        {
          "text": "Mahal kita!"
        },
        {
          "text": "사랑해!"
        },
        {
          "text": "난 너가 싫어"
        },
        {
          "text": "😍😍😍"
        }
      ]
    },
    "card_text": "\n\n# twitter-XLM-roBERTa-base for Sentiment Analysis\n\nThis is a multilingual XLM-roBERTa-base model trained on ~198M tweets and finetuned for sentiment analysis. The sentiment fine-tuning was done on 8 languages (Ar, En, Fr, De, Hi, It, Sp, Pt) but it can be used for more languages (see paper for details).\n\n- Paper: [XLM-T: A Multilingual Language Model Toolkit for Twitter](https://arxiv.org/abs/2104.12250). \n- Git Repo: [XLM-T official repository](https://github.com/cardiffnlp/xlm-t).\n\nThis model has been integrated into the [TweetNLP library](https://github.com/cardiffnlp/tweetnlp).\n\n## Example Pipeline\n```python\nfrom transformers import pipeline\nmodel_path = \"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\nsentiment_task = pipeline(\"sentiment-analysis\", model=model_path, tokenizer=model_path)\nsentiment_task(\"T'estimo!\")\n```\n```\n[{'label': 'Positive', 'score': 0.6600581407546997}]\n```\n\n## Full classification example\n\n```python\nfrom transformers import AutoModelForSequenceClassification\nfrom transformers import TFAutoModelForSequenceClassification\nfrom transformers import AutoTokenizer, AutoConfig\nimport numpy as np\nfrom scipy.special import softmax\n\n# Preprocess text (username and link placeholders)\ndef preprocess(text):\n    new_text = []\n    for t in text.split(\" \"):\n        t = '@user' if t.startswith('@') and len(t) > 1 else t\n        t = 'http' if t.startswith('http') else t\n        new_text.append(t)\n    return \" \".join(new_text)\n\nMODEL = f\"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL)\nconfig = AutoConfig.from_pretrained(MODEL)\n\n# PT\nmodel = AutoModelForSequenceClassification.from_pretrained(MODEL)\nmodel.save_pretrained(MODEL)\n\ntext = \"Good night 😊\"\ntext = preprocess(text)\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\nscores = output[0][0].detach().numpy()\nscores = softmax(scores)\n\n# # TF\n# model = TFAutoModelForSequenceClassification.from_pretrained(MODEL)\n# model.save_pretrained(MODEL)\n\n# text = \"Good night 😊\"\n# encoded_input = tokenizer(text, return_tensors='tf')\n# output = model(encoded_input)\n# scores = output[0][0].numpy()\n# scores = softmax(scores)\n\n# Print labels and scores\nranking = np.argsort(scores)\nranking = ranking[::-1]\nfor i in range(scores.shape[0]):\n    l = config.id2label[ranking[i]]\n    s = scores[ranking[i]]\n    print(f\"{i+1}) {l} {np.round(float(s), 4)}\")\n\n```\n\nOutput: \n\n```\n1) Positive 0.7673\n2) Neutral 0.2015\n3) Negative 0.0313\n```\n\n### Reference\n```\n@inproceedings{barbieri-etal-2022-xlm,\n    title = \"{XLM}-{T}: Multilingual Language Models in {T}witter for Sentiment Analysis and Beyond\",\n    author = \"Barbieri, Francesco  and\n      Espinosa Anke, Luis  and\n      Camacho-Collados, Jose\",\n    booktitle = \"Proceedings of the Thirteenth Language Resources and Evaluation Conference\",\n    month = jun,\n    year = \"2022\",\n    address = \"Marseille, France\",\n    publisher = \"European Language Resources Association\",\n    url = \"https://aclanthology.org/2022.lrec-1.27\",\n    pages = \"258--266\"\n}\n\n```\n\n",
    "card_content": "---\nlanguage: multilingual\nwidget:\n- text: 🤗\n- text: T'estimo! ❤️\n- text: I love you!\n- text: I hate you 🤮\n- text: Mahal kita!\n- text: 사랑해!\n- text: 난 너가 싫어\n- text: 😍😍😍\n---\n\n\n# twitter-XLM-roBERTa-base for Sentiment Analysis\n\nThis is a multilingual XLM-roBERTa-base model trained on ~198M tweets and finetuned for sentiment analysis. The sentiment fine-tuning was done on 8 languages (Ar, En, Fr, De, Hi, It, Sp, Pt) but it can be used for more languages (see paper for details).\n\n- Paper: [XLM-T: A Multilingual Language Model Toolkit for Twitter](https://arxiv.org/abs/2104.12250). \n- Git Repo: [XLM-T official repository](https://github.com/cardiffnlp/xlm-t).\n\nThis model has been integrated into the [TweetNLP library](https://github.com/cardiffnlp/tweetnlp).\n\n## Example Pipeline\n```python\nfrom transformers import pipeline\nmodel_path = \"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\nsentiment_task = pipeline(\"sentiment-analysis\", model=model_path, tokenizer=model_path)\nsentiment_task(\"T'estimo!\")\n```\n```\n[{'label': 'Positive', 'score': 0.6600581407546997}]\n```\n\n## Full classification example\n\n```python\nfrom transformers import AutoModelForSequenceClassification\nfrom transformers import TFAutoModelForSequenceClassification\nfrom transformers import AutoTokenizer, AutoConfig\nimport numpy as np\nfrom scipy.special import softmax\n\n# Preprocess text (username and link placeholders)\ndef preprocess(text):\n    new_text = []\n    for t in text.split(\" \"):\n        t = '@user' if t.startswith('@') and len(t) > 1 else t\n        t = 'http' if t.startswith('http') else t\n        new_text.append(t)\n    return \" \".join(new_text)\n\nMODEL = f\"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL)\nconfig = AutoConfig.from_pretrained(MODEL)\n\n# PT\nmodel = AutoModelForSequenceClassification.from_pretrained(MODEL)\nmodel.save_pretrained(MODEL)\n\ntext = \"Good night 😊\"\ntext = preprocess(text)\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\nscores = output[0][0].detach().numpy()\nscores = softmax(scores)\n\n# # TF\n# model = TFAutoModelForSequenceClassification.from_pretrained(MODEL)\n# model.save_pretrained(MODEL)\n\n# text = \"Good night 😊\"\n# encoded_input = tokenizer(text, return_tensors='tf')\n# output = model(encoded_input)\n# scores = output[0][0].numpy()\n# scores = softmax(scores)\n\n# Print labels and scores\nranking = np.argsort(scores)\nranking = ranking[::-1]\nfor i in range(scores.shape[0]):\n    l = config.id2label[ranking[i]]\n    s = scores[ranking[i]]\n    print(f\"{i+1}) {l} {np.round(float(s), 4)}\")\n\n```\n\nOutput: \n\n```\n1) Positive 0.7673\n2) Neutral 0.2015\n3) Negative 0.0313\n```\n\n### Reference\n```\n@inproceedings{barbieri-etal-2022-xlm,\n    title = \"{XLM}-{T}: Multilingual Language Models in {T}witter for Sentiment Analysis and Beyond\",\n    author = \"Barbieri, Francesco  and\n      Espinosa Anke, Luis  and\n      Camacho-Collados, Jose\",\n    booktitle = \"Proceedings of the Thirteenth Language Resources and Evaluation Conference\",\n    month = jun,\n    year = \"2022\",\n    address = \"Marseille, France\",\n    publisher = \"European Language Resources Association\",\n    url = \"https://aclanthology.org/2022.lrec-1.27\",\n    pages = \"258--266\"\n}\n\n```\n\n",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModelForSequenceClassification",
      "custom_class": null,
      "pipeline_tag": "text-classification",
      "processor": "AutoTokenizer"
    },
    "safetensors": null,
    "model_index": null,
    "trending_score": null
  },
  {
    "model_id": "facebook/roberta-hate-speech-dynabench-r4-target",
    "model_name": "facebook/roberta-hate-speech-dynabench-r4-target",
    "author": "facebook",
    "downloads": 1212681,
    "downloads_all_time": null,
    "likes": 77,
    "tags": [
      "transformers",
      "pytorch",
      "safetensors",
      "roberta",
      "text-classification",
      "en",
      "arxiv:2012.15761",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/facebook/roberta-hate-speech-dynabench-r4-target",
    "dependencies": null,
    "last_modified": "2023-03-16T20:03:57+00:00",
    "created_at": "2022-06-10T22:24:39+00:00",
    "analysis_date": "2025-03-22T00:54:57.353848",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "roberta",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": "en"
    },
    "card_text": "\n# LFTW R4 Target\n\nThe R4 Target model from [Learning from the Worst: Dynamically Generated Datasets to Improve Online Hate Detection](https://arxiv.org/abs/2012.15761)\n\n## Citation Information\n\n```bibtex\n@inproceedings{vidgen2021lftw,\n  title={Learning from the Worst: Dynamically Generated Datasets to Improve Online Hate Detection},\n  author={Bertie Vidgen and Tristan Thrush and Zeerak Waseem and Douwe Kiela},\n  booktitle={ACL},\n  year={2021}\n}\n```\n\nThanks to Kushal Tirumala and Adina Williams for helping the authors put the model on the hub!",
    "card_content": "---\nlanguage: en\n---\n\n# LFTW R4 Target\n\nThe R4 Target model from [Learning from the Worst: Dynamically Generated Datasets to Improve Online Hate Detection](https://arxiv.org/abs/2012.15761)\n\n## Citation Information\n\n```bibtex\n@inproceedings{vidgen2021lftw,\n  title={Learning from the Worst: Dynamically Generated Datasets to Improve Online Hate Detection},\n  author={Bertie Vidgen and Tristan Thrush and Zeerak Waseem and Douwe Kiela},\n  booktitle={ACL},\n  year={2021}\n}\n```\n\nThanks to Kushal Tirumala and Adina Williams for helping the authors put the model on the hub!",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModelForSequenceClassification",
      "custom_class": null,
      "pipeline_tag": "text-classification",
      "processor": "AutoTokenizer"
    },
    "safetensors": {
      "parameters": {
        "I64": 514,
        "F32": 124647170
      },
      "total": 124647684
    },
    "model_index": null,
    "trending_score": null
  },
  {
    "model_id": "nlptown/bert-base-multilingual-uncased-sentiment",
    "model_name": "nlptown/bert-base-multilingual-uncased-sentiment",
    "author": "nlptown",
    "downloads": 1176119,
    "downloads_all_time": null,
    "likes": 358,
    "tags": [
      "transformers",
      "pytorch",
      "tf",
      "jax",
      "safetensors",
      "bert",
      "text-classification",
      "en",
      "nl",
      "de",
      "fr",
      "it",
      "es",
      "doi:10.57967/hf/1515",
      "license:mit",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment",
    "dependencies": null,
    "last_modified": "2025-01-02T20:13:01+00:00",
    "created_at": "2022-03-02T23:29:05+00:00",
    "analysis_date": "2025-03-22T00:54:58.311576",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "bert",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": [
        "en",
        "nl",
        "de",
        "fr",
        "it",
        "es"
      ],
      "license": "mit"
    },
    "card_text": "\n# bert-base-multilingual-uncased-sentiment\n\nVisit the [NLP Town website](https://www.nlp.town) for an updated version of this model, with a 40% error reduction on product reviews.\n\nThis is a bert-base-multilingual-uncased model finetuned for sentiment analysis on product reviews in six languages: English, Dutch, German, French, Spanish, and Italian. It predicts the sentiment of the review as a number of stars (between 1 and 5).\n\nThis model is intended for direct use as a sentiment analysis model for product reviews in any of the six languages above or for further finetuning on related sentiment analysis tasks.\n\n## Training data\n\nHere is the number of product reviews we used for finetuning the model: \n\n| Language | Number of reviews |\n| -------- | ----------------- |\n| English  | 150k           |\n| Dutch    | 80k            |\n| German   | 137k           |\n| French   | 140k           |\n| Italian  | 72k            |\n| Spanish  | 50k            |\n\n## Accuracy\n\nThe fine-tuned model obtained the following accuracy on 5,000 held-out product reviews in each of the languages:\n\n- Accuracy (exact) is the exact match for the number of stars.\n- Accuracy (off-by-1) is the percentage of reviews where the number of stars the model predicts differs by a maximum of 1 from the number given by the human reviewer. \n\n\n| Language | Accuracy (exact) | Accuracy (off-by-1) |\n| -------- | ---------------------- | ------------------- |\n| English  | 67%                 | 95%\n| Dutch    | 57%                 | 93%\n| German   | 61%                 | 94%\n| French   | 59%                 | 94%\n| Italian  | 59%                 | 95%\n| Spanish  | 58%                 | 95%\n\n## Contact \n\nIn addition to this model, [NLP Town](http://nlp.town) offers custom models for many languages and NLP tasks.\n\nIf you found this model useful, you can [buy us a coffee](https://www.buymeacoffee.com/yvespeirsman).\n\nFeel free to contact us for questions, feedback and/or requests for similar models.",
    "card_content": "---\nlanguage:\n- en\n- nl\n- de\n- fr\n- it\n- es\nlicense: mit\n---\n\n# bert-base-multilingual-uncased-sentiment\n\nVisit the [NLP Town website](https://www.nlp.town) for an updated version of this model, with a 40% error reduction on product reviews.\n\nThis is a bert-base-multilingual-uncased model finetuned for sentiment analysis on product reviews in six languages: English, Dutch, German, French, Spanish, and Italian. It predicts the sentiment of the review as a number of stars (between 1 and 5).\n\nThis model is intended for direct use as a sentiment analysis model for product reviews in any of the six languages above or for further finetuning on related sentiment analysis tasks.\n\n## Training data\n\nHere is the number of product reviews we used for finetuning the model: \n\n| Language | Number of reviews |\n| -------- | ----------------- |\n| English  | 150k           |\n| Dutch    | 80k            |\n| German   | 137k           |\n| French   | 140k           |\n| Italian  | 72k            |\n| Spanish  | 50k            |\n\n## Accuracy\n\nThe fine-tuned model obtained the following accuracy on 5,000 held-out product reviews in each of the languages:\n\n- Accuracy (exact) is the exact match for the number of stars.\n- Accuracy (off-by-1) is the percentage of reviews where the number of stars the model predicts differs by a maximum of 1 from the number given by the human reviewer. \n\n\n| Language | Accuracy (exact) | Accuracy (off-by-1) |\n| -------- | ---------------------- | ------------------- |\n| English  | 67%                 | 95%\n| Dutch    | 57%                 | 93%\n| German   | 61%                 | 94%\n| French   | 59%                 | 94%\n| Italian  | 59%                 | 95%\n| Spanish  | 58%                 | 95%\n\n## Contact \n\nIn addition to this model, [NLP Town](http://nlp.town) offers custom models for many languages and NLP tasks.\n\nIf you found this model useful, you can [buy us a coffee](https://www.buymeacoffee.com/yvespeirsman).\n\nFeel free to contact us for questions, feedback and/or requests for similar models.",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModelForSequenceClassification",
      "custom_class": null,
      "pipeline_tag": "text-classification",
      "processor": "AutoTokenizer"
    },
    "safetensors": {
      "parameters": {
        "F32": 167360261
      },
      "total": 167360261
    },
    "model_index": null,
    "trending_score": null
  },
  {
    "model_id": "lucadiliello/BLEURT-20-D12",
    "model_name": "lucadiliello/BLEURT-20-D12",
    "author": "lucadiliello",
    "downloads": 982989,
    "downloads_all_time": null,
    "likes": 0,
    "tags": [
      "transformers",
      "pytorch",
      "bleurt",
      "text-classification",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/lucadiliello/BLEURT-20-D12",
    "dependencies": [
      [
        "torch",
        null
      ],
      [
        "bleurt_pytorch",
        null
      ]
    ],
    "last_modified": "2023-01-19T15:55:33+00:00",
    "created_at": "2023-01-19T15:18:25+00:00",
    "analysis_date": "2025-03-22T00:54:59.442918",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "bleurt",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {},
    "card_text": "This model is based on a custom Transformer model that can be installed with:\n\n```bash\npip install git+https://github.com/lucadiliello/bleurt-pytorch.git\n```\n\nNow load the model and make predictions with:\n\n```python\nimport torch\nfrom bleurt_pytorch import BleurtConfig, BleurtForSequenceClassification, BleurtTokenizer\n\nconfig = BleurtConfig.from_pretrained('lucadiliello/BLEURT-20-D12')\nmodel = BleurtForSequenceClassification.from_pretrained('lucadiliello/BLEURT-20-D12')\ntokenizer = BleurtTokenizer.from_pretrained('lucadiliello/BLEURT-20-D12')\n\nreferences = [\"a bird chirps by the window\", \"this is a random sentence\"]\ncandidates = [\"a bird chirps by the window\", \"this looks like a random sentence\"]\n\nmodel.eval()\nwith torch.no_grad():\n    inputs = tokenizer(references, candidates, padding='longest', return_tensors='pt')\n    res = model(**inputs).logits.flatten().tolist()\nprint(res)\n# [0.9604414105415344, 0.8080050349235535]\n```\n\nTake a look at this [repository](https://github.com/lucadiliello/bleurt-pytorch) for the definition of `BleurtConfig`, `BleurtForSequenceClassification` and `BleurtTokenizer` in PyTorch.",
    "card_content": "---\n{}\n---\nThis model is based on a custom Transformer model that can be installed with:\n\n```bash\npip install git+https://github.com/lucadiliello/bleurt-pytorch.git\n```\n\nNow load the model and make predictions with:\n\n```python\nimport torch\nfrom bleurt_pytorch import BleurtConfig, BleurtForSequenceClassification, BleurtTokenizer\n\nconfig = BleurtConfig.from_pretrained('lucadiliello/BLEURT-20-D12')\nmodel = BleurtForSequenceClassification.from_pretrained('lucadiliello/BLEURT-20-D12')\ntokenizer = BleurtTokenizer.from_pretrained('lucadiliello/BLEURT-20-D12')\n\nreferences = [\"a bird chirps by the window\", \"this is a random sentence\"]\ncandidates = [\"a bird chirps by the window\", \"this looks like a random sentence\"]\n\nmodel.eval()\nwith torch.no_grad():\n    inputs = tokenizer(references, candidates, padding='longest', return_tensors='pt')\n    res = model(**inputs).logits.flatten().tolist()\nprint(res)\n# [0.9604414105415344, 0.8080050349235535]\n```\n\nTake a look at this [repository](https://github.com/lucadiliello/bleurt-pytorch) for the definition of `BleurtConfig`, `BleurtForSequenceClassification` and `BleurtTokenizer` in PyTorch.",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModelForSequenceClassification",
      "custom_class": null,
      "pipeline_tag": "text-classification",
      "processor": null
    },
    "safetensors": null,
    "model_index": null,
    "trending_score": null
  },
  {
    "model_id": "BAAI/bge-reranker-v2-m3",
    "model_name": "BAAI/bge-reranker-v2-m3",
    "author": "BAAI",
    "downloads": 977288,
    "downloads_all_time": null,
    "likes": 569,
    "tags": [
      "sentence-transformers",
      "safetensors",
      "xlm-roberta",
      "text-classification",
      "transformers",
      "text-embeddings-inference",
      "multilingual",
      "arxiv:2312.15503",
      "arxiv:2402.03216",
      "license:apache-2.0",
      "region:us"
    ],
    "card_url": "https://huggingface.co/BAAI/bge-reranker-v2-m3",
    "dependencies": [
      [
        "FlagEmbedding",
        null
      ],
      [
        "torch",
        null
      ],
      [
        "transformers",
        null
      ]
    ],
    "last_modified": "2024-06-24T14:08:45+00:00",
    "created_at": "2024-03-15T13:32:18+00:00",
    "analysis_date": "2025-03-22T00:55:01.389342",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "xlm-roberta",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "license": "apache-2.0",
      "pipeline_tag": "text-classification",
      "tags": [
        "transformers",
        "sentence-transformers",
        "text-embeddings-inference"
      ],
      "language": [
        "multilingual"
      ]
    },
    "card_text": "\n# Reranker\n\n**More details please refer to our Github: [FlagEmbedding](https://github.com/FlagOpen/FlagEmbedding/tree/master).**\n\n- [Model List](#model-list)\n- [Usage](#usage)\n- [Fine-tuning](#fine-tune)\n- [Evaluation](#evaluation)\n- [Citation](#citation)\n\nDifferent from embedding model, reranker uses question and document as input and directly output similarity instead of embedding. \nYou can get a relevance score by inputting query and passage to the reranker. \nAnd the score can be mapped to a float value in [0,1] by sigmoid function.\n\n\n## Model List\n\n| Model                                                                     | Base model                                                           | Language | layerwise |                           feature                            |\n|:--------------------------------------------------------------------------|:--------:|:-----------------------------------------------------------------------------------------------------------------------------------:|:----------------------------------------------------------------------------------------------:|:----------------------------------------------------------------------------------------------:|\n| [BAAI/bge-reranker-base](https://huggingface.co/BAAI/bge-reranker-base) | [xlm-roberta-base](https://huggingface.co/xlm-roberta-base) | Chinese and English |     -     | Lightweight reranker model, easy to deploy, with fast inference. |\n| [BAAI/bge-reranker-large](https://huggingface.co/BAAI/bge-reranker-large) | [xlm-roberta-large](https://huggingface.co/FacebookAI/xlm-roberta-large) | Chinese and English |     -     | Lightweight reranker model, easy to deploy, with fast inference. |\n| [BAAI/bge-reranker-v2-m3](https://huggingface.co/BAAI/bge-reranker-v2-m3) | [bge-m3](https://huggingface.co/BAAI/bge-m3) |    Multilingual     |     -     | Lightweight reranker model, possesses strong multilingual capabilities, easy to deploy, with fast inference. |\n| [BAAI/bge-reranker-v2-gemma](https://huggingface.co/BAAI/bge-reranker-v2-gemma) |      [gemma-2b](https://huggingface.co/google/gemma-2b)      |    Multilingual     |     -     | Suitable for multilingual contexts, performs well in both English proficiency and multilingual capabilities. |\n| [BAAI/bge-reranker-v2-minicpm-layerwise](https://huggingface.co/BAAI/bge-reranker-v2-minicpm-layerwise) | [MiniCPM-2B-dpo-bf16](https://huggingface.co/openbmb/MiniCPM-2B-dpo-bf16) |    Multilingual     |   8-40    | Suitable for multilingual contexts, performs well in both English and Chinese proficiency, allows freedom to select layers for output, facilitating accelerated inference. |\n\n\nYou can select the model according your senario and resource. \n- For **multilingual**, utilize [BAAI/bge-reranker-v2-m3](https://huggingface.co/BAAI/bge-reranker-v2-m3) and [BAAI/bge-reranker-v2-gemma](https://huggingface.co/BAAI/bge-reranker-v2-gemma)\n\n- For **Chinese or English**, utilize [BAAI/bge-reranker-v2-m3](https://huggingface.co/BAAI/bge-reranker-v2-m3) and [BAAI/bge-reranker-v2-minicpm-layerwise](https://huggingface.co/BAAI/bge-reranker-v2-minicpm-layerwise). \n\n- For **efficiency**, utilize [BAAI/bge-reranker-v2-m3](https://huggingface.co/BAAI/bge-reranker-v2-m3) and the low layer of [BAAI/bge-reranker-v2-minicpm-layerwise](https://huggingface.co/BAAI/bge-reranker-v2-minicpm-layerwise). \n\n- For better performance, recommand [BAAI/bge-reranker-v2-minicpm-layerwise](https://huggingface.co/BAAI/bge-reranker-v2-minicpm-layerwise) and [BAAI/bge-reranker-v2-gemma](https://huggingface.co/BAAI/bge-reranker-v2-gemma)\n\n## Usage \n### Using FlagEmbedding\n\n```\npip install -U FlagEmbedding\n```\n\n#### For normal reranker (bge-reranker-base / bge-reranker-large / bge-reranker-v2-m3 )\n\nGet relevance scores (higher scores indicate more relevance):\n\n```python\nfrom FlagEmbedding import FlagReranker\nreranker = FlagReranker('BAAI/bge-reranker-v2-m3', use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\n\nscore = reranker.compute_score(['query', 'passage'])\nprint(score) # -5.65234375\n\n# You can map the scores into 0-1 by set \"normalize=True\", which will apply sigmoid function to the score\nscore = reranker.compute_score(['query', 'passage'], normalize=True)\nprint(score) # 0.003497010252573502\n\nscores = reranker.compute_score([['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']])\nprint(scores) # [-8.1875, 5.26171875]\n\n# You can map the scores into 0-1 by set \"normalize=True\", which will apply sigmoid function to the score\nscores = reranker.compute_score([['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']], normalize=True)\nprint(scores) # [0.00027803096387751553, 0.9948403768236574]\n```\n\n#### For LLM-based reranker\n\n```python\nfrom FlagEmbedding import FlagLLMReranker\nreranker = FlagLLMReranker('BAAI/bge-reranker-v2-gemma', use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\n# reranker = FlagLLMReranker('BAAI/bge-reranker-v2-gemma', use_bf16=True) # You can also set use_bf16=True to speed up computation with a slight performance degradation\n\nscore = reranker.compute_score(['query', 'passage'])\nprint(score)\n\nscores = reranker.compute_score([['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']])\nprint(scores)\n```\n\n#### For LLM-based layerwise reranker\n\n```python\nfrom FlagEmbedding import LayerWiseFlagLLMReranker\nreranker = LayerWiseFlagLLMReranker('BAAI/bge-reranker-v2-minicpm-layerwise', use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\n# reranker = LayerWiseFlagLLMReranker('BAAI/bge-reranker-v2-minicpm-layerwise', use_bf16=True) # You can also set use_bf16=True to speed up computation with a slight performance degradation\n\nscore = reranker.compute_score(['query', 'passage'], cutoff_layers=[28]) # Adjusting 'cutoff_layers' to pick which layers are used for computing the score.\nprint(score)\n\nscores = reranker.compute_score([['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']], cutoff_layers=[28])\nprint(scores)\n```\n\n### Using Huggingface transformers\n\n#### For normal reranker (bge-reranker-base / bge-reranker-large / bge-reranker-v2-m3 )\n\nGet relevance scores (higher scores indicate more relevance):\n\n```python\nimport torch\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained('BAAI/bge-reranker-v2-m3')\nmodel = AutoModelForSequenceClassification.from_pretrained('BAAI/bge-reranker-v2-m3')\nmodel.eval()\n\npairs = [['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']]\nwith torch.no_grad():\n    inputs = tokenizer(pairs, padding=True, truncation=True, return_tensors='pt', max_length=512)\n    scores = model(**inputs, return_dict=True).logits.view(-1, ).float()\n    print(scores)\n```\n\n#### For LLM-based reranker\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ndef get_inputs(pairs, tokenizer, prompt=None, max_length=1024):\n    if prompt is None:\n        prompt = \"Given a query A and a passage B, determine whether the passage contains an answer to the query by providing a prediction of either 'Yes' or 'No'.\"\n    sep = \"\\n\"\n    prompt_inputs = tokenizer(prompt,\n                              return_tensors=None,\n                              add_special_tokens=False)['input_ids']\n    sep_inputs = tokenizer(sep,\n                           return_tensors=None,\n                           add_special_tokens=False)['input_ids']\n    inputs = []\n    for query, passage in pairs:\n        query_inputs = tokenizer(f'A: {query}',\n                                 return_tensors=None,\n                                 add_special_tokens=False,\n                                 max_length=max_length * 3 // 4,\n                                 truncation=True)\n        passage_inputs = tokenizer(f'B: {passage}',\n                                   return_tensors=None,\n                                   add_special_tokens=False,\n                                   max_length=max_length,\n                                   truncation=True)\n        item = tokenizer.prepare_for_model(\n            [tokenizer.bos_token_id] + query_inputs['input_ids'],\n            sep_inputs + passage_inputs['input_ids'],\n            truncation='only_second',\n            max_length=max_length,\n            padding=False,\n            return_attention_mask=False,\n            return_token_type_ids=False,\n            add_special_tokens=False\n        )\n        item['input_ids'] = item['input_ids'] + sep_inputs + prompt_inputs\n        item['attention_mask'] = [1] * len(item['input_ids'])\n        inputs.append(item)\n    return tokenizer.pad(\n            inputs,\n            padding=True,\n            max_length=max_length + len(sep_inputs) + len(prompt_inputs),\n            pad_to_multiple_of=8,\n            return_tensors='pt',\n    )\n\ntokenizer = AutoTokenizer.from_pretrained('BAAI/bge-reranker-v2-gemma')\nmodel = AutoModelForCausalLM.from_pretrained('BAAI/bge-reranker-v2-gemma')\nyes_loc = tokenizer('Yes', add_special_tokens=False)['input_ids'][0]\nmodel.eval()\n\npairs = [['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']]\nwith torch.no_grad():\n    inputs = get_inputs(pairs, tokenizer)\n    scores = model(**inputs, return_dict=True).logits[:, -1, yes_loc].view(-1, ).float()\n    print(scores)\n```\n\n#### For LLM-based layerwise reranker\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ndef get_inputs(pairs, tokenizer, prompt=None, max_length=1024):\n    if prompt is None:\n        prompt = \"Given a query A and a passage B, determine whether the passage contains an answer to the query by providing a prediction of either 'Yes' or 'No'.\"\n    sep = \"\\n\"\n    prompt_inputs = tokenizer(prompt,\n                              return_tensors=None,\n                              add_special_tokens=False)['input_ids']\n    sep_inputs = tokenizer(sep,\n                           return_tensors=None,\n                           add_special_tokens=False)['input_ids']\n    inputs = []\n    for query, passage in pairs:\n        query_inputs = tokenizer(f'A: {query}',\n                                 return_tensors=None,\n                                 add_special_tokens=False,\n                                 max_length=max_length * 3 // 4,\n                                 truncation=True)\n        passage_inputs = tokenizer(f'B: {passage}',\n                                   return_tensors=None,\n                                   add_special_tokens=False,\n                                   max_length=max_length,\n                                   truncation=True)\n        item = tokenizer.prepare_for_model(\n            [tokenizer.bos_token_id] + query_inputs['input_ids'],\n            sep_inputs + passage_inputs['input_ids'],\n            truncation='only_second',\n            max_length=max_length,\n            padding=False,\n            return_attention_mask=False,\n            return_token_type_ids=False,\n            add_special_tokens=False\n        )\n        item['input_ids'] = item['input_ids'] + sep_inputs + prompt_inputs\n        item['attention_mask'] = [1] * len(item['input_ids'])\n        inputs.append(item)\n    return tokenizer.pad(\n            inputs,\n            padding=True,\n            max_length=max_length + len(sep_inputs) + len(prompt_inputs),\n            pad_to_multiple_of=8,\n            return_tensors='pt',\n    )\n\ntokenizer = AutoTokenizer.from_pretrained('BAAI/bge-reranker-v2-minicpm-layerwise', trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained('BAAI/bge-reranker-v2-minicpm-layerwise', trust_remote_code=True, torch_dtype=torch.bfloat16)\nmodel = model.to('cuda')\nmodel.eval()\n\npairs = [['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']]\nwith torch.no_grad():\n    inputs = get_inputs(pairs, tokenizer).to(model.device)\n    all_scores = model(**inputs, return_dict=True, cutoff_layers=[28])\n    all_scores = [scores[:, -1].view(-1, ).float() for scores in all_scores[0]]\n    print(all_scores)\n```\n\n## Fine-tune\n\n### Data Format\n\nTrain data should be a json file, where each line is a dict like this:\n\n```\n{\"query\": str, \"pos\": List[str], \"neg\":List[str], \"prompt\": str}\n```\n\n`query` is the query, and `pos` is a list of positive texts, `neg` is a list of negative texts, `prompt` indicates the relationship between query and texts. If you have no negative texts for a query, you can random sample some from the entire corpus as the negatives.\n\nSee [toy_finetune_data.jsonl](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/llm_reranker/toy_finetune_data.jsonl) for a toy data file.\n\n### Train\n\nYou can fine-tune the reranker with the following code:\n\n**For llm-based reranker**\n\n```shell\ntorchrun --nproc_per_node {number of gpus} \\\n-m FlagEmbedding.llm_reranker.finetune_for_instruction.run \\\n--output_dir {path to save model} \\\n--model_name_or_path google/gemma-2b \\\n--train_data ./toy_finetune_data.jsonl \\\n--learning_rate 2e-4 \\\n--num_train_epochs 1 \\\n--per_device_train_batch_size 1 \\\n--gradient_accumulation_steps 16 \\\n--dataloader_drop_last True \\\n--query_max_len 512 \\\n--passage_max_len 512 \\\n--train_group_size 16 \\\n--logging_steps 1 \\\n--save_steps 2000 \\\n--save_total_limit 50 \\\n--ddp_find_unused_parameters False \\\n--gradient_checkpointing \\\n--deepspeed stage1.json \\\n--warmup_ratio 0.1 \\\n--bf16 \\\n--use_lora True \\\n--lora_rank 32 \\\n--lora_alpha 64 \\\n--use_flash_attn True \\\n--target_modules q_proj k_proj v_proj o_proj\n```\n\n**For llm-based layerwise reranker**\n\n```shell\ntorchrun --nproc_per_node {number of gpus} \\\n-m FlagEmbedding.llm_reranker.finetune_for_layerwise.run \\\n--output_dir {path to save model} \\\n--model_name_or_path openbmb/MiniCPM-2B-dpo-bf16 \\\n--train_data ./toy_finetune_data.jsonl \\\n--learning_rate 2e-4 \\\n--num_train_epochs 1 \\\n--per_device_train_batch_size 1 \\\n--gradient_accumulation_steps 16 \\\n--dataloader_drop_last True \\\n--query_max_len 512 \\\n--passage_max_len 512 \\\n--train_group_size 16 \\\n--logging_steps 1 \\\n--save_steps 2000 \\\n--save_total_limit 50 \\\n--ddp_find_unused_parameters False \\\n--gradient_checkpointing \\\n--deepspeed stage1.json \\\n--warmup_ratio 0.1 \\\n--bf16 \\\n--use_lora True \\\n--lora_rank 32 \\\n--lora_alpha 64 \\\n--use_flash_attn True \\\n--target_modules q_proj k_proj v_proj o_proj \\\n--start_layer 8 \\\n--head_multi True \\\n--head_type simple \\\n--lora_extra_parameters linear_head\n```\n\nOur rerankers are initialized from [google/gemma-2b](https://huggingface.co/google/gemma-2b) (for llm-based reranker) and [openbmb/MiniCPM-2B-dpo-bf16](https://huggingface.co/openbmb/MiniCPM-2B-dpo-bf16) (for llm-based layerwise reranker), and we train it on a mixture of multilingual datasets:\n\n- [bge-m3-data](https://huggingface.co/datasets/Shitao/bge-m3-data)\n- [quora train data](https://huggingface.co/datasets/quora)\n- [fever train data](https://fever.ai/dataset/fever.html)\n\n## Evaluation\n\n- llama-index.\n\n![image-20240317193909373](./assets/llama-index.png)\n\n\n- BEIR.   \n\nrereank the top 100 results from bge-en-v1.5 large.\n\n![image-20240317174633333](./assets/BEIR-bge-en-v1.5.png)\n\nrereank the top 100 results from e5 mistral 7b instruct.\n\n![image-20240317172949713](./assets/BEIR-e5-mistral.png)\n\n- CMTEB-retrieval.   \nIt rereank the top 100 results from bge-zh-v1.5 large.\n\n![image-20240317173026235](./assets/CMTEB-retrieval-bge-zh-v1.5.png)\n\n- miracl (multi-language).   \nIt rereank the top 100 results from bge-m3.\n\n![image-20240317173117639](./assets/miracl-bge-m3.png)\n\n\n\n## Citation\n\nIf you find this repository useful, please consider giving a star and citation\n\n```bibtex\n@misc{li2023making,\n      title={Making Large Language Models A Better Foundation For Dense Retrieval}, \n      author={Chaofan Li and Zheng Liu and Shitao Xiao and Yingxia Shao},\n      year={2023},\n      eprint={2312.15503},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n@misc{chen2024bge,\n      title={BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation}, \n      author={Jianlv Chen and Shitao Xiao and Peitian Zhang and Kun Luo and Defu Lian and Zheng Liu},\n      year={2024},\n      eprint={2402.03216},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```",
    "card_content": "---\nlicense: apache-2.0\npipeline_tag: text-classification\ntags:\n- transformers\n- sentence-transformers\n- text-embeddings-inference\nlanguage:\n- multilingual\n---\n\n# Reranker\n\n**More details please refer to our Github: [FlagEmbedding](https://github.com/FlagOpen/FlagEmbedding/tree/master).**\n\n- [Model List](#model-list)\n- [Usage](#usage)\n- [Fine-tuning](#fine-tune)\n- [Evaluation](#evaluation)\n- [Citation](#citation)\n\nDifferent from embedding model, reranker uses question and document as input and directly output similarity instead of embedding. \nYou can get a relevance score by inputting query and passage to the reranker. \nAnd the score can be mapped to a float value in [0,1] by sigmoid function.\n\n\n## Model List\n\n| Model                                                                     | Base model                                                           | Language | layerwise |                           feature                            |\n|:--------------------------------------------------------------------------|:--------:|:-----------------------------------------------------------------------------------------------------------------------------------:|:----------------------------------------------------------------------------------------------:|:----------------------------------------------------------------------------------------------:|\n| [BAAI/bge-reranker-base](https://huggingface.co/BAAI/bge-reranker-base) | [xlm-roberta-base](https://huggingface.co/xlm-roberta-base) | Chinese and English |     -     | Lightweight reranker model, easy to deploy, with fast inference. |\n| [BAAI/bge-reranker-large](https://huggingface.co/BAAI/bge-reranker-large) | [xlm-roberta-large](https://huggingface.co/FacebookAI/xlm-roberta-large) | Chinese and English |     -     | Lightweight reranker model, easy to deploy, with fast inference. |\n| [BAAI/bge-reranker-v2-m3](https://huggingface.co/BAAI/bge-reranker-v2-m3) | [bge-m3](https://huggingface.co/BAAI/bge-m3) |    Multilingual     |     -     | Lightweight reranker model, possesses strong multilingual capabilities, easy to deploy, with fast inference. |\n| [BAAI/bge-reranker-v2-gemma](https://huggingface.co/BAAI/bge-reranker-v2-gemma) |      [gemma-2b](https://huggingface.co/google/gemma-2b)      |    Multilingual     |     -     | Suitable for multilingual contexts, performs well in both English proficiency and multilingual capabilities. |\n| [BAAI/bge-reranker-v2-minicpm-layerwise](https://huggingface.co/BAAI/bge-reranker-v2-minicpm-layerwise) | [MiniCPM-2B-dpo-bf16](https://huggingface.co/openbmb/MiniCPM-2B-dpo-bf16) |    Multilingual     |   8-40    | Suitable for multilingual contexts, performs well in both English and Chinese proficiency, allows freedom to select layers for output, facilitating accelerated inference. |\n\n\nYou can select the model according your senario and resource. \n- For **multilingual**, utilize [BAAI/bge-reranker-v2-m3](https://huggingface.co/BAAI/bge-reranker-v2-m3) and [BAAI/bge-reranker-v2-gemma](https://huggingface.co/BAAI/bge-reranker-v2-gemma)\n\n- For **Chinese or English**, utilize [BAAI/bge-reranker-v2-m3](https://huggingface.co/BAAI/bge-reranker-v2-m3) and [BAAI/bge-reranker-v2-minicpm-layerwise](https://huggingface.co/BAAI/bge-reranker-v2-minicpm-layerwise). \n\n- For **efficiency**, utilize [BAAI/bge-reranker-v2-m3](https://huggingface.co/BAAI/bge-reranker-v2-m3) and the low layer of [BAAI/bge-reranker-v2-minicpm-layerwise](https://huggingface.co/BAAI/bge-reranker-v2-minicpm-layerwise). \n\n- For better performance, recommand [BAAI/bge-reranker-v2-minicpm-layerwise](https://huggingface.co/BAAI/bge-reranker-v2-minicpm-layerwise) and [BAAI/bge-reranker-v2-gemma](https://huggingface.co/BAAI/bge-reranker-v2-gemma)\n\n## Usage \n### Using FlagEmbedding\n\n```\npip install -U FlagEmbedding\n```\n\n#### For normal reranker (bge-reranker-base / bge-reranker-large / bge-reranker-v2-m3 )\n\nGet relevance scores (higher scores indicate more relevance):\n\n```python\nfrom FlagEmbedding import FlagReranker\nreranker = FlagReranker('BAAI/bge-reranker-v2-m3', use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\n\nscore = reranker.compute_score(['query', 'passage'])\nprint(score) # -5.65234375\n\n# You can map the scores into 0-1 by set \"normalize=True\", which will apply sigmoid function to the score\nscore = reranker.compute_score(['query', 'passage'], normalize=True)\nprint(score) # 0.003497010252573502\n\nscores = reranker.compute_score([['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']])\nprint(scores) # [-8.1875, 5.26171875]\n\n# You can map the scores into 0-1 by set \"normalize=True\", which will apply sigmoid function to the score\nscores = reranker.compute_score([['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']], normalize=True)\nprint(scores) # [0.00027803096387751553, 0.9948403768236574]\n```\n\n#### For LLM-based reranker\n\n```python\nfrom FlagEmbedding import FlagLLMReranker\nreranker = FlagLLMReranker('BAAI/bge-reranker-v2-gemma', use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\n# reranker = FlagLLMReranker('BAAI/bge-reranker-v2-gemma', use_bf16=True) # You can also set use_bf16=True to speed up computation with a slight performance degradation\n\nscore = reranker.compute_score(['query', 'passage'])\nprint(score)\n\nscores = reranker.compute_score([['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']])\nprint(scores)\n```\n\n#### For LLM-based layerwise reranker\n\n```python\nfrom FlagEmbedding import LayerWiseFlagLLMReranker\nreranker = LayerWiseFlagLLMReranker('BAAI/bge-reranker-v2-minicpm-layerwise', use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\n# reranker = LayerWiseFlagLLMReranker('BAAI/bge-reranker-v2-minicpm-layerwise', use_bf16=True) # You can also set use_bf16=True to speed up computation with a slight performance degradation\n\nscore = reranker.compute_score(['query', 'passage'], cutoff_layers=[28]) # Adjusting 'cutoff_layers' to pick which layers are used for computing the score.\nprint(score)\n\nscores = reranker.compute_score([['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']], cutoff_layers=[28])\nprint(scores)\n```\n\n### Using Huggingface transformers\n\n#### For normal reranker (bge-reranker-base / bge-reranker-large / bge-reranker-v2-m3 )\n\nGet relevance scores (higher scores indicate more relevance):\n\n```python\nimport torch\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained('BAAI/bge-reranker-v2-m3')\nmodel = AutoModelForSequenceClassification.from_pretrained('BAAI/bge-reranker-v2-m3')\nmodel.eval()\n\npairs = [['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']]\nwith torch.no_grad():\n    inputs = tokenizer(pairs, padding=True, truncation=True, return_tensors='pt', max_length=512)\n    scores = model(**inputs, return_dict=True).logits.view(-1, ).float()\n    print(scores)\n```\n\n#### For LLM-based reranker\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ndef get_inputs(pairs, tokenizer, prompt=None, max_length=1024):\n    if prompt is None:\n        prompt = \"Given a query A and a passage B, determine whether the passage contains an answer to the query by providing a prediction of either 'Yes' or 'No'.\"\n    sep = \"\\n\"\n    prompt_inputs = tokenizer(prompt,\n                              return_tensors=None,\n                              add_special_tokens=False)['input_ids']\n    sep_inputs = tokenizer(sep,\n                           return_tensors=None,\n                           add_special_tokens=False)['input_ids']\n    inputs = []\n    for query, passage in pairs:\n        query_inputs = tokenizer(f'A: {query}',\n                                 return_tensors=None,\n                                 add_special_tokens=False,\n                                 max_length=max_length * 3 // 4,\n                                 truncation=True)\n        passage_inputs = tokenizer(f'B: {passage}',\n                                   return_tensors=None,\n                                   add_special_tokens=False,\n                                   max_length=max_length,\n                                   truncation=True)\n        item = tokenizer.prepare_for_model(\n            [tokenizer.bos_token_id] + query_inputs['input_ids'],\n            sep_inputs + passage_inputs['input_ids'],\n            truncation='only_second',\n            max_length=max_length,\n            padding=False,\n            return_attention_mask=False,\n            return_token_type_ids=False,\n            add_special_tokens=False\n        )\n        item['input_ids'] = item['input_ids'] + sep_inputs + prompt_inputs\n        item['attention_mask'] = [1] * len(item['input_ids'])\n        inputs.append(item)\n    return tokenizer.pad(\n            inputs,\n            padding=True,\n            max_length=max_length + len(sep_inputs) + len(prompt_inputs),\n            pad_to_multiple_of=8,\n            return_tensors='pt',\n    )\n\ntokenizer = AutoTokenizer.from_pretrained('BAAI/bge-reranker-v2-gemma')\nmodel = AutoModelForCausalLM.from_pretrained('BAAI/bge-reranker-v2-gemma')\nyes_loc = tokenizer('Yes', add_special_tokens=False)['input_ids'][0]\nmodel.eval()\n\npairs = [['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']]\nwith torch.no_grad():\n    inputs = get_inputs(pairs, tokenizer)\n    scores = model(**inputs, return_dict=True).logits[:, -1, yes_loc].view(-1, ).float()\n    print(scores)\n```\n\n#### For LLM-based layerwise reranker\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ndef get_inputs(pairs, tokenizer, prompt=None, max_length=1024):\n    if prompt is None:\n        prompt = \"Given a query A and a passage B, determine whether the passage contains an answer to the query by providing a prediction of either 'Yes' or 'No'.\"\n    sep = \"\\n\"\n    prompt_inputs = tokenizer(prompt,\n                              return_tensors=None,\n                              add_special_tokens=False)['input_ids']\n    sep_inputs = tokenizer(sep,\n                           return_tensors=None,\n                           add_special_tokens=False)['input_ids']\n    inputs = []\n    for query, passage in pairs:\n        query_inputs = tokenizer(f'A: {query}',\n                                 return_tensors=None,\n                                 add_special_tokens=False,\n                                 max_length=max_length * 3 // 4,\n                                 truncation=True)\n        passage_inputs = tokenizer(f'B: {passage}',\n                                   return_tensors=None,\n                                   add_special_tokens=False,\n                                   max_length=max_length,\n                                   truncation=True)\n        item = tokenizer.prepare_for_model(\n            [tokenizer.bos_token_id] + query_inputs['input_ids'],\n            sep_inputs + passage_inputs['input_ids'],\n            truncation='only_second',\n            max_length=max_length,\n            padding=False,\n            return_attention_mask=False,\n            return_token_type_ids=False,\n            add_special_tokens=False\n        )\n        item['input_ids'] = item['input_ids'] + sep_inputs + prompt_inputs\n        item['attention_mask'] = [1] * len(item['input_ids'])\n        inputs.append(item)\n    return tokenizer.pad(\n            inputs,\n            padding=True,\n            max_length=max_length + len(sep_inputs) + len(prompt_inputs),\n            pad_to_multiple_of=8,\n            return_tensors='pt',\n    )\n\ntokenizer = AutoTokenizer.from_pretrained('BAAI/bge-reranker-v2-minicpm-layerwise', trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained('BAAI/bge-reranker-v2-minicpm-layerwise', trust_remote_code=True, torch_dtype=torch.bfloat16)\nmodel = model.to('cuda')\nmodel.eval()\n\npairs = [['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']]\nwith torch.no_grad():\n    inputs = get_inputs(pairs, tokenizer).to(model.device)\n    all_scores = model(**inputs, return_dict=True, cutoff_layers=[28])\n    all_scores = [scores[:, -1].view(-1, ).float() for scores in all_scores[0]]\n    print(all_scores)\n```\n\n## Fine-tune\n\n### Data Format\n\nTrain data should be a json file, where each line is a dict like this:\n\n```\n{\"query\": str, \"pos\": List[str], \"neg\":List[str], \"prompt\": str}\n```\n\n`query` is the query, and `pos` is a list of positive texts, `neg` is a list of negative texts, `prompt` indicates the relationship between query and texts. If you have no negative texts for a query, you can random sample some from the entire corpus as the negatives.\n\nSee [toy_finetune_data.jsonl](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/llm_reranker/toy_finetune_data.jsonl) for a toy data file.\n\n### Train\n\nYou can fine-tune the reranker with the following code:\n\n**For llm-based reranker**\n\n```shell\ntorchrun --nproc_per_node {number of gpus} \\\n-m FlagEmbedding.llm_reranker.finetune_for_instruction.run \\\n--output_dir {path to save model} \\\n--model_name_or_path google/gemma-2b \\\n--train_data ./toy_finetune_data.jsonl \\\n--learning_rate 2e-4 \\\n--num_train_epochs 1 \\\n--per_device_train_batch_size 1 \\\n--gradient_accumulation_steps 16 \\\n--dataloader_drop_last True \\\n--query_max_len 512 \\\n--passage_max_len 512 \\\n--train_group_size 16 \\\n--logging_steps 1 \\\n--save_steps 2000 \\\n--save_total_limit 50 \\\n--ddp_find_unused_parameters False \\\n--gradient_checkpointing \\\n--deepspeed stage1.json \\\n--warmup_ratio 0.1 \\\n--bf16 \\\n--use_lora True \\\n--lora_rank 32 \\\n--lora_alpha 64 \\\n--use_flash_attn True \\\n--target_modules q_proj k_proj v_proj o_proj\n```\n\n**For llm-based layerwise reranker**\n\n```shell\ntorchrun --nproc_per_node {number of gpus} \\\n-m FlagEmbedding.llm_reranker.finetune_for_layerwise.run \\\n--output_dir {path to save model} \\\n--model_name_or_path openbmb/MiniCPM-2B-dpo-bf16 \\\n--train_data ./toy_finetune_data.jsonl \\\n--learning_rate 2e-4 \\\n--num_train_epochs 1 \\\n--per_device_train_batch_size 1 \\\n--gradient_accumulation_steps 16 \\\n--dataloader_drop_last True \\\n--query_max_len 512 \\\n--passage_max_len 512 \\\n--train_group_size 16 \\\n--logging_steps 1 \\\n--save_steps 2000 \\\n--save_total_limit 50 \\\n--ddp_find_unused_parameters False \\\n--gradient_checkpointing \\\n--deepspeed stage1.json \\\n--warmup_ratio 0.1 \\\n--bf16 \\\n--use_lora True \\\n--lora_rank 32 \\\n--lora_alpha 64 \\\n--use_flash_attn True \\\n--target_modules q_proj k_proj v_proj o_proj \\\n--start_layer 8 \\\n--head_multi True \\\n--head_type simple \\\n--lora_extra_parameters linear_head\n```\n\nOur rerankers are initialized from [google/gemma-2b](https://huggingface.co/google/gemma-2b) (for llm-based reranker) and [openbmb/MiniCPM-2B-dpo-bf16](https://huggingface.co/openbmb/MiniCPM-2B-dpo-bf16) (for llm-based layerwise reranker), and we train it on a mixture of multilingual datasets:\n\n- [bge-m3-data](https://huggingface.co/datasets/Shitao/bge-m3-data)\n- [quora train data](https://huggingface.co/datasets/quora)\n- [fever train data](https://fever.ai/dataset/fever.html)\n\n## Evaluation\n\n- llama-index.\n\n![image-20240317193909373](./assets/llama-index.png)\n\n\n- BEIR.   \n\nrereank the top 100 results from bge-en-v1.5 large.\n\n![image-20240317174633333](./assets/BEIR-bge-en-v1.5.png)\n\nrereank the top 100 results from e5 mistral 7b instruct.\n\n![image-20240317172949713](./assets/BEIR-e5-mistral.png)\n\n- CMTEB-retrieval.   \nIt rereank the top 100 results from bge-zh-v1.5 large.\n\n![image-20240317173026235](./assets/CMTEB-retrieval-bge-zh-v1.5.png)\n\n- miracl (multi-language).   \nIt rereank the top 100 results from bge-m3.\n\n![image-20240317173117639](./assets/miracl-bge-m3.png)\n\n\n\n## Citation\n\nIf you find this repository useful, please consider giving a star and citation\n\n```bibtex\n@misc{li2023making,\n      title={Making Large Language Models A Better Foundation For Dense Retrieval}, \n      author={Chaofan Li and Zheng Liu and Shitao Xiao and Yingxia Shao},\n      year={2023},\n      eprint={2312.15503},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n@misc{chen2024bge,\n      title={BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation}, \n      author={Jianlv Chen and Shitao Xiao and Peitian Zhang and Kun Luo and Defu Lian and Zheng Liu},\n      year={2024},\n      eprint={2402.03216},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```",
    "library_name": "sentence-transformers",
    "transformers_info": {
      "auto_model": "AutoModelForSequenceClassification",
      "custom_class": null,
      "pipeline_tag": "text-classification",
      "processor": "AutoTokenizer"
    },
    "safetensors": {
      "parameters": {
        "F32": 567755777
      },
      "total": 567755777
    },
    "model_index": null,
    "trending_score": null
  },
  {
    "model_id": "cardiffnlp/twitter-roberta-base-emotion",
    "model_name": "cardiffnlp/twitter-roberta-base-emotion",
    "author": "cardiffnlp",
    "downloads": 959865,
    "downloads_all_time": null,
    "likes": 43,
    "tags": [
      "transformers",
      "pytorch",
      "tf",
      "jax",
      "roberta",
      "text-classification",
      "arxiv:2010.12421",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/cardiffnlp/twitter-roberta-base-emotion",
    "dependencies": [
      [
        "transformers",
        null
      ],
      [
        "numpy",
        null
      ],
      [
        "scipy",
        null
      ],
      [
        "csv",
        null
      ],
      [
        "urllib",
        null
      ]
    ],
    "last_modified": "2023-05-28T05:08:00+00:00",
    "created_at": "2022-03-02T23:29:05+00:00",
    "analysis_date": "2025-03-22T00:55:02.710623",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "roberta",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {},
    "card_text": "# Twitter-roBERTa-base for Emotion Recognition\n\nThis is a RoBERTa-base model trained on ~58M tweets and finetuned for emotion recognition with the TweetEval benchmark.\n\n- Paper: [_TweetEval_ benchmark (Findings of EMNLP 2020)](https://arxiv.org/pdf/2010.12421.pdf). \n- Git Repo: [Tweeteval official repository](https://github.com/cardiffnlp/tweeteval).\n\n<b>New!</b> We just released a new emotion recognition model trained with more emotion types and with a newer RoBERTa-based model. \nSee [twitter-roberta-base-emotion-multilabel-latest](https://huggingface.co/cardiffnlp/twitter-roberta-base-emotion-multilabel-latest) and [TweetNLP](https://github.com/cardiffnlp/tweetnlp) for more details.\n\n## Example of classification\n\n```python\nfrom transformers import AutoModelForSequenceClassification\nfrom transformers import TFAutoModelForSequenceClassification\nfrom transformers import AutoTokenizer\nimport numpy as np\nfrom scipy.special import softmax\nimport csv\nimport urllib.request\n\n# Preprocess text (username and link placeholders)\ndef preprocess(text):\n    new_text = []\n    for t in text.split(\" \"):\n        t = '@user' if t.startswith('@') and len(t) > 1 else t\n        t = 'http' if t.startswith('http') else t\n        new_text.append(t)\n    return \" \".join(new_text)\n\n# Tasks:\n# emoji, emotion, hate, irony, offensive, sentiment\n# stance/abortion, stance/atheism, stance/climate, stance/feminist, stance/hillary\n\ntask='emotion'\nMODEL = f\"cardiffnlp/twitter-roberta-base-{task}\"\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL)\n\n# download label mapping\nmapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt\"\nwith urllib.request.urlopen(mapping_link) as f:\n    html = f.read().decode('utf-8').split(\"\\n\")\n    csvreader = csv.reader(html, delimiter='\\t')\nlabels = [row[1] for row in csvreader if len(row) > 1]\n\n# PT\nmodel = AutoModelForSequenceClassification.from_pretrained(MODEL)\nmodel.save_pretrained(MODEL)\n\ntext = \"Celebrating my promotion 😎\"\ntext = preprocess(text)\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\nscores = output[0][0].detach().numpy()\nscores = softmax(scores)\n\n# # TF\n# model = TFAutoModelForSequenceClassification.from_pretrained(MODEL)\n# model.save_pretrained(MODEL)\n\n# text = \"Celebrating my promotion 😎\"\n# encoded_input = tokenizer(text, return_tensors='tf')\n# output = model(encoded_input)\n# scores = output[0][0].numpy()\n# scores = softmax(scores)\n\nranking = np.argsort(scores)\nranking = ranking[::-1]\nfor i in range(scores.shape[0]):\n    l = labels[ranking[i]]\n    s = scores[ranking[i]]\n    print(f\"{i+1}) {l} {np.round(float(s), 4)}\")\n\n```\n\nOutput: \n\n```\n1) joy 0.9382\n2) optimism 0.0362\n3) anger 0.0145\n4) sadness 0.0112\n```\n",
    "card_content": "---\n{}\n---\n# Twitter-roBERTa-base for Emotion Recognition\n\nThis is a RoBERTa-base model trained on ~58M tweets and finetuned for emotion recognition with the TweetEval benchmark.\n\n- Paper: [_TweetEval_ benchmark (Findings of EMNLP 2020)](https://arxiv.org/pdf/2010.12421.pdf). \n- Git Repo: [Tweeteval official repository](https://github.com/cardiffnlp/tweeteval).\n\n<b>New!</b> We just released a new emotion recognition model trained with more emotion types and with a newer RoBERTa-based model. \nSee [twitter-roberta-base-emotion-multilabel-latest](https://huggingface.co/cardiffnlp/twitter-roberta-base-emotion-multilabel-latest) and [TweetNLP](https://github.com/cardiffnlp/tweetnlp) for more details.\n\n## Example of classification\n\n```python\nfrom transformers import AutoModelForSequenceClassification\nfrom transformers import TFAutoModelForSequenceClassification\nfrom transformers import AutoTokenizer\nimport numpy as np\nfrom scipy.special import softmax\nimport csv\nimport urllib.request\n\n# Preprocess text (username and link placeholders)\ndef preprocess(text):\n    new_text = []\n    for t in text.split(\" \"):\n        t = '@user' if t.startswith('@') and len(t) > 1 else t\n        t = 'http' if t.startswith('http') else t\n        new_text.append(t)\n    return \" \".join(new_text)\n\n# Tasks:\n# emoji, emotion, hate, irony, offensive, sentiment\n# stance/abortion, stance/atheism, stance/climate, stance/feminist, stance/hillary\n\ntask='emotion'\nMODEL = f\"cardiffnlp/twitter-roberta-base-{task}\"\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL)\n\n# download label mapping\nmapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt\"\nwith urllib.request.urlopen(mapping_link) as f:\n    html = f.read().decode('utf-8').split(\"\\n\")\n    csvreader = csv.reader(html, delimiter='\\t')\nlabels = [row[1] for row in csvreader if len(row) > 1]\n\n# PT\nmodel = AutoModelForSequenceClassification.from_pretrained(MODEL)\nmodel.save_pretrained(MODEL)\n\ntext = \"Celebrating my promotion 😎\"\ntext = preprocess(text)\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\nscores = output[0][0].detach().numpy()\nscores = softmax(scores)\n\n# # TF\n# model = TFAutoModelForSequenceClassification.from_pretrained(MODEL)\n# model.save_pretrained(MODEL)\n\n# text = \"Celebrating my promotion 😎\"\n# encoded_input = tokenizer(text, return_tensors='tf')\n# output = model(encoded_input)\n# scores = output[0][0].numpy()\n# scores = softmax(scores)\n\nranking = np.argsort(scores)\nranking = ranking[::-1]\nfor i in range(scores.shape[0]):\n    l = labels[ranking[i]]\n    s = scores[ranking[i]]\n    print(f\"{i+1}) {l} {np.round(float(s), 4)}\")\n\n```\n\nOutput: \n\n```\n1) joy 0.9382\n2) optimism 0.0362\n3) anger 0.0145\n4) sadness 0.0112\n```\n",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModelForSequenceClassification",
      "custom_class": null,
      "pipeline_tag": "text-classification",
      "processor": "AutoTokenizer"
    },
    "safetensors": null,
    "model_index": null,
    "trending_score": null
  },
  {
    "model_id": "yiyanghkust/finbert-tone",
    "model_name": "yiyanghkust/finbert-tone",
    "author": "yiyanghkust",
    "downloads": 946165,
    "downloads_all_time": null,
    "likes": 174,
    "tags": [
      "transformers",
      "pytorch",
      "tf",
      "text-classification",
      "financial-sentiment-analysis",
      "sentiment-analysis",
      "en",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/yiyanghkust/finbert-tone",
    "dependencies": [
      [
        "transformers",
        null
      ]
    ],
    "last_modified": "2022-10-17T00:35:39+00:00",
    "created_at": "2022-03-02T23:29:05+00:00",
    "analysis_date": "2025-03-22T00:55:03.634496",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "unknown",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": "en",
      "tags": [
        "financial-sentiment-analysis",
        "sentiment-analysis"
      ],
      "widget": [
        {
          "text": "growth is strong and we have plenty of liquidity"
        }
      ]
    },
    "card_text": "\n`FinBERT` is a BERT model pre-trained on financial communication text. The purpose is to enhance financial NLP research and practice. It is trained on the following three financial communication corpus. The total corpora size is 4.9B tokens.\n- Corporate Reports 10-K & 10-Q: 2.5B tokens\n- Earnings Call Transcripts: 1.3B tokens\n- Analyst Reports: 1.1B tokens\n\nMore technical details on `FinBERT`: [Click Link](https://github.com/yya518/FinBERT)\n\nThis released `finbert-tone` model is the `FinBERT` model fine-tuned on 10,000 manually annotated (positive, negative, neutral) sentences from analyst reports. This model achieves superior performance on financial tone analysis task. If you are simply interested in using `FinBERT` for financial tone analysis, give it a try.\n\nIf you use the model in your academic work, please cite the following paper:\n\nHuang, Allen H., Hui Wang, and Yi Yang. \"FinBERT: A Large Language Model for Extracting Information from Financial Text.\" *Contemporary Accounting Research* (2022).\n\n\n# How to use \nYou can use this model with Transformers pipeline for sentiment analysis.\n```python\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom transformers import pipeline\n\nfinbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone',num_labels=3)\ntokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')\n\nnlp = pipeline(\"sentiment-analysis\", model=finbert, tokenizer=tokenizer)\n\nsentences = [\"there is a shortage of capital, and we need extra financing\",  \n             \"growth is strong and we have plenty of liquidity\", \n             \"there are doubts about our finances\", \n             \"profits are flat\"]\nresults = nlp(sentences)\nprint(results)  #LABEL_0: neutral; LABEL_1: positive; LABEL_2: negative\n\n```",
    "card_content": "---\nlanguage: en\ntags:\n- financial-sentiment-analysis\n- sentiment-analysis\nwidget:\n- text: growth is strong and we have plenty of liquidity\n---\n\n`FinBERT` is a BERT model pre-trained on financial communication text. The purpose is to enhance financial NLP research and practice. It is trained on the following three financial communication corpus. The total corpora size is 4.9B tokens.\n- Corporate Reports 10-K & 10-Q: 2.5B tokens\n- Earnings Call Transcripts: 1.3B tokens\n- Analyst Reports: 1.1B tokens\n\nMore technical details on `FinBERT`: [Click Link](https://github.com/yya518/FinBERT)\n\nThis released `finbert-tone` model is the `FinBERT` model fine-tuned on 10,000 manually annotated (positive, negative, neutral) sentences from analyst reports. This model achieves superior performance on financial tone analysis task. If you are simply interested in using `FinBERT` for financial tone analysis, give it a try.\n\nIf you use the model in your academic work, please cite the following paper:\n\nHuang, Allen H., Hui Wang, and Yi Yang. \"FinBERT: A Large Language Model for Extracting Information from Financial Text.\" *Contemporary Accounting Research* (2022).\n\n\n# How to use \nYou can use this model with Transformers pipeline for sentiment analysis.\n```python\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom transformers import pipeline\n\nfinbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone',num_labels=3)\ntokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')\n\nnlp = pipeline(\"sentiment-analysis\", model=finbert, tokenizer=tokenizer)\n\nsentences = [\"there is a shortage of capital, and we need extra financing\",  \n             \"growth is strong and we have plenty of liquidity\", \n             \"there are doubts about our finances\", \n             \"profits are flat\"]\nresults = nlp(sentences)\nprint(results)  #LABEL_0: neutral; LABEL_1: positive; LABEL_2: negative\n\n```",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModelForSequenceClassification",
      "custom_class": null,
      "pipeline_tag": "text-classification",
      "processor": null
    },
    "safetensors": null,
    "model_index": null,
    "trending_score": null
  },
  {
    "model_id": "cross-encoder/ms-marco-MiniLM-L12-v2",
    "model_name": "cross-encoder/ms-marco-MiniLM-L12-v2",
    "author": "cross-encoder",
    "downloads": 866899,
    "downloads_all_time": null,
    "likes": 68,
    "tags": [
      "transformers",
      "pytorch",
      "jax",
      "safetensors",
      "bert",
      "text-classification",
      "license:apache-2.0",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/cross-encoder/ms-marco-MiniLM-L12-v2",
    "dependencies": [
      [
        "sentence_transformers",
        null
      ],
      [
        "transformers",
        null
      ],
      [
        "torch",
        null
      ]
    ],
    "last_modified": "2025-03-07T14:58:25+00:00",
    "created_at": "2022-03-02T23:29:05+00:00",
    "analysis_date": "2025-03-22T00:55:04.923744",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "bert",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "license": "apache-2.0"
    },
    "card_text": "# Cross-Encoder for MS Marco\n\nThis model was trained on the [MS Marco Passage Ranking](https://github.com/microsoft/MSMARCO-Passage-Ranking) task.\n\nThe model can be used for Information Retrieval: Given a query, encode the query will all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. See [SBERT.net Retrieve & Re-rank](https://www.sbert.net/examples/applications/retrieve_rerank/README.html) for more details. The training code is available here: [SBERT.net Training MS Marco](https://github.com/UKPLab/sentence-transformers/tree/master/examples/training/ms_marco)\n\n\n## Usage with SentenceTransformers\n\nThe usage is easy when you have [SentenceTransformers](https://www.sbert.net/) installed. Then you can use the pre-trained models like this:\n```python\nfrom sentence_transformers import CrossEncoder\n\nmodel = CrossEncoder('cross-encoder/ms-marco-MiniLM-L12-v2')\nscores = model.predict([\n    (\"How many people live in Berlin?\", \"Berlin had a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\"),\n    (\"How many people live in Berlin?\", \"Berlin is well known for its museums.\"),\n])\nprint(scores)\n# [ 9.218911  -4.0780287]\n```\n\n\n## Usage with Transformers\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\n\nmodel = AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-MiniLM-L12-v2')\ntokenizer = AutoTokenizer.from_pretrained('cross-encoder/ms-marco-MiniLM-L12-v2')\n\nfeatures = tokenizer(['How many people live in Berlin?', 'How many people live in Berlin?'], ['Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.', 'New York City is famous for the Metropolitan Museum of Art.'],  padding=True, truncation=True, return_tensors=\"pt\")\n\nmodel.eval()\nwith torch.no_grad():\n    scores = model(**features).logits\n    print(scores)\n```\n\n\n\n## Performance\nIn the following table, we provide various pre-trained Cross-Encoders together with their performance on the [TREC Deep Learning 2019](https://microsoft.github.io/TREC-2019-Deep-Learning/) and the [MS Marco Passage Reranking](https://github.com/microsoft/MSMARCO-Passage-Ranking/) dataset. \n\n\n| Model-Name        | NDCG@10 (TREC DL 19) | MRR@10 (MS Marco Dev)  | Docs / Sec |\n| ------------- |:-------------| -----| --- | \n| **Version 2 models** | | | \n| cross-encoder/ms-marco-TinyBERT-L2-v2 | 69.84 | 32.56 | 9000\n| cross-encoder/ms-marco-MiniLM-L2-v2 | 71.01 | 34.85 | 4100\n| cross-encoder/ms-marco-MiniLM-L4-v2 | 73.04 | 37.70 | 2500\n| cross-encoder/ms-marco-MiniLM-L6-v2 | 74.30 | 39.01 | 1800\n| cross-encoder/ms-marco-MiniLM-L12-v2 | 74.31 | 39.02 | 960\n| **Version 1 models** | | | \n| cross-encoder/ms-marco-TinyBERT-L2  | 67.43 | 30.15  | 9000\n| cross-encoder/ms-marco-TinyBERT-L4  | 68.09 | 34.50  | 2900\n| cross-encoder/ms-marco-TinyBERT-L6 |  69.57 | 36.13  | 680\n| cross-encoder/ms-marco-electra-base | 71.99 | 36.41 | 340\n| **Other models** | | | \n| nboost/pt-tinybert-msmarco | 63.63 | 28.80 | 2900 \n| nboost/pt-bert-base-uncased-msmarco | 70.94 | 34.75 | 340 \n| nboost/pt-bert-large-msmarco | 73.36 | 36.48 | 100 \n| Capreolus/electra-base-msmarco | 71.23 | 36.89 | 340 \n| amberoad/bert-multilingual-passage-reranking-msmarco | 68.40 | 35.54 | 330 \n| sebastian-hofstaetter/distilbert-cat-margin_mse-T2-msmarco | 72.82 | 37.88 | 720\n \n Note: Runtime was computed on a V100 GPU.\n",
    "card_content": "---\nlicense: apache-2.0\n---\n# Cross-Encoder for MS Marco\n\nThis model was trained on the [MS Marco Passage Ranking](https://github.com/microsoft/MSMARCO-Passage-Ranking) task.\n\nThe model can be used for Information Retrieval: Given a query, encode the query will all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. See [SBERT.net Retrieve & Re-rank](https://www.sbert.net/examples/applications/retrieve_rerank/README.html) for more details. The training code is available here: [SBERT.net Training MS Marco](https://github.com/UKPLab/sentence-transformers/tree/master/examples/training/ms_marco)\n\n\n## Usage with SentenceTransformers\n\nThe usage is easy when you have [SentenceTransformers](https://www.sbert.net/) installed. Then you can use the pre-trained models like this:\n```python\nfrom sentence_transformers import CrossEncoder\n\nmodel = CrossEncoder('cross-encoder/ms-marco-MiniLM-L12-v2')\nscores = model.predict([\n    (\"How many people live in Berlin?\", \"Berlin had a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\"),\n    (\"How many people live in Berlin?\", \"Berlin is well known for its museums.\"),\n])\nprint(scores)\n# [ 9.218911  -4.0780287]\n```\n\n\n## Usage with Transformers\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\n\nmodel = AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-MiniLM-L12-v2')\ntokenizer = AutoTokenizer.from_pretrained('cross-encoder/ms-marco-MiniLM-L12-v2')\n\nfeatures = tokenizer(['How many people live in Berlin?', 'How many people live in Berlin?'], ['Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.', 'New York City is famous for the Metropolitan Museum of Art.'],  padding=True, truncation=True, return_tensors=\"pt\")\n\nmodel.eval()\nwith torch.no_grad():\n    scores = model(**features).logits\n    print(scores)\n```\n\n\n\n## Performance\nIn the following table, we provide various pre-trained Cross-Encoders together with their performance on the [TREC Deep Learning 2019](https://microsoft.github.io/TREC-2019-Deep-Learning/) and the [MS Marco Passage Reranking](https://github.com/microsoft/MSMARCO-Passage-Ranking/) dataset. \n\n\n| Model-Name        | NDCG@10 (TREC DL 19) | MRR@10 (MS Marco Dev)  | Docs / Sec |\n| ------------- |:-------------| -----| --- | \n| **Version 2 models** | | | \n| cross-encoder/ms-marco-TinyBERT-L2-v2 | 69.84 | 32.56 | 9000\n| cross-encoder/ms-marco-MiniLM-L2-v2 | 71.01 | 34.85 | 4100\n| cross-encoder/ms-marco-MiniLM-L4-v2 | 73.04 | 37.70 | 2500\n| cross-encoder/ms-marco-MiniLM-L6-v2 | 74.30 | 39.01 | 1800\n| cross-encoder/ms-marco-MiniLM-L12-v2 | 74.31 | 39.02 | 960\n| **Version 1 models** | | | \n| cross-encoder/ms-marco-TinyBERT-L2  | 67.43 | 30.15  | 9000\n| cross-encoder/ms-marco-TinyBERT-L4  | 68.09 | 34.50  | 2900\n| cross-encoder/ms-marco-TinyBERT-L6 |  69.57 | 36.13  | 680\n| cross-encoder/ms-marco-electra-base | 71.99 | 36.41 | 340\n| **Other models** | | | \n| nboost/pt-tinybert-msmarco | 63.63 | 28.80 | 2900 \n| nboost/pt-bert-base-uncased-msmarco | 70.94 | 34.75 | 340 \n| nboost/pt-bert-large-msmarco | 73.36 | 36.48 | 100 \n| Capreolus/electra-base-msmarco | 71.23 | 36.89 | 340 \n| amberoad/bert-multilingual-passage-reranking-msmarco | 68.40 | 35.54 | 330 \n| sebastian-hofstaetter/distilbert-cat-margin_mse-T2-msmarco | 72.82 | 37.88 | 720\n \n Note: Runtime was computed on a V100 GPU.\n",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModelForSequenceClassification",
      "custom_class": null,
      "pipeline_tag": "text-classification",
      "processor": "AutoTokenizer"
    },
    "safetensors": {
      "parameters": {
        "I64": 512,
        "F32": 33360385
      },
      "total": 33360897
    },
    "model_index": null,
    "trending_score": null
  },
  {
    "model_id": "michellejieli/emotion_text_classifier",
    "model_name": "michellejieli/emotion_text_classifier",
    "author": "michellejieli",
    "downloads": 826857,
    "downloads_all_time": null,
    "likes": 120,
    "tags": [
      "transformers",
      "pytorch",
      "roberta",
      "text-classification",
      "distilroberta",
      "sentiment",
      "emotion",
      "twitter",
      "reddit",
      "en",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/michellejieli/emotion_text_classifier",
    "dependencies": [
      [
        "transformers",
        null
      ]
    ],
    "last_modified": "2023-05-03T00:39:47+00:00",
    "created_at": "2022-10-22T22:44:07+00:00",
    "analysis_date": "2025-03-22T00:55:06.493456",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "roberta",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": "en",
      "tags": [
        "distilroberta",
        "sentiment",
        "emotion",
        "twitter",
        "reddit"
      ],
      "widget": [
        {
          "text": "Oh my God, he's lost it. He's totally lost it."
        },
        {
          "text": "What?"
        },
        {
          "text": "Wow, congratulations! So excited for you!"
        }
      ]
    },
    "card_text": "\n# Fine-tuned DistilRoBERTa-base for Emotion Classification 🤬🤢😀😐😭😲\n\n# Model Description \n\nDistilRoBERTa-base is a transformer model that performs sentiment analysis. I fine-tuned the model on transcripts from the Friends show with the goal of classifying emotions from text data, specifically dialogue from Netflix shows or movies. The model predicts 6 Ekman emotions and a neutral class. These emotions include anger, disgust, fear, joy, neutrality, sadness, and surprise.\n\nThe model is a fine-tuned version of [Emotion English DistilRoBERTa-base](https://huggingface.co/j-hartmann/emotion-english-distilroberta-base/) and [DistilRoBERTa-base](https://huggingface.co/j-hartmann/emotion-english-distilroberta-base). This model was initially trained on the following table from [Emotion English DistilRoBERTa-base](https://huggingface.co/j-hartmann/emotion-english-distilroberta-base/):\n\n|Name|anger|disgust|fear|joy|neutral|sadness|surprise|\n|---|---|---|---|---|---|---|---|\n|Crowdflower (2016)|Yes|-|-|Yes|Yes|Yes|Yes|\n|Emotion Dataset, Elvis et al. (2018)|Yes|-|Yes|Yes|-|Yes|Yes|\n|GoEmotions, Demszky et al. (2020)|Yes|Yes|Yes|Yes|Yes|Yes|Yes|\n|ISEAR, Vikash (2018)|Yes|Yes|Yes|Yes|-|Yes|-|\n|MELD, Poria et al. (2019)|Yes|Yes|Yes|Yes|Yes|Yes|Yes|\n|SemEval-2018, EI-reg, Mohammad et al. (2018) |Yes|-|Yes|Yes|-|Yes|-|\n\nIt was fine-tuned on:\n|Name|anger|disgust|fear|joy|neutral|sadness|surprise|\n|---|---|---|---|---|---|---|---|\n|Emotion Lines (Friends)|Yes|Yes|Yes|Yes|Yes|Yes|Yes|\n\n# How to Use \n\n```python\nfrom transformers import pipeline\nclassifier = pipeline(\"sentiment-analysis\", model=\"michellejieli/emotion_text_classifier\")\nclassifier(\"I love this!\")\n```\n\n```python\nOutput:\n[{'label': 'joy', 'score': 0.9887555241584778}]\n```\n\n# Contact\n\nPlease reach out to [michelleli1999@gmail.com](mailto:michelleli1999@gmail.com) if you have any questions or feedback.\n\n\n# Reference\n\n```\nJochen Hartmann, \"Emotion English DistilRoBERTa-base\". https://huggingface.co/j-hartmann/emotion-english-distilroberta-base/, 2022.\nAshritha R Murthy and K M Anil Kumar 2021 IOP Conf. Ser.: Mater. Sci. Eng. 1110 012009\n```",
    "card_content": "---\nlanguage: en\ntags:\n- distilroberta\n- sentiment\n- emotion\n- twitter\n- reddit\nwidget:\n- text: Oh my God, he's lost it. He's totally lost it.\n- text: What?\n- text: Wow, congratulations! So excited for you!\n---\n\n# Fine-tuned DistilRoBERTa-base for Emotion Classification 🤬🤢😀😐😭😲\n\n# Model Description \n\nDistilRoBERTa-base is a transformer model that performs sentiment analysis. I fine-tuned the model on transcripts from the Friends show with the goal of classifying emotions from text data, specifically dialogue from Netflix shows or movies. The model predicts 6 Ekman emotions and a neutral class. These emotions include anger, disgust, fear, joy, neutrality, sadness, and surprise.\n\nThe model is a fine-tuned version of [Emotion English DistilRoBERTa-base](https://huggingface.co/j-hartmann/emotion-english-distilroberta-base/) and [DistilRoBERTa-base](https://huggingface.co/j-hartmann/emotion-english-distilroberta-base). This model was initially trained on the following table from [Emotion English DistilRoBERTa-base](https://huggingface.co/j-hartmann/emotion-english-distilroberta-base/):\n\n|Name|anger|disgust|fear|joy|neutral|sadness|surprise|\n|---|---|---|---|---|---|---|---|\n|Crowdflower (2016)|Yes|-|-|Yes|Yes|Yes|Yes|\n|Emotion Dataset, Elvis et al. (2018)|Yes|-|Yes|Yes|-|Yes|Yes|\n|GoEmotions, Demszky et al. (2020)|Yes|Yes|Yes|Yes|Yes|Yes|Yes|\n|ISEAR, Vikash (2018)|Yes|Yes|Yes|Yes|-|Yes|-|\n|MELD, Poria et al. (2019)|Yes|Yes|Yes|Yes|Yes|Yes|Yes|\n|SemEval-2018, EI-reg, Mohammad et al. (2018) |Yes|-|Yes|Yes|-|Yes|-|\n\nIt was fine-tuned on:\n|Name|anger|disgust|fear|joy|neutral|sadness|surprise|\n|---|---|---|---|---|---|---|---|\n|Emotion Lines (Friends)|Yes|Yes|Yes|Yes|Yes|Yes|Yes|\n\n# How to Use \n\n```python\nfrom transformers import pipeline\nclassifier = pipeline(\"sentiment-analysis\", model=\"michellejieli/emotion_text_classifier\")\nclassifier(\"I love this!\")\n```\n\n```python\nOutput:\n[{'label': 'joy', 'score': 0.9887555241584778}]\n```\n\n# Contact\n\nPlease reach out to [michelleli1999@gmail.com](mailto:michelleli1999@gmail.com) if you have any questions or feedback.\n\n\n# Reference\n\n```\nJochen Hartmann, \"Emotion English DistilRoBERTa-base\". https://huggingface.co/j-hartmann/emotion-english-distilroberta-base/, 2022.\nAshritha R Murthy and K M Anil Kumar 2021 IOP Conf. Ser.: Mater. Sci. Eng. 1110 012009\n```",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModelForSequenceClassification",
      "custom_class": null,
      "pipeline_tag": "text-classification",
      "processor": "AutoTokenizer"
    },
    "safetensors": null,
    "model_index": null,
    "trending_score": null
  },
  {
    "model_id": "sadickam/sdgBERT",
    "model_name": "sadickam/sdgBERT",
    "author": "sadickam",
    "downloads": 807335,
    "downloads_all_time": null,
    "likes": 11,
    "tags": [
      "transformers",
      "safetensors",
      "bert",
      "text-classification",
      "en",
      "license:mit",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/sadickam/sdgBERT",
    "dependencies": [
      [
        "transformers",
        null
      ]
    ],
    "last_modified": "2025-01-30T20:50:38+00:00",
    "created_at": "2023-01-15T23:34:42+00:00",
    "analysis_date": "2025-03-22T00:55:07.627711",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "bert",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "license": "mit",
      "language": [
        "en"
      ],
      "metrics": [
        "accuracy",
        "matthews_correlation"
      ],
      "widget": [
        {
          "text": "Highway work zones create potential risks for both traffic and workers in addition to traffic congestion and delays that result in increased road user delay."
        },
        {
          "text": "A circular economy is a way of achieving sustainable consumption and production, as well as nature positive outcomes."
        }
      ]
    },
    "card_text": "\n# sadickam/sdgBERT (previously - sadickam/sdg-classification-bert)\n\n<!-- Provide a quick summary of what the model is/does. -->\n\nsgdBERT (previously named \"sdg-classification-bert\"), is an NLP model for classifying text with respect to the United Nations sustainable development goals (SDG).\n\n![image](https://user-images.githubusercontent.com/73560591/216751462-ced482ba-5d8e-48aa-9a48-5557979a35f1.png)\nSource:https://www.un.org/development/desa/disabilities/about-us/sustainable-development-goals-sdgs-and-disability.html\n\n\n## Model Details\n\n### Model Description\n\n<!-- Provide a longer summary of what this model is. -->\n\nThis text classification model was developed by fine-tuning the bert-base-uncased pre-trained model. The training data for this fine-tuned model was sourced from the publicly available OSDG Community Dataset (OSDG-CD) at https://zenodo.org/record/5550238#.ZBulfcJByF4.\nThis model was made as part of academic research at Deakin University. The goal was to make a transformer-based SDG text classification model that anyone could use. Only the first 16 UN SDGs supported. The primary model details are highlighted below:\n\n- **Model type:** Text classification\n- **Language(s) (NLP):** English\n- **License:** mit\n- **Finetuned from model [optional]:** bert-base-uncased\n\n### Model Sources\n<!-- Provide the basic links for the model. -->\n- **Repository:** https://huggingface.co/sadickam/sdg-classification-bert  \n- **Demo:** option 1 (copy/past text and csv): https://sadickam-sdg-text-classifier.hf.space/; option 2 (PDF documents): https://sadickam-document-sdg-app-cpu.hf.space\n\n\n### Direct Use\n\n<!-- This section is for the model use without fine-tuning or plugging into a larger ecosystem/app. -->\n\nThis is a fine-tuned model and therefore requires no further training.\n\n\n## How to Get Started with the Model\n\nUse the code below to get started with the model.\n```python\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\ntokenizer = AutoTokenizer.from_pretrained(\"sadickam/sdg-classification-bert\")\nmodel = AutoModelForSequenceClassification.from_pretrained(\"sadickam/sdg-classification-bert\")\n```\n\n\n## Training Data\n\n<!-- This should link to a Data Card, perhaps with a short stub of information on what the training data is all about as well as documentation related to data pre-processing or additional filtering. -->\nThe training data includes text from a wide range of industries and academic research fields. Hence, this fine-tuned model is not for a specific industry. \n\nSee training here: https://zenodo.org/record/5550238#.ZBulfcJByF4\n\n\n## Training Hyperparameters\n\n- Num_epoch = 3\n- Learning rate = 5e-5\n- Batch size = 16\n\n\n## Evaluation\n\n#### Metrics\n\n<!-- These are the evaluation metrics being used, ideally with a description of why. -->\n- Accuracy = 0.90\n- Matthews correlation = 0.89\n\n\n## Citation\nWill be provided soon\n<!-- Sadick, A.M. (2023). SDG classification with BERT. https://huggingface.co/sadickam/sdg-classification-bert -->\n\n<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->\n\n\n## Model Card Contact\ns.sadick@deakin.edu.au",
    "card_content": "---\nlicense: mit\nlanguage:\n- en\nmetrics:\n- accuracy\n- matthews_correlation\nwidget:\n- text: Highway work zones create potential risks for both traffic and workers in\n    addition to traffic congestion and delays that result in increased road user delay.\n- text: A circular economy is a way of achieving sustainable consumption and production,\n    as well as nature positive outcomes.\n---\n\n# sadickam/sdgBERT (previously - sadickam/sdg-classification-bert)\n\n<!-- Provide a quick summary of what the model is/does. -->\n\nsgdBERT (previously named \"sdg-classification-bert\"), is an NLP model for classifying text with respect to the United Nations sustainable development goals (SDG).\n\n![image](https://user-images.githubusercontent.com/73560591/216751462-ced482ba-5d8e-48aa-9a48-5557979a35f1.png)\nSource:https://www.un.org/development/desa/disabilities/about-us/sustainable-development-goals-sdgs-and-disability.html\n\n\n## Model Details\n\n### Model Description\n\n<!-- Provide a longer summary of what this model is. -->\n\nThis text classification model was developed by fine-tuning the bert-base-uncased pre-trained model. The training data for this fine-tuned model was sourced from the publicly available OSDG Community Dataset (OSDG-CD) at https://zenodo.org/record/5550238#.ZBulfcJByF4.\nThis model was made as part of academic research at Deakin University. The goal was to make a transformer-based SDG text classification model that anyone could use. Only the first 16 UN SDGs supported. The primary model details are highlighted below:\n\n- **Model type:** Text classification\n- **Language(s) (NLP):** English\n- **License:** mit\n- **Finetuned from model [optional]:** bert-base-uncased\n\n### Model Sources\n<!-- Provide the basic links for the model. -->\n- **Repository:** https://huggingface.co/sadickam/sdg-classification-bert  \n- **Demo:** option 1 (copy/past text and csv): https://sadickam-sdg-text-classifier.hf.space/; option 2 (PDF documents): https://sadickam-document-sdg-app-cpu.hf.space\n\n\n### Direct Use\n\n<!-- This section is for the model use without fine-tuning or plugging into a larger ecosystem/app. -->\n\nThis is a fine-tuned model and therefore requires no further training.\n\n\n## How to Get Started with the Model\n\nUse the code below to get started with the model.\n```python\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\ntokenizer = AutoTokenizer.from_pretrained(\"sadickam/sdg-classification-bert\")\nmodel = AutoModelForSequenceClassification.from_pretrained(\"sadickam/sdg-classification-bert\")\n```\n\n\n## Training Data\n\n<!-- This should link to a Data Card, perhaps with a short stub of information on what the training data is all about as well as documentation related to data pre-processing or additional filtering. -->\nThe training data includes text from a wide range of industries and academic research fields. Hence, this fine-tuned model is not for a specific industry. \n\nSee training here: https://zenodo.org/record/5550238#.ZBulfcJByF4\n\n\n## Training Hyperparameters\n\n- Num_epoch = 3\n- Learning rate = 5e-5\n- Batch size = 16\n\n\n## Evaluation\n\n#### Metrics\n\n<!-- These are the evaluation metrics being used, ideally with a description of why. -->\n- Accuracy = 0.90\n- Matthews correlation = 0.89\n\n\n## Citation\nWill be provided soon\n<!-- Sadick, A.M. (2023). SDG classification with BERT. https://huggingface.co/sadickam/sdg-classification-bert -->\n\n<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->\n\n\n## Model Card Contact\ns.sadick@deakin.edu.au",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModelForSequenceClassification",
      "custom_class": null,
      "pipeline_tag": "text-classification",
      "processor": "AutoTokenizer"
    },
    "safetensors": {
      "parameters": {
        "F32": 109494544
      },
      "total": 109494544
    },
    "model_index": null,
    "trending_score": null
  },
  {
    "model_id": "TieIncred/distilbert-base-uncased-finetuned-emotional",
    "model_name": "TieIncred/distilbert-base-uncased-finetuned-emotional",
    "author": "TieIncred",
    "downloads": 778378,
    "downloads_all_time": null,
    "likes": 1,
    "tags": [
      "transformers",
      "pytorch",
      "tensorboard",
      "distilbert",
      "text-classification",
      "generated_from_trainer",
      "dataset:emotion",
      "base_model:distilbert/distilbert-base-uncased",
      "base_model:finetune:distilbert/distilbert-base-uncased",
      "license:apache-2.0",
      "model-index",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/TieIncred/distilbert-base-uncased-finetuned-emotional",
    "dependencies": [
      [
        "transformers",
        "4.16.2"
      ],
      [
        "pytorch",
        "2.1.0+cu118"
      ],
      [
        "datasets",
        "2.9.0"
      ],
      [
        "tokenizers",
        "0.14.1"
      ]
    ],
    "last_modified": "2024-01-11T10:30:18+00:00",
    "created_at": "2023-10-24T18:34:32+00:00",
    "analysis_date": "2025-03-22T00:55:08.958583",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "distilbert",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "license": "apache-2.0",
      "tags": [
        "generated_from_trainer"
      ],
      "datasets": [
        "emotion"
      ],
      "metrics": [
        "accuracy",
        "f1"
      ],
      "base_model": "distilbert-base-uncased",
      "model-index": [
        {
          "name": "distilbert-base-uncased-finetuned-emotional",
          "results": [
            {
              "task": {
                "type": "text-classification",
                "name": "Text Classification"
              },
              "dataset": {
                "name": "emotion",
                "type": "emotion",
                "args": "split"
              },
              "metrics": [
                {
                  "type": "accuracy",
                  "value": 0.9305,
                  "name": "Accuracy"
                },
                {
                  "type": "f1",
                  "value": 0.9309021978510164,
                  "name": "F1"
                }
              ]
            }
          ]
        }
      ]
    },
    "card_text": "\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# distilbert-base-uncased-finetuned-emotional\n\nThis model is a fine-tuned version of [distilbert-base-uncased](https://huggingface.co/distilbert-base-uncased) on the emotion dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.1658\n- Accuracy: 0.9305\n- F1: 0.9309\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 64\n- eval_batch_size: 64\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 2\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss | Accuracy | F1     |\n|:-------------:|:-----:|:----:|:---------------:|:--------:|:------:|\n| 0.1683        | 1.0   | 250  | 0.1738          | 0.9295   | 0.9294 |\n| 0.1085        | 2.0   | 500  | 0.1658          | 0.9305   | 0.9309 |\n\n\n### Framework versions\n\n- Transformers 4.16.2\n- Pytorch 2.1.0+cu118\n- Datasets 2.9.0\n- Tokenizers 0.14.1\n",
    "card_content": "---\nlicense: apache-2.0\ntags:\n- generated_from_trainer\ndatasets:\n- emotion\nmetrics:\n- accuracy\n- f1\nbase_model: distilbert-base-uncased\nmodel-index:\n- name: distilbert-base-uncased-finetuned-emotional\n  results:\n  - task:\n      type: text-classification\n      name: Text Classification\n    dataset:\n      name: emotion\n      type: emotion\n      args: split\n    metrics:\n    - type: accuracy\n      value: 0.9305\n      name: Accuracy\n    - type: f1\n      value: 0.9309021978510164\n      name: F1\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# distilbert-base-uncased-finetuned-emotional\n\nThis model is a fine-tuned version of [distilbert-base-uncased](https://huggingface.co/distilbert-base-uncased) on the emotion dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.1658\n- Accuracy: 0.9305\n- F1: 0.9309\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 64\n- eval_batch_size: 64\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 2\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss | Accuracy | F1     |\n|:-------------:|:-----:|:----:|:---------------:|:--------:|:------:|\n| 0.1683        | 1.0   | 250  | 0.1738          | 0.9295   | 0.9294 |\n| 0.1085        | 2.0   | 500  | 0.1658          | 0.9305   | 0.9309 |\n\n\n### Framework versions\n\n- Transformers 4.16.2\n- Pytorch 2.1.0+cu118\n- Datasets 2.9.0\n- Tokenizers 0.14.1\n",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModelForSequenceClassification",
      "custom_class": null,
      "pipeline_tag": "text-classification",
      "processor": "AutoTokenizer"
    },
    "safetensors": null,
    "model_index": [
      {
        "name": "distilbert-base-uncased-finetuned-emotional",
        "results": [
          {
            "task": {
              "type": "text-classification",
              "name": "Text Classification"
            },
            "dataset": {
              "name": "emotion",
              "type": "emotion",
              "args": "split"
            },
            "metrics": [
              {
                "type": "accuracy",
                "value": 0.9305,
                "name": "Accuracy",
                "verified": false
              },
              {
                "type": "f1",
                "value": 0.9309021978510164,
                "name": "F1",
                "verified": false
              }
            ]
          }
        ]
      }
    ],
    "trending_score": null
  },
  {
    "model_id": "lxyuan/distilbert-base-multilingual-cased-sentiments-student",
    "model_name": "lxyuan/distilbert-base-multilingual-cased-sentiments-student",
    "author": "lxyuan",
    "downloads": 761118,
    "downloads_all_time": null,
    "likes": 277,
    "tags": [
      "transformers",
      "pytorch",
      "onnx",
      "safetensors",
      "distilbert",
      "text-classification",
      "sentiment-analysis",
      "zero-shot-distillation",
      "distillation",
      "zero-shot-classification",
      "debarta-v3",
      "en",
      "ar",
      "de",
      "es",
      "fr",
      "ja",
      "zh",
      "id",
      "hi",
      "it",
      "ms",
      "pt",
      "dataset:tyqiangz/multilingual-sentiments",
      "doi:10.57967/hf/1422",
      "license:apache-2.0",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/lxyuan/distilbert-base-multilingual-cased-sentiments-student",
    "dependencies": [
      [
        "transformers",
        "4.28.1"
      ],
      [
        "pytorch",
        "2.0.0+cu118"
      ],
      [
        "datasets",
        "2.11.0"
      ],
      [
        "tokenizers",
        "0.13.3"
      ]
    ],
    "last_modified": "2025-03-03T02:06:53+00:00",
    "created_at": "2023-05-05T16:22:55+00:00",
    "analysis_date": "2025-03-22T00:55:10.557700",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "distilbert",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "license": "apache-2.0",
      "tags": [
        "sentiment-analysis",
        "text-classification",
        "zero-shot-distillation",
        "distillation",
        "zero-shot-classification",
        "debarta-v3"
      ],
      "datasets": [
        "tyqiangz/multilingual-sentiments"
      ],
      "language": [
        "en",
        "ar",
        "de",
        "es",
        "fr",
        "ja",
        "zh",
        "id",
        "hi",
        "it",
        "ms",
        "pt"
      ],
      "model-index": [
        {
          "name": "distilbert-base-multilingual-cased-sentiments-student",
          "results": []
        }
      ]
    },
    "card_text": "\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# distilbert-base-multilingual-cased-sentiments-student\n\nThis model is distilled from the zero-shot classification pipeline on the Multilingual Sentiment \ndataset using this [script](https://github.com/huggingface/transformers/tree/main/examples/research_projects/zero-shot-distillation). \n\nIn reality the multilingual-sentiment dataset is annotated of course, \nbut we'll pretend and ignore the annotations for the sake of example.\n\n\n    Teacher model: MoritzLaurer/mDeBERTa-v3-base-mnli-xnli\n    Teacher hypothesis template: \"The sentiment of this text is {}.\"\n    Student model: distilbert-base-multilingual-cased\n\n\n## Inference example\n\n```python\nfrom transformers import pipeline\n\ndistilled_student_sentiment_classifier = pipeline(\n    model=\"lxyuan/distilbert-base-multilingual-cased-sentiments-student\", \n    return_all_scores=True\n)\n\n# english\ndistilled_student_sentiment_classifier (\"I love this movie and i would watch it again and again!\")\n>> [[{'label': 'positive', 'score': 0.9731044769287109},\n  {'label': 'neutral', 'score': 0.016910076141357422},\n  {'label': 'negative', 'score': 0.009985478594899178}]]\n\n# malay\ndistilled_student_sentiment_classifier(\"Saya suka filem ini dan saya akan menontonnya lagi dan lagi!\")\n[[{'label': 'positive', 'score': 0.9760093688964844},\n  {'label': 'neutral', 'score': 0.01804516464471817},\n  {'label': 'negative', 'score': 0.005945465061813593}]]\n\n# japanese\ndistilled_student_sentiment_classifier(\"私はこの映画が大好きで、何度も見ます！\")\n>> [[{'label': 'positive', 'score': 0.9342429041862488},\n  {'label': 'neutral', 'score': 0.040193185210227966},\n  {'label': 'negative', 'score': 0.025563929229974747}]]\n\n\n```\n\n\n## Training procedure\n\nNotebook link: [here](https://github.com/LxYuan0420/nlp/blob/main/notebooks/Distilling_Zero_Shot_multilingual_distilbert_sentiments_student.ipynb)\n\n### Training hyperparameters\n\nResult can be reproduce using the following commands:\n\n```bash\npython transformers/examples/research_projects/zero-shot-distillation/distill_classifier.py \\\n--data_file ./multilingual-sentiments/train_unlabeled.txt \\\n--class_names_file ./multilingual-sentiments/class_names.txt \\\n--hypothesis_template \"The sentiment of this text is {}.\" \\\n--teacher_name_or_path MoritzLaurer/mDeBERTa-v3-base-mnli-xnli \\\n--teacher_batch_size 32 \\\n--student_name_or_path distilbert-base-multilingual-cased \\\n--output_dir ./distilbert-base-multilingual-cased-sentiments-student \\\n--per_device_train_batch_size 16 \\\n--fp16\n```\n\nIf you are training this model on Colab, make the following code changes to avoid Out-of-memory error message:\n```bash\n###### modify L78 to disable fast tokenizer \ndefault=False,\n\n###### update dataset map part at L313\ndataset = dataset.map(tokenizer, input_columns=\"text\", fn_kwargs={\"padding\": \"max_length\", \"truncation\": True, \"max_length\": 512})\n\n###### add following lines to L213\ndel model\nprint(f\"Manually deleted Teacher model, free some memory for student model.\")\n\n###### add following lines to L337\ntrainer.push_to_hub()\ntokenizer.push_to_hub(\"distilbert-base-multilingual-cased-sentiments-student\")\n  \n```\n\n### Training log\n```bash\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n{'train_runtime': 2009.8864, 'train_samples_per_second': 73.0, 'train_steps_per_second': 4.563, 'train_loss': 0.6473459283913797, 'epoch': 1.0}\n100%|███████████████████████████████████████| 9171/9171 [33:29<00:00,  4.56it/s]\n[INFO|trainer.py:762] 2023-05-06 10:56:18,555 >> The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n[INFO|trainer.py:3129] 2023-05-06 10:56:18,557 >> ***** Running Evaluation *****\n[INFO|trainer.py:3131] 2023-05-06 10:56:18,557 >>   Num examples = 146721\n[INFO|trainer.py:3134] 2023-05-06 10:56:18,557 >>   Batch size = 128\n100%|███████████████████████████████████████| 1147/1147 [08:59<00:00,  2.13it/s]\n05/06/2023 11:05:18 - INFO - __main__ - Agreement of student and teacher predictions: 88.29%\n[INFO|trainer.py:2868] 2023-05-06 11:05:18,251 >> Saving model checkpoint to ./distilbert-base-multilingual-cased-sentiments-student\n[INFO|configuration_utils.py:457] 2023-05-06 11:05:18,251 >> Configuration saved in ./distilbert-base-multilingual-cased-sentiments-student/config.json\n[INFO|modeling_utils.py:1847] 2023-05-06 11:05:18,905 >> Model weights saved in ./distilbert-base-multilingual-cased-sentiments-student/pytorch_model.bin\n[INFO|tokenization_utils_base.py:2171] 2023-05-06 11:05:18,905 >> tokenizer config file saved in ./distilbert-base-multilingual-cased-sentiments-student/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2178] 2023-05-06 11:05:18,905 >> Special tokens file saved in ./distilbert-base-multilingual-cased-sentiments-student/special_tokens_map.json\n\n```\n\n### Framework versions\n\n- Transformers 4.28.1\n- Pytorch 2.0.0+cu118\n- Datasets 2.11.0\n- Tokenizers 0.13.3",
    "card_content": "---\nlicense: apache-2.0\ntags:\n- sentiment-analysis\n- text-classification\n- zero-shot-distillation\n- distillation\n- zero-shot-classification\n- debarta-v3\ndatasets:\n- tyqiangz/multilingual-sentiments\nlanguage:\n- en\n- ar\n- de\n- es\n- fr\n- ja\n- zh\n- id\n- hi\n- it\n- ms\n- pt\nmodel-index:\n- name: distilbert-base-multilingual-cased-sentiments-student\n  results: []\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# distilbert-base-multilingual-cased-sentiments-student\n\nThis model is distilled from the zero-shot classification pipeline on the Multilingual Sentiment \ndataset using this [script](https://github.com/huggingface/transformers/tree/main/examples/research_projects/zero-shot-distillation). \n\nIn reality the multilingual-sentiment dataset is annotated of course, \nbut we'll pretend and ignore the annotations for the sake of example.\n\n\n    Teacher model: MoritzLaurer/mDeBERTa-v3-base-mnli-xnli\n    Teacher hypothesis template: \"The sentiment of this text is {}.\"\n    Student model: distilbert-base-multilingual-cased\n\n\n## Inference example\n\n```python\nfrom transformers import pipeline\n\ndistilled_student_sentiment_classifier = pipeline(\n    model=\"lxyuan/distilbert-base-multilingual-cased-sentiments-student\", \n    return_all_scores=True\n)\n\n# english\ndistilled_student_sentiment_classifier (\"I love this movie and i would watch it again and again!\")\n>> [[{'label': 'positive', 'score': 0.9731044769287109},\n  {'label': 'neutral', 'score': 0.016910076141357422},\n  {'label': 'negative', 'score': 0.009985478594899178}]]\n\n# malay\ndistilled_student_sentiment_classifier(\"Saya suka filem ini dan saya akan menontonnya lagi dan lagi!\")\n[[{'label': 'positive', 'score': 0.9760093688964844},\n  {'label': 'neutral', 'score': 0.01804516464471817},\n  {'label': 'negative', 'score': 0.005945465061813593}]]\n\n# japanese\ndistilled_student_sentiment_classifier(\"私はこの映画が大好きで、何度も見ます！\")\n>> [[{'label': 'positive', 'score': 0.9342429041862488},\n  {'label': 'neutral', 'score': 0.040193185210227966},\n  {'label': 'negative', 'score': 0.025563929229974747}]]\n\n\n```\n\n\n## Training procedure\n\nNotebook link: [here](https://github.com/LxYuan0420/nlp/blob/main/notebooks/Distilling_Zero_Shot_multilingual_distilbert_sentiments_student.ipynb)\n\n### Training hyperparameters\n\nResult can be reproduce using the following commands:\n\n```bash\npython transformers/examples/research_projects/zero-shot-distillation/distill_classifier.py \\\n--data_file ./multilingual-sentiments/train_unlabeled.txt \\\n--class_names_file ./multilingual-sentiments/class_names.txt \\\n--hypothesis_template \"The sentiment of this text is {}.\" \\\n--teacher_name_or_path MoritzLaurer/mDeBERTa-v3-base-mnli-xnli \\\n--teacher_batch_size 32 \\\n--student_name_or_path distilbert-base-multilingual-cased \\\n--output_dir ./distilbert-base-multilingual-cased-sentiments-student \\\n--per_device_train_batch_size 16 \\\n--fp16\n```\n\nIf you are training this model on Colab, make the following code changes to avoid Out-of-memory error message:\n```bash\n###### modify L78 to disable fast tokenizer \ndefault=False,\n\n###### update dataset map part at L313\ndataset = dataset.map(tokenizer, input_columns=\"text\", fn_kwargs={\"padding\": \"max_length\", \"truncation\": True, \"max_length\": 512})\n\n###### add following lines to L213\ndel model\nprint(f\"Manually deleted Teacher model, free some memory for student model.\")\n\n###### add following lines to L337\ntrainer.push_to_hub()\ntokenizer.push_to_hub(\"distilbert-base-multilingual-cased-sentiments-student\")\n  \n```\n\n### Training log\n```bash\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n{'train_runtime': 2009.8864, 'train_samples_per_second': 73.0, 'train_steps_per_second': 4.563, 'train_loss': 0.6473459283913797, 'epoch': 1.0}\n100%|███████████████████████████████████████| 9171/9171 [33:29<00:00,  4.56it/s]\n[INFO|trainer.py:762] 2023-05-06 10:56:18,555 >> The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n[INFO|trainer.py:3129] 2023-05-06 10:56:18,557 >> ***** Running Evaluation *****\n[INFO|trainer.py:3131] 2023-05-06 10:56:18,557 >>   Num examples = 146721\n[INFO|trainer.py:3134] 2023-05-06 10:56:18,557 >>   Batch size = 128\n100%|███████████████████████████████████████| 1147/1147 [08:59<00:00,  2.13it/s]\n05/06/2023 11:05:18 - INFO - __main__ - Agreement of student and teacher predictions: 88.29%\n[INFO|trainer.py:2868] 2023-05-06 11:05:18,251 >> Saving model checkpoint to ./distilbert-base-multilingual-cased-sentiments-student\n[INFO|configuration_utils.py:457] 2023-05-06 11:05:18,251 >> Configuration saved in ./distilbert-base-multilingual-cased-sentiments-student/config.json\n[INFO|modeling_utils.py:1847] 2023-05-06 11:05:18,905 >> Model weights saved in ./distilbert-base-multilingual-cased-sentiments-student/pytorch_model.bin\n[INFO|tokenization_utils_base.py:2171] 2023-05-06 11:05:18,905 >> tokenizer config file saved in ./distilbert-base-multilingual-cased-sentiments-student/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2178] 2023-05-06 11:05:18,905 >> Special tokens file saved in ./distilbert-base-multilingual-cased-sentiments-student/special_tokens_map.json\n\n```\n\n### Framework versions\n\n- Transformers 4.28.1\n- Pytorch 2.0.0+cu118\n- Datasets 2.11.0\n- Tokenizers 0.13.3",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModelForSequenceClassification",
      "custom_class": null,
      "pipeline_tag": "text-classification",
      "processor": "AutoTokenizer"
    },
    "safetensors": {
      "parameters": {
        "F32": 135326979
      },
      "total": 135326979
    },
    "model_index": [
      {
        "name": "distilbert-base-multilingual-cased-sentiments-student",
        "results": []
      }
    ],
    "trending_score": null
  },
  {
    "model_id": "microsoft/deberta-large-mnli",
    "model_name": "microsoft/deberta-large-mnli",
    "author": "microsoft",
    "downloads": 699476,
    "downloads_all_time": null,
    "likes": 18,
    "tags": [
      "transformers",
      "pytorch",
      "deberta",
      "text-classification",
      "deberta-v1",
      "deberta-mnli",
      "en",
      "arxiv:2006.03654",
      "license:mit",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/microsoft/deberta-large-mnli",
    "dependencies": [
      [
        "torch",
        null
      ],
      [
        "transformers",
        null
      ]
    ],
    "last_modified": "2021-05-21T20:07:51+00:00",
    "created_at": "2022-03-02T23:29:05+00:00",
    "analysis_date": "2025-03-22T00:55:12.212317",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "deberta",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": "en",
      "tags": [
        "deberta-v1",
        "deberta-mnli"
      ],
      "tasks": "mnli",
      "thumbnail": "https://huggingface.co/front/thumbnails/microsoft.png",
      "license": "mit",
      "widget": [
        {
          "text": "[CLS] I love you. [SEP] I like you. [SEP]"
        }
      ]
    },
    "card_text": "\n## DeBERTa: Decoding-enhanced BERT with Disentangled Attention\n\n[DeBERTa](https://arxiv.org/abs/2006.03654) improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder. It outperforms BERT and RoBERTa on  majority of NLU tasks with 80GB training data. \n\nPlease check the [official repository](https://github.com/microsoft/DeBERTa) for more details and updates.\n\nThis is the DeBERTa large model fine-tuned with MNLI task.\n\n#### Fine-tuning on NLU tasks\n\nWe present the dev results on SQuAD 1.1/2.0 and several GLUE benchmark tasks.\n\n| Model                     | SQuAD 1.1 | SQuAD 2.0 | MNLI-m/mm   | SST-2 | QNLI | CoLA | RTE    | MRPC  | QQP   |STS-B |\n|---------------------------|-----------|-----------|-------------|-------|------|------|--------|-------|-------|------|\n|                           | F1/EM     | F1/EM     | Acc         | Acc   | Acc  | MCC  | Acc    |Acc/F1 |Acc/F1 |P/S   |\n| BERT-Large                | 90.9/84.1 | 81.8/79.0 | 86.6/-      | 93.2  | 92.3 | 60.6 | 70.4   | 88.0/-       | 91.3/- |90.0/- |\n| RoBERTa-Large             | 94.6/88.9 | 89.4/86.5 | 90.2/-      | 96.4  | 93.9 | 68.0 | 86.6   | 90.9/-       | 92.2/- |92.4/- |\n| XLNet-Large               | 95.1/89.7 | 90.6/87.9 | 90.8/-      | 97.0  | 94.9 | 69.0 | 85.9   | 90.8/-       | 92.3/- |92.5/- |\n| [DeBERTa-Large](https://huggingface.co/microsoft/deberta-large)<sup>1</sup> | 95.5/90.1 | 90.7/88.0 | 91.3/91.1| 96.5|95.3| 69.5| 91.0| 92.6/94.6| 92.3/- |92.8/92.5 |\n| [DeBERTa-XLarge](https://huggingface.co/microsoft/deberta-xlarge)<sup>1</sup> | -/-  | -/-  | 91.5/91.2| 97.0 | - | -    | 93.1   | 92.1/94.3    | -    |92.9/92.7|\n| [DeBERTa-V2-XLarge](https://huggingface.co/microsoft/deberta-v2-xlarge)<sup>1</sup>|95.8/90.8| 91.4/88.9|91.7/91.6| **97.5**| 95.8|71.1|**93.9**|92.0/94.2|92.3/89.8|92.9/92.9|\n|**[DeBERTa-V2-XXLarge](https://huggingface.co/microsoft/deberta-v2-xxlarge)<sup>1,2</sup>**|**96.1/91.4**|**92.2/89.7**|**91.7/91.9**|97.2|**96.0**|**72.0**| 93.5| **93.1/94.9**|**92.7/90.3** |**93.2/93.1** |\n--------\n#### Notes.\n - <sup>1</sup> Following RoBERTa, for RTE, MRPC, STS-B, we fine-tune the tasks based on [DeBERTa-Large-MNLI](https://huggingface.co/microsoft/deberta-large-mnli), [DeBERTa-XLarge-MNLI](https://huggingface.co/microsoft/deberta-xlarge-mnli), [DeBERTa-V2-XLarge-MNLI](https://huggingface.co/microsoft/deberta-v2-xlarge-mnli), [DeBERTa-V2-XXLarge-MNLI](https://huggingface.co/microsoft/deberta-v2-xxlarge-mnli). The results of SST-2/QQP/QNLI/SQuADv2 will also be slightly improved when start from MNLI fine-tuned models, however, we only report the numbers fine-tuned from pretrained base models for those 4 tasks.\n - <sup>2</sup> To try the **XXLarge** model with **[HF transformers](https://huggingface.co/transformers/main_classes/trainer.html)**, you need to specify **--sharded_ddp**\n \n```bash  \ncd transformers/examples/text-classification/\nexport TASK_NAME=mrpc\npython -m torch.distributed.launch --nproc_per_node=8 run_glue.py   --model_name_or_path microsoft/deberta-v2-xxlarge   \\\\\n--task_name $TASK_NAME   --do_train   --do_eval   --max_seq_length 128   --per_device_train_batch_size 4   \\\\\n--learning_rate 3e-6   --num_train_epochs 3   --output_dir /tmp/$TASK_NAME/ --overwrite_output_dir --sharded_ddp --fp16\n```\n\n### Citation\n\nIf you find DeBERTa useful for your work, please cite the following paper:\n\n``` latex\n@inproceedings{\nhe2021deberta,\ntitle={DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION},\nauthor={Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=XPZIaotutsD}\n}\n```\n\n",
    "card_content": "---\nlanguage: en\ntags:\n- deberta-v1\n- deberta-mnli\ntasks: mnli\nthumbnail: https://huggingface.co/front/thumbnails/microsoft.png\nlicense: mit\nwidget:\n- text: '[CLS] I love you. [SEP] I like you. [SEP]'\n---\n\n## DeBERTa: Decoding-enhanced BERT with Disentangled Attention\n\n[DeBERTa](https://arxiv.org/abs/2006.03654) improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder. It outperforms BERT and RoBERTa on  majority of NLU tasks with 80GB training data. \n\nPlease check the [official repository](https://github.com/microsoft/DeBERTa) for more details and updates.\n\nThis is the DeBERTa large model fine-tuned with MNLI task.\n\n#### Fine-tuning on NLU tasks\n\nWe present the dev results on SQuAD 1.1/2.0 and several GLUE benchmark tasks.\n\n| Model                     | SQuAD 1.1 | SQuAD 2.0 | MNLI-m/mm   | SST-2 | QNLI | CoLA | RTE    | MRPC  | QQP   |STS-B |\n|---------------------------|-----------|-----------|-------------|-------|------|------|--------|-------|-------|------|\n|                           | F1/EM     | F1/EM     | Acc         | Acc   | Acc  | MCC  | Acc    |Acc/F1 |Acc/F1 |P/S   |\n| BERT-Large                | 90.9/84.1 | 81.8/79.0 | 86.6/-      | 93.2  | 92.3 | 60.6 | 70.4   | 88.0/-       | 91.3/- |90.0/- |\n| RoBERTa-Large             | 94.6/88.9 | 89.4/86.5 | 90.2/-      | 96.4  | 93.9 | 68.0 | 86.6   | 90.9/-       | 92.2/- |92.4/- |\n| XLNet-Large               | 95.1/89.7 | 90.6/87.9 | 90.8/-      | 97.0  | 94.9 | 69.0 | 85.9   | 90.8/-       | 92.3/- |92.5/- |\n| [DeBERTa-Large](https://huggingface.co/microsoft/deberta-large)<sup>1</sup> | 95.5/90.1 | 90.7/88.0 | 91.3/91.1| 96.5|95.3| 69.5| 91.0| 92.6/94.6| 92.3/- |92.8/92.5 |\n| [DeBERTa-XLarge](https://huggingface.co/microsoft/deberta-xlarge)<sup>1</sup> | -/-  | -/-  | 91.5/91.2| 97.0 | - | -    | 93.1   | 92.1/94.3    | -    |92.9/92.7|\n| [DeBERTa-V2-XLarge](https://huggingface.co/microsoft/deberta-v2-xlarge)<sup>1</sup>|95.8/90.8| 91.4/88.9|91.7/91.6| **97.5**| 95.8|71.1|**93.9**|92.0/94.2|92.3/89.8|92.9/92.9|\n|**[DeBERTa-V2-XXLarge](https://huggingface.co/microsoft/deberta-v2-xxlarge)<sup>1,2</sup>**|**96.1/91.4**|**92.2/89.7**|**91.7/91.9**|97.2|**96.0**|**72.0**| 93.5| **93.1/94.9**|**92.7/90.3** |**93.2/93.1** |\n--------\n#### Notes.\n - <sup>1</sup> Following RoBERTa, for RTE, MRPC, STS-B, we fine-tune the tasks based on [DeBERTa-Large-MNLI](https://huggingface.co/microsoft/deberta-large-mnli), [DeBERTa-XLarge-MNLI](https://huggingface.co/microsoft/deberta-xlarge-mnli), [DeBERTa-V2-XLarge-MNLI](https://huggingface.co/microsoft/deberta-v2-xlarge-mnli), [DeBERTa-V2-XXLarge-MNLI](https://huggingface.co/microsoft/deberta-v2-xxlarge-mnli). The results of SST-2/QQP/QNLI/SQuADv2 will also be slightly improved when start from MNLI fine-tuned models, however, we only report the numbers fine-tuned from pretrained base models for those 4 tasks.\n - <sup>2</sup> To try the **XXLarge** model with **[HF transformers](https://huggingface.co/transformers/main_classes/trainer.html)**, you need to specify **--sharded_ddp**\n \n```bash  \ncd transformers/examples/text-classification/\nexport TASK_NAME=mrpc\npython -m torch.distributed.launch --nproc_per_node=8 run_glue.py   --model_name_or_path microsoft/deberta-v2-xxlarge   \\\\\n--task_name $TASK_NAME   --do_train   --do_eval   --max_seq_length 128   --per_device_train_batch_size 4   \\\\\n--learning_rate 3e-6   --num_train_epochs 3   --output_dir /tmp/$TASK_NAME/ --overwrite_output_dir --sharded_ddp --fp16\n```\n\n### Citation\n\nIf you find DeBERTa useful for your work, please cite the following paper:\n\n``` latex\n@inproceedings{\nhe2021deberta,\ntitle={DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION},\nauthor={Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=XPZIaotutsD}\n}\n```\n\n",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModelForSequenceClassification",
      "custom_class": null,
      "pipeline_tag": "text-classification",
      "processor": "AutoTokenizer"
    },
    "safetensors": null,
    "model_index": null,
    "trending_score": null
  },
  {
    "model_id": "unitary/toxic-bert",
    "model_name": "unitary/toxic-bert",
    "author": "unitary",
    "downloads": 688473,
    "downloads_all_time": null,
    "likes": 164,
    "tags": [
      "transformers",
      "pytorch",
      "jax",
      "safetensors",
      "bert",
      "text-classification",
      "arxiv:1703.04009",
      "arxiv:1905.12516",
      "license:apache-2.0",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/unitary/toxic-bert",
    "dependencies": [
      [
        "transformers",
        null
      ],
      [
        "pytorch_lightning",
        null
      ],
      [
        "torch",
        null
      ],
      [
        "pandas",
        null
      ],
      [
        "detoxify",
        null
      ]
    ],
    "last_modified": "2024-03-13T17:41:49+00:00",
    "created_at": "2022-03-02T23:29:05+00:00",
    "analysis_date": "2025-03-22T00:55:13.957347",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "bert",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "license": "apache-2.0"
    },
    "card_text": "\n      \n<div align=\"center\">    \n\n**⚠️ Disclaimer:**\nThe huggingface models currently give different results to the detoxify library (see issue [here](https://github.com/unitaryai/detoxify/issues/15)). For the most up to date models we recommend using the models from https://github.com/unitaryai/detoxify\n\n# 🙊 Detoxify\n##  Toxic Comment Classification with ⚡ Pytorch Lightning and 🤗 Transformers   \n\n![CI testing](https://github.com/unitaryai/detoxify/workflows/CI%20testing/badge.svg)\n![Lint](https://github.com/unitaryai/detoxify/workflows/Lint/badge.svg)\n\n</div>\n \n![Examples image](examples.png)\n\n## Description   \n\nTrained models & code to predict toxic comments on 3 Jigsaw challenges: Toxic comment classification, Unintended Bias in Toxic comments, Multilingual toxic comment classification.\n\nBuilt by [Laura Hanu](https://laurahanu.github.io/) at [Unitary](https://www.unitary.ai/), where we are working to stop harmful content online by interpreting visual content in context. \n\nDependencies:\n- For inference:\n  - 🤗 Transformers\n  - ⚡ Pytorch lightning \n- For training will also need:\n  - Kaggle API (to download data)\n\n\n| Challenge | Year | Goal | Original Data Source | Detoxify Model Name | Top Kaggle Leaderboard Score | Detoxify Score\n|-|-|-|-|-|-|-|\n| [Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge) | 2018 |  build a multi-headed model that’s capable of detecting different types of of toxicity like threats, obscenity, insults, and identity-based hate. | Wikipedia Comments | `original` | 0.98856 | 0.98636\n| [Jigsaw Unintended Bias in Toxicity Classification](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification) | 2019 | build a model that recognizes toxicity and minimizes this type of unintended bias with respect to mentions of identities. You'll be using a dataset labeled for identity mentions and optimizing a metric designed to measure unintended bias. | Civil Comments | `unbiased` | 0.94734 | 0.93639\n| [Jigsaw Multilingual Toxic Comment Classification](https://www.kaggle.com/c/jigsaw-multilingual-toxic-comment-classification) | 2020 | build effective multilingual models | Wikipedia Comments + Civil Comments | `multilingual` | 0.9536 | 0.91655*\n\n*Score not directly comparable since it is obtained on the validation set provided and not on the test set. To update when the test labels are made available. \n\nIt is also noteworthy to mention that the top leadearboard scores have been achieved using model ensembles. The purpose of this library was to build something user-friendly and straightforward to use.\n\n## Limitations and ethical considerations\n\nIf words that are associated with swearing, insults or profanity are present in a comment, it is likely that it will be classified as toxic, regardless of the tone or the intent of the author e.g. humorous/self-deprecating. This could present some biases towards already vulnerable minority groups.\n\nThe intended use of this library is for research purposes, fine-tuning on carefully constructed datasets that reflect real world demographics  and/or to aid content moderators in flagging out harmful content quicker.\n\nSome useful resources about the risk of different biases in toxicity or hate speech detection are:\n- [The Risk of Racial Bias in Hate Speech Detection](https://homes.cs.washington.edu/~msap/pdfs/sap2019risk.pdf)\n- [Automated Hate Speech Detection and the Problem of Offensive Language](https://arxiv.org/pdf/1703.04009.pdf%201.pdf)\n- [Racial Bias in Hate Speech and Abusive Language Detection Datasets](https://arxiv.org/pdf/1905.12516.pdf)\n\n## Quick prediction\n\n\nThe `multilingual` model has been trained on 7 different languages so it should only be tested on: `english`, `french`, `spanish`, `italian`, `portuguese`, `turkish` or `russian`.\n\n```bash\n# install detoxify  \n\npip install detoxify\n\n```\n```python\n\nfrom detoxify import Detoxify\n\n# each model takes in either a string or a list of strings\n\nresults = Detoxify('original').predict('example text')\n\nresults = Detoxify('unbiased').predict(['example text 1','example text 2'])\n\nresults = Detoxify('multilingual').predict(['example text','exemple de texte','texto de ejemplo','testo di esempio','texto de exemplo','örnek metin','пример текста'])\n\n# optional to display results nicely (will need to pip install pandas)\n\nimport pandas as pd\n\nprint(pd.DataFrame(results, index=input_text).round(5))\n\n```\nFor more details check the Prediction section.\n\n\n## Labels\nAll challenges have a toxicity label. The toxicity labels represent the aggregate ratings of up to 10 annotators according the following schema:\n- **Very Toxic** (a very hateful, aggressive, or disrespectful comment that is very likely to make you leave a discussion or give up on sharing your perspective)\n- **Toxic** (a rude, disrespectful, or unreasonable comment that is somewhat likely to make you leave a discussion or give up on sharing your perspective)\n- **Hard to Say**\n- **Not Toxic**\n\nMore information about the labelling schema can be found [here](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/data).\n\n### Toxic Comment Classification Challenge\nThis challenge includes the following labels:\n\n- `toxic`\n- `severe_toxic`\n- `obscene`\n- `threat`\n- `insult`\n- `identity_hate`\n\n### Jigsaw Unintended Bias in Toxicity Classification\nThis challenge has 2 types of labels: the main toxicity labels and some additional identity labels that represent the identities mentioned in the comments. \n\nOnly identities with more than 500 examples in the test set (combined public and private) are included during training as additional labels and in the evaluation calculation.\n\n- `toxicity`\n- `severe_toxicity`\n- `obscene`\n- `threat`\n- `insult`\n- `identity_attack`\n- `sexual_explicit`\n\nIdentity labels used:\n- `male`\n- `female`\n- `homosexual_gay_or_lesbian`\n- `christian`\n- `jewish`\n- `muslim`\n- `black`\n- `white`\n- `psychiatric_or_mental_illness`\n\nA complete list of all the identity labels available can be found [here](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/data).\n\n\n### Jigsaw Multilingual Toxic Comment Classification\n\nSince this challenge combines the data from the previous 2 challenges, it includes all labels from above, however the final evaluation is only on:\n\n- `toxicity`\n\n## How to run   \n\nFirst, install dependencies   \n```bash\n# clone project   \n\ngit clone https://github.com/unitaryai/detoxify\n\n# create virtual env\n\npython3 -m venv toxic-env\nsource toxic-env/bin/activate\n\n# install project   \n\npip install -e detoxify\ncd detoxify\n\n# for training\npip install -r requirements.txt\n\n ```   \n\n## Prediction\n\nTrained models summary:\n\n|Model name| Transformer type| Data from\n|:--:|:--:|:--:|\n|`original`| `bert-base-uncased` | Toxic Comment Classification Challenge\n|`unbiased`| `roberta-base`| Unintended Bias in Toxicity Classification\n|`multilingual`| `xlm-roberta-base`| Multilingual Toxic Comment Classification\n\nFor a quick prediction can run the example script on a comment directly or from a txt containing a list of comments. \n```bash\n\n# load model via torch.hub\n\npython run_prediction.py --input 'example' --model_name original\n\n# load model from from checkpoint path\n\npython run_prediction.py --input 'example' --from_ckpt_path model_path\n\n# save results to a .csv file\n\npython run_prediction.py --input test_set.txt --model_name original --save_to results.csv\n\n# to see usage\n\npython run_prediction.py --help\n\n```\n\nCheckpoints can be downloaded from the latest release or via the Pytorch hub API with the following names:\n- `toxic_bert`\n- `unbiased_toxic_roberta`\n- `multilingual_toxic_xlm_r`\n```bash\nmodel = torch.hub.load('unitaryai/detoxify','toxic_bert')\n```\n\nImporting detoxify in python:\n\n```python\n\nfrom detoxify import Detoxify\n\nresults = Detoxify('original').predict('some text')\n\nresults = Detoxify('unbiased').predict(['example text 1','example text 2'])\n\nresults = Detoxify('multilingual').predict(['example text','exemple de texte','texto de ejemplo','testo di esempio','texto de exemplo','örnek metin','пример текста'])\n\n# to display results nicely\n\nimport pandas as pd\n\nprint(pd.DataFrame(results,index=input_text).round(5))\n\n```\n\n\n## Training\n\n If you do not already have a Kaggle account: \n - you need to create one to be able to download the data\n \n - go to My Account and click on Create New API Token - this will download a kaggle.json file\n\n - make sure this file is located in ~/.kaggle\n\n ```bash\n\n# create data directory\n\nmkdir jigsaw_data\ncd jigsaw_data\n\n# download data\n\nkaggle competitions download -c jigsaw-toxic-comment-classification-challenge\n\nkaggle competitions download -c jigsaw-unintended-bias-in-toxicity-classification\n\nkaggle competitions download -c jigsaw-multilingual-toxic-comment-classification\n\n```\n## Start Training\n ### Toxic Comment Classification Challenge\n\n ```bash\n\npython create_val_set.py\n\npython train.py --config configs/Toxic_comment_classification_BERT.json\n``` \n ### Unintended Bias in Toxicicity Challenge\n\n```bash\n\npython train.py --config configs/Unintended_bias_toxic_comment_classification_RoBERTa.json\n\n```\n ### Multilingual Toxic Comment Classification\n\n This is trained in 2 stages. First, train on all available data, and second, train only on the translated versions of the first challenge. \n \n The [translated data](https://www.kaggle.com/miklgr500/jigsaw-train-multilingual-coments-google-api) can be downloaded from Kaggle in french, spanish, italian, portuguese, turkish, and russian (the languages available in the test set).\n\n ```bash\n\n# stage 1\n\npython train.py --config configs/Multilingual_toxic_comment_classification_XLMR.json\n\n# stage 2\n\npython train.py --config configs/Multilingual_toxic_comment_classification_XLMR_stage2.json\n\n```\n### Monitor progress with tensorboard\n\n ```bash\n\ntensorboard --logdir=./saved\n\n```\n## Model Evaluation\n\n### Toxic Comment Classification Challenge\n\nThis challenge is evaluated on the mean AUC score of all the labels.\n\n```bash\n\npython evaluate.py --checkpoint saved/lightning_logs/checkpoints/example_checkpoint.pth --test_csv test.csv\n\n```\n### Unintended Bias in Toxicicity Challenge\n\nThis challenge is evaluated on a novel bias metric that combines different AUC scores to balance overall performance. More information on this metric [here](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/overview/evaluation).\n\n```bash\n\npython evaluate.py --checkpoint saved/lightning_logs/checkpoints/example_checkpoint.pth --test_csv test.csv\n\n# to get the final bias metric\npython model_eval/compute_bias_metric.py\n\n```\n### Multilingual Toxic Comment Classification\n\nThis challenge is evaluated on the AUC score of the main toxic label.\n\n```bash\n\npython evaluate.py --checkpoint saved/lightning_logs/checkpoints/example_checkpoint.pth --test_csv test.csv\n\n```\n\n### Citation   \n```\n@misc{Detoxify,\n  title={Detoxify},\n  author={Hanu, Laura and {Unitary team}},\n  howpublished={Github. https://github.com/unitaryai/detoxify},\n  year={2020}\n}\n```",
    "card_content": "---\nlicense: apache-2.0\n---\n\n      \n<div align=\"center\">    \n\n**⚠️ Disclaimer:**\nThe huggingface models currently give different results to the detoxify library (see issue [here](https://github.com/unitaryai/detoxify/issues/15)). For the most up to date models we recommend using the models from https://github.com/unitaryai/detoxify\n\n# 🙊 Detoxify\n##  Toxic Comment Classification with ⚡ Pytorch Lightning and 🤗 Transformers   \n\n![CI testing](https://github.com/unitaryai/detoxify/workflows/CI%20testing/badge.svg)\n![Lint](https://github.com/unitaryai/detoxify/workflows/Lint/badge.svg)\n\n</div>\n \n![Examples image](examples.png)\n\n## Description   \n\nTrained models & code to predict toxic comments on 3 Jigsaw challenges: Toxic comment classification, Unintended Bias in Toxic comments, Multilingual toxic comment classification.\n\nBuilt by [Laura Hanu](https://laurahanu.github.io/) at [Unitary](https://www.unitary.ai/), where we are working to stop harmful content online by interpreting visual content in context. \n\nDependencies:\n- For inference:\n  - 🤗 Transformers\n  - ⚡ Pytorch lightning \n- For training will also need:\n  - Kaggle API (to download data)\n\n\n| Challenge | Year | Goal | Original Data Source | Detoxify Model Name | Top Kaggle Leaderboard Score | Detoxify Score\n|-|-|-|-|-|-|-|\n| [Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge) | 2018 |  build a multi-headed model that’s capable of detecting different types of of toxicity like threats, obscenity, insults, and identity-based hate. | Wikipedia Comments | `original` | 0.98856 | 0.98636\n| [Jigsaw Unintended Bias in Toxicity Classification](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification) | 2019 | build a model that recognizes toxicity and minimizes this type of unintended bias with respect to mentions of identities. You'll be using a dataset labeled for identity mentions and optimizing a metric designed to measure unintended bias. | Civil Comments | `unbiased` | 0.94734 | 0.93639\n| [Jigsaw Multilingual Toxic Comment Classification](https://www.kaggle.com/c/jigsaw-multilingual-toxic-comment-classification) | 2020 | build effective multilingual models | Wikipedia Comments + Civil Comments | `multilingual` | 0.9536 | 0.91655*\n\n*Score not directly comparable since it is obtained on the validation set provided and not on the test set. To update when the test labels are made available. \n\nIt is also noteworthy to mention that the top leadearboard scores have been achieved using model ensembles. The purpose of this library was to build something user-friendly and straightforward to use.\n\n## Limitations and ethical considerations\n\nIf words that are associated with swearing, insults or profanity are present in a comment, it is likely that it will be classified as toxic, regardless of the tone or the intent of the author e.g. humorous/self-deprecating. This could present some biases towards already vulnerable minority groups.\n\nThe intended use of this library is for research purposes, fine-tuning on carefully constructed datasets that reflect real world demographics  and/or to aid content moderators in flagging out harmful content quicker.\n\nSome useful resources about the risk of different biases in toxicity or hate speech detection are:\n- [The Risk of Racial Bias in Hate Speech Detection](https://homes.cs.washington.edu/~msap/pdfs/sap2019risk.pdf)\n- [Automated Hate Speech Detection and the Problem of Offensive Language](https://arxiv.org/pdf/1703.04009.pdf%201.pdf)\n- [Racial Bias in Hate Speech and Abusive Language Detection Datasets](https://arxiv.org/pdf/1905.12516.pdf)\n\n## Quick prediction\n\n\nThe `multilingual` model has been trained on 7 different languages so it should only be tested on: `english`, `french`, `spanish`, `italian`, `portuguese`, `turkish` or `russian`.\n\n```bash\n# install detoxify  \n\npip install detoxify\n\n```\n```python\n\nfrom detoxify import Detoxify\n\n# each model takes in either a string or a list of strings\n\nresults = Detoxify('original').predict('example text')\n\nresults = Detoxify('unbiased').predict(['example text 1','example text 2'])\n\nresults = Detoxify('multilingual').predict(['example text','exemple de texte','texto de ejemplo','testo di esempio','texto de exemplo','örnek metin','пример текста'])\n\n# optional to display results nicely (will need to pip install pandas)\n\nimport pandas as pd\n\nprint(pd.DataFrame(results, index=input_text).round(5))\n\n```\nFor more details check the Prediction section.\n\n\n## Labels\nAll challenges have a toxicity label. The toxicity labels represent the aggregate ratings of up to 10 annotators according the following schema:\n- **Very Toxic** (a very hateful, aggressive, or disrespectful comment that is very likely to make you leave a discussion or give up on sharing your perspective)\n- **Toxic** (a rude, disrespectful, or unreasonable comment that is somewhat likely to make you leave a discussion or give up on sharing your perspective)\n- **Hard to Say**\n- **Not Toxic**\n\nMore information about the labelling schema can be found [here](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/data).\n\n### Toxic Comment Classification Challenge\nThis challenge includes the following labels:\n\n- `toxic`\n- `severe_toxic`\n- `obscene`\n- `threat`\n- `insult`\n- `identity_hate`\n\n### Jigsaw Unintended Bias in Toxicity Classification\nThis challenge has 2 types of labels: the main toxicity labels and some additional identity labels that represent the identities mentioned in the comments. \n\nOnly identities with more than 500 examples in the test set (combined public and private) are included during training as additional labels and in the evaluation calculation.\n\n- `toxicity`\n- `severe_toxicity`\n- `obscene`\n- `threat`\n- `insult`\n- `identity_attack`\n- `sexual_explicit`\n\nIdentity labels used:\n- `male`\n- `female`\n- `homosexual_gay_or_lesbian`\n- `christian`\n- `jewish`\n- `muslim`\n- `black`\n- `white`\n- `psychiatric_or_mental_illness`\n\nA complete list of all the identity labels available can be found [here](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/data).\n\n\n### Jigsaw Multilingual Toxic Comment Classification\n\nSince this challenge combines the data from the previous 2 challenges, it includes all labels from above, however the final evaluation is only on:\n\n- `toxicity`\n\n## How to run   \n\nFirst, install dependencies   \n```bash\n# clone project   \n\ngit clone https://github.com/unitaryai/detoxify\n\n# create virtual env\n\npython3 -m venv toxic-env\nsource toxic-env/bin/activate\n\n# install project   \n\npip install -e detoxify\ncd detoxify\n\n# for training\npip install -r requirements.txt\n\n ```   \n\n## Prediction\n\nTrained models summary:\n\n|Model name| Transformer type| Data from\n|:--:|:--:|:--:|\n|`original`| `bert-base-uncased` | Toxic Comment Classification Challenge\n|`unbiased`| `roberta-base`| Unintended Bias in Toxicity Classification\n|`multilingual`| `xlm-roberta-base`| Multilingual Toxic Comment Classification\n\nFor a quick prediction can run the example script on a comment directly or from a txt containing a list of comments. \n```bash\n\n# load model via torch.hub\n\npython run_prediction.py --input 'example' --model_name original\n\n# load model from from checkpoint path\n\npython run_prediction.py --input 'example' --from_ckpt_path model_path\n\n# save results to a .csv file\n\npython run_prediction.py --input test_set.txt --model_name original --save_to results.csv\n\n# to see usage\n\npython run_prediction.py --help\n\n```\n\nCheckpoints can be downloaded from the latest release or via the Pytorch hub API with the following names:\n- `toxic_bert`\n- `unbiased_toxic_roberta`\n- `multilingual_toxic_xlm_r`\n```bash\nmodel = torch.hub.load('unitaryai/detoxify','toxic_bert')\n```\n\nImporting detoxify in python:\n\n```python\n\nfrom detoxify import Detoxify\n\nresults = Detoxify('original').predict('some text')\n\nresults = Detoxify('unbiased').predict(['example text 1','example text 2'])\n\nresults = Detoxify('multilingual').predict(['example text','exemple de texte','texto de ejemplo','testo di esempio','texto de exemplo','örnek metin','пример текста'])\n\n# to display results nicely\n\nimport pandas as pd\n\nprint(pd.DataFrame(results,index=input_text).round(5))\n\n```\n\n\n## Training\n\n If you do not already have a Kaggle account: \n - you need to create one to be able to download the data\n \n - go to My Account and click on Create New API Token - this will download a kaggle.json file\n\n - make sure this file is located in ~/.kaggle\n\n ```bash\n\n# create data directory\n\nmkdir jigsaw_data\ncd jigsaw_data\n\n# download data\n\nkaggle competitions download -c jigsaw-toxic-comment-classification-challenge\n\nkaggle competitions download -c jigsaw-unintended-bias-in-toxicity-classification\n\nkaggle competitions download -c jigsaw-multilingual-toxic-comment-classification\n\n```\n## Start Training\n ### Toxic Comment Classification Challenge\n\n ```bash\n\npython create_val_set.py\n\npython train.py --config configs/Toxic_comment_classification_BERT.json\n``` \n ### Unintended Bias in Toxicicity Challenge\n\n```bash\n\npython train.py --config configs/Unintended_bias_toxic_comment_classification_RoBERTa.json\n\n```\n ### Multilingual Toxic Comment Classification\n\n This is trained in 2 stages. First, train on all available data, and second, train only on the translated versions of the first challenge. \n \n The [translated data](https://www.kaggle.com/miklgr500/jigsaw-train-multilingual-coments-google-api) can be downloaded from Kaggle in french, spanish, italian, portuguese, turkish, and russian (the languages available in the test set).\n\n ```bash\n\n# stage 1\n\npython train.py --config configs/Multilingual_toxic_comment_classification_XLMR.json\n\n# stage 2\n\npython train.py --config configs/Multilingual_toxic_comment_classification_XLMR_stage2.json\n\n```\n### Monitor progress with tensorboard\n\n ```bash\n\ntensorboard --logdir=./saved\n\n```\n## Model Evaluation\n\n### Toxic Comment Classification Challenge\n\nThis challenge is evaluated on the mean AUC score of all the labels.\n\n```bash\n\npython evaluate.py --checkpoint saved/lightning_logs/checkpoints/example_checkpoint.pth --test_csv test.csv\n\n```\n### Unintended Bias in Toxicicity Challenge\n\nThis challenge is evaluated on a novel bias metric that combines different AUC scores to balance overall performance. More information on this metric [here](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/overview/evaluation).\n\n```bash\n\npython evaluate.py --checkpoint saved/lightning_logs/checkpoints/example_checkpoint.pth --test_csv test.csv\n\n# to get the final bias metric\npython model_eval/compute_bias_metric.py\n\n```\n### Multilingual Toxic Comment Classification\n\nThis challenge is evaluated on the AUC score of the main toxic label.\n\n```bash\n\npython evaluate.py --checkpoint saved/lightning_logs/checkpoints/example_checkpoint.pth --test_csv test.csv\n\n```\n\n### Citation   \n```\n@misc{Detoxify,\n  title={Detoxify},\n  author={Hanu, Laura and {Unitary team}},\n  howpublished={Github. https://github.com/unitaryai/detoxify},\n  year={2020}\n}\n```",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModelForSequenceClassification",
      "custom_class": null,
      "pipeline_tag": "text-classification",
      "processor": "AutoTokenizer"
    },
    "safetensors": {
      "parameters": {
        "I64": 512,
        "F32": 109486854
      },
      "total": 109487366
    },
    "model_index": null,
    "trending_score": null
  },
  {
    "model_id": "j-hartmann/emotion-english-distilroberta-base",
    "model_name": "j-hartmann/emotion-english-distilroberta-base",
    "author": "j-hartmann",
    "downloads": 684073,
    "downloads_all_time": null,
    "likes": 389,
    "tags": [
      "transformers",
      "pytorch",
      "tf",
      "roberta",
      "text-classification",
      "distilroberta",
      "sentiment",
      "emotion",
      "twitter",
      "reddit",
      "en",
      "arxiv:2210.00434",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/j-hartmann/emotion-english-distilroberta-base",
    "dependencies": [
      [
        "transformers",
        null
      ]
    ],
    "last_modified": "2023-01-02T13:03:10+00:00",
    "created_at": "2022-03-02T23:29:05+00:00",
    "analysis_date": "2025-03-22T00:55:15.143977",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "roberta",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": "en",
      "tags": [
        "distilroberta",
        "sentiment",
        "emotion",
        "twitter",
        "reddit"
      ],
      "widget": [
        {
          "text": "Oh wow. I didn't know that."
        },
        {
          "text": "This movie always makes me cry.."
        },
        {
          "text": "Oh Happy Day"
        }
      ]
    },
    "card_text": "\n# Emotion English DistilRoBERTa-base\n\n# Description ℹ\n\nWith this model, you can classify emotions in English text data. The model was trained on 6 diverse datasets (see Appendix below) and predicts Ekman's 6 basic emotions, plus a neutral class:\n\n1) anger 🤬\n2) disgust 🤢\n3) fear 😨\n4) joy 😀\n5) neutral 😐\n6) sadness 😭\n7) surprise 😲\n\nThe model is a fine-tuned checkpoint of [DistilRoBERTa-base](https://huggingface.co/distilroberta-base). For a 'non-distilled' emotion model, please refer to the model card of the [RoBERTa-large](https://huggingface.co/j-hartmann/emotion-english-roberta-large) version.\n\n# Application 🚀\n\na) Run emotion model with 3 lines of code on single text example using Hugging Face's pipeline command on Google Colab:\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/j-hartmann/emotion-english-distilroberta-base/blob/main/simple_emotion_pipeline.ipynb)\n\n```python\nfrom transformers import pipeline\nclassifier = pipeline(\"text-classification\", model=\"j-hartmann/emotion-english-distilroberta-base\", return_all_scores=True)\nclassifier(\"I love this!\")\n```\n\n```python\nOutput:\n[[{'label': 'anger', 'score': 0.004419783595949411},\n  {'label': 'disgust', 'score': 0.0016119900392368436},\n  {'label': 'fear', 'score': 0.0004138521908316761},\n  {'label': 'joy', 'score': 0.9771687984466553},\n  {'label': 'neutral', 'score': 0.005764586851000786},\n  {'label': 'sadness', 'score': 0.002092392183840275},\n  {'label': 'surprise', 'score': 0.008528684265911579}]]\n```\n\nb) Run emotion model on multiple examples and full datasets (e.g., .csv files) on Google Colab:\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/j-hartmann/emotion-english-distilroberta-base/blob/main/emotion_prediction_example.ipynb)\n\n# Contact 💻\n\nPlease reach out to [jochen.hartmann@tum.de](mailto:jochen.hartmann@tum.de) if you have any questions or feedback.\n\nThanks to Samuel Domdey and [chrsiebert](https://huggingface.co/siebert) for their support in making this model available.\n\n# Reference ✅\n\nFor attribution, please cite the following reference if you use this model. A working paper will be available soon.\n\n```\nJochen Hartmann, \"Emotion English DistilRoBERTa-base\". https://huggingface.co/j-hartmann/emotion-english-distilroberta-base/, 2022.\n```\n\nBibTex citation:\n\n```\n@misc{hartmann2022emotionenglish,\n  author={Hartmann, Jochen},\n  title={Emotion English DistilRoBERTa-base},\n  year={2022},\n  howpublished = {\\url{https://huggingface.co/j-hartmann/emotion-english-distilroberta-base/}},\n}\n```\n\n# Appendix 📚\n\nPlease find an overview of the datasets used for training below. All datasets contain English text. The table summarizes which emotions are available in each of the datasets. The datasets represent a diverse collection of text types. Specifically, they contain emotion labels for texts from Twitter, Reddit, student self-reports, and utterances from TV dialogues. As MELD (Multimodal EmotionLines Dataset) extends the popular EmotionLines dataset, EmotionLines itself is not included here. \n\n|Name|anger|disgust|fear|joy|neutral|sadness|surprise|\n|---|---|---|---|---|---|---|---|\n|Crowdflower (2016)|Yes|-|-|Yes|Yes|Yes|Yes|\n|Emotion Dataset, Elvis et al. (2018)|Yes|-|Yes|Yes|-|Yes|Yes|\n|GoEmotions, Demszky et al. (2020)|Yes|Yes|Yes|Yes|Yes|Yes|Yes|\n|ISEAR, Vikash (2018)|Yes|Yes|Yes|Yes|-|Yes|-|\n|MELD, Poria et al. (2019)|Yes|Yes|Yes|Yes|Yes|Yes|Yes|\n|SemEval-2018, EI-reg, Mohammad et al. (2018) |Yes|-|Yes|Yes|-|Yes|-|\n\nThe model is trained on a balanced subset from the datasets listed above (2,811 observations per emotion, i.e., nearly 20k observations in total). 80% of this balanced subset is used for training and 20% for evaluation. The evaluation accuracy is 66% (vs. the random-chance baseline of 1/7 = 14%).\n\n# Scientific Applications 📖\n\nBelow you can find a list of papers using \"Emotion English DistilRoBERTa-base\". If you would like your paper to be added to the list, please send me an email.\n\n- Butt, S., Sharma, S., Sharma, R., Sidorov, G., & Gelbukh, A. (2022). What goes on inside rumour and non-rumour tweets and their reactions: A Psycholinguistic Analyses. Computers in Human Behavior, 107345.\n- Kuang, Z., Zong, S., Zhang, J., Chen, J., & Liu, H. (2022). Music-to-Text Synaesthesia: Generating Descriptive Text from Music Recordings. arXiv preprint arXiv:2210.00434.\n- Rozado, D., Hughes, R., & Halberstadt, J. (2022). Longitudinal analysis of sentiment and emotion in news media headlines using automated labelling with Transformer language models. Plos one, 17(10), e0276367.",
    "card_content": "---\nlanguage: en\ntags:\n- distilroberta\n- sentiment\n- emotion\n- twitter\n- reddit\nwidget:\n- text: Oh wow. I didn't know that.\n- text: This movie always makes me cry..\n- text: Oh Happy Day\n---\n\n# Emotion English DistilRoBERTa-base\n\n# Description ℹ\n\nWith this model, you can classify emotions in English text data. The model was trained on 6 diverse datasets (see Appendix below) and predicts Ekman's 6 basic emotions, plus a neutral class:\n\n1) anger 🤬\n2) disgust 🤢\n3) fear 😨\n4) joy 😀\n5) neutral 😐\n6) sadness 😭\n7) surprise 😲\n\nThe model is a fine-tuned checkpoint of [DistilRoBERTa-base](https://huggingface.co/distilroberta-base). For a 'non-distilled' emotion model, please refer to the model card of the [RoBERTa-large](https://huggingface.co/j-hartmann/emotion-english-roberta-large) version.\n\n# Application 🚀\n\na) Run emotion model with 3 lines of code on single text example using Hugging Face's pipeline command on Google Colab:\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/j-hartmann/emotion-english-distilroberta-base/blob/main/simple_emotion_pipeline.ipynb)\n\n```python\nfrom transformers import pipeline\nclassifier = pipeline(\"text-classification\", model=\"j-hartmann/emotion-english-distilroberta-base\", return_all_scores=True)\nclassifier(\"I love this!\")\n```\n\n```python\nOutput:\n[[{'label': 'anger', 'score': 0.004419783595949411},\n  {'label': 'disgust', 'score': 0.0016119900392368436},\n  {'label': 'fear', 'score': 0.0004138521908316761},\n  {'label': 'joy', 'score': 0.9771687984466553},\n  {'label': 'neutral', 'score': 0.005764586851000786},\n  {'label': 'sadness', 'score': 0.002092392183840275},\n  {'label': 'surprise', 'score': 0.008528684265911579}]]\n```\n\nb) Run emotion model on multiple examples and full datasets (e.g., .csv files) on Google Colab:\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/j-hartmann/emotion-english-distilroberta-base/blob/main/emotion_prediction_example.ipynb)\n\n# Contact 💻\n\nPlease reach out to [jochen.hartmann@tum.de](mailto:jochen.hartmann@tum.de) if you have any questions or feedback.\n\nThanks to Samuel Domdey and [chrsiebert](https://huggingface.co/siebert) for their support in making this model available.\n\n# Reference ✅\n\nFor attribution, please cite the following reference if you use this model. A working paper will be available soon.\n\n```\nJochen Hartmann, \"Emotion English DistilRoBERTa-base\". https://huggingface.co/j-hartmann/emotion-english-distilroberta-base/, 2022.\n```\n\nBibTex citation:\n\n```\n@misc{hartmann2022emotionenglish,\n  author={Hartmann, Jochen},\n  title={Emotion English DistilRoBERTa-base},\n  year={2022},\n  howpublished = {\\url{https://huggingface.co/j-hartmann/emotion-english-distilroberta-base/}},\n}\n```\n\n# Appendix 📚\n\nPlease find an overview of the datasets used for training below. All datasets contain English text. The table summarizes which emotions are available in each of the datasets. The datasets represent a diverse collection of text types. Specifically, they contain emotion labels for texts from Twitter, Reddit, student self-reports, and utterances from TV dialogues. As MELD (Multimodal EmotionLines Dataset) extends the popular EmotionLines dataset, EmotionLines itself is not included here. \n\n|Name|anger|disgust|fear|joy|neutral|sadness|surprise|\n|---|---|---|---|---|---|---|---|\n|Crowdflower (2016)|Yes|-|-|Yes|Yes|Yes|Yes|\n|Emotion Dataset, Elvis et al. (2018)|Yes|-|Yes|Yes|-|Yes|Yes|\n|GoEmotions, Demszky et al. (2020)|Yes|Yes|Yes|Yes|Yes|Yes|Yes|\n|ISEAR, Vikash (2018)|Yes|Yes|Yes|Yes|-|Yes|-|\n|MELD, Poria et al. (2019)|Yes|Yes|Yes|Yes|Yes|Yes|Yes|\n|SemEval-2018, EI-reg, Mohammad et al. (2018) |Yes|-|Yes|Yes|-|Yes|-|\n\nThe model is trained on a balanced subset from the datasets listed above (2,811 observations per emotion, i.e., nearly 20k observations in total). 80% of this balanced subset is used for training and 20% for evaluation. The evaluation accuracy is 66% (vs. the random-chance baseline of 1/7 = 14%).\n\n# Scientific Applications 📖\n\nBelow you can find a list of papers using \"Emotion English DistilRoBERTa-base\". If you would like your paper to be added to the list, please send me an email.\n\n- Butt, S., Sharma, S., Sharma, R., Sidorov, G., & Gelbukh, A. (2022). What goes on inside rumour and non-rumour tweets and their reactions: A Psycholinguistic Analyses. Computers in Human Behavior, 107345.\n- Kuang, Z., Zong, S., Zhang, J., Chen, J., & Liu, H. (2022). Music-to-Text Synaesthesia: Generating Descriptive Text from Music Recordings. arXiv preprint arXiv:2210.00434.\n- Rozado, D., Hughes, R., & Halberstadt, J. (2022). Longitudinal analysis of sentiment and emotion in news media headlines using automated labelling with Transformer language models. Plos one, 17(10), e0276367.",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModelForSequenceClassification",
      "custom_class": null,
      "pipeline_tag": "text-classification",
      "processor": "AutoTokenizer"
    },
    "safetensors": null,
    "model_index": null,
    "trending_score": null
  },
  {
    "model_id": "finiteautomata/beto-sentiment-analysis",
    "model_name": "finiteautomata/beto-sentiment-analysis",
    "author": "finiteautomata",
    "downloads": 677313,
    "downloads_all_time": null,
    "likes": 30,
    "tags": [
      "transformers",
      "pytorch",
      "jax",
      "bert",
      "text-classification",
      "sentiment-analysis",
      "es",
      "arxiv:2106.09462",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/finiteautomata/beto-sentiment-analysis",
    "dependencies": [
      [
        "pysentimiento",
        null
      ]
    ],
    "last_modified": "2023-02-25T14:23:57+00:00",
    "created_at": "2022-03-02T23:29:05+00:00",
    "analysis_date": "2025-03-22T00:55:16.181789",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "bert",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": [
        "es"
      ],
      "tags": [
        "sentiment-analysis"
      ]
    },
    "card_text": "\n# Sentiment Analysis in Spanish\n## beto-sentiment-analysis\n\n**NOTE: this model will be removed soon -- use [pysentimiento/robertuito-sentiment-analysis](https://huggingface.co/pysentimiento/robertuito-sentiment-analysis) instead**\n\nRepository: [https://github.com/finiteautomata/pysentimiento/](https://github.com/pysentimiento/pysentimiento/)\n\n\nModel trained with TASS 2020 corpus (around ~5k tweets) of several dialects of Spanish. Base model is [BETO](https://github.com/dccuchile/beto), a BERT model trained in Spanish.\n\nUses `POS`, `NEG`, `NEU` labels.\n\n## License\n\n`pysentimiento` is an open-source library for non-commercial use and scientific research purposes only. Please be aware that models are trained with third-party datasets and are subject to their respective licenses. \n\n1. [TASS Dataset license](http://tass.sepln.org/tass_data/download.php)\n2. [SEMEval 2017 Dataset license]()\n\n## Citation\n\nIf you use this model in your work, please cite the following papers:\n\n```\n@misc{perez2021pysentimiento,\n      title={pysentimiento: A Python Toolkit for Sentiment Analysis and SocialNLP tasks},\n      author={Juan Manuel Pérez and Juan Carlos Giudici and Franco Luque},\n      year={2021},\n      eprint={2106.09462},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n\n@article{canete2020spanish,\n  title={Spanish pre-trained bert model and evaluation data},\n  author={Ca{\\~n}ete, Jos{\\'e} and Chaperon, Gabriel and Fuentes, Rodrigo and Ho, Jou-Hui and Kang, Hojin and P{\\'e}rez, Jorge},\n  journal={Pml4dc at iclr},\n  volume={2020},\n  number={2020},\n  pages={1--10},\n  year={2020}\n}\n```\n\nEnjoy! 🤗\n",
    "card_content": "---\nlanguage:\n- es\ntags:\n- sentiment-analysis\n---\n\n# Sentiment Analysis in Spanish\n## beto-sentiment-analysis\n\n**NOTE: this model will be removed soon -- use [pysentimiento/robertuito-sentiment-analysis](https://huggingface.co/pysentimiento/robertuito-sentiment-analysis) instead**\n\nRepository: [https://github.com/finiteautomata/pysentimiento/](https://github.com/pysentimiento/pysentimiento/)\n\n\nModel trained with TASS 2020 corpus (around ~5k tweets) of several dialects of Spanish. Base model is [BETO](https://github.com/dccuchile/beto), a BERT model trained in Spanish.\n\nUses `POS`, `NEG`, `NEU` labels.\n\n## License\n\n`pysentimiento` is an open-source library for non-commercial use and scientific research purposes only. Please be aware that models are trained with third-party datasets and are subject to their respective licenses. \n\n1. [TASS Dataset license](http://tass.sepln.org/tass_data/download.php)\n2. [SEMEval 2017 Dataset license]()\n\n## Citation\n\nIf you use this model in your work, please cite the following papers:\n\n```\n@misc{perez2021pysentimiento,\n      title={pysentimiento: A Python Toolkit for Sentiment Analysis and SocialNLP tasks},\n      author={Juan Manuel Pérez and Juan Carlos Giudici and Franco Luque},\n      year={2021},\n      eprint={2106.09462},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n\n@article{canete2020spanish,\n  title={Spanish pre-trained bert model and evaluation data},\n  author={Ca{\\~n}ete, Jos{\\'e} and Chaperon, Gabriel and Fuentes, Rodrigo and Ho, Jou-Hui and Kang, Hojin and P{\\'e}rez, Jorge},\n  journal={Pml4dc at iclr},\n  volume={2020},\n  number={2020},\n  pages={1--10},\n  year={2020}\n}\n```\n\nEnjoy! 🤗\n",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModelForSequenceClassification",
      "custom_class": null,
      "pipeline_tag": "text-classification",
      "processor": "AutoTokenizer"
    },
    "safetensors": null,
    "model_index": null,
    "trending_score": null
  },
  {
    "model_id": "Elron/bleurt-tiny-512",
    "model_name": "Elron/bleurt-tiny-512",
    "author": "Elron",
    "downloads": 658761,
    "downloads_all_time": null,
    "likes": 4,
    "tags": [
      "transformers",
      "pytorch",
      "bert",
      "text-classification",
      "arxiv:1910.09700",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/Elron/bleurt-tiny-512",
    "dependencies": [
      [
        "transformers",
        null
      ],
      [
        "torch",
        null
      ]
    ],
    "last_modified": "2022-11-26T15:13:43+00:00",
    "created_at": "2022-03-02T23:29:04+00:00",
    "analysis_date": "2025-03-22T00:55:18.878783",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "bert",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "tags": [
        "text-classification",
        "bert"
      ]
    },
    "card_text": "\n# Model Card for bleurt-tiny-512 \n \n# Model Details\n \n## Model Description\n \nPytorch version of the original BLEURT models from ACL paper\n \n- **Developed by:** Elron Bandel, Thibault Sellam, Dipanjan Das and Ankur P. Parikh of Google Research\n- **Shared by [Optional]:** Elron Bandel\n- **Model type:** Text Classification \n- **Language(s) (NLP):** More information needed\n- **License:** More information needed \n- **Parent Model:** BERT\n- **Resources for more information:**\n     - [GitHub Repo](https://github.com/google-research/bleurt/tree/master)\n \t  - [Associated Paper](https://aclanthology.org/2020.acl-main.704/)\n    - [Blog Post](https://ai.googleblog.com/2020/05/evaluating-natural-language-generation.html)\n \t\n\n\n# Uses\n \n\n## Direct Use\nThis model can be used for the task of Text Classification \n \n## Downstream Use [Optional]\n \nMore information needed.\n \n## Out-of-Scope Use\n \nThe model should not be used to intentionally create hostile or alienating environments for people. \n \n# Bias, Risks, and Limitations\n \n \nSignificant research has explored bias and fairness issues with language models (see, e.g., [Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)). Predictions generated by the model may include disturbing and harmful stereotypes across protected classes; identity characteristics; and sensitive, social, and occupational groups.\n\n\n\n## Recommendations\n \n \nUsers (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.\n\n# Training Details\n \n## Training Data\nThe model authors note in the [associated paper](https://aclanthology.org/2020.acl-main.704.pdf): \n> We use years 2017 to 2019 of the WMT Metrics Shared Task, to-English language pairs. For each year, we used the of- ficial WMT test set, which include several thou- sand pairs of sentences with human ratings from the news domain. The training sets contain 5,360, 9,492, and 147,691 records for each year. \n \n \n## Training Procedure\n\n \n### Preprocessing\n \nMore information needed \n \n### Speeds, Sizes, Times\nMore information needed \n\n \n# Evaluation\n \n \n## Testing Data, Factors & Metrics\n \n### Testing Data\n \nThe test sets for years 2018 and 2019 [of the WMT Metrics Shared Task, to-English language pairs.]  are noisier,\n \n \n \n### Factors\nMore information needed\n \n### Metrics\n \nMore information needed\n \n \n## Results \n \nMore information needed\n\n \n# Model Examination\n \nMore information needed\n \n# Environmental Impact\n \nCarbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).\n \n- **Hardware Type:** More information needed\n- **Hours used:** More information needed\n- **Cloud Provider:** More information needed\n- **Compute Region:** More information needed\n- **Carbon Emitted:** More information needed\n \n# Technical Specifications [optional]\n \n## Model Architecture and Objective\n\nMore information needed \n \n## Compute Infrastructure\n \nMore information needed \n \n### Hardware\n \n \nMore information needed\n \n### Software\n \nMore information needed.\n \n# Citation\n\n \n**BibTeX:**\n \n \n```bibtex\n@inproceedings{sellam2020bleurt,\n  title = {BLEURT: Learning Robust Metrics for Text Generation},\n  author = {Thibault Sellam and Dipanjan Das and Ankur P Parikh},\n  year = {2020},\n  booktitle = {Proceedings of ACL}\n}\n```\n \n \n \n \n# Glossary [optional]\nMore information needed \n \n# More Information [optional]\nMore information needed \n\n \n# Model Card Authors [optional]\n \n Elron Bandel in collaboration with Ezi Ozoani and the Hugging Face team\n\n\n# Model Card Contact\n \nMore information needed\n \n# How to Get Started with the Model\n \nUse the code below to get started with the model.\n \n<details>\n<summary> Click to expand </summary>\n\n```python\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"Elron/bleurt-tiny-512\")\nmodel = AutoModelForSequenceClassification.from_pretrained(\"Elron/bleurt-tiny-512\")\nmodel.eval()\n\nreferences = [\"hello world\", \"hello world\"]\ncandidates = [\"hi universe\", \"bye world\"]\n\nwith torch.no_grad():\n  scores = model(**tokenizer(references, candidates, return_tensors='pt'))[0].squeeze()\n\nprint(scores) # tensor([-0.9414, -0.5678])\n ```\n\nSee [this notebook](https://colab.research.google.com/drive/1KsCUkFW45d5_ROSv2aHtXgeBa2Z98r03?usp=sharing) for model conversion code. \n</details>\n",
    "card_content": "---\ntags:\n- text-classification\n- bert\n---\n\n# Model Card for bleurt-tiny-512 \n \n# Model Details\n \n## Model Description\n \nPytorch version of the original BLEURT models from ACL paper\n \n- **Developed by:** Elron Bandel, Thibault Sellam, Dipanjan Das and Ankur P. Parikh of Google Research\n- **Shared by [Optional]:** Elron Bandel\n- **Model type:** Text Classification \n- **Language(s) (NLP):** More information needed\n- **License:** More information needed \n- **Parent Model:** BERT\n- **Resources for more information:**\n     - [GitHub Repo](https://github.com/google-research/bleurt/tree/master)\n \t  - [Associated Paper](https://aclanthology.org/2020.acl-main.704/)\n    - [Blog Post](https://ai.googleblog.com/2020/05/evaluating-natural-language-generation.html)\n \t\n\n\n# Uses\n \n\n## Direct Use\nThis model can be used for the task of Text Classification \n \n## Downstream Use [Optional]\n \nMore information needed.\n \n## Out-of-Scope Use\n \nThe model should not be used to intentionally create hostile or alienating environments for people. \n \n# Bias, Risks, and Limitations\n \n \nSignificant research has explored bias and fairness issues with language models (see, e.g., [Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)). Predictions generated by the model may include disturbing and harmful stereotypes across protected classes; identity characteristics; and sensitive, social, and occupational groups.\n\n\n\n## Recommendations\n \n \nUsers (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.\n\n# Training Details\n \n## Training Data\nThe model authors note in the [associated paper](https://aclanthology.org/2020.acl-main.704.pdf): \n> We use years 2017 to 2019 of the WMT Metrics Shared Task, to-English language pairs. For each year, we used the of- ficial WMT test set, which include several thou- sand pairs of sentences with human ratings from the news domain. The training sets contain 5,360, 9,492, and 147,691 records for each year. \n \n \n## Training Procedure\n\n \n### Preprocessing\n \nMore information needed \n \n### Speeds, Sizes, Times\nMore information needed \n\n \n# Evaluation\n \n \n## Testing Data, Factors & Metrics\n \n### Testing Data\n \nThe test sets for years 2018 and 2019 [of the WMT Metrics Shared Task, to-English language pairs.]  are noisier,\n \n \n \n### Factors\nMore information needed\n \n### Metrics\n \nMore information needed\n \n \n## Results \n \nMore information needed\n\n \n# Model Examination\n \nMore information needed\n \n# Environmental Impact\n \nCarbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).\n \n- **Hardware Type:** More information needed\n- **Hours used:** More information needed\n- **Cloud Provider:** More information needed\n- **Compute Region:** More information needed\n- **Carbon Emitted:** More information needed\n \n# Technical Specifications [optional]\n \n## Model Architecture and Objective\n\nMore information needed \n \n## Compute Infrastructure\n \nMore information needed \n \n### Hardware\n \n \nMore information needed\n \n### Software\n \nMore information needed.\n \n# Citation\n\n \n**BibTeX:**\n \n \n```bibtex\n@inproceedings{sellam2020bleurt,\n  title = {BLEURT: Learning Robust Metrics for Text Generation},\n  author = {Thibault Sellam and Dipanjan Das and Ankur P Parikh},\n  year = {2020},\n  booktitle = {Proceedings of ACL}\n}\n```\n \n \n \n \n# Glossary [optional]\nMore information needed \n \n# More Information [optional]\nMore information needed \n\n \n# Model Card Authors [optional]\n \n Elron Bandel in collaboration with Ezi Ozoani and the Hugging Face team\n\n\n# Model Card Contact\n \nMore information needed\n \n# How to Get Started with the Model\n \nUse the code below to get started with the model.\n \n<details>\n<summary> Click to expand </summary>\n\n```python\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"Elron/bleurt-tiny-512\")\nmodel = AutoModelForSequenceClassification.from_pretrained(\"Elron/bleurt-tiny-512\")\nmodel.eval()\n\nreferences = [\"hello world\", \"hello world\"]\ncandidates = [\"hi universe\", \"bye world\"]\n\nwith torch.no_grad():\n  scores = model(**tokenizer(references, candidates, return_tensors='pt'))[0].squeeze()\n\nprint(scores) # tensor([-0.9414, -0.5678])\n ```\n\nSee [this notebook](https://colab.research.google.com/drive/1KsCUkFW45d5_ROSv2aHtXgeBa2Z98r03?usp=sharing) for model conversion code. \n</details>\n",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModelForSequenceClassification",
      "custom_class": null,
      "pipeline_tag": "text-classification",
      "processor": "AutoTokenizer"
    },
    "safetensors": null,
    "model_index": null,
    "trending_score": null
  },
  {
    "model_id": "mixedbread-ai/mxbai-rerank-base-v1",
    "model_name": "mixedbread-ai/mxbai-rerank-base-v1",
    "author": "mixedbread-ai",
    "downloads": 611569,
    "downloads_all_time": null,
    "likes": 40,
    "tags": [
      "transformers",
      "onnx",
      "safetensors",
      "deberta-v2",
      "text-classification",
      "reranker",
      "transformers.js",
      "en",
      "license:apache-2.0",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/mixedbread-ai/mxbai-rerank-base-v1",
    "dependencies": null,
    "last_modified": "2025-03-13T04:18:02+00:00",
    "created_at": "2024-02-29T14:36:24+00:00",
    "analysis_date": "2025-03-22T00:55:23.665203",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "deberta-v2",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "library_name": "transformers",
      "tags": [
        "reranker",
        "transformers.js"
      ],
      "license": "apache-2.0",
      "language": [
        "en"
      ]
    },
    "card_text": "<br><br>\n\n<p align=\"center\">\n<svg xmlns=\"http://www.w3.org/2000/svg\" xml:space=\"preserve\" viewBox=\"0 0 2020 1130\" width=\"150\" height=\"150\" aria-hidden=\"true\"><path fill=\"#e95a0f\" d=\"M398.167 621.992c-1.387-20.362-4.092-40.739-3.851-61.081.355-30.085 6.873-59.139 21.253-85.976 10.487-19.573 24.09-36.822 40.662-51.515 16.394-14.535 34.338-27.046 54.336-36.182 15.224-6.955 31.006-12.609 47.829-14.168 11.809-1.094 23.753-2.514 35.524-1.836 23.033 1.327 45.131 7.255 66.255 16.75 16.24 7.3 31.497 16.165 45.651 26.969 12.997 9.921 24.412 21.37 34.158 34.509 11.733 15.817 20.849 33.037 25.987 52.018 3.468 12.81 6.438 25.928 7.779 39.097 1.722 16.908 1.642 34.003 2.235 51.021.427 12.253.224 24.547 1.117 36.762 1.677 22.93 4.062 45.764 11.8 67.7 5.376 15.239 12.499 29.55 20.846 43.681l-18.282 20.328c-1.536 1.71-2.795 3.665-4.254 5.448l-19.323 23.533c-13.859-5.449-27.446-11.803-41.657-16.086-13.622-4.106-27.793-6.765-41.905-8.775-15.256-2.173-30.701-3.475-46.105-4.049-23.571-.879-47.178-1.056-70.769-1.029-10.858.013-21.723 1.116-32.57 1.926-5.362.4-10.69 1.255-16.464 1.477-2.758-7.675-5.284-14.865-7.367-22.181-3.108-10.92-4.325-22.554-13.16-31.095-2.598-2.512-5.069-5.341-6.883-8.443-6.366-10.884-12.48-21.917-18.571-32.959-4.178-7.573-8.411-14.375-17.016-18.559-10.34-5.028-19.538-12.387-29.311-18.611-3.173-2.021-6.414-4.312-9.952-5.297-5.857-1.63-11.98-2.301-17.991-3.376z\"></path><path fill=\"#ed6d7b\" d=\"M1478.998 758.842c-12.025.042-24.05.085-36.537-.373-.14-8.536.231-16.569.453-24.607.033-1.179-.315-2.986-1.081-3.4-.805-.434-2.376.338-3.518.81-.856.354-1.562 1.069-3.589 2.521-.239-3.308-.664-5.586-.519-7.827.488-7.544 2.212-15.166 1.554-22.589-1.016-11.451 1.397-14.592-12.332-14.419-3.793.048-3.617-2.803-3.332-5.331.499-4.422 1.45-8.803 1.77-13.233.311-4.316.068-8.672.068-12.861-2.554-.464-4.326-.86-6.12-1.098-4.415-.586-6.051-2.251-5.065-7.31 1.224-6.279.848-12.862 1.276-19.306.19-2.86-.971-4.473-3.794-4.753-4.113-.407-8.242-1.057-12.352-.975-4.663.093-5.192-2.272-4.751-6.012.733-6.229 1.252-12.483 1.875-18.726l1.102-10.495c-5.905-.309-11.146-.805-16.385-.778-3.32.017-5.174-1.4-5.566-4.4-1.172-8.968-2.479-17.944-3.001-26.96-.26-4.484-1.936-5.705-6.005-5.774-9.284-.158-18.563-.594-27.843-.953-7.241-.28-10.137-2.764-11.3-9.899-.746-4.576-2.715-7.801-7.777-8.207-7.739-.621-15.511-.992-23.207-1.961-7.327-.923-14.587-2.415-21.853-3.777-5.021-.941-10.003-2.086-15.003-3.14 4.515-22.952 13.122-44.382 26.284-63.587 18.054-26.344 41.439-47.239 69.102-63.294 15.847-9.197 32.541-16.277 50.376-20.599 16.655-4.036 33.617-5.715 50.622-4.385 33.334 2.606 63.836 13.955 92.415 31.15 15.864 9.545 30.241 20.86 42.269 34.758 8.113 9.374 15.201 19.78 21.718 30.359 10.772 17.484 16.846 36.922 20.611 56.991 1.783 9.503 2.815 19.214 3.318 28.876.758 14.578.755 29.196.65 44.311l-51.545 20.013c-7.779 3.059-15.847 5.376-21.753 12.365-4.73 5.598-10.658 10.316-16.547 14.774-9.9 7.496-18.437 15.988-25.083 26.631-3.333 5.337-7.901 10.381-12.999 14.038-11.355 8.144-17.397 18.973-19.615 32.423l-6.988 41.011z\"></path><path fill=\"#ec663e\" d=\"M318.11 923.047c-.702 17.693-.832 35.433-2.255 53.068-1.699 21.052-6.293 41.512-14.793 61.072-9.001 20.711-21.692 38.693-38.496 53.583-16.077 14.245-34.602 24.163-55.333 30.438-21.691 6.565-43.814 8.127-66.013 6.532-22.771-1.636-43.88-9.318-62.74-22.705-20.223-14.355-35.542-32.917-48.075-54.096-9.588-16.203-16.104-33.55-19.201-52.015-2.339-13.944-2.307-28.011-.403-42.182 2.627-19.545 9.021-37.699 17.963-55.067 11.617-22.564 27.317-41.817 48.382-56.118 15.819-10.74 33.452-17.679 52.444-20.455 8.77-1.282 17.696-1.646 26.568-2.055 11.755-.542 23.534-.562 35.289-1.11 8.545-.399 17.067-1.291 26.193-1.675 1.349 1.77 2.24 3.199 2.835 4.742 4.727 12.261 10.575 23.865 18.636 34.358 7.747 10.084 14.83 20.684 22.699 30.666 3.919 4.972 8.37 9.96 13.609 13.352 7.711 4.994 16.238 8.792 24.617 12.668 5.852 2.707 12.037 4.691 18.074 6.998z\"></path><path fill=\"#ea580e\" d=\"M1285.167 162.995c3.796-29.75 13.825-56.841 32.74-80.577 16.339-20.505 36.013-36.502 59.696-47.614 14.666-6.881 29.971-11.669 46.208-12.749 10.068-.669 20.239-1.582 30.255-.863 16.6 1.191 32.646 5.412 47.9 12.273 19.39 8.722 36.44 20.771 50.582 36.655 15.281 17.162 25.313 37.179 31.49 59.286 5.405 19.343 6.31 39.161 4.705 58.825-2.37 29.045-11.836 55.923-30.451 78.885-10.511 12.965-22.483 24.486-37.181 33.649-5.272-5.613-10.008-11.148-14.539-16.846-5.661-7.118-10.958-14.533-16.78-21.513-4.569-5.478-9.548-10.639-14.624-15.658-3.589-3.549-7.411-6.963-11.551-9.827-5.038-3.485-10.565-6.254-15.798-9.468-8.459-5.195-17.011-9.669-26.988-11.898-12.173-2.72-24.838-4.579-35.622-11.834-1.437-.967-3.433-1.192-5.213-1.542-12.871-2.529-25.454-5.639-36.968-12.471-5.21-3.091-11.564-4.195-17.011-6.965-4.808-2.445-8.775-6.605-13.646-8.851-8.859-4.085-18.114-7.311-27.204-10.896z\"></path><path fill=\"#f8ab00\" d=\"M524.963 311.12c-9.461-5.684-19.513-10.592-28.243-17.236-12.877-9.801-24.031-21.578-32.711-35.412-11.272-17.965-19.605-37.147-21.902-58.403-1.291-11.951-2.434-24.073-1.87-36.034.823-17.452 4.909-34.363 11.581-50.703 8.82-21.603 22.25-39.792 39.568-55.065 18.022-15.894 39.162-26.07 62.351-32.332 19.22-5.19 38.842-6.177 58.37-4.674 23.803 1.831 45.56 10.663 65.062 24.496 17.193 12.195 31.688 27.086 42.894 45.622-11.403 8.296-22.633 16.117-34.092 23.586-17.094 11.142-34.262 22.106-48.036 37.528-8.796 9.848-17.201 20.246-27.131 28.837-16.859 14.585-27.745 33.801-41.054 51.019-11.865 15.349-20.663 33.117-30.354 50.08-5.303 9.283-9.654 19.11-14.434 28.692z\"></path><path fill=\"#ea5227\" d=\"M1060.11 1122.049c-7.377 1.649-14.683 4.093-22.147 4.763-11.519 1.033-23.166 1.441-34.723 1.054-19.343-.647-38.002-4.7-55.839-12.65-15.078-6.72-28.606-15.471-40.571-26.836-24.013-22.81-42.053-49.217-49.518-81.936-1.446-6.337-1.958-12.958-2.235-19.477-.591-13.926-.219-27.909-1.237-41.795-.916-12.5-3.16-24.904-4.408-37.805 1.555-1.381 3.134-2.074 3.778-3.27 4.729-8.79 12.141-15.159 19.083-22.03 5.879-5.818 10.688-12.76 16.796-18.293 6.993-6.335 11.86-13.596 14.364-22.612l8.542-29.993c8.015 1.785 15.984 3.821 24.057 5.286 8.145 1.478 16.371 2.59 24.602 3.493 8.453.927 16.956 1.408 25.891 2.609 1.119 16.09 1.569 31.667 2.521 47.214.676 11.045 1.396 22.154 3.234 33.043 2.418 14.329 5.708 28.527 9.075 42.674 3.499 14.705 4.028 29.929 10.415 44.188 10.157 22.674 18.29 46.25 28.281 69.004 7.175 16.341 12.491 32.973 15.078 50.615.645 4.4 3.256 8.511 4.963 12.755z\"></path><path fill=\"#ea5330\" d=\"M1060.512 1122.031c-2.109-4.226-4.72-8.337-5.365-12.737-2.587-17.642-7.904-34.274-15.078-50.615-9.991-22.755-18.124-46.33-28.281-69.004-6.387-14.259-6.916-29.482-10.415-44.188-3.366-14.147-6.656-28.346-9.075-42.674-1.838-10.889-2.558-21.999-3.234-33.043-.951-15.547-1.401-31.124-2.068-47.146 8.568-.18 17.146.487 25.704.286l41.868-1.4c.907 3.746 1.245 7.04 1.881 10.276l8.651 42.704c.903 4.108 2.334 8.422 4.696 11.829 7.165 10.338 14.809 20.351 22.456 30.345 4.218 5.512 8.291 11.304 13.361 15.955 8.641 7.927 18.065 14.995 27.071 22.532 12.011 10.052 24.452 19.302 40.151 22.854-1.656 11.102-2.391 22.44-5.172 33.253-4.792 18.637-12.38 36.209-23.412 52.216-13.053 18.94-29.086 34.662-49.627 45.055-10.757 5.443-22.443 9.048-34.111 13.501z\"></path><path fill=\"#f8aa05\" d=\"M1989.106 883.951c5.198 8.794 11.46 17.148 15.337 26.491 5.325 12.833 9.744 26.207 12.873 39.737 2.95 12.757 3.224 25.908 1.987 39.219-1.391 14.973-4.643 29.268-10.349 43.034-5.775 13.932-13.477 26.707-23.149 38.405-14.141 17.104-31.215 30.458-50.807 40.488-14.361 7.352-29.574 12.797-45.741 14.594-10.297 1.144-20.732 2.361-31.031 1.894-24.275-1.1-47.248-7.445-68.132-20.263-6.096-3.741-11.925-7.917-17.731-12.342 5.319-5.579 10.361-10.852 15.694-15.811l37.072-34.009c.975-.892 2.113-1.606 3.08-2.505 6.936-6.448 14.765-12.2 20.553-19.556 8.88-11.285 20.064-19.639 31.144-28.292 4.306-3.363 9.06-6.353 12.673-10.358 5.868-6.504 10.832-13.814 16.422-20.582 6.826-8.264 13.727-16.481 20.943-24.401 4.065-4.461 8.995-8.121 13.249-12.424 14.802-14.975 28.77-30.825 45.913-43.317z\"></path><path fill=\"#ed6876\" d=\"M1256.099 523.419c5.065.642 10.047 1.787 15.068 2.728 7.267 1.362 14.526 2.854 21.853 3.777 7.696.97 15.468 1.34 23.207 1.961 5.062.406 7.031 3.631 7.777 8.207 1.163 7.135 4.059 9.62 11.3 9.899l27.843.953c4.069.069 5.745 1.291 6.005 5.774.522 9.016 1.829 17.992 3.001 26.96.392 3 2.246 4.417 5.566 4.4 5.239-.026 10.48.469 16.385.778l-1.102 10.495-1.875 18.726c-.44 3.74.088 6.105 4.751 6.012 4.11-.082 8.239.568 12.352.975 2.823.28 3.984 1.892 3.794 4.753-.428 6.444-.052 13.028-1.276 19.306-.986 5.059.651 6.724 5.065 7.31 1.793.238 3.566.634 6.12 1.098 0 4.189.243 8.545-.068 12.861-.319 4.43-1.27 8.811-1.77 13.233-.285 2.528-.461 5.379 3.332 5.331 13.729-.173 11.316 2.968 12.332 14.419.658 7.423-1.066 15.045-1.554 22.589-.145 2.241.28 4.519.519 7.827 2.026-1.452 2.733-2.167 3.589-2.521 1.142-.472 2.713-1.244 3.518-.81.767.414 1.114 2.221 1.081 3.4l-.917 24.539c-11.215.82-22.45.899-33.636 1.674l-43.952 3.436c-1.086-3.01-2.319-5.571-2.296-8.121.084-9.297-4.468-16.583-9.091-24.116-3.872-6.308-8.764-13.052-9.479-19.987-1.071-10.392-5.716-15.936-14.889-18.979-1.097-.364-2.16-.844-3.214-1.327-7.478-3.428-15.548-5.918-19.059-14.735-.904-2.27-3.657-3.775-5.461-5.723-2.437-2.632-4.615-5.525-7.207-7.987-2.648-2.515-5.352-5.346-8.589-6.777-4.799-2.121-10.074-3.185-15.175-4.596l-15.785-4.155c.274-12.896 1.722-25.901.54-38.662-1.647-17.783-3.457-35.526-2.554-53.352.528-10.426 2.539-20.777 3.948-31.574z\"></path><path fill=\"#f6a200\" d=\"M525.146 311.436c4.597-9.898 8.947-19.725 14.251-29.008 9.691-16.963 18.49-34.73 30.354-50.08 13.309-17.218 24.195-36.434 41.054-51.019 9.93-8.591 18.335-18.989 27.131-28.837 13.774-15.422 30.943-26.386 48.036-37.528 11.459-7.469 22.688-15.29 34.243-23.286 11.705 16.744 19.716 35.424 22.534 55.717 2.231 16.066 2.236 32.441 2.753 49.143-4.756 1.62-9.284 2.234-13.259 4.056-6.43 2.948-12.193 7.513-18.774 9.942-19.863 7.331-33.806 22.349-47.926 36.784-7.86 8.035-13.511 18.275-19.886 27.705-4.434 6.558-9.345 13.037-12.358 20.254-4.249 10.177-6.94 21.004-10.296 31.553-12.33.053-24.741 1.027-36.971-.049-20.259-1.783-40.227-5.567-58.755-14.69-.568-.28-1.295-.235-2.132-.658z\"></path><path fill=\"#f7a80d\" d=\"M1989.057 883.598c-17.093 12.845-31.061 28.695-45.863 43.67-4.254 4.304-9.184 7.963-13.249 12.424-7.216 7.92-14.117 16.137-20.943 24.401-5.59 6.768-10.554 14.078-16.422 20.582-3.614 4.005-8.367 6.995-12.673 10.358-11.08 8.653-22.264 17.007-31.144 28.292-5.788 7.356-13.617 13.108-20.553 19.556-.967.899-2.105 1.614-3.08 2.505l-37.072 34.009c-5.333 4.96-10.375 10.232-15.859 15.505-21.401-17.218-37.461-38.439-48.623-63.592 3.503-1.781 7.117-2.604 9.823-4.637 8.696-6.536 20.392-8.406 27.297-17.714.933-1.258 2.646-1.973 4.065-2.828 17.878-10.784 36.338-20.728 53.441-32.624 10.304-7.167 18.637-17.23 27.583-26.261 3.819-3.855 7.436-8.091 10.3-12.681 12.283-19.68 24.43-39.446 40.382-56.471 12.224-13.047 17.258-29.524 22.539-45.927 15.85 4.193 29.819 12.129 42.632 22.08 10.583 8.219 19.782 17.883 27.42 29.351z\"></path><path fill=\"#ef7a72\" d=\"M1479.461 758.907c1.872-13.734 4.268-27.394 6.525-41.076 2.218-13.45 8.26-24.279 19.615-32.423 5.099-3.657 9.667-8.701 12.999-14.038 6.646-10.643 15.183-19.135 25.083-26.631 5.888-4.459 11.817-9.176 16.547-14.774 5.906-6.99 13.974-9.306 21.753-12.365l51.48-19.549c.753 11.848.658 23.787 1.641 35.637 1.771 21.353 4.075 42.672 11.748 62.955.17.449.107.985-.019 2.158-6.945 4.134-13.865 7.337-20.437 11.143-3.935 2.279-7.752 5.096-10.869 8.384-6.011 6.343-11.063 13.624-17.286 19.727-9.096 8.92-12.791 20.684-18.181 31.587-.202.409-.072.984-.096 1.481-8.488-1.72-16.937-3.682-25.476-5.094-9.689-1.602-19.426-3.084-29.201-3.949-15.095-1.335-30.241-2.1-45.828-3.172z\"></path><path fill=\"#e94e3b\" d=\"M957.995 766.838c-20.337-5.467-38.791-14.947-55.703-27.254-8.2-5.967-15.451-13.238-22.958-20.37 2.969-3.504 5.564-6.772 8.598-9.563 7.085-6.518 11.283-14.914 15.8-23.153 4.933-8.996 10.345-17.743 14.966-26.892 2.642-5.231 5.547-11.01 5.691-16.611.12-4.651.194-8.932 2.577-12.742 8.52-13.621 15.483-28.026 18.775-43.704 2.11-10.049 7.888-18.774 7.81-29.825-.064-9.089 4.291-18.215 6.73-27.313 3.212-11.983 7.369-23.797 9.492-35.968 3.202-18.358 5.133-36.945 7.346-55.466l4.879-45.8c6.693.288 13.386.575 20.54 1.365.13 3.458-.41 6.407-.496 9.37l-1.136 42.595c-.597 11.552-2.067 23.058-3.084 34.59l-3.845 44.478c-.939 10.202-1.779 20.432-3.283 30.557-.96 6.464-4.46 12.646-1.136 19.383.348.706-.426 1.894-.448 2.864-.224 9.918-5.99 19.428-2.196 29.646.103.279-.033.657-.092.983l-8.446 46.205c-1.231 6.469-2.936 12.846-4.364 19.279-1.5 6.757-2.602 13.621-4.456 20.277-3.601 12.93-10.657 25.3-5.627 39.47.368 1.036.234 2.352.017 3.476l-5.949 30.123z\"></path><path fill=\"#ea5043\" d=\"M958.343 767.017c1.645-10.218 3.659-20.253 5.602-30.302.217-1.124.351-2.44-.017-3.476-5.03-14.17 2.026-26.539 5.627-39.47 1.854-6.656 2.956-13.52 4.456-20.277 1.428-6.433 3.133-12.81 4.364-19.279l8.446-46.205c.059-.326.196-.705.092-.983-3.794-10.218 1.972-19.728 2.196-29.646.022-.97.796-2.158.448-2.864-3.324-6.737.176-12.919 1.136-19.383 1.504-10.125 2.344-20.355 3.283-30.557l3.845-44.478c1.017-11.532 2.488-23.038 3.084-34.59.733-14.18.722-28.397 1.136-42.595.086-2.963.626-5.912.956-9.301 5.356-.48 10.714-.527 16.536-.081 2.224 15.098 1.855 29.734 1.625 44.408-.157 10.064 1.439 20.142 1.768 30.23.334 10.235-.035 20.49.116 30.733.084 5.713.789 11.418.861 17.13.054 4.289-.469 8.585-.702 12.879-.072 1.323-.138 2.659-.031 3.975l2.534 34.405-1.707 36.293-1.908 48.69c-.182 8.103.993 16.237.811 24.34-.271 12.076-1.275 24.133-1.787 36.207-.102 2.414-.101 5.283 1.06 7.219 4.327 7.22 4.463 15.215 4.736 23.103.365 10.553.088 21.128.086 31.693-11.44 2.602-22.84.688-34.106-.916-11.486-1.635-22.806-4.434-34.546-6.903z\"></path><path fill=\"#eb5d19\" d=\"M398.091 622.45c6.086.617 12.21 1.288 18.067 2.918 3.539.985 6.779 3.277 9.952 5.297 9.773 6.224 18.971 13.583 29.311 18.611 8.606 4.184 12.839 10.986 17.016 18.559l18.571 32.959c1.814 3.102 4.285 5.931 6.883 8.443 8.835 8.542 10.052 20.175 13.16 31.095 2.082 7.317 4.609 14.507 6.946 22.127-29.472 3.021-58.969 5.582-87.584 15.222-1.185-2.302-1.795-4.362-2.769-6.233-4.398-8.449-6.703-18.174-14.942-24.299-2.511-1.866-5.103-3.814-7.047-6.218-8.358-10.332-17.028-20.276-28.772-26.973 4.423-11.478 9.299-22.806 13.151-34.473 4.406-13.348 6.724-27.18 6.998-41.313.098-5.093.643-10.176 1.06-15.722z\"></path><path fill=\"#e94c32\" d=\"M981.557 392.109c-1.172 15.337-2.617 30.625-4.438 45.869-2.213 18.521-4.144 37.108-7.346 55.466-2.123 12.171-6.28 23.985-9.492 35.968-2.439 9.098-6.794 18.224-6.73 27.313.078 11.051-5.7 19.776-7.81 29.825-3.292 15.677-10.255 30.082-18.775 43.704-2.383 3.81-2.458 8.091-2.577 12.742-.144 5.6-3.049 11.38-5.691 16.611-4.621 9.149-10.033 17.896-14.966 26.892-4.517 8.239-8.715 16.635-15.8 23.153-3.034 2.791-5.629 6.06-8.735 9.255-12.197-10.595-21.071-23.644-29.301-37.24-7.608-12.569-13.282-25.962-17.637-40.37 13.303-6.889 25.873-13.878 35.311-25.315.717-.869 1.934-1.312 2.71-2.147 5.025-5.405 10.515-10.481 14.854-16.397 6.141-8.374 10.861-17.813 17.206-26.008 8.22-10.618 13.657-22.643 20.024-34.466 4.448-.626 6.729-3.21 8.114-6.89 1.455-3.866 2.644-7.895 4.609-11.492 4.397-8.05 9.641-15.659 13.708-23.86 3.354-6.761 5.511-14.116 8.203-21.206 5.727-15.082 7.277-31.248 12.521-46.578 3.704-10.828 3.138-23.116 4.478-34.753l7.56-.073z\"></path><path fill=\"#f7a617\" d=\"M1918.661 831.99c-4.937 16.58-9.971 33.057-22.196 46.104-15.952 17.025-28.099 36.791-40.382 56.471-2.864 4.59-6.481 8.825-10.3 12.681-8.947 9.031-17.279 19.094-27.583 26.261-17.103 11.896-35.564 21.84-53.441 32.624-1.419.856-3.132 1.571-4.065 2.828-6.904 9.308-18.6 11.178-27.297 17.714-2.705 2.033-6.319 2.856-9.874 4.281-3.413-9.821-6.916-19.583-9.36-29.602-1.533-6.284-1.474-12.957-1.665-19.913 1.913-.78 3.374-1.057 4.81-1.431 15.822-4.121 31.491-8.029 43.818-20.323 9.452-9.426 20.371-17.372 30.534-26.097 6.146-5.277 13.024-10.052 17.954-16.326 14.812-18.848 28.876-38.285 43.112-57.581 2.624-3.557 5.506-7.264 6.83-11.367 2.681-8.311 4.375-16.94 6.476-25.438 17.89.279 35.333 3.179 52.629 9.113z\"></path><path fill=\"#ea553a\" d=\"M1172.91 977.582c-15.775-3.127-28.215-12.377-40.227-22.43-9.005-7.537-18.43-14.605-27.071-22.532-5.07-4.651-9.143-10.443-13.361-15.955-7.647-9.994-15.291-20.007-22.456-30.345-2.361-3.407-3.792-7.72-4.696-11.829-3.119-14.183-5.848-28.453-8.651-42.704-.636-3.236-.974-6.53-1.452-10.209 15.234-2.19 30.471-3.969 46.408-5.622 2.692 5.705 4.882 11.222 6.63 16.876 2.9 9.381 7.776 17.194 15.035 24.049 7.056 6.662 13.305 14.311 19.146 22.099 9.509 12.677 23.01 19.061 36.907 25.054-1.048 7.441-2.425 14.854-3.066 22.33-.956 11.162-1.393 22.369-2.052 33.557l-1.096 17.661z\"></path><path fill=\"#ea5453\" d=\"M1163.123 704.036c-4.005 5.116-7.685 10.531-12.075 15.293-12.842 13.933-27.653 25.447-44.902 34.538-3.166-5.708-5.656-11.287-8.189-17.251-3.321-12.857-6.259-25.431-9.963-37.775-4.6-15.329-10.6-30.188-11.349-46.562-.314-6.871-1.275-14.287-7.114-19.644-1.047-.961-1.292-3.053-1.465-4.67l-4.092-39.927c-.554-5.245-.383-10.829-2.21-15.623-3.622-9.503-4.546-19.253-4.688-29.163-.088-6.111 1.068-12.256.782-18.344-.67-14.281-1.76-28.546-2.9-42.8-.657-8.222-1.951-16.395-2.564-24.62-.458-6.137-.285-12.322-.104-18.21.959 5.831 1.076 11.525 2.429 16.909 2.007 7.986 5.225 15.664 7.324 23.632 3.222 12.23 1.547 25.219 6.728 37.355 4.311 10.099 6.389 21.136 9.732 31.669 2.228 7.02 6.167 13.722 7.121 20.863 1.119 8.376 6.1 13.974 10.376 20.716l2.026 10.576c1.711 9.216 3.149 18.283 8.494 26.599 6.393 9.946 11.348 20.815 16.943 31.276 4.021 7.519 6.199 16.075 12.925 22.065l24.462 22.26c.556.503 1.507.571 2.274.841z\"></path><path fill=\"#ea5b15\" d=\"M1285.092 163.432c9.165 3.148 18.419 6.374 27.279 10.459 4.871 2.246 8.838 6.406 13.646 8.851 5.446 2.77 11.801 3.874 17.011 6.965 11.514 6.831 24.097 9.942 36.968 12.471 1.78.35 3.777.576 5.213 1.542 10.784 7.255 23.448 9.114 35.622 11.834 9.977 2.23 18.529 6.703 26.988 11.898 5.233 3.214 10.76 5.983 15.798 9.468 4.14 2.864 7.962 6.279 11.551 9.827 5.076 5.02 10.056 10.181 14.624 15.658 5.822 6.98 11.119 14.395 16.78 21.513 4.531 5.698 9.267 11.233 14.222 16.987-10.005 5.806-20.07 12.004-30.719 16.943-7.694 3.569-16.163 5.464-24.688 7.669-2.878-7.088-5.352-13.741-7.833-20.392-.802-2.15-1.244-4.55-2.498-6.396-4.548-6.7-9.712-12.999-14.011-19.847-6.672-10.627-15.34-18.93-26.063-25.376-9.357-5.625-18.367-11.824-27.644-17.587-6.436-3.997-12.902-8.006-19.659-11.405-5.123-2.577-11.107-3.536-16.046-6.37-17.187-9.863-35.13-17.887-54.031-23.767-4.403-1.37-8.953-2.267-13.436-3.382l.926-27.565z\"></path><path fill=\"#ea504b\" d=\"M1098 737l7.789 16.893c-15.04 9.272-31.679 15.004-49.184 17.995-9.464 1.617-19.122 2.097-29.151 3.019-.457-10.636-.18-21.211-.544-31.764-.273-7.888-.409-15.883-4.736-23.103-1.16-1.936-1.162-4.805-1.06-7.219l1.787-36.207c.182-8.103-.993-16.237-.811-24.34.365-16.236 1.253-32.461 1.908-48.69.484-12 .942-24.001 1.98-36.069 5.57 10.19 10.632 20.42 15.528 30.728 1.122 2.362 2.587 5.09 2.339 7.488-1.536 14.819 5.881 26.839 12.962 38.33 10.008 16.241 16.417 33.54 20.331 51.964 2.285 10.756 4.729 21.394 11.958 30.165L1098 737z\"></path><path fill=\"#f6a320\" d=\"M1865.78 822.529c-1.849 8.846-3.544 17.475-6.224 25.786-1.323 4.102-4.206 7.81-6.83 11.367l-43.112 57.581c-4.93 6.273-11.808 11.049-17.954 16.326-10.162 8.725-21.082 16.671-30.534 26.097-12.327 12.294-27.997 16.202-43.818 20.323-1.436.374-2.897.651-4.744.986-1.107-17.032-1.816-34.076-2.079-51.556 1.265-.535 2.183-.428 2.888-.766 10.596-5.072 20.8-11.059 32.586-13.273 1.69-.317 3.307-1.558 4.732-2.662l26.908-21.114c4.992-4.003 11.214-7.393 14.381-12.585 11.286-18.5 22.363-37.263 27.027-58.87l36.046 1.811c3.487.165 6.983.14 10.727.549z\"></path><path fill=\"#ec6333\" d=\"M318.448 922.814c-6.374-2.074-12.56-4.058-18.412-6.765-8.379-3.876-16.906-7.675-24.617-12.668-5.239-3.392-9.69-8.381-13.609-13.352-7.87-9.983-14.953-20.582-22.699-30.666-8.061-10.493-13.909-22.097-18.636-34.358-.595-1.543-1.486-2.972-2.382-4.783 6.84-1.598 13.797-3.023 20.807-4.106 18.852-2.912 36.433-9.493 53.737-17.819.697.888.889 1.555 1.292 2.051l17.921 21.896c4.14 4.939 8.06 10.191 12.862 14.412 5.67 4.984 12.185 9.007 18.334 13.447-8.937 16.282-16.422 33.178-20.696 51.31-1.638 6.951-2.402 14.107-3.903 21.403z\"></path><path fill=\"#f49700\" d=\"M623.467 326.903c2.893-10.618 5.584-21.446 9.833-31.623 3.013-7.217 7.924-13.696 12.358-20.254 6.375-9.43 12.026-19.67 19.886-27.705 14.12-14.434 28.063-29.453 47.926-36.784 6.581-2.429 12.344-6.994 18.774-9.942 3.975-1.822 8.503-2.436 13.186-3.592 1.947 18.557 3.248 37.15 8.307 55.686-15.453 7.931-28.853 18.092-40.46 29.996-10.417 10.683-19.109 23.111-28.013 35.175-3.238 4.388-4.888 9.948-7.262 14.973-17.803-3.987-35.767-6.498-54.535-5.931z\"></path><path fill=\"#ea544c\" d=\"M1097.956 736.615c-2.925-3.218-5.893-6.822-8.862-10.425-7.229-8.771-9.672-19.409-11.958-30.165-3.914-18.424-10.323-35.722-20.331-51.964-7.081-11.491-14.498-23.511-12.962-38.33.249-2.398-1.217-5.126-2.339-7.488l-15.232-31.019-3.103-34.338c-.107-1.316-.041-2.653.031-3.975.233-4.294.756-8.59.702-12.879-.072-5.713-.776-11.417-.861-17.13l-.116-30.733c-.329-10.088-1.926-20.166-1.768-30.23.23-14.674.599-29.31-1.162-44.341 9.369-.803 18.741-1.179 28.558-1.074 1.446 15.814 2.446 31.146 3.446 46.478.108 6.163-.064 12.348.393 18.485.613 8.225 1.907 16.397 2.564 24.62l2.9 42.8c.286 6.088-.869 12.234-.782 18.344.142 9.91 1.066 19.661 4.688 29.163 1.827 4.794 1.657 10.377 2.21 15.623l4.092 39.927c.172 1.617.417 3.71 1.465 4.67 5.839 5.357 6.8 12.773 7.114 19.644.749 16.374 6.749 31.233 11.349 46.562 3.704 12.344 6.642 24.918 9.963 37.775z\"></path><path fill=\"#ec5c61\" d=\"M1204.835 568.008c1.254 25.351-1.675 50.16-10.168 74.61-8.598-4.883-18.177-8.709-24.354-15.59-7.44-8.289-13.929-17.442-21.675-25.711-8.498-9.072-16.731-18.928-21.084-31.113-.54-1.513-1.691-2.807-2.594-4.564-4.605-9.247-7.706-18.544-7.96-29.09-.835-7.149-1.214-13.944-2.609-20.523-2.215-10.454-5.626-20.496-7.101-31.302-2.513-18.419-7.207-36.512-5.347-55.352.24-2.43-.17-4.949-.477-7.402l-4.468-34.792c2.723-.379 5.446-.757 8.585-.667 1.749 8.781 2.952 17.116 4.448 25.399 1.813 10.037 3.64 20.084 5.934 30.017 1.036 4.482 3.953 8.573 4.73 13.064 1.794 10.377 4.73 20.253 9.272 29.771 2.914 6.105 4.761 12.711 7.496 18.912 2.865 6.496 6.264 12.755 9.35 19.156 3.764 7.805 7.667 15.013 16.1 19.441 7.527 3.952 13.713 10.376 20.983 14.924 6.636 4.152 13.932 7.25 20.937 10.813z\"></path><path fill=\"#ed676f\" d=\"M1140.75 379.231c18.38-4.858 36.222-11.21 53.979-18.971 3.222 3.368 5.693 6.744 8.719 9.512 2.333 2.134 5.451 5.07 8.067 4.923 7.623-.429 12.363 2.688 17.309 8.215 5.531 6.18 12.744 10.854 19.224 16.184-5.121 7.193-10.461 14.241-15.323 21.606-13.691 20.739-22.99 43.255-26.782 67.926-.543 3.536-1.281 7.043-2.366 10.925-14.258-6.419-26.411-14.959-32.731-29.803-1.087-2.553-2.596-4.93-3.969-7.355-1.694-2.993-3.569-5.89-5.143-8.943-1.578-3.062-2.922-6.249-4.295-9.413-1.57-3.621-3.505-7.163-4.47-10.946-1.257-4.93-.636-10.572-2.725-15.013-5.831-12.397-7.467-25.628-9.497-38.847z\"></path><path fill=\"#ed656e\" d=\"M1254.103 647.439c5.325.947 10.603 2.272 15.847 3.722 5.101 1.41 10.376 2.475 15.175 4.596 3.237 1.431 5.942 4.262 8.589 6.777 2.592 2.462 4.77 5.355 7.207 7.987 1.804 1.948 4.557 3.453 5.461 5.723 3.51 8.817 11.581 11.307 19.059 14.735 1.053.483 2.116.963 3.214 1.327 9.172 3.043 13.818 8.587 14.889 18.979.715 6.935 5.607 13.679 9.479 19.987 4.623 7.533 9.175 14.819 9.091 24.116-.023 2.55 1.21 5.111 1.874 8.055-19.861 2.555-39.795 4.296-59.597 9.09l-11.596-23.203c-1.107-2.169-2.526-4.353-4.307-5.975-7.349-6.694-14.863-13.209-22.373-19.723l-17.313-14.669c-2.776-2.245-5.935-4.017-8.92-6.003l11.609-38.185c1.508-5.453 1.739-11.258 2.613-17.336z\"></path><path fill=\"#ec6168\" d=\"M1140.315 379.223c2.464 13.227 4.101 26.459 9.931 38.856 2.089 4.441 1.468 10.083 2.725 15.013.965 3.783 2.9 7.325 4.47 10.946 1.372 3.164 2.716 6.351 4.295 9.413 1.574 3.053 3.449 5.95 5.143 8.943 1.372 2.425 2.882 4.803 3.969 7.355 6.319 14.844 18.473 23.384 32.641 30.212.067 5.121-.501 10.201-.435 15.271l.985 38.117c.151 4.586.616 9.162.868 14.201-7.075-3.104-14.371-6.202-21.007-10.354-7.269-4.548-13.456-10.972-20.983-14.924-8.434-4.428-12.337-11.637-16.1-19.441-3.087-6.401-6.485-12.66-9.35-19.156-2.735-6.201-4.583-12.807-7.496-18.912-4.542-9.518-7.477-19.394-9.272-29.771-.777-4.491-3.694-8.581-4.73-13.064-2.294-9.933-4.121-19.98-5.934-30.017-1.496-8.283-2.699-16.618-4.036-25.335 10.349-2.461 20.704-4.511 31.054-6.582.957-.191 1.887-.515 3.264-.769z\"></path><path fill=\"#e94c28\" d=\"M922 537c-6.003 11.784-11.44 23.81-19.66 34.428-6.345 8.196-11.065 17.635-17.206 26.008-4.339 5.916-9.828 10.992-14.854 16.397-.776.835-1.993 1.279-2.71 2.147-9.439 11.437-22.008 18.427-35.357 24.929-4.219-10.885-6.942-22.155-7.205-33.905l-.514-49.542c7.441-2.893 14.452-5.197 21.334-7.841 1.749-.672 3.101-2.401 4.604-3.681 6.749-5.745 12.845-12.627 20.407-16.944 7.719-4.406 14.391-9.101 18.741-16.889.626-1.122 1.689-2.077 2.729-2.877 7.197-5.533 12.583-12.51 16.906-20.439.68-1.247 2.495-1.876 4.105-2.651 2.835 1.408 5.267 2.892 7.884 3.892 3.904 1.491 4.392 3.922 2.833 7.439-1.47 3.318-2.668 6.756-4.069 10.106-1.247 2.981-.435 5.242 2.413 6.544 2.805 1.282 3.125 3.14 1.813 5.601l-6.907 12.799L922 537z\"></path><path fill=\"#eb5659\" d=\"M1124.995 566c.868 1.396 2.018 2.691 2.559 4.203 4.353 12.185 12.586 22.041 21.084 31.113 7.746 8.269 14.235 17.422 21.675 25.711 6.176 6.881 15.756 10.707 24.174 15.932-6.073 22.316-16.675 42.446-31.058 60.937-1.074-.131-2.025-.199-2.581-.702l-24.462-22.26c-6.726-5.99-8.904-14.546-12.925-22.065-5.594-10.461-10.55-21.33-16.943-31.276-5.345-8.315-6.783-17.383-8.494-26.599-.63-3.394-1.348-6.772-1.738-10.848-.371-6.313-1.029-11.934-1.745-18.052l6.34 4.04 1.288-.675-2.143-15.385 9.454 1.208v-8.545L1124.995 566z\"></path><path fill=\"#f5a02d\" d=\"M1818.568 820.096c-4.224 21.679-15.302 40.442-26.587 58.942-3.167 5.192-9.389 8.582-14.381 12.585l-26.908 21.114c-1.425 1.104-3.042 2.345-4.732 2.662-11.786 2.214-21.99 8.201-32.586 13.273-.705.338-1.624.231-2.824.334a824.35 824.35 0 0 1-8.262-42.708c4.646-2.14 9.353-3.139 13.269-5.47 5.582-3.323 11.318-6.942 15.671-11.652 7.949-8.6 14.423-18.572 22.456-27.081 8.539-9.046 13.867-19.641 18.325-30.922l46.559 8.922z\"></path><path fill=\"#eb5a57\" d=\"M1124.96 565.639c-5.086-4.017-10.208-8.395-15.478-12.901v8.545l-9.454-1.208 2.143 15.385-1.288.675-6.34-4.04c.716 6.118 1.375 11.74 1.745 17.633-4.564-6.051-9.544-11.649-10.663-20.025-.954-7.141-4.892-13.843-7.121-20.863-3.344-10.533-5.421-21.57-9.732-31.669-5.181-12.135-3.506-25.125-6.728-37.355-2.099-7.968-5.317-15.646-7.324-23.632-1.353-5.384-1.47-11.078-2.429-16.909l-3.294-46.689a278.63 278.63 0 0 1 27.57-2.084c2.114 12.378 3.647 24.309 5.479 36.195 1.25 8.111 2.832 16.175 4.422 24.23 1.402 7.103 2.991 14.169 4.55 21.241 1.478 6.706.273 14.002 4.6 20.088 5.401 7.597 7.176 16.518 9.467 25.337 1.953 7.515 5.804 14.253 11.917 19.406.254 10.095 3.355 19.392 7.96 28.639z\"></path><path fill=\"#ea541c\" d=\"M911.651 810.999c-2.511 10.165-5.419 20.146-8.2 30.162-2.503 9.015-7.37 16.277-14.364 22.612-6.108 5.533-10.917 12.475-16.796 18.293-6.942 6.871-14.354 13.24-19.083 22.03-.644 1.196-2.222 1.889-3.705 2.857-2.39-7.921-4.101-15.991-6.566-23.823-5.451-17.323-12.404-33.976-23.414-48.835l21.627-21.095c3.182-3.29 5.532-7.382 8.295-11.083l10.663-14.163c9.528 4.78 18.925 9.848 28.625 14.247 7.324 3.321 15.036 5.785 22.917 8.799z\"></path><path fill=\"#eb5d19\" d=\"M1284.092 191.421c4.557.69 9.107 1.587 13.51 2.957 18.901 5.881 36.844 13.904 54.031 23.767 4.938 2.834 10.923 3.792 16.046 6.37 6.757 3.399 13.224 7.408 19.659 11.405l27.644 17.587c10.723 6.446 19.392 14.748 26.063 25.376 4.299 6.848 9.463 13.147 14.011 19.847 1.254 1.847 1.696 4.246 2.498 6.396l7.441 20.332c-11.685 1.754-23.379 3.133-35.533 4.037-.737-2.093-.995-3.716-1.294-5.33-3.157-17.057-14.048-30.161-23.034-44.146-3.027-4.71-7.786-8.529-12.334-11.993-9.346-7.116-19.004-13.834-28.688-20.491-6.653-4.573-13.311-9.251-20.431-13.002-8.048-4.24-16.479-7.85-24.989-11.091-11.722-4.465-23.673-8.328-35.527-12.449l.927-19.572z\"></path><path fill=\"#eb5e24\" d=\"M1283.09 211.415c11.928 3.699 23.88 7.562 35.602 12.027 8.509 3.241 16.941 6.852 24.989 11.091 7.12 3.751 13.778 8.429 20.431 13.002 9.684 6.657 19.342 13.375 28.688 20.491 4.548 3.463 9.307 7.283 12.334 11.993 8.986 13.985 19.877 27.089 23.034 44.146.299 1.615.557 3.237.836 5.263-13.373-.216-26.749-.839-40.564-1.923-2.935-9.681-4.597-18.92-12.286-26.152-15.577-14.651-30.4-30.102-45.564-45.193-.686-.683-1.626-1.156-2.516-1.584l-47.187-22.615 2.203-20.546z\"></path><path fill=\"#e9511f\" d=\"M913 486.001c-1.29.915-3.105 1.543-3.785 2.791-4.323 7.929-9.709 14.906-16.906 20.439-1.04.8-2.103 1.755-2.729 2.877-4.35 7.788-11.022 12.482-18.741 16.889-7.562 4.317-13.658 11.199-20.407 16.944-1.503 1.28-2.856 3.009-4.604 3.681-6.881 2.643-13.893 4.948-21.262 7.377-.128-11.151.202-22.302.378-33.454.03-1.892-.6-3.795-.456-6.12 13.727-1.755 23.588-9.527 33.278-17.663 2.784-2.337 6.074-4.161 8.529-6.784l29.057-31.86c1.545-1.71 3.418-3.401 4.221-5.459 5.665-14.509 11.49-28.977 16.436-43.736 2.817-8.407 4.074-17.338 6.033-26.032 5.039.714 10.078 1.427 15.536 2.629-.909 8.969-2.31 17.438-3.546 25.931-2.41 16.551-5.84 32.839-11.991 48.461L913 486.001z\"></path><path fill=\"#ea5741\" d=\"M1179.451 903.828c-14.224-5.787-27.726-12.171-37.235-24.849-5.841-7.787-12.09-15.436-19.146-22.099-7.259-6.854-12.136-14.667-15.035-24.049-1.748-5.654-3.938-11.171-6.254-17.033 15.099-4.009 30.213-8.629 44.958-15.533l28.367 36.36c6.09 8.015 13.124 14.75 22.72 18.375-7.404 14.472-13.599 29.412-17.48 45.244-.271 1.106-.382 2.25-.895 3.583z\"></path><path fill=\"#ea522a\" d=\"M913.32 486.141c2.693-7.837 5.694-15.539 8.722-23.231 6.151-15.622 9.581-31.91 11.991-48.461l3.963-25.861c7.582.317 15.168 1.031 22.748 1.797 4.171.421 8.333.928 12.877 1.596-.963 11.836-.398 24.125-4.102 34.953-5.244 15.33-6.794 31.496-12.521 46.578-2.692 7.09-4.849 14.445-8.203 21.206-4.068 8.201-9.311 15.81-13.708 23.86-1.965 3.597-3.154 7.627-4.609 11.492-1.385 3.68-3.666 6.265-8.114 6.89-1.994-1.511-3.624-3.059-5.077-4.44l6.907-12.799c1.313-2.461.993-4.318-1.813-5.601-2.849-1.302-3.66-3.563-2.413-6.544 1.401-3.35 2.599-6.788 4.069-10.106 1.558-3.517 1.071-5.948-2.833-7.439-2.617-1-5.049-2.484-7.884-3.892z\"></path><path fill=\"#eb5e24\" d=\"M376.574 714.118c12.053 6.538 20.723 16.481 29.081 26.814 1.945 2.404 4.537 4.352 7.047 6.218 8.24 6.125 10.544 15.85 14.942 24.299.974 1.871 1.584 3.931 2.376 6.29-7.145 3.719-14.633 6.501-21.386 10.517-9.606 5.713-18.673 12.334-28.425 18.399-3.407-3.73-6.231-7.409-9.335-10.834l-30.989-33.862c11.858-11.593 22.368-24.28 31.055-38.431 1.86-3.031 3.553-6.164 5.632-9.409z\"></path><path fill=\"#e95514\" d=\"M859.962 787.636c-3.409 5.037-6.981 9.745-10.516 14.481-2.763 3.701-5.113 7.792-8.295 11.083-6.885 7.118-14.186 13.834-21.65 20.755-13.222-17.677-29.417-31.711-48.178-42.878-.969-.576-2.068-.934-3.27-1.709 6.28-8.159 12.733-15.993 19.16-23.849 1.459-1.783 2.718-3.738 4.254-5.448l18.336-19.969c4.909 5.34 9.619 10.738 14.081 16.333 9.72 12.19 21.813 21.566 34.847 29.867.411.262.725.674 1.231 1.334z\"></path><path fill=\"#eb5f2d\" d=\"M339.582 762.088l31.293 33.733c3.104 3.425 5.928 7.104 9.024 10.979-12.885 11.619-24.548 24.139-33.899 38.704-.872 1.359-1.56 2.837-2.644 4.428-6.459-4.271-12.974-8.294-18.644-13.278-4.802-4.221-8.722-9.473-12.862-14.412l-17.921-21.896c-.403-.496-.595-1.163-.926-2.105 16.738-10.504 32.58-21.87 46.578-36.154z\"></path><path fill=\"#f28d00\" d=\"M678.388 332.912c1.989-5.104 3.638-10.664 6.876-15.051 8.903-12.064 17.596-24.492 28.013-35.175 11.607-11.904 25.007-22.064 40.507-29.592 4.873 11.636 9.419 23.412 13.67 35.592-5.759 4.084-11.517 7.403-16.594 11.553-4.413 3.607-8.124 8.092-12.023 12.301-5.346 5.772-10.82 11.454-15.782 17.547-3.929 4.824-7.17 10.208-10.716 15.344l-33.95-12.518z\"></path><path fill=\"#f08369\" d=\"M1580.181 771.427c-.191-.803-.322-1.377-.119-1.786 5.389-10.903 9.084-22.666 18.181-31.587 6.223-6.103 11.276-13.385 17.286-19.727 3.117-3.289 6.933-6.105 10.869-8.384 6.572-3.806 13.492-7.009 20.461-10.752 1.773 3.23 3.236 6.803 4.951 10.251l12.234 24.993c-1.367 1.966-2.596 3.293-3.935 4.499-7.845 7.07-16.315 13.564-23.407 21.32-6.971 7.623-12.552 16.517-18.743 24.854l-37.777-13.68z\"></path><path fill=\"#f18b5e\" d=\"M1618.142 785.4c6.007-8.63 11.588-17.524 18.559-25.147 7.092-7.755 15.562-14.249 23.407-21.32 1.338-1.206 2.568-2.534 3.997-4.162l28.996 33.733c1.896 2.205 4.424 3.867 6.66 6.394-6.471 7.492-12.967 14.346-19.403 21.255l-18.407 19.953c-12.958-12.409-27.485-22.567-43.809-30.706z\"></path><path fill=\"#f49c3a\" d=\"M1771.617 811.1c-4.066 11.354-9.394 21.949-17.933 30.995-8.032 8.509-14.507 18.481-22.456 27.081-4.353 4.71-10.089 8.329-15.671 11.652-3.915 2.331-8.623 3.331-13.318 5.069-4.298-9.927-8.255-19.998-12.1-30.743 4.741-4.381 9.924-7.582 13.882-11.904 7.345-8.021 14.094-16.603 20.864-25.131 4.897-6.168 9.428-12.626 14.123-18.955l32.61 11.936z\"></path><path fill=\"#f08000\" d=\"M712.601 345.675c3.283-5.381 6.524-10.765 10.453-15.589 4.962-6.093 10.435-11.774 15.782-17.547 3.899-4.21 7.61-8.695 12.023-12.301 5.078-4.15 10.836-7.469 16.636-11.19a934.12 934.12 0 0 1 23.286 35.848c-4.873 6.234-9.676 11.895-14.63 17.421l-25.195 27.801c-11.713-9.615-24.433-17.645-38.355-24.443z\"></path><path fill=\"#ed6e04\" d=\"M751.11 370.42c8.249-9.565 16.693-18.791 25.041-28.103 4.954-5.526 9.757-11.187 14.765-17.106 7.129 6.226 13.892 13.041 21.189 19.225 5.389 4.567 11.475 8.312 17.53 12.92-5.51 7.863-10.622 15.919-17.254 22.427-8.881 8.716-18.938 16.233-28.49 24.264-5.703-6.587-11.146-13.427-17.193-19.682-4.758-4.921-10.261-9.121-15.587-13.944z\"></path><path fill=\"#ea541c\" d=\"M921.823 385.544c-1.739 9.04-2.995 17.971-5.813 26.378-4.946 14.759-10.771 29.227-16.436 43.736-.804 2.058-2.676 3.749-4.221 5.459l-29.057 31.86c-2.455 2.623-5.745 4.447-8.529 6.784-9.69 8.135-19.551 15.908-33.208 17.237-1.773-9.728-3.147-19.457-4.091-29.6l36.13-16.763c.581-.267 1.046-.812 1.525-1.269 8.033-7.688 16.258-15.19 24.011-23.152 4.35-4.467 9.202-9.144 11.588-14.69 6.638-15.425 15.047-30.299 17.274-47.358 3.536.344 7.072.688 10.829 1.377z\"></path><path fill=\"#f3944d\" d=\"M1738.688 798.998c-4.375 6.495-8.906 12.953-13.803 19.121-6.771 8.528-13.519 17.11-20.864 25.131-3.958 4.322-9.141 7.523-13.925 11.54-8.036-13.464-16.465-26.844-27.999-38.387 5.988-6.951 12.094-13.629 18.261-20.25l19.547-20.95 38.783 23.794z\"></path><path fill=\"#ec6168\" d=\"M1239.583 703.142c3.282 1.805 6.441 3.576 9.217 5.821 5.88 4.755 11.599 9.713 17.313 14.669l22.373 19.723c1.781 1.622 3.2 3.806 4.307 5.975 3.843 7.532 7.477 15.171 11.194 23.136-10.764 4.67-21.532 8.973-32.69 12.982l-22.733-27.366c-2.003-2.416-4.096-4.758-6.194-7.093-3.539-3.94-6.927-8.044-10.74-11.701-2.57-2.465-5.762-4.283-8.675-6.39l16.627-29.755z\"></path><path fill=\"#ec663e\" d=\"M1351.006 332.839l-28.499 10.33c-.294.107-.533.367-1.194.264-11.067-19.018-27.026-32.559-44.225-44.855-4.267-3.051-8.753-5.796-13.138-8.682l9.505-24.505c10.055 4.069 19.821 8.227 29.211 13.108 3.998 2.078 7.299 5.565 10.753 8.598 3.077 2.701 5.743 5.891 8.926 8.447 4.116 3.304 9.787 5.345 12.62 9.432 6.083 8.777 10.778 18.517 16.041 27.863z\"></path><path fill=\"#eb5e5b\" d=\"M1222.647 733.051c3.223 1.954 6.415 3.771 8.985 6.237 3.813 3.658 7.201 7.761 10.74 11.701l6.194 7.093 22.384 27.409c-13.056 6.836-25.309 14.613-36.736 24.161l-39.323-44.7 24.494-27.846c1.072-1.224 1.974-2.598 3.264-4.056z\"></path><path fill=\"#ea580e\" d=\"M876.001 376.171c5.874 1.347 11.748 2.694 17.812 4.789-.81 5.265-2.687 9.791-2.639 14.296.124 11.469-4.458 20.383-12.73 27.863-2.075 1.877-3.659 4.286-5.668 6.248l-22.808 21.967c-.442.422-1.212.488-1.813.757l-23.113 10.389-9.875 4.514c-2.305-6.09-4.609-12.181-6.614-18.676 7.64-4.837 15.567-8.54 22.18-13.873 9.697-7.821 18.931-16.361 27.443-25.455 5.613-5.998 12.679-11.331 14.201-20.475.699-4.2 2.384-8.235 3.623-12.345z\"></path><path fill=\"#e95514\" d=\"M815.103 467.384c3.356-1.894 6.641-3.415 9.94-4.903l23.113-10.389c.6-.269 1.371-.335 1.813-.757l22.808-21.967c2.008-1.962 3.593-4.371 5.668-6.248 8.272-7.48 12.854-16.394 12.73-27.863-.049-4.505 1.828-9.031 2.847-13.956 5.427.559 10.836 1.526 16.609 2.68-1.863 17.245-10.272 32.119-16.91 47.544-2.387 5.546-7.239 10.223-11.588 14.69-7.753 7.962-15.978 15.464-24.011 23.152-.478.458-.944 1.002-1.525 1.269l-36.069 16.355c-2.076-6.402-3.783-12.81-5.425-19.607z\"></path><path fill=\"#eb620b\" d=\"M783.944 404.402c9.499-8.388 19.556-15.905 28.437-24.621 6.631-6.508 11.744-14.564 17.575-22.273 9.271 4.016 18.501 8.375 27.893 13.43-4.134 7.07-8.017 13.778-12.833 19.731-5.785 7.15-12.109 13.917-18.666 20.376-7.99 7.869-16.466 15.244-24.731 22.832l-17.674-29.475z\"></path><path fill=\"#ea544c\" d=\"M1197.986 854.686c-9.756-3.309-16.79-10.044-22.88-18.059l-28.001-36.417c8.601-5.939 17.348-11.563 26.758-17.075 1.615 1.026 2.639 1.876 3.505 2.865l26.664 30.44c3.723 4.139 7.995 7.785 12.017 11.656l-18.064 26.591z\"></path><path fill=\"#ec6333\" d=\"M1351.41 332.903c-5.667-9.409-10.361-19.149-16.445-27.926-2.833-4.087-8.504-6.128-12.62-9.432-3.184-2.555-5.849-5.745-8.926-8.447-3.454-3.033-6.756-6.52-10.753-8.598-9.391-4.88-19.157-9.039-29.138-13.499 1.18-5.441 2.727-10.873 4.81-16.607 11.918 4.674 24.209 8.261 34.464 14.962 14.239 9.304 29.011 18.453 39.595 32.464 2.386 3.159 5.121 6.077 7.884 8.923 6.564 6.764 10.148 14.927 11.723 24.093l-20.594 4.067z\"></path><path fill=\"#eb5e5b\" d=\"M1117 536.549c-6.113-4.702-9.965-11.44-11.917-18.955-2.292-8.819-4.066-17.74-9.467-25.337-4.327-6.085-3.122-13.382-4.6-20.088l-4.55-21.241c-1.59-8.054-3.172-16.118-4.422-24.23l-5.037-36.129c6.382-1.43 12.777-2.462 19.582-3.443 1.906 11.646 3.426 23.24 4.878 34.842.307 2.453.717 4.973.477 7.402-1.86 18.84 2.834 36.934 5.347 55.352 1.474 10.806 4.885 20.848 7.101 31.302 1.394 6.579 1.774 13.374 2.609 20.523z\"></path><path fill=\"#ec644b\" d=\"M1263.638 290.071c4.697 2.713 9.183 5.458 13.45 8.509 17.199 12.295 33.158 25.836 43.873 44.907-8.026 4.725-16.095 9.106-24.83 13.372-11.633-15.937-25.648-28.515-41.888-38.689-1.609-1.008-3.555-1.48-5.344-2.2 2.329-3.852 4.766-7.645 6.959-11.573l7.78-14.326z\"></path><path fill=\"#eb5f2d\" d=\"M1372.453 328.903c-2.025-9.233-5.608-17.396-12.172-24.16-2.762-2.846-5.498-5.764-7.884-8.923-10.584-14.01-25.356-23.16-39.595-32.464-10.256-6.701-22.546-10.289-34.284-15.312.325-5.246 1.005-10.444 2.027-15.863l47.529 22.394c.89.428 1.83.901 2.516 1.584l45.564 45.193c7.69 7.233 9.352 16.472 11.849 26.084-5.032.773-10.066 1.154-15.55 1.466z\"></path><path fill=\"#e95a0f\" d=\"M801.776 434.171c8.108-7.882 16.584-15.257 24.573-23.126 6.558-6.459 12.881-13.226 18.666-20.376 4.817-5.953 8.7-12.661 13.011-19.409 5.739 1.338 11.463 3.051 17.581 4.838-.845 4.183-2.53 8.219-3.229 12.418-1.522 9.144-8.588 14.477-14.201 20.475-8.512 9.094-17.745 17.635-27.443 25.455-6.613 5.333-14.54 9.036-22.223 13.51-2.422-4.469-4.499-8.98-6.735-13.786z\"></path><path fill=\"#eb5e5b\" d=\"M1248.533 316.002c2.155.688 4.101 1.159 5.71 2.168 16.24 10.174 30.255 22.752 41.532 38.727-7.166 5.736-14.641 11.319-22.562 16.731-1.16-1.277-1.684-2.585-2.615-3.46l-38.694-36.2 14.203-15.029c.803-.86 1.38-1.93 2.427-2.936z\"></path><path fill=\"#eb5a57\" d=\"M1216.359 827.958c-4.331-3.733-8.603-7.379-12.326-11.518l-26.664-30.44c-.866-.989-1.89-1.839-3.152-2.902 6.483-6.054 13.276-11.959 20.371-18.005l39.315 44.704c-5.648 6.216-11.441 12.12-17.544 18.161z\"></path><path fill=\"#ec6168\" d=\"M1231.598 334.101l38.999 36.066c.931.876 1.456 2.183 2.303 3.608-4.283 4.279-8.7 8.24-13.769 12.091-4.2-3.051-7.512-6.349-11.338-8.867-12.36-8.136-22.893-18.27-32.841-29.093l16.646-13.805z\"></path><path fill=\"#ed656e\" d=\"M1214.597 347.955c10.303 10.775 20.836 20.908 33.196 29.044 3.825 2.518 7.137 5.816 10.992 8.903-3.171 4.397-6.65 8.648-10.432 13.046-6.785-5.184-13.998-9.858-19.529-16.038-4.946-5.527-9.687-8.644-17.309-8.215-2.616.147-5.734-2.788-8.067-4.923-3.026-2.769-5.497-6.144-8.35-9.568 6.286-4.273 12.715-8.237 19.499-12.25z\"></path></svg>\n</p>\n\n<p align=\"center\">\n<b>The crispy rerank family from <a href=\"https://mixedbread.ai\"><b>Mixedbread</b></a>.</b>\n</p>\n\n<p align=\"center\">\n<sup> 🍞 Looking for a simple end-to-end retrieval solution? Meet Omni, our multimodal and multilingual model. <a href=\"https://mixedbread.com\"><b>Get in touch for access.</a> </sup>\n</p>\n\n# mxbai-rerank-base-v1\n\nThis is the base model in our family of powerful reranker models. You can learn more about the models in our [blog post](https://www.mixedbread.ai/blog/mxbai-rerank-v1).\n\nWe have three models:\n\n- [mxbai-rerank-xsmall-v1](https://huggingface.co/mixedbread-ai/mxbai-rerank-xsmall-v1)\n- [mxbai-rerank-base-v1](https://huggingface.co/mixedbread-ai/mxbai-rerank-base-v1) (🍞)\n- [mxbai-rerank-large-v1](https://huggingface.co/mixedbread-ai/mxbai-rerank-large-v1)\n\n## Quickstart\n\nCurrently, the best way to use our models is with the most recent version of sentence-transformers.\n\n`pip install -U sentence-transformers`\n\nLet's say you have a query, and you want to rerank a set of documents. You can do that with only one line of code:\n\n```python\nfrom sentence_transformers import CrossEncoder\n\n# Load the model, here we use our base sized model\nmodel = CrossEncoder(\"mixedbread-ai/mxbai-rerank-base-v1\")\n\n\n# Example query and documents\nquery = \"Who wrote 'To Kill a Mockingbird'?\"\ndocuments = [\n    \"'To Kill a Mockingbird' is a novel by Harper Lee published in 1960. It was immediately successful, winning the Pulitzer Prize, and has become a classic of modern American literature.\",\n    \"The novel 'Moby-Dick' was written by Herman Melville and first published in 1851. It is considered a masterpiece of American literature and deals with complex themes of obsession, revenge, and the conflict between good and evil.\",\n    \"Harper Lee, an American novelist widely known for her novel 'To Kill a Mockingbird', was born in 1926 in Monroeville, Alabama. She received the Pulitzer Prize for Fiction in 1961.\",\n    \"Jane Austen was an English novelist known primarily for her six major novels, which interpret, critique and comment upon the British landed gentry at the end of the 18th century.\",\n    \"The 'Harry Potter' series, which consists of seven fantasy novels written by British author J.K. Rowling, is among the most popular and critically acclaimed books of the modern era.\",\n    \"'The Great Gatsby', a novel written by American author F. Scott Fitzgerald, was published in 1925. The story is set in the Jazz Age and follows the life of millionaire Jay Gatsby and his pursuit of Daisy Buchanan.\"\n]\n\n# Lets get the scores\nresults = model.rank(query, documents, return_documents=True, top_k=3)\n```\n\n\n<details>\n  <summary>JavaScript Example</summary>\n\nInstall [transformers.js](https://github.com/xenova/transformers.js)\n\n`npm i @xenova/transformers`\n\nLet's say you have a query, and you want to rerank a set of documents. In JavaScript, you need to add a function:\n\n```javascript\nimport { AutoTokenizer, AutoModelForSequenceClassification } from '@xenova/transformers';\n\nconst model_id = 'mixedbread-ai/mxbai-rerank-base-v1';\nconst model = await AutoModelForSequenceClassification.from_pretrained(model_id);\nconst tokenizer = await AutoTokenizer.from_pretrained(model_id);\n\n/**\n * Performs ranking with the CrossEncoder on the given query and documents. Returns a sorted list with the document indices and scores.\n * @param {string} query A single query\n * @param {string[]} documents A list of documents\n * @param {Object} options Options for ranking\n * @param {number} [options.top_k=undefined] Return the top-k documents. If undefined, all documents are returned.\n * @param {number} [options.return_documents=false] If true, also returns the documents. If false, only returns the indices and scores.\n */\nasync function rank(query, documents, {\n    top_k = undefined,\n    return_documents = false,\n} = {}) {\n    const inputs = tokenizer(\n        new Array(documents.length).fill(query),\n        {\n            text_pair: documents,\n            padding: true,\n            truncation: true,\n        }\n    )\n    const { logits } = await model(inputs);\n    return logits\n        .sigmoid()\n        .tolist()\n        .map(([score], i) => ({\n            corpus_id: i,\n            score,\n            ...(return_documents ? { text: documents[i] } : {})\n        }))\n        .sort((a, b) => b.score - a.score)\n        .slice(0, top_k);\n}\n\n// Example usage:\nconst query = \"Who wrote 'To Kill a Mockingbird'?\"\nconst documents = [\n    \"'To Kill a Mockingbird' is a novel by Harper Lee published in 1960. It was immediately successful, winning the Pulitzer Prize, and has become a classic of modern American literature.\",\n    \"The novel 'Moby-Dick' was written by Herman Melville and first published in 1851. It is considered a masterpiece of American literature and deals with complex themes of obsession, revenge, and the conflict between good and evil.\",\n    \"Harper Lee, an American novelist widely known for her novel 'To Kill a Mockingbird', was born in 1926 in Monroeville, Alabama. She received the Pulitzer Prize for Fiction in 1961.\",\n    \"Jane Austen was an English novelist known primarily for her six major novels, which interpret, critique and comment upon the British landed gentry at the end of the 18th century.\",\n    \"The 'Harry Potter' series, which consists of seven fantasy novels written by British author J.K. Rowling, is among the most popular and critically acclaimed books of the modern era.\",\n    \"'The Great Gatsby', a novel written by American author F. Scott Fitzgerald, was published in 1925. The story is set in the Jazz Age and follows the life of millionaire Jay Gatsby and his pursuit of Daisy Buchanan.\"\n]\n\nconst results = await rank(query, documents, { return_documents: true, top_k: 3 });\nconsole.log(results);\n```\n</details>\n\n## Using API\n\nYou can use the large model via our API as follows:\n\n```python\nfrom mixedbread_ai.client import MixedbreadAI\n\nmxbai = MixedbreadAI(api_key=\"{MIXEDBREAD_API_KEY}\")\n\nres = mxbai.reranking(\n  model=\"mixedbread-ai/mxbai-rerank-large-v1\",\n  query=\"Who is the author of To Kill a Mockingbird?\",\n  input=[\n    \"To Kill a Mockingbird is a novel by Harper Lee published in 1960. It was immediately successful, winning the Pulitzer Prize, and has become a classic of modern American literature.\",\n    \"The novel Moby-Dick was written by Herman Melville and first published in 1851. It is considered a masterpiece of American literature and deals with complex themes of obsession, revenge, and the conflict between good and evil.\",\n    \"Harper Lee, an American novelist widely known for her novel To Kill a Mockingbird, was born in 1926 in Monroeville, Alabama. She received the Pulitzer Prize for Fiction in 1961.\",\n    \"Jane Austen was an English novelist known primarily for her six major novels, which interpret, critique and comment upon the British landed gentry at the end of the 18th century.\",\n    \"The Harry Potter series, which consists of seven fantasy novels written by British author J.K. Rowling, is among the most popular and critically acclaimed books of the modern era.\",\n    \"The Great Gatsby, a novel written by American author F. Scott Fitzgerald, was published in 1925. The story is set in the Jazz Age and follows the life of millionaire Jay Gatsby and his pursuit of Daisy Buchanan.\"\n  ],\n  top_k=3,\n  return_input=false\n)\n\nprint(res.data)\n```\n\nThe API comes with additional features, such as a continous trained reranker! Check out the [docs](https://www.mixedbread.ai/docs) for more information.\n\n## Evaluation\n\nOur reranker models are designed to elevate your search. They work extremely well in combination with keyword search and can even outperform semantic search systems in many cases.\n\n| Model                                                                                 | NDCG@10  | Accuracy@3 |\n| ------------------------------------------------------------------------------------- | -------- | ---------- |\n| Lexical Search (Lucene)                                                               | 38.0     | 66.4       |\n| [BAAI/bge-reranker-base](https://huggingface.co/BAAI/bge-reranker-base)               | 41.6     | 66.9       |\n| [BAAI/bge-reranker-large](https://huggingface.co/BAAI/bge-reranker-large)             | 45.2     | 70.6       |\n| cohere-embed-v3 (semantic search)                                                     | 47.5     | 70.9       |\n| [mxbai-rerank-xsmall-v1](https://huggingface.co/mixedbread-ai/mxbai-rerank-xsmall-v1) | **43.9** | **70.0**   |\n| [mxbai-rerank-base-v1](https://huggingface.co/mixedbread-ai/mxbai-rerank-base-v1)     | **46.9** | **72.3**   |\n| [mxbai-rerank-large-v1](https://huggingface.co/mixedbread-ai/mxbai-rerank-large-v1)   | **48.8** | **74.9**   |\n\nThe reported results are aggregated from 11 datasets of BEIR. We used [Pyserini](https://github.com/castorini/pyserini/) to evaluate the models. Find more in our [blog-post](https://www.mixedbread.ai/blog/mxbai-rerank-v1) and on this [spreadsheet](https://docs.google.com/spreadsheets/d/15ELkSMFv-oHa5TRiIjDvhIstH9dlc3pnZeO-iGz4Ld4/edit?usp=sharing).\n\n## Community\nPlease join our [Discord Community](https://discord.gg/jDfMHzAVfU) and share your feedback and thoughts! We are here to help and also always happy to chat.\n\n## Citation\n\n```bibtex\n@online{rerank2024mxbai,\n  title={Boost Your Search With The Crispy Mixedbread Rerank Models},\n  author={Aamir Shakir and Darius Koenig and Julius Lipp and Sean Lee},\n  year={2024},\n  url={https://www.mixedbread.ai/blog/mxbai-rerank-v1},\n}\n```\n\n## License\nApache 2.0",
    "card_content": "---\nlibrary_name: transformers\ntags:\n- reranker\n- transformers.js\nlicense: apache-2.0\nlanguage:\n- en\n---\n<br><br>\n\n<p align=\"center\">\n<svg xmlns=\"http://www.w3.org/2000/svg\" xml:space=\"preserve\" viewBox=\"0 0 2020 1130\" width=\"150\" height=\"150\" aria-hidden=\"true\"><path fill=\"#e95a0f\" d=\"M398.167 621.992c-1.387-20.362-4.092-40.739-3.851-61.081.355-30.085 6.873-59.139 21.253-85.976 10.487-19.573 24.09-36.822 40.662-51.515 16.394-14.535 34.338-27.046 54.336-36.182 15.224-6.955 31.006-12.609 47.829-14.168 11.809-1.094 23.753-2.514 35.524-1.836 23.033 1.327 45.131 7.255 66.255 16.75 16.24 7.3 31.497 16.165 45.651 26.969 12.997 9.921 24.412 21.37 34.158 34.509 11.733 15.817 20.849 33.037 25.987 52.018 3.468 12.81 6.438 25.928 7.779 39.097 1.722 16.908 1.642 34.003 2.235 51.021.427 12.253.224 24.547 1.117 36.762 1.677 22.93 4.062 45.764 11.8 67.7 5.376 15.239 12.499 29.55 20.846 43.681l-18.282 20.328c-1.536 1.71-2.795 3.665-4.254 5.448l-19.323 23.533c-13.859-5.449-27.446-11.803-41.657-16.086-13.622-4.106-27.793-6.765-41.905-8.775-15.256-2.173-30.701-3.475-46.105-4.049-23.571-.879-47.178-1.056-70.769-1.029-10.858.013-21.723 1.116-32.57 1.926-5.362.4-10.69 1.255-16.464 1.477-2.758-7.675-5.284-14.865-7.367-22.181-3.108-10.92-4.325-22.554-13.16-31.095-2.598-2.512-5.069-5.341-6.883-8.443-6.366-10.884-12.48-21.917-18.571-32.959-4.178-7.573-8.411-14.375-17.016-18.559-10.34-5.028-19.538-12.387-29.311-18.611-3.173-2.021-6.414-4.312-9.952-5.297-5.857-1.63-11.98-2.301-17.991-3.376z\"></path><path fill=\"#ed6d7b\" d=\"M1478.998 758.842c-12.025.042-24.05.085-36.537-.373-.14-8.536.231-16.569.453-24.607.033-1.179-.315-2.986-1.081-3.4-.805-.434-2.376.338-3.518.81-.856.354-1.562 1.069-3.589 2.521-.239-3.308-.664-5.586-.519-7.827.488-7.544 2.212-15.166 1.554-22.589-1.016-11.451 1.397-14.592-12.332-14.419-3.793.048-3.617-2.803-3.332-5.331.499-4.422 1.45-8.803 1.77-13.233.311-4.316.068-8.672.068-12.861-2.554-.464-4.326-.86-6.12-1.098-4.415-.586-6.051-2.251-5.065-7.31 1.224-6.279.848-12.862 1.276-19.306.19-2.86-.971-4.473-3.794-4.753-4.113-.407-8.242-1.057-12.352-.975-4.663.093-5.192-2.272-4.751-6.012.733-6.229 1.252-12.483 1.875-18.726l1.102-10.495c-5.905-.309-11.146-.805-16.385-.778-3.32.017-5.174-1.4-5.566-4.4-1.172-8.968-2.479-17.944-3.001-26.96-.26-4.484-1.936-5.705-6.005-5.774-9.284-.158-18.563-.594-27.843-.953-7.241-.28-10.137-2.764-11.3-9.899-.746-4.576-2.715-7.801-7.777-8.207-7.739-.621-15.511-.992-23.207-1.961-7.327-.923-14.587-2.415-21.853-3.777-5.021-.941-10.003-2.086-15.003-3.14 4.515-22.952 13.122-44.382 26.284-63.587 18.054-26.344 41.439-47.239 69.102-63.294 15.847-9.197 32.541-16.277 50.376-20.599 16.655-4.036 33.617-5.715 50.622-4.385 33.334 2.606 63.836 13.955 92.415 31.15 15.864 9.545 30.241 20.86 42.269 34.758 8.113 9.374 15.201 19.78 21.718 30.359 10.772 17.484 16.846 36.922 20.611 56.991 1.783 9.503 2.815 19.214 3.318 28.876.758 14.578.755 29.196.65 44.311l-51.545 20.013c-7.779 3.059-15.847 5.376-21.753 12.365-4.73 5.598-10.658 10.316-16.547 14.774-9.9 7.496-18.437 15.988-25.083 26.631-3.333 5.337-7.901 10.381-12.999 14.038-11.355 8.144-17.397 18.973-19.615 32.423l-6.988 41.011z\"></path><path fill=\"#ec663e\" d=\"M318.11 923.047c-.702 17.693-.832 35.433-2.255 53.068-1.699 21.052-6.293 41.512-14.793 61.072-9.001 20.711-21.692 38.693-38.496 53.583-16.077 14.245-34.602 24.163-55.333 30.438-21.691 6.565-43.814 8.127-66.013 6.532-22.771-1.636-43.88-9.318-62.74-22.705-20.223-14.355-35.542-32.917-48.075-54.096-9.588-16.203-16.104-33.55-19.201-52.015-2.339-13.944-2.307-28.011-.403-42.182 2.627-19.545 9.021-37.699 17.963-55.067 11.617-22.564 27.317-41.817 48.382-56.118 15.819-10.74 33.452-17.679 52.444-20.455 8.77-1.282 17.696-1.646 26.568-2.055 11.755-.542 23.534-.562 35.289-1.11 8.545-.399 17.067-1.291 26.193-1.675 1.349 1.77 2.24 3.199 2.835 4.742 4.727 12.261 10.575 23.865 18.636 34.358 7.747 10.084 14.83 20.684 22.699 30.666 3.919 4.972 8.37 9.96 13.609 13.352 7.711 4.994 16.238 8.792 24.617 12.668 5.852 2.707 12.037 4.691 18.074 6.998z\"></path><path fill=\"#ea580e\" d=\"M1285.167 162.995c3.796-29.75 13.825-56.841 32.74-80.577 16.339-20.505 36.013-36.502 59.696-47.614 14.666-6.881 29.971-11.669 46.208-12.749 10.068-.669 20.239-1.582 30.255-.863 16.6 1.191 32.646 5.412 47.9 12.273 19.39 8.722 36.44 20.771 50.582 36.655 15.281 17.162 25.313 37.179 31.49 59.286 5.405 19.343 6.31 39.161 4.705 58.825-2.37 29.045-11.836 55.923-30.451 78.885-10.511 12.965-22.483 24.486-37.181 33.649-5.272-5.613-10.008-11.148-14.539-16.846-5.661-7.118-10.958-14.533-16.78-21.513-4.569-5.478-9.548-10.639-14.624-15.658-3.589-3.549-7.411-6.963-11.551-9.827-5.038-3.485-10.565-6.254-15.798-9.468-8.459-5.195-17.011-9.669-26.988-11.898-12.173-2.72-24.838-4.579-35.622-11.834-1.437-.967-3.433-1.192-5.213-1.542-12.871-2.529-25.454-5.639-36.968-12.471-5.21-3.091-11.564-4.195-17.011-6.965-4.808-2.445-8.775-6.605-13.646-8.851-8.859-4.085-18.114-7.311-27.204-10.896z\"></path><path fill=\"#f8ab00\" d=\"M524.963 311.12c-9.461-5.684-19.513-10.592-28.243-17.236-12.877-9.801-24.031-21.578-32.711-35.412-11.272-17.965-19.605-37.147-21.902-58.403-1.291-11.951-2.434-24.073-1.87-36.034.823-17.452 4.909-34.363 11.581-50.703 8.82-21.603 22.25-39.792 39.568-55.065 18.022-15.894 39.162-26.07 62.351-32.332 19.22-5.19 38.842-6.177 58.37-4.674 23.803 1.831 45.56 10.663 65.062 24.496 17.193 12.195 31.688 27.086 42.894 45.622-11.403 8.296-22.633 16.117-34.092 23.586-17.094 11.142-34.262 22.106-48.036 37.528-8.796 9.848-17.201 20.246-27.131 28.837-16.859 14.585-27.745 33.801-41.054 51.019-11.865 15.349-20.663 33.117-30.354 50.08-5.303 9.283-9.654 19.11-14.434 28.692z\"></path><path fill=\"#ea5227\" d=\"M1060.11 1122.049c-7.377 1.649-14.683 4.093-22.147 4.763-11.519 1.033-23.166 1.441-34.723 1.054-19.343-.647-38.002-4.7-55.839-12.65-15.078-6.72-28.606-15.471-40.571-26.836-24.013-22.81-42.053-49.217-49.518-81.936-1.446-6.337-1.958-12.958-2.235-19.477-.591-13.926-.219-27.909-1.237-41.795-.916-12.5-3.16-24.904-4.408-37.805 1.555-1.381 3.134-2.074 3.778-3.27 4.729-8.79 12.141-15.159 19.083-22.03 5.879-5.818 10.688-12.76 16.796-18.293 6.993-6.335 11.86-13.596 14.364-22.612l8.542-29.993c8.015 1.785 15.984 3.821 24.057 5.286 8.145 1.478 16.371 2.59 24.602 3.493 8.453.927 16.956 1.408 25.891 2.609 1.119 16.09 1.569 31.667 2.521 47.214.676 11.045 1.396 22.154 3.234 33.043 2.418 14.329 5.708 28.527 9.075 42.674 3.499 14.705 4.028 29.929 10.415 44.188 10.157 22.674 18.29 46.25 28.281 69.004 7.175 16.341 12.491 32.973 15.078 50.615.645 4.4 3.256 8.511 4.963 12.755z\"></path><path fill=\"#ea5330\" d=\"M1060.512 1122.031c-2.109-4.226-4.72-8.337-5.365-12.737-2.587-17.642-7.904-34.274-15.078-50.615-9.991-22.755-18.124-46.33-28.281-69.004-6.387-14.259-6.916-29.482-10.415-44.188-3.366-14.147-6.656-28.346-9.075-42.674-1.838-10.889-2.558-21.999-3.234-33.043-.951-15.547-1.401-31.124-2.068-47.146 8.568-.18 17.146.487 25.704.286l41.868-1.4c.907 3.746 1.245 7.04 1.881 10.276l8.651 42.704c.903 4.108 2.334 8.422 4.696 11.829 7.165 10.338 14.809 20.351 22.456 30.345 4.218 5.512 8.291 11.304 13.361 15.955 8.641 7.927 18.065 14.995 27.071 22.532 12.011 10.052 24.452 19.302 40.151 22.854-1.656 11.102-2.391 22.44-5.172 33.253-4.792 18.637-12.38 36.209-23.412 52.216-13.053 18.94-29.086 34.662-49.627 45.055-10.757 5.443-22.443 9.048-34.111 13.501z\"></path><path fill=\"#f8aa05\" d=\"M1989.106 883.951c5.198 8.794 11.46 17.148 15.337 26.491 5.325 12.833 9.744 26.207 12.873 39.737 2.95 12.757 3.224 25.908 1.987 39.219-1.391 14.973-4.643 29.268-10.349 43.034-5.775 13.932-13.477 26.707-23.149 38.405-14.141 17.104-31.215 30.458-50.807 40.488-14.361 7.352-29.574 12.797-45.741 14.594-10.297 1.144-20.732 2.361-31.031 1.894-24.275-1.1-47.248-7.445-68.132-20.263-6.096-3.741-11.925-7.917-17.731-12.342 5.319-5.579 10.361-10.852 15.694-15.811l37.072-34.009c.975-.892 2.113-1.606 3.08-2.505 6.936-6.448 14.765-12.2 20.553-19.556 8.88-11.285 20.064-19.639 31.144-28.292 4.306-3.363 9.06-6.353 12.673-10.358 5.868-6.504 10.832-13.814 16.422-20.582 6.826-8.264 13.727-16.481 20.943-24.401 4.065-4.461 8.995-8.121 13.249-12.424 14.802-14.975 28.77-30.825 45.913-43.317z\"></path><path fill=\"#ed6876\" d=\"M1256.099 523.419c5.065.642 10.047 1.787 15.068 2.728 7.267 1.362 14.526 2.854 21.853 3.777 7.696.97 15.468 1.34 23.207 1.961 5.062.406 7.031 3.631 7.777 8.207 1.163 7.135 4.059 9.62 11.3 9.899l27.843.953c4.069.069 5.745 1.291 6.005 5.774.522 9.016 1.829 17.992 3.001 26.96.392 3 2.246 4.417 5.566 4.4 5.239-.026 10.48.469 16.385.778l-1.102 10.495-1.875 18.726c-.44 3.74.088 6.105 4.751 6.012 4.11-.082 8.239.568 12.352.975 2.823.28 3.984 1.892 3.794 4.753-.428 6.444-.052 13.028-1.276 19.306-.986 5.059.651 6.724 5.065 7.31 1.793.238 3.566.634 6.12 1.098 0 4.189.243 8.545-.068 12.861-.319 4.43-1.27 8.811-1.77 13.233-.285 2.528-.461 5.379 3.332 5.331 13.729-.173 11.316 2.968 12.332 14.419.658 7.423-1.066 15.045-1.554 22.589-.145 2.241.28 4.519.519 7.827 2.026-1.452 2.733-2.167 3.589-2.521 1.142-.472 2.713-1.244 3.518-.81.767.414 1.114 2.221 1.081 3.4l-.917 24.539c-11.215.82-22.45.899-33.636 1.674l-43.952 3.436c-1.086-3.01-2.319-5.571-2.296-8.121.084-9.297-4.468-16.583-9.091-24.116-3.872-6.308-8.764-13.052-9.479-19.987-1.071-10.392-5.716-15.936-14.889-18.979-1.097-.364-2.16-.844-3.214-1.327-7.478-3.428-15.548-5.918-19.059-14.735-.904-2.27-3.657-3.775-5.461-5.723-2.437-2.632-4.615-5.525-7.207-7.987-2.648-2.515-5.352-5.346-8.589-6.777-4.799-2.121-10.074-3.185-15.175-4.596l-15.785-4.155c.274-12.896 1.722-25.901.54-38.662-1.647-17.783-3.457-35.526-2.554-53.352.528-10.426 2.539-20.777 3.948-31.574z\"></path><path fill=\"#f6a200\" d=\"M525.146 311.436c4.597-9.898 8.947-19.725 14.251-29.008 9.691-16.963 18.49-34.73 30.354-50.08 13.309-17.218 24.195-36.434 41.054-51.019 9.93-8.591 18.335-18.989 27.131-28.837 13.774-15.422 30.943-26.386 48.036-37.528 11.459-7.469 22.688-15.29 34.243-23.286 11.705 16.744 19.716 35.424 22.534 55.717 2.231 16.066 2.236 32.441 2.753 49.143-4.756 1.62-9.284 2.234-13.259 4.056-6.43 2.948-12.193 7.513-18.774 9.942-19.863 7.331-33.806 22.349-47.926 36.784-7.86 8.035-13.511 18.275-19.886 27.705-4.434 6.558-9.345 13.037-12.358 20.254-4.249 10.177-6.94 21.004-10.296 31.553-12.33.053-24.741 1.027-36.971-.049-20.259-1.783-40.227-5.567-58.755-14.69-.568-.28-1.295-.235-2.132-.658z\"></path><path fill=\"#f7a80d\" d=\"M1989.057 883.598c-17.093 12.845-31.061 28.695-45.863 43.67-4.254 4.304-9.184 7.963-13.249 12.424-7.216 7.92-14.117 16.137-20.943 24.401-5.59 6.768-10.554 14.078-16.422 20.582-3.614 4.005-8.367 6.995-12.673 10.358-11.08 8.653-22.264 17.007-31.144 28.292-5.788 7.356-13.617 13.108-20.553 19.556-.967.899-2.105 1.614-3.08 2.505l-37.072 34.009c-5.333 4.96-10.375 10.232-15.859 15.505-21.401-17.218-37.461-38.439-48.623-63.592 3.503-1.781 7.117-2.604 9.823-4.637 8.696-6.536 20.392-8.406 27.297-17.714.933-1.258 2.646-1.973 4.065-2.828 17.878-10.784 36.338-20.728 53.441-32.624 10.304-7.167 18.637-17.23 27.583-26.261 3.819-3.855 7.436-8.091 10.3-12.681 12.283-19.68 24.43-39.446 40.382-56.471 12.224-13.047 17.258-29.524 22.539-45.927 15.85 4.193 29.819 12.129 42.632 22.08 10.583 8.219 19.782 17.883 27.42 29.351z\"></path><path fill=\"#ef7a72\" d=\"M1479.461 758.907c1.872-13.734 4.268-27.394 6.525-41.076 2.218-13.45 8.26-24.279 19.615-32.423 5.099-3.657 9.667-8.701 12.999-14.038 6.646-10.643 15.183-19.135 25.083-26.631 5.888-4.459 11.817-9.176 16.547-14.774 5.906-6.99 13.974-9.306 21.753-12.365l51.48-19.549c.753 11.848.658 23.787 1.641 35.637 1.771 21.353 4.075 42.672 11.748 62.955.17.449.107.985-.019 2.158-6.945 4.134-13.865 7.337-20.437 11.143-3.935 2.279-7.752 5.096-10.869 8.384-6.011 6.343-11.063 13.624-17.286 19.727-9.096 8.92-12.791 20.684-18.181 31.587-.202.409-.072.984-.096 1.481-8.488-1.72-16.937-3.682-25.476-5.094-9.689-1.602-19.426-3.084-29.201-3.949-15.095-1.335-30.241-2.1-45.828-3.172z\"></path><path fill=\"#e94e3b\" d=\"M957.995 766.838c-20.337-5.467-38.791-14.947-55.703-27.254-8.2-5.967-15.451-13.238-22.958-20.37 2.969-3.504 5.564-6.772 8.598-9.563 7.085-6.518 11.283-14.914 15.8-23.153 4.933-8.996 10.345-17.743 14.966-26.892 2.642-5.231 5.547-11.01 5.691-16.611.12-4.651.194-8.932 2.577-12.742 8.52-13.621 15.483-28.026 18.775-43.704 2.11-10.049 7.888-18.774 7.81-29.825-.064-9.089 4.291-18.215 6.73-27.313 3.212-11.983 7.369-23.797 9.492-35.968 3.202-18.358 5.133-36.945 7.346-55.466l4.879-45.8c6.693.288 13.386.575 20.54 1.365.13 3.458-.41 6.407-.496 9.37l-1.136 42.595c-.597 11.552-2.067 23.058-3.084 34.59l-3.845 44.478c-.939 10.202-1.779 20.432-3.283 30.557-.96 6.464-4.46 12.646-1.136 19.383.348.706-.426 1.894-.448 2.864-.224 9.918-5.99 19.428-2.196 29.646.103.279-.033.657-.092.983l-8.446 46.205c-1.231 6.469-2.936 12.846-4.364 19.279-1.5 6.757-2.602 13.621-4.456 20.277-3.601 12.93-10.657 25.3-5.627 39.47.368 1.036.234 2.352.017 3.476l-5.949 30.123z\"></path><path fill=\"#ea5043\" d=\"M958.343 767.017c1.645-10.218 3.659-20.253 5.602-30.302.217-1.124.351-2.44-.017-3.476-5.03-14.17 2.026-26.539 5.627-39.47 1.854-6.656 2.956-13.52 4.456-20.277 1.428-6.433 3.133-12.81 4.364-19.279l8.446-46.205c.059-.326.196-.705.092-.983-3.794-10.218 1.972-19.728 2.196-29.646.022-.97.796-2.158.448-2.864-3.324-6.737.176-12.919 1.136-19.383 1.504-10.125 2.344-20.355 3.283-30.557l3.845-44.478c1.017-11.532 2.488-23.038 3.084-34.59.733-14.18.722-28.397 1.136-42.595.086-2.963.626-5.912.956-9.301 5.356-.48 10.714-.527 16.536-.081 2.224 15.098 1.855 29.734 1.625 44.408-.157 10.064 1.439 20.142 1.768 30.23.334 10.235-.035 20.49.116 30.733.084 5.713.789 11.418.861 17.13.054 4.289-.469 8.585-.702 12.879-.072 1.323-.138 2.659-.031 3.975l2.534 34.405-1.707 36.293-1.908 48.69c-.182 8.103.993 16.237.811 24.34-.271 12.076-1.275 24.133-1.787 36.207-.102 2.414-.101 5.283 1.06 7.219 4.327 7.22 4.463 15.215 4.736 23.103.365 10.553.088 21.128.086 31.693-11.44 2.602-22.84.688-34.106-.916-11.486-1.635-22.806-4.434-34.546-6.903z\"></path><path fill=\"#eb5d19\" d=\"M398.091 622.45c6.086.617 12.21 1.288 18.067 2.918 3.539.985 6.779 3.277 9.952 5.297 9.773 6.224 18.971 13.583 29.311 18.611 8.606 4.184 12.839 10.986 17.016 18.559l18.571 32.959c1.814 3.102 4.285 5.931 6.883 8.443 8.835 8.542 10.052 20.175 13.16 31.095 2.082 7.317 4.609 14.507 6.946 22.127-29.472 3.021-58.969 5.582-87.584 15.222-1.185-2.302-1.795-4.362-2.769-6.233-4.398-8.449-6.703-18.174-14.942-24.299-2.511-1.866-5.103-3.814-7.047-6.218-8.358-10.332-17.028-20.276-28.772-26.973 4.423-11.478 9.299-22.806 13.151-34.473 4.406-13.348 6.724-27.18 6.998-41.313.098-5.093.643-10.176 1.06-15.722z\"></path><path fill=\"#e94c32\" d=\"M981.557 392.109c-1.172 15.337-2.617 30.625-4.438 45.869-2.213 18.521-4.144 37.108-7.346 55.466-2.123 12.171-6.28 23.985-9.492 35.968-2.439 9.098-6.794 18.224-6.73 27.313.078 11.051-5.7 19.776-7.81 29.825-3.292 15.677-10.255 30.082-18.775 43.704-2.383 3.81-2.458 8.091-2.577 12.742-.144 5.6-3.049 11.38-5.691 16.611-4.621 9.149-10.033 17.896-14.966 26.892-4.517 8.239-8.715 16.635-15.8 23.153-3.034 2.791-5.629 6.06-8.735 9.255-12.197-10.595-21.071-23.644-29.301-37.24-7.608-12.569-13.282-25.962-17.637-40.37 13.303-6.889 25.873-13.878 35.311-25.315.717-.869 1.934-1.312 2.71-2.147 5.025-5.405 10.515-10.481 14.854-16.397 6.141-8.374 10.861-17.813 17.206-26.008 8.22-10.618 13.657-22.643 20.024-34.466 4.448-.626 6.729-3.21 8.114-6.89 1.455-3.866 2.644-7.895 4.609-11.492 4.397-8.05 9.641-15.659 13.708-23.86 3.354-6.761 5.511-14.116 8.203-21.206 5.727-15.082 7.277-31.248 12.521-46.578 3.704-10.828 3.138-23.116 4.478-34.753l7.56-.073z\"></path><path fill=\"#f7a617\" d=\"M1918.661 831.99c-4.937 16.58-9.971 33.057-22.196 46.104-15.952 17.025-28.099 36.791-40.382 56.471-2.864 4.59-6.481 8.825-10.3 12.681-8.947 9.031-17.279 19.094-27.583 26.261-17.103 11.896-35.564 21.84-53.441 32.624-1.419.856-3.132 1.571-4.065 2.828-6.904 9.308-18.6 11.178-27.297 17.714-2.705 2.033-6.319 2.856-9.874 4.281-3.413-9.821-6.916-19.583-9.36-29.602-1.533-6.284-1.474-12.957-1.665-19.913 1.913-.78 3.374-1.057 4.81-1.431 15.822-4.121 31.491-8.029 43.818-20.323 9.452-9.426 20.371-17.372 30.534-26.097 6.146-5.277 13.024-10.052 17.954-16.326 14.812-18.848 28.876-38.285 43.112-57.581 2.624-3.557 5.506-7.264 6.83-11.367 2.681-8.311 4.375-16.94 6.476-25.438 17.89.279 35.333 3.179 52.629 9.113z\"></path><path fill=\"#ea553a\" d=\"M1172.91 977.582c-15.775-3.127-28.215-12.377-40.227-22.43-9.005-7.537-18.43-14.605-27.071-22.532-5.07-4.651-9.143-10.443-13.361-15.955-7.647-9.994-15.291-20.007-22.456-30.345-2.361-3.407-3.792-7.72-4.696-11.829-3.119-14.183-5.848-28.453-8.651-42.704-.636-3.236-.974-6.53-1.452-10.209 15.234-2.19 30.471-3.969 46.408-5.622 2.692 5.705 4.882 11.222 6.63 16.876 2.9 9.381 7.776 17.194 15.035 24.049 7.056 6.662 13.305 14.311 19.146 22.099 9.509 12.677 23.01 19.061 36.907 25.054-1.048 7.441-2.425 14.854-3.066 22.33-.956 11.162-1.393 22.369-2.052 33.557l-1.096 17.661z\"></path><path fill=\"#ea5453\" d=\"M1163.123 704.036c-4.005 5.116-7.685 10.531-12.075 15.293-12.842 13.933-27.653 25.447-44.902 34.538-3.166-5.708-5.656-11.287-8.189-17.251-3.321-12.857-6.259-25.431-9.963-37.775-4.6-15.329-10.6-30.188-11.349-46.562-.314-6.871-1.275-14.287-7.114-19.644-1.047-.961-1.292-3.053-1.465-4.67l-4.092-39.927c-.554-5.245-.383-10.829-2.21-15.623-3.622-9.503-4.546-19.253-4.688-29.163-.088-6.111 1.068-12.256.782-18.344-.67-14.281-1.76-28.546-2.9-42.8-.657-8.222-1.951-16.395-2.564-24.62-.458-6.137-.285-12.322-.104-18.21.959 5.831 1.076 11.525 2.429 16.909 2.007 7.986 5.225 15.664 7.324 23.632 3.222 12.23 1.547 25.219 6.728 37.355 4.311 10.099 6.389 21.136 9.732 31.669 2.228 7.02 6.167 13.722 7.121 20.863 1.119 8.376 6.1 13.974 10.376 20.716l2.026 10.576c1.711 9.216 3.149 18.283 8.494 26.599 6.393 9.946 11.348 20.815 16.943 31.276 4.021 7.519 6.199 16.075 12.925 22.065l24.462 22.26c.556.503 1.507.571 2.274.841z\"></path><path fill=\"#ea5b15\" d=\"M1285.092 163.432c9.165 3.148 18.419 6.374 27.279 10.459 4.871 2.246 8.838 6.406 13.646 8.851 5.446 2.77 11.801 3.874 17.011 6.965 11.514 6.831 24.097 9.942 36.968 12.471 1.78.35 3.777.576 5.213 1.542 10.784 7.255 23.448 9.114 35.622 11.834 9.977 2.23 18.529 6.703 26.988 11.898 5.233 3.214 10.76 5.983 15.798 9.468 4.14 2.864 7.962 6.279 11.551 9.827 5.076 5.02 10.056 10.181 14.624 15.658 5.822 6.98 11.119 14.395 16.78 21.513 4.531 5.698 9.267 11.233 14.222 16.987-10.005 5.806-20.07 12.004-30.719 16.943-7.694 3.569-16.163 5.464-24.688 7.669-2.878-7.088-5.352-13.741-7.833-20.392-.802-2.15-1.244-4.55-2.498-6.396-4.548-6.7-9.712-12.999-14.011-19.847-6.672-10.627-15.34-18.93-26.063-25.376-9.357-5.625-18.367-11.824-27.644-17.587-6.436-3.997-12.902-8.006-19.659-11.405-5.123-2.577-11.107-3.536-16.046-6.37-17.187-9.863-35.13-17.887-54.031-23.767-4.403-1.37-8.953-2.267-13.436-3.382l.926-27.565z\"></path><path fill=\"#ea504b\" d=\"M1098 737l7.789 16.893c-15.04 9.272-31.679 15.004-49.184 17.995-9.464 1.617-19.122 2.097-29.151 3.019-.457-10.636-.18-21.211-.544-31.764-.273-7.888-.409-15.883-4.736-23.103-1.16-1.936-1.162-4.805-1.06-7.219l1.787-36.207c.182-8.103-.993-16.237-.811-24.34.365-16.236 1.253-32.461 1.908-48.69.484-12 .942-24.001 1.98-36.069 5.57 10.19 10.632 20.42 15.528 30.728 1.122 2.362 2.587 5.09 2.339 7.488-1.536 14.819 5.881 26.839 12.962 38.33 10.008 16.241 16.417 33.54 20.331 51.964 2.285 10.756 4.729 21.394 11.958 30.165L1098 737z\"></path><path fill=\"#f6a320\" d=\"M1865.78 822.529c-1.849 8.846-3.544 17.475-6.224 25.786-1.323 4.102-4.206 7.81-6.83 11.367l-43.112 57.581c-4.93 6.273-11.808 11.049-17.954 16.326-10.162 8.725-21.082 16.671-30.534 26.097-12.327 12.294-27.997 16.202-43.818 20.323-1.436.374-2.897.651-4.744.986-1.107-17.032-1.816-34.076-2.079-51.556 1.265-.535 2.183-.428 2.888-.766 10.596-5.072 20.8-11.059 32.586-13.273 1.69-.317 3.307-1.558 4.732-2.662l26.908-21.114c4.992-4.003 11.214-7.393 14.381-12.585 11.286-18.5 22.363-37.263 27.027-58.87l36.046 1.811c3.487.165 6.983.14 10.727.549z\"></path><path fill=\"#ec6333\" d=\"M318.448 922.814c-6.374-2.074-12.56-4.058-18.412-6.765-8.379-3.876-16.906-7.675-24.617-12.668-5.239-3.392-9.69-8.381-13.609-13.352-7.87-9.983-14.953-20.582-22.699-30.666-8.061-10.493-13.909-22.097-18.636-34.358-.595-1.543-1.486-2.972-2.382-4.783 6.84-1.598 13.797-3.023 20.807-4.106 18.852-2.912 36.433-9.493 53.737-17.819.697.888.889 1.555 1.292 2.051l17.921 21.896c4.14 4.939 8.06 10.191 12.862 14.412 5.67 4.984 12.185 9.007 18.334 13.447-8.937 16.282-16.422 33.178-20.696 51.31-1.638 6.951-2.402 14.107-3.903 21.403z\"></path><path fill=\"#f49700\" d=\"M623.467 326.903c2.893-10.618 5.584-21.446 9.833-31.623 3.013-7.217 7.924-13.696 12.358-20.254 6.375-9.43 12.026-19.67 19.886-27.705 14.12-14.434 28.063-29.453 47.926-36.784 6.581-2.429 12.344-6.994 18.774-9.942 3.975-1.822 8.503-2.436 13.186-3.592 1.947 18.557 3.248 37.15 8.307 55.686-15.453 7.931-28.853 18.092-40.46 29.996-10.417 10.683-19.109 23.111-28.013 35.175-3.238 4.388-4.888 9.948-7.262 14.973-17.803-3.987-35.767-6.498-54.535-5.931z\"></path><path fill=\"#ea544c\" d=\"M1097.956 736.615c-2.925-3.218-5.893-6.822-8.862-10.425-7.229-8.771-9.672-19.409-11.958-30.165-3.914-18.424-10.323-35.722-20.331-51.964-7.081-11.491-14.498-23.511-12.962-38.33.249-2.398-1.217-5.126-2.339-7.488l-15.232-31.019-3.103-34.338c-.107-1.316-.041-2.653.031-3.975.233-4.294.756-8.59.702-12.879-.072-5.713-.776-11.417-.861-17.13l-.116-30.733c-.329-10.088-1.926-20.166-1.768-30.23.23-14.674.599-29.31-1.162-44.341 9.369-.803 18.741-1.179 28.558-1.074 1.446 15.814 2.446 31.146 3.446 46.478.108 6.163-.064 12.348.393 18.485.613 8.225 1.907 16.397 2.564 24.62l2.9 42.8c.286 6.088-.869 12.234-.782 18.344.142 9.91 1.066 19.661 4.688 29.163 1.827 4.794 1.657 10.377 2.21 15.623l4.092 39.927c.172 1.617.417 3.71 1.465 4.67 5.839 5.357 6.8 12.773 7.114 19.644.749 16.374 6.749 31.233 11.349 46.562 3.704 12.344 6.642 24.918 9.963 37.775z\"></path><path fill=\"#ec5c61\" d=\"M1204.835 568.008c1.254 25.351-1.675 50.16-10.168 74.61-8.598-4.883-18.177-8.709-24.354-15.59-7.44-8.289-13.929-17.442-21.675-25.711-8.498-9.072-16.731-18.928-21.084-31.113-.54-1.513-1.691-2.807-2.594-4.564-4.605-9.247-7.706-18.544-7.96-29.09-.835-7.149-1.214-13.944-2.609-20.523-2.215-10.454-5.626-20.496-7.101-31.302-2.513-18.419-7.207-36.512-5.347-55.352.24-2.43-.17-4.949-.477-7.402l-4.468-34.792c2.723-.379 5.446-.757 8.585-.667 1.749 8.781 2.952 17.116 4.448 25.399 1.813 10.037 3.64 20.084 5.934 30.017 1.036 4.482 3.953 8.573 4.73 13.064 1.794 10.377 4.73 20.253 9.272 29.771 2.914 6.105 4.761 12.711 7.496 18.912 2.865 6.496 6.264 12.755 9.35 19.156 3.764 7.805 7.667 15.013 16.1 19.441 7.527 3.952 13.713 10.376 20.983 14.924 6.636 4.152 13.932 7.25 20.937 10.813z\"></path><path fill=\"#ed676f\" d=\"M1140.75 379.231c18.38-4.858 36.222-11.21 53.979-18.971 3.222 3.368 5.693 6.744 8.719 9.512 2.333 2.134 5.451 5.07 8.067 4.923 7.623-.429 12.363 2.688 17.309 8.215 5.531 6.18 12.744 10.854 19.224 16.184-5.121 7.193-10.461 14.241-15.323 21.606-13.691 20.739-22.99 43.255-26.782 67.926-.543 3.536-1.281 7.043-2.366 10.925-14.258-6.419-26.411-14.959-32.731-29.803-1.087-2.553-2.596-4.93-3.969-7.355-1.694-2.993-3.569-5.89-5.143-8.943-1.578-3.062-2.922-6.249-4.295-9.413-1.57-3.621-3.505-7.163-4.47-10.946-1.257-4.93-.636-10.572-2.725-15.013-5.831-12.397-7.467-25.628-9.497-38.847z\"></path><path fill=\"#ed656e\" d=\"M1254.103 647.439c5.325.947 10.603 2.272 15.847 3.722 5.101 1.41 10.376 2.475 15.175 4.596 3.237 1.431 5.942 4.262 8.589 6.777 2.592 2.462 4.77 5.355 7.207 7.987 1.804 1.948 4.557 3.453 5.461 5.723 3.51 8.817 11.581 11.307 19.059 14.735 1.053.483 2.116.963 3.214 1.327 9.172 3.043 13.818 8.587 14.889 18.979.715 6.935 5.607 13.679 9.479 19.987 4.623 7.533 9.175 14.819 9.091 24.116-.023 2.55 1.21 5.111 1.874 8.055-19.861 2.555-39.795 4.296-59.597 9.09l-11.596-23.203c-1.107-2.169-2.526-4.353-4.307-5.975-7.349-6.694-14.863-13.209-22.373-19.723l-17.313-14.669c-2.776-2.245-5.935-4.017-8.92-6.003l11.609-38.185c1.508-5.453 1.739-11.258 2.613-17.336z\"></path><path fill=\"#ec6168\" d=\"M1140.315 379.223c2.464 13.227 4.101 26.459 9.931 38.856 2.089 4.441 1.468 10.083 2.725 15.013.965 3.783 2.9 7.325 4.47 10.946 1.372 3.164 2.716 6.351 4.295 9.413 1.574 3.053 3.449 5.95 5.143 8.943 1.372 2.425 2.882 4.803 3.969 7.355 6.319 14.844 18.473 23.384 32.641 30.212.067 5.121-.501 10.201-.435 15.271l.985 38.117c.151 4.586.616 9.162.868 14.201-7.075-3.104-14.371-6.202-21.007-10.354-7.269-4.548-13.456-10.972-20.983-14.924-8.434-4.428-12.337-11.637-16.1-19.441-3.087-6.401-6.485-12.66-9.35-19.156-2.735-6.201-4.583-12.807-7.496-18.912-4.542-9.518-7.477-19.394-9.272-29.771-.777-4.491-3.694-8.581-4.73-13.064-2.294-9.933-4.121-19.98-5.934-30.017-1.496-8.283-2.699-16.618-4.036-25.335 10.349-2.461 20.704-4.511 31.054-6.582.957-.191 1.887-.515 3.264-.769z\"></path><path fill=\"#e94c28\" d=\"M922 537c-6.003 11.784-11.44 23.81-19.66 34.428-6.345 8.196-11.065 17.635-17.206 26.008-4.339 5.916-9.828 10.992-14.854 16.397-.776.835-1.993 1.279-2.71 2.147-9.439 11.437-22.008 18.427-35.357 24.929-4.219-10.885-6.942-22.155-7.205-33.905l-.514-49.542c7.441-2.893 14.452-5.197 21.334-7.841 1.749-.672 3.101-2.401 4.604-3.681 6.749-5.745 12.845-12.627 20.407-16.944 7.719-4.406 14.391-9.101 18.741-16.889.626-1.122 1.689-2.077 2.729-2.877 7.197-5.533 12.583-12.51 16.906-20.439.68-1.247 2.495-1.876 4.105-2.651 2.835 1.408 5.267 2.892 7.884 3.892 3.904 1.491 4.392 3.922 2.833 7.439-1.47 3.318-2.668 6.756-4.069 10.106-1.247 2.981-.435 5.242 2.413 6.544 2.805 1.282 3.125 3.14 1.813 5.601l-6.907 12.799L922 537z\"></path><path fill=\"#eb5659\" d=\"M1124.995 566c.868 1.396 2.018 2.691 2.559 4.203 4.353 12.185 12.586 22.041 21.084 31.113 7.746 8.269 14.235 17.422 21.675 25.711 6.176 6.881 15.756 10.707 24.174 15.932-6.073 22.316-16.675 42.446-31.058 60.937-1.074-.131-2.025-.199-2.581-.702l-24.462-22.26c-6.726-5.99-8.904-14.546-12.925-22.065-5.594-10.461-10.55-21.33-16.943-31.276-5.345-8.315-6.783-17.383-8.494-26.599-.63-3.394-1.348-6.772-1.738-10.848-.371-6.313-1.029-11.934-1.745-18.052l6.34 4.04 1.288-.675-2.143-15.385 9.454 1.208v-8.545L1124.995 566z\"></path><path fill=\"#f5a02d\" d=\"M1818.568 820.096c-4.224 21.679-15.302 40.442-26.587 58.942-3.167 5.192-9.389 8.582-14.381 12.585l-26.908 21.114c-1.425 1.104-3.042 2.345-4.732 2.662-11.786 2.214-21.99 8.201-32.586 13.273-.705.338-1.624.231-2.824.334a824.35 824.35 0 0 1-8.262-42.708c4.646-2.14 9.353-3.139 13.269-5.47 5.582-3.323 11.318-6.942 15.671-11.652 7.949-8.6 14.423-18.572 22.456-27.081 8.539-9.046 13.867-19.641 18.325-30.922l46.559 8.922z\"></path><path fill=\"#eb5a57\" d=\"M1124.96 565.639c-5.086-4.017-10.208-8.395-15.478-12.901v8.545l-9.454-1.208 2.143 15.385-1.288.675-6.34-4.04c.716 6.118 1.375 11.74 1.745 17.633-4.564-6.051-9.544-11.649-10.663-20.025-.954-7.141-4.892-13.843-7.121-20.863-3.344-10.533-5.421-21.57-9.732-31.669-5.181-12.135-3.506-25.125-6.728-37.355-2.099-7.968-5.317-15.646-7.324-23.632-1.353-5.384-1.47-11.078-2.429-16.909l-3.294-46.689a278.63 278.63 0 0 1 27.57-2.084c2.114 12.378 3.647 24.309 5.479 36.195 1.25 8.111 2.832 16.175 4.422 24.23 1.402 7.103 2.991 14.169 4.55 21.241 1.478 6.706.273 14.002 4.6 20.088 5.401 7.597 7.176 16.518 9.467 25.337 1.953 7.515 5.804 14.253 11.917 19.406.254 10.095 3.355 19.392 7.96 28.639z\"></path><path fill=\"#ea541c\" d=\"M911.651 810.999c-2.511 10.165-5.419 20.146-8.2 30.162-2.503 9.015-7.37 16.277-14.364 22.612-6.108 5.533-10.917 12.475-16.796 18.293-6.942 6.871-14.354 13.24-19.083 22.03-.644 1.196-2.222 1.889-3.705 2.857-2.39-7.921-4.101-15.991-6.566-23.823-5.451-17.323-12.404-33.976-23.414-48.835l21.627-21.095c3.182-3.29 5.532-7.382 8.295-11.083l10.663-14.163c9.528 4.78 18.925 9.848 28.625 14.247 7.324 3.321 15.036 5.785 22.917 8.799z\"></path><path fill=\"#eb5d19\" d=\"M1284.092 191.421c4.557.69 9.107 1.587 13.51 2.957 18.901 5.881 36.844 13.904 54.031 23.767 4.938 2.834 10.923 3.792 16.046 6.37 6.757 3.399 13.224 7.408 19.659 11.405l27.644 17.587c10.723 6.446 19.392 14.748 26.063 25.376 4.299 6.848 9.463 13.147 14.011 19.847 1.254 1.847 1.696 4.246 2.498 6.396l7.441 20.332c-11.685 1.754-23.379 3.133-35.533 4.037-.737-2.093-.995-3.716-1.294-5.33-3.157-17.057-14.048-30.161-23.034-44.146-3.027-4.71-7.786-8.529-12.334-11.993-9.346-7.116-19.004-13.834-28.688-20.491-6.653-4.573-13.311-9.251-20.431-13.002-8.048-4.24-16.479-7.85-24.989-11.091-11.722-4.465-23.673-8.328-35.527-12.449l.927-19.572z\"></path><path fill=\"#eb5e24\" d=\"M1283.09 211.415c11.928 3.699 23.88 7.562 35.602 12.027 8.509 3.241 16.941 6.852 24.989 11.091 7.12 3.751 13.778 8.429 20.431 13.002 9.684 6.657 19.342 13.375 28.688 20.491 4.548 3.463 9.307 7.283 12.334 11.993 8.986 13.985 19.877 27.089 23.034 44.146.299 1.615.557 3.237.836 5.263-13.373-.216-26.749-.839-40.564-1.923-2.935-9.681-4.597-18.92-12.286-26.152-15.577-14.651-30.4-30.102-45.564-45.193-.686-.683-1.626-1.156-2.516-1.584l-47.187-22.615 2.203-20.546z\"></path><path fill=\"#e9511f\" d=\"M913 486.001c-1.29.915-3.105 1.543-3.785 2.791-4.323 7.929-9.709 14.906-16.906 20.439-1.04.8-2.103 1.755-2.729 2.877-4.35 7.788-11.022 12.482-18.741 16.889-7.562 4.317-13.658 11.199-20.407 16.944-1.503 1.28-2.856 3.009-4.604 3.681-6.881 2.643-13.893 4.948-21.262 7.377-.128-11.151.202-22.302.378-33.454.03-1.892-.6-3.795-.456-6.12 13.727-1.755 23.588-9.527 33.278-17.663 2.784-2.337 6.074-4.161 8.529-6.784l29.057-31.86c1.545-1.71 3.418-3.401 4.221-5.459 5.665-14.509 11.49-28.977 16.436-43.736 2.817-8.407 4.074-17.338 6.033-26.032 5.039.714 10.078 1.427 15.536 2.629-.909 8.969-2.31 17.438-3.546 25.931-2.41 16.551-5.84 32.839-11.991 48.461L913 486.001z\"></path><path fill=\"#ea5741\" d=\"M1179.451 903.828c-14.224-5.787-27.726-12.171-37.235-24.849-5.841-7.787-12.09-15.436-19.146-22.099-7.259-6.854-12.136-14.667-15.035-24.049-1.748-5.654-3.938-11.171-6.254-17.033 15.099-4.009 30.213-8.629 44.958-15.533l28.367 36.36c6.09 8.015 13.124 14.75 22.72 18.375-7.404 14.472-13.599 29.412-17.48 45.244-.271 1.106-.382 2.25-.895 3.583z\"></path><path fill=\"#ea522a\" d=\"M913.32 486.141c2.693-7.837 5.694-15.539 8.722-23.231 6.151-15.622 9.581-31.91 11.991-48.461l3.963-25.861c7.582.317 15.168 1.031 22.748 1.797 4.171.421 8.333.928 12.877 1.596-.963 11.836-.398 24.125-4.102 34.953-5.244 15.33-6.794 31.496-12.521 46.578-2.692 7.09-4.849 14.445-8.203 21.206-4.068 8.201-9.311 15.81-13.708 23.86-1.965 3.597-3.154 7.627-4.609 11.492-1.385 3.68-3.666 6.265-8.114 6.89-1.994-1.511-3.624-3.059-5.077-4.44l6.907-12.799c1.313-2.461.993-4.318-1.813-5.601-2.849-1.302-3.66-3.563-2.413-6.544 1.401-3.35 2.599-6.788 4.069-10.106 1.558-3.517 1.071-5.948-2.833-7.439-2.617-1-5.049-2.484-7.884-3.892z\"></path><path fill=\"#eb5e24\" d=\"M376.574 714.118c12.053 6.538 20.723 16.481 29.081 26.814 1.945 2.404 4.537 4.352 7.047 6.218 8.24 6.125 10.544 15.85 14.942 24.299.974 1.871 1.584 3.931 2.376 6.29-7.145 3.719-14.633 6.501-21.386 10.517-9.606 5.713-18.673 12.334-28.425 18.399-3.407-3.73-6.231-7.409-9.335-10.834l-30.989-33.862c11.858-11.593 22.368-24.28 31.055-38.431 1.86-3.031 3.553-6.164 5.632-9.409z\"></path><path fill=\"#e95514\" d=\"M859.962 787.636c-3.409 5.037-6.981 9.745-10.516 14.481-2.763 3.701-5.113 7.792-8.295 11.083-6.885 7.118-14.186 13.834-21.65 20.755-13.222-17.677-29.417-31.711-48.178-42.878-.969-.576-2.068-.934-3.27-1.709 6.28-8.159 12.733-15.993 19.16-23.849 1.459-1.783 2.718-3.738 4.254-5.448l18.336-19.969c4.909 5.34 9.619 10.738 14.081 16.333 9.72 12.19 21.813 21.566 34.847 29.867.411.262.725.674 1.231 1.334z\"></path><path fill=\"#eb5f2d\" d=\"M339.582 762.088l31.293 33.733c3.104 3.425 5.928 7.104 9.024 10.979-12.885 11.619-24.548 24.139-33.899 38.704-.872 1.359-1.56 2.837-2.644 4.428-6.459-4.271-12.974-8.294-18.644-13.278-4.802-4.221-8.722-9.473-12.862-14.412l-17.921-21.896c-.403-.496-.595-1.163-.926-2.105 16.738-10.504 32.58-21.87 46.578-36.154z\"></path><path fill=\"#f28d00\" d=\"M678.388 332.912c1.989-5.104 3.638-10.664 6.876-15.051 8.903-12.064 17.596-24.492 28.013-35.175 11.607-11.904 25.007-22.064 40.507-29.592 4.873 11.636 9.419 23.412 13.67 35.592-5.759 4.084-11.517 7.403-16.594 11.553-4.413 3.607-8.124 8.092-12.023 12.301-5.346 5.772-10.82 11.454-15.782 17.547-3.929 4.824-7.17 10.208-10.716 15.344l-33.95-12.518z\"></path><path fill=\"#f08369\" d=\"M1580.181 771.427c-.191-.803-.322-1.377-.119-1.786 5.389-10.903 9.084-22.666 18.181-31.587 6.223-6.103 11.276-13.385 17.286-19.727 3.117-3.289 6.933-6.105 10.869-8.384 6.572-3.806 13.492-7.009 20.461-10.752 1.773 3.23 3.236 6.803 4.951 10.251l12.234 24.993c-1.367 1.966-2.596 3.293-3.935 4.499-7.845 7.07-16.315 13.564-23.407 21.32-6.971 7.623-12.552 16.517-18.743 24.854l-37.777-13.68z\"></path><path fill=\"#f18b5e\" d=\"M1618.142 785.4c6.007-8.63 11.588-17.524 18.559-25.147 7.092-7.755 15.562-14.249 23.407-21.32 1.338-1.206 2.568-2.534 3.997-4.162l28.996 33.733c1.896 2.205 4.424 3.867 6.66 6.394-6.471 7.492-12.967 14.346-19.403 21.255l-18.407 19.953c-12.958-12.409-27.485-22.567-43.809-30.706z\"></path><path fill=\"#f49c3a\" d=\"M1771.617 811.1c-4.066 11.354-9.394 21.949-17.933 30.995-8.032 8.509-14.507 18.481-22.456 27.081-4.353 4.71-10.089 8.329-15.671 11.652-3.915 2.331-8.623 3.331-13.318 5.069-4.298-9.927-8.255-19.998-12.1-30.743 4.741-4.381 9.924-7.582 13.882-11.904 7.345-8.021 14.094-16.603 20.864-25.131 4.897-6.168 9.428-12.626 14.123-18.955l32.61 11.936z\"></path><path fill=\"#f08000\" d=\"M712.601 345.675c3.283-5.381 6.524-10.765 10.453-15.589 4.962-6.093 10.435-11.774 15.782-17.547 3.899-4.21 7.61-8.695 12.023-12.301 5.078-4.15 10.836-7.469 16.636-11.19a934.12 934.12 0 0 1 23.286 35.848c-4.873 6.234-9.676 11.895-14.63 17.421l-25.195 27.801c-11.713-9.615-24.433-17.645-38.355-24.443z\"></path><path fill=\"#ed6e04\" d=\"M751.11 370.42c8.249-9.565 16.693-18.791 25.041-28.103 4.954-5.526 9.757-11.187 14.765-17.106 7.129 6.226 13.892 13.041 21.189 19.225 5.389 4.567 11.475 8.312 17.53 12.92-5.51 7.863-10.622 15.919-17.254 22.427-8.881 8.716-18.938 16.233-28.49 24.264-5.703-6.587-11.146-13.427-17.193-19.682-4.758-4.921-10.261-9.121-15.587-13.944z\"></path><path fill=\"#ea541c\" d=\"M921.823 385.544c-1.739 9.04-2.995 17.971-5.813 26.378-4.946 14.759-10.771 29.227-16.436 43.736-.804 2.058-2.676 3.749-4.221 5.459l-29.057 31.86c-2.455 2.623-5.745 4.447-8.529 6.784-9.69 8.135-19.551 15.908-33.208 17.237-1.773-9.728-3.147-19.457-4.091-29.6l36.13-16.763c.581-.267 1.046-.812 1.525-1.269 8.033-7.688 16.258-15.19 24.011-23.152 4.35-4.467 9.202-9.144 11.588-14.69 6.638-15.425 15.047-30.299 17.274-47.358 3.536.344 7.072.688 10.829 1.377z\"></path><path fill=\"#f3944d\" d=\"M1738.688 798.998c-4.375 6.495-8.906 12.953-13.803 19.121-6.771 8.528-13.519 17.11-20.864 25.131-3.958 4.322-9.141 7.523-13.925 11.54-8.036-13.464-16.465-26.844-27.999-38.387 5.988-6.951 12.094-13.629 18.261-20.25l19.547-20.95 38.783 23.794z\"></path><path fill=\"#ec6168\" d=\"M1239.583 703.142c3.282 1.805 6.441 3.576 9.217 5.821 5.88 4.755 11.599 9.713 17.313 14.669l22.373 19.723c1.781 1.622 3.2 3.806 4.307 5.975 3.843 7.532 7.477 15.171 11.194 23.136-10.764 4.67-21.532 8.973-32.69 12.982l-22.733-27.366c-2.003-2.416-4.096-4.758-6.194-7.093-3.539-3.94-6.927-8.044-10.74-11.701-2.57-2.465-5.762-4.283-8.675-6.39l16.627-29.755z\"></path><path fill=\"#ec663e\" d=\"M1351.006 332.839l-28.499 10.33c-.294.107-.533.367-1.194.264-11.067-19.018-27.026-32.559-44.225-44.855-4.267-3.051-8.753-5.796-13.138-8.682l9.505-24.505c10.055 4.069 19.821 8.227 29.211 13.108 3.998 2.078 7.299 5.565 10.753 8.598 3.077 2.701 5.743 5.891 8.926 8.447 4.116 3.304 9.787 5.345 12.62 9.432 6.083 8.777 10.778 18.517 16.041 27.863z\"></path><path fill=\"#eb5e5b\" d=\"M1222.647 733.051c3.223 1.954 6.415 3.771 8.985 6.237 3.813 3.658 7.201 7.761 10.74 11.701l6.194 7.093 22.384 27.409c-13.056 6.836-25.309 14.613-36.736 24.161l-39.323-44.7 24.494-27.846c1.072-1.224 1.974-2.598 3.264-4.056z\"></path><path fill=\"#ea580e\" d=\"M876.001 376.171c5.874 1.347 11.748 2.694 17.812 4.789-.81 5.265-2.687 9.791-2.639 14.296.124 11.469-4.458 20.383-12.73 27.863-2.075 1.877-3.659 4.286-5.668 6.248l-22.808 21.967c-.442.422-1.212.488-1.813.757l-23.113 10.389-9.875 4.514c-2.305-6.09-4.609-12.181-6.614-18.676 7.64-4.837 15.567-8.54 22.18-13.873 9.697-7.821 18.931-16.361 27.443-25.455 5.613-5.998 12.679-11.331 14.201-20.475.699-4.2 2.384-8.235 3.623-12.345z\"></path><path fill=\"#e95514\" d=\"M815.103 467.384c3.356-1.894 6.641-3.415 9.94-4.903l23.113-10.389c.6-.269 1.371-.335 1.813-.757l22.808-21.967c2.008-1.962 3.593-4.371 5.668-6.248 8.272-7.48 12.854-16.394 12.73-27.863-.049-4.505 1.828-9.031 2.847-13.956 5.427.559 10.836 1.526 16.609 2.68-1.863 17.245-10.272 32.119-16.91 47.544-2.387 5.546-7.239 10.223-11.588 14.69-7.753 7.962-15.978 15.464-24.011 23.152-.478.458-.944 1.002-1.525 1.269l-36.069 16.355c-2.076-6.402-3.783-12.81-5.425-19.607z\"></path><path fill=\"#eb620b\" d=\"M783.944 404.402c9.499-8.388 19.556-15.905 28.437-24.621 6.631-6.508 11.744-14.564 17.575-22.273 9.271 4.016 18.501 8.375 27.893 13.43-4.134 7.07-8.017 13.778-12.833 19.731-5.785 7.15-12.109 13.917-18.666 20.376-7.99 7.869-16.466 15.244-24.731 22.832l-17.674-29.475z\"></path><path fill=\"#ea544c\" d=\"M1197.986 854.686c-9.756-3.309-16.79-10.044-22.88-18.059l-28.001-36.417c8.601-5.939 17.348-11.563 26.758-17.075 1.615 1.026 2.639 1.876 3.505 2.865l26.664 30.44c3.723 4.139 7.995 7.785 12.017 11.656l-18.064 26.591z\"></path><path fill=\"#ec6333\" d=\"M1351.41 332.903c-5.667-9.409-10.361-19.149-16.445-27.926-2.833-4.087-8.504-6.128-12.62-9.432-3.184-2.555-5.849-5.745-8.926-8.447-3.454-3.033-6.756-6.52-10.753-8.598-9.391-4.88-19.157-9.039-29.138-13.499 1.18-5.441 2.727-10.873 4.81-16.607 11.918 4.674 24.209 8.261 34.464 14.962 14.239 9.304 29.011 18.453 39.595 32.464 2.386 3.159 5.121 6.077 7.884 8.923 6.564 6.764 10.148 14.927 11.723 24.093l-20.594 4.067z\"></path><path fill=\"#eb5e5b\" d=\"M1117 536.549c-6.113-4.702-9.965-11.44-11.917-18.955-2.292-8.819-4.066-17.74-9.467-25.337-4.327-6.085-3.122-13.382-4.6-20.088l-4.55-21.241c-1.59-8.054-3.172-16.118-4.422-24.23l-5.037-36.129c6.382-1.43 12.777-2.462 19.582-3.443 1.906 11.646 3.426 23.24 4.878 34.842.307 2.453.717 4.973.477 7.402-1.86 18.84 2.834 36.934 5.347 55.352 1.474 10.806 4.885 20.848 7.101 31.302 1.394 6.579 1.774 13.374 2.609 20.523z\"></path><path fill=\"#ec644b\" d=\"M1263.638 290.071c4.697 2.713 9.183 5.458 13.45 8.509 17.199 12.295 33.158 25.836 43.873 44.907-8.026 4.725-16.095 9.106-24.83 13.372-11.633-15.937-25.648-28.515-41.888-38.689-1.609-1.008-3.555-1.48-5.344-2.2 2.329-3.852 4.766-7.645 6.959-11.573l7.78-14.326z\"></path><path fill=\"#eb5f2d\" d=\"M1372.453 328.903c-2.025-9.233-5.608-17.396-12.172-24.16-2.762-2.846-5.498-5.764-7.884-8.923-10.584-14.01-25.356-23.16-39.595-32.464-10.256-6.701-22.546-10.289-34.284-15.312.325-5.246 1.005-10.444 2.027-15.863l47.529 22.394c.89.428 1.83.901 2.516 1.584l45.564 45.193c7.69 7.233 9.352 16.472 11.849 26.084-5.032.773-10.066 1.154-15.55 1.466z\"></path><path fill=\"#e95a0f\" d=\"M801.776 434.171c8.108-7.882 16.584-15.257 24.573-23.126 6.558-6.459 12.881-13.226 18.666-20.376 4.817-5.953 8.7-12.661 13.011-19.409 5.739 1.338 11.463 3.051 17.581 4.838-.845 4.183-2.53 8.219-3.229 12.418-1.522 9.144-8.588 14.477-14.201 20.475-8.512 9.094-17.745 17.635-27.443 25.455-6.613 5.333-14.54 9.036-22.223 13.51-2.422-4.469-4.499-8.98-6.735-13.786z\"></path><path fill=\"#eb5e5b\" d=\"M1248.533 316.002c2.155.688 4.101 1.159 5.71 2.168 16.24 10.174 30.255 22.752 41.532 38.727-7.166 5.736-14.641 11.319-22.562 16.731-1.16-1.277-1.684-2.585-2.615-3.46l-38.694-36.2 14.203-15.029c.803-.86 1.38-1.93 2.427-2.936z\"></path><path fill=\"#eb5a57\" d=\"M1216.359 827.958c-4.331-3.733-8.603-7.379-12.326-11.518l-26.664-30.44c-.866-.989-1.89-1.839-3.152-2.902 6.483-6.054 13.276-11.959 20.371-18.005l39.315 44.704c-5.648 6.216-11.441 12.12-17.544 18.161z\"></path><path fill=\"#ec6168\" d=\"M1231.598 334.101l38.999 36.066c.931.876 1.456 2.183 2.303 3.608-4.283 4.279-8.7 8.24-13.769 12.091-4.2-3.051-7.512-6.349-11.338-8.867-12.36-8.136-22.893-18.27-32.841-29.093l16.646-13.805z\"></path><path fill=\"#ed656e\" d=\"M1214.597 347.955c10.303 10.775 20.836 20.908 33.196 29.044 3.825 2.518 7.137 5.816 10.992 8.903-3.171 4.397-6.65 8.648-10.432 13.046-6.785-5.184-13.998-9.858-19.529-16.038-4.946-5.527-9.687-8.644-17.309-8.215-2.616.147-5.734-2.788-8.067-4.923-3.026-2.769-5.497-6.144-8.35-9.568 6.286-4.273 12.715-8.237 19.499-12.25z\"></path></svg>\n</p>\n\n<p align=\"center\">\n<b>The crispy rerank family from <a href=\"https://mixedbread.ai\"><b>Mixedbread</b></a>.</b>\n</p>\n\n<p align=\"center\">\n<sup> 🍞 Looking for a simple end-to-end retrieval solution? Meet Omni, our multimodal and multilingual model. <a href=\"https://mixedbread.com\"><b>Get in touch for access.</a> </sup>\n</p>\n\n# mxbai-rerank-base-v1\n\nThis is the base model in our family of powerful reranker models. You can learn more about the models in our [blog post](https://www.mixedbread.ai/blog/mxbai-rerank-v1).\n\nWe have three models:\n\n- [mxbai-rerank-xsmall-v1](https://huggingface.co/mixedbread-ai/mxbai-rerank-xsmall-v1)\n- [mxbai-rerank-base-v1](https://huggingface.co/mixedbread-ai/mxbai-rerank-base-v1) (🍞)\n- [mxbai-rerank-large-v1](https://huggingface.co/mixedbread-ai/mxbai-rerank-large-v1)\n\n## Quickstart\n\nCurrently, the best way to use our models is with the most recent version of sentence-transformers.\n\n`pip install -U sentence-transformers`\n\nLet's say you have a query, and you want to rerank a set of documents. You can do that with only one line of code:\n\n```python\nfrom sentence_transformers import CrossEncoder\n\n# Load the model, here we use our base sized model\nmodel = CrossEncoder(\"mixedbread-ai/mxbai-rerank-base-v1\")\n\n\n# Example query and documents\nquery = \"Who wrote 'To Kill a Mockingbird'?\"\ndocuments = [\n    \"'To Kill a Mockingbird' is a novel by Harper Lee published in 1960. It was immediately successful, winning the Pulitzer Prize, and has become a classic of modern American literature.\",\n    \"The novel 'Moby-Dick' was written by Herman Melville and first published in 1851. It is considered a masterpiece of American literature and deals with complex themes of obsession, revenge, and the conflict between good and evil.\",\n    \"Harper Lee, an American novelist widely known for her novel 'To Kill a Mockingbird', was born in 1926 in Monroeville, Alabama. She received the Pulitzer Prize for Fiction in 1961.\",\n    \"Jane Austen was an English novelist known primarily for her six major novels, which interpret, critique and comment upon the British landed gentry at the end of the 18th century.\",\n    \"The 'Harry Potter' series, which consists of seven fantasy novels written by British author J.K. Rowling, is among the most popular and critically acclaimed books of the modern era.\",\n    \"'The Great Gatsby', a novel written by American author F. Scott Fitzgerald, was published in 1925. The story is set in the Jazz Age and follows the life of millionaire Jay Gatsby and his pursuit of Daisy Buchanan.\"\n]\n\n# Lets get the scores\nresults = model.rank(query, documents, return_documents=True, top_k=3)\n```\n\n\n<details>\n  <summary>JavaScript Example</summary>\n\nInstall [transformers.js](https://github.com/xenova/transformers.js)\n\n`npm i @xenova/transformers`\n\nLet's say you have a query, and you want to rerank a set of documents. In JavaScript, you need to add a function:\n\n```javascript\nimport { AutoTokenizer, AutoModelForSequenceClassification } from '@xenova/transformers';\n\nconst model_id = 'mixedbread-ai/mxbai-rerank-base-v1';\nconst model = await AutoModelForSequenceClassification.from_pretrained(model_id);\nconst tokenizer = await AutoTokenizer.from_pretrained(model_id);\n\n/**\n * Performs ranking with the CrossEncoder on the given query and documents. Returns a sorted list with the document indices and scores.\n * @param {string} query A single query\n * @param {string[]} documents A list of documents\n * @param {Object} options Options for ranking\n * @param {number} [options.top_k=undefined] Return the top-k documents. If undefined, all documents are returned.\n * @param {number} [options.return_documents=false] If true, also returns the documents. If false, only returns the indices and scores.\n */\nasync function rank(query, documents, {\n    top_k = undefined,\n    return_documents = false,\n} = {}) {\n    const inputs = tokenizer(\n        new Array(documents.length).fill(query),\n        {\n            text_pair: documents,\n            padding: true,\n            truncation: true,\n        }\n    )\n    const { logits } = await model(inputs);\n    return logits\n        .sigmoid()\n        .tolist()\n        .map(([score], i) => ({\n            corpus_id: i,\n            score,\n            ...(return_documents ? { text: documents[i] } : {})\n        }))\n        .sort((a, b) => b.score - a.score)\n        .slice(0, top_k);\n}\n\n// Example usage:\nconst query = \"Who wrote 'To Kill a Mockingbird'?\"\nconst documents = [\n    \"'To Kill a Mockingbird' is a novel by Harper Lee published in 1960. It was immediately successful, winning the Pulitzer Prize, and has become a classic of modern American literature.\",\n    \"The novel 'Moby-Dick' was written by Herman Melville and first published in 1851. It is considered a masterpiece of American literature and deals with complex themes of obsession, revenge, and the conflict between good and evil.\",\n    \"Harper Lee, an American novelist widely known for her novel 'To Kill a Mockingbird', was born in 1926 in Monroeville, Alabama. She received the Pulitzer Prize for Fiction in 1961.\",\n    \"Jane Austen was an English novelist known primarily for her six major novels, which interpret, critique and comment upon the British landed gentry at the end of the 18th century.\",\n    \"The 'Harry Potter' series, which consists of seven fantasy novels written by British author J.K. Rowling, is among the most popular and critically acclaimed books of the modern era.\",\n    \"'The Great Gatsby', a novel written by American author F. Scott Fitzgerald, was published in 1925. The story is set in the Jazz Age and follows the life of millionaire Jay Gatsby and his pursuit of Daisy Buchanan.\"\n]\n\nconst results = await rank(query, documents, { return_documents: true, top_k: 3 });\nconsole.log(results);\n```\n</details>\n\n## Using API\n\nYou can use the large model via our API as follows:\n\n```python\nfrom mixedbread_ai.client import MixedbreadAI\n\nmxbai = MixedbreadAI(api_key=\"{MIXEDBREAD_API_KEY}\")\n\nres = mxbai.reranking(\n  model=\"mixedbread-ai/mxbai-rerank-large-v1\",\n  query=\"Who is the author of To Kill a Mockingbird?\",\n  input=[\n    \"To Kill a Mockingbird is a novel by Harper Lee published in 1960. It was immediately successful, winning the Pulitzer Prize, and has become a classic of modern American literature.\",\n    \"The novel Moby-Dick was written by Herman Melville and first published in 1851. It is considered a masterpiece of American literature and deals with complex themes of obsession, revenge, and the conflict between good and evil.\",\n    \"Harper Lee, an American novelist widely known for her novel To Kill a Mockingbird, was born in 1926 in Monroeville, Alabama. She received the Pulitzer Prize for Fiction in 1961.\",\n    \"Jane Austen was an English novelist known primarily for her six major novels, which interpret, critique and comment upon the British landed gentry at the end of the 18th century.\",\n    \"The Harry Potter series, which consists of seven fantasy novels written by British author J.K. Rowling, is among the most popular and critically acclaimed books of the modern era.\",\n    \"The Great Gatsby, a novel written by American author F. Scott Fitzgerald, was published in 1925. The story is set in the Jazz Age and follows the life of millionaire Jay Gatsby and his pursuit of Daisy Buchanan.\"\n  ],\n  top_k=3,\n  return_input=false\n)\n\nprint(res.data)\n```\n\nThe API comes with additional features, such as a continous trained reranker! Check out the [docs](https://www.mixedbread.ai/docs) for more information.\n\n## Evaluation\n\nOur reranker models are designed to elevate your search. They work extremely well in combination with keyword search and can even outperform semantic search systems in many cases.\n\n| Model                                                                                 | NDCG@10  | Accuracy@3 |\n| ------------------------------------------------------------------------------------- | -------- | ---------- |\n| Lexical Search (Lucene)                                                               | 38.0     | 66.4       |\n| [BAAI/bge-reranker-base](https://huggingface.co/BAAI/bge-reranker-base)               | 41.6     | 66.9       |\n| [BAAI/bge-reranker-large](https://huggingface.co/BAAI/bge-reranker-large)             | 45.2     | 70.6       |\n| cohere-embed-v3 (semantic search)                                                     | 47.5     | 70.9       |\n| [mxbai-rerank-xsmall-v1](https://huggingface.co/mixedbread-ai/mxbai-rerank-xsmall-v1) | **43.9** | **70.0**   |\n| [mxbai-rerank-base-v1](https://huggingface.co/mixedbread-ai/mxbai-rerank-base-v1)     | **46.9** | **72.3**   |\n| [mxbai-rerank-large-v1](https://huggingface.co/mixedbread-ai/mxbai-rerank-large-v1)   | **48.8** | **74.9**   |\n\nThe reported results are aggregated from 11 datasets of BEIR. We used [Pyserini](https://github.com/castorini/pyserini/) to evaluate the models. Find more in our [blog-post](https://www.mixedbread.ai/blog/mxbai-rerank-v1) and on this [spreadsheet](https://docs.google.com/spreadsheets/d/15ELkSMFv-oHa5TRiIjDvhIstH9dlc3pnZeO-iGz4Ld4/edit?usp=sharing).\n\n## Community\nPlease join our [Discord Community](https://discord.gg/jDfMHzAVfU) and share your feedback and thoughts! We are here to help and also always happy to chat.\n\n## Citation\n\n```bibtex\n@online{rerank2024mxbai,\n  title={Boost Your Search With The Crispy Mixedbread Rerank Models},\n  author={Aamir Shakir and Darius Koenig and Julius Lipp and Sean Lee},\n  year={2024},\n  url={https://www.mixedbread.ai/blog/mxbai-rerank-v1},\n}\n```\n\n## License\nApache 2.0",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModelForSequenceClassification",
      "custom_class": null,
      "pipeline_tag": "text-classification",
      "processor": "AutoTokenizer"
    },
    "safetensors": {
      "parameters": {
        "F16": 184422913
      },
      "total": 184422913
    },
    "model_index": null,
    "trending_score": null
  },
  {
    "model_id": "martin-ha/toxic-comment-model",
    "model_name": "martin-ha/toxic-comment-model",
    "author": "martin-ha",
    "downloads": 596517,
    "downloads_all_time": null,
    "likes": 61,
    "tags": [
      "transformers",
      "pytorch",
      "distilbert",
      "text-classification",
      "en",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/martin-ha/toxic-comment-model",
    "dependencies": [
      [
        "transformers",
        null
      ]
    ],
    "last_modified": "2022-05-06T02:24:31+00:00",
    "created_at": "2022-03-02T23:29:05+00:00",
    "analysis_date": "2025-03-22T00:55:25.096388",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "distilbert",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": "en"
    },
    "card_text": "\n## Model description\nThis model is a fine-tuned version of the [DistilBERT model](https://huggingface.co/transformers/model_doc/distilbert.html) to classify toxic comments. \n\n## How to use\n\nYou can use the model with the following code.\n\n```python\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, TextClassificationPipeline\n\nmodel_path = \"martin-ha/toxic-comment-model\"\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_path)\n\npipeline =  TextClassificationPipeline(model=model, tokenizer=tokenizer)\nprint(pipeline('This is a test text.'))\n```\n\n## Limitations and Bias\n\nThis model is intended to use for classify toxic online classifications. However, one limitation of the model is that it performs poorly for some comments that mention a specific identity subgroup, like Muslim. The following table shows a evaluation score for different identity group. You can learn the specific meaning of this metrics [here](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/overview/evaluation). But basically, those metrics shows how well a model performs for a specific group. The larger the number, the better.\n\n| **subgroup**                  | **subgroup_size** | **subgroup_auc** | **bpsn_auc** | **bnsp_auc** |\n| ----------------------------- | ----------------- | ---------------- | ------------ | ------------ |\n| muslim                        | 108               | 0.689            | 0.811        | 0.88         |\n| jewish                        | 40                | 0.749            | 0.86         | 0.825        |\n| homosexual_gay_or_lesbian     | 56                | 0.795            | 0.706        | 0.972        |\n| black                         | 84                | 0.866            | 0.758        | 0.975        |\n| white                         | 112               | 0.876            | 0.784        | 0.97         |\n| female                        | 306               | 0.898            | 0.887        | 0.948        |\n| christian                     | 231               | 0.904            | 0.917        | 0.93         |\n| male                          | 225               | 0.922            | 0.862        | 0.967        |\n| psychiatric_or_mental_illness | 26                | 0.924            | 0.907        | 0.95         |\n\nThe table above shows that the model performs poorly for the muslim and jewish group. In fact, you pass the sentence \"Muslims are people who follow or practice Islam, an Abrahamic monotheistic religion.\" Into the model, the model will classify it as toxic. Be mindful for this type of potential bias.\n\n## Training data\nThe training data comes this [Kaggle competition](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/data). We use 10% of the `train.csv` data to train the model.\n\n## Training procedure\n\nYou can see [this documentation and codes](https://github.com/MSIA/wenyang_pan_nlp_project_2021) for how we train the model. It takes about 3 hours in a P-100 GPU.\n\n## Evaluation results\n\nThe model achieves 94% accuracy and 0.59 f1-score in a 10000 rows held-out test set.",
    "card_content": "---\nlanguage: en\n---\n\n## Model description\nThis model is a fine-tuned version of the [DistilBERT model](https://huggingface.co/transformers/model_doc/distilbert.html) to classify toxic comments. \n\n## How to use\n\nYou can use the model with the following code.\n\n```python\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, TextClassificationPipeline\n\nmodel_path = \"martin-ha/toxic-comment-model\"\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_path)\n\npipeline =  TextClassificationPipeline(model=model, tokenizer=tokenizer)\nprint(pipeline('This is a test text.'))\n```\n\n## Limitations and Bias\n\nThis model is intended to use for classify toxic online classifications. However, one limitation of the model is that it performs poorly for some comments that mention a specific identity subgroup, like Muslim. The following table shows a evaluation score for different identity group. You can learn the specific meaning of this metrics [here](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/overview/evaluation). But basically, those metrics shows how well a model performs for a specific group. The larger the number, the better.\n\n| **subgroup**                  | **subgroup_size** | **subgroup_auc** | **bpsn_auc** | **bnsp_auc** |\n| ----------------------------- | ----------------- | ---------------- | ------------ | ------------ |\n| muslim                        | 108               | 0.689            | 0.811        | 0.88         |\n| jewish                        | 40                | 0.749            | 0.86         | 0.825        |\n| homosexual_gay_or_lesbian     | 56                | 0.795            | 0.706        | 0.972        |\n| black                         | 84                | 0.866            | 0.758        | 0.975        |\n| white                         | 112               | 0.876            | 0.784        | 0.97         |\n| female                        | 306               | 0.898            | 0.887        | 0.948        |\n| christian                     | 231               | 0.904            | 0.917        | 0.93         |\n| male                          | 225               | 0.922            | 0.862        | 0.967        |\n| psychiatric_or_mental_illness | 26                | 0.924            | 0.907        | 0.95         |\n\nThe table above shows that the model performs poorly for the muslim and jewish group. In fact, you pass the sentence \"Muslims are people who follow or practice Islam, an Abrahamic monotheistic religion.\" Into the model, the model will classify it as toxic. Be mindful for this type of potential bias.\n\n## Training data\nThe training data comes this [Kaggle competition](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/data). We use 10% of the `train.csv` data to train the model.\n\n## Training procedure\n\nYou can see [this documentation and codes](https://github.com/MSIA/wenyang_pan_nlp_project_2021) for how we train the model. It takes about 3 hours in a P-100 GPU.\n\n## Evaluation results\n\nThe model achieves 94% accuracy and 0.59 f1-score in a 10000 rows held-out test set.",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModelForSequenceClassification",
      "custom_class": null,
      "pipeline_tag": "text-classification",
      "processor": "AutoTokenizer"
    },
    "safetensors": null,
    "model_index": null,
    "trending_score": null
  },
  {
    "model_id": "SamLowe/roberta-base-go_emotions",
    "model_name": "SamLowe/roberta-base-go_emotions",
    "author": "SamLowe",
    "downloads": 559008,
    "downloads_all_time": null,
    "likes": 551,
    "tags": [
      "transformers",
      "pytorch",
      "safetensors",
      "roberta",
      "text-classification",
      "emotions",
      "multi-class-classification",
      "multi-label-classification",
      "en",
      "dataset:go_emotions",
      "doi:10.57967/hf/3548",
      "license:mit",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/SamLowe/roberta-base-go_emotions",
    "dependencies": [
      [
        "transformers",
        null
      ]
    ],
    "last_modified": "2023-10-04T10:00:58+00:00",
    "created_at": "2022-09-15T13:04:21+00:00",
    "analysis_date": "2025-03-22T00:55:26.272917",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "roberta",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": "en",
      "tags": [
        "text-classification",
        "pytorch",
        "roberta",
        "emotions",
        "multi-class-classification",
        "multi-label-classification"
      ],
      "datasets": [
        "go_emotions"
      ],
      "license": "mit",
      "widget": [
        {
          "text": "I am not having a great day."
        }
      ]
    },
    "card_text": "\n#### Overview\n\nModel trained from [roberta-base](https://huggingface.co/roberta-base) on the [go_emotions](https://huggingface.co/datasets/go_emotions) dataset for multi-label classification.\n\n##### ONNX version also available\n\nA version of this model in ONNX format (including an INT8 quantized ONNX version) is now available at [https://huggingface.co/SamLowe/roberta-base-go_emotions-onnx](https://huggingface.co/SamLowe/roberta-base-go_emotions-onnx). These are faster for inference, esp for smaller batch sizes, massively reduce the size of the dependencies required for inference, make inference of the model more multi-platform, and in the case of the quantized version reduce the model file/download size by 75% whilst retaining almost all the accuracy if you only need inference.\n\n#### Dataset used for the model\n\n[go_emotions](https://huggingface.co/datasets/go_emotions) is based on Reddit data and has 28 labels. It is a multi-label dataset where one or multiple labels may apply for any given input text, hence this model is a multi-label classification model with 28 'probability' float outputs for any given input text. Typically a threshold of 0.5 is applied to the probabilities for the prediction for each label.\n\n#### How the model was created\n\nThe model was trained using `AutoModelForSequenceClassification.from_pretrained` with `problem_type=\"multi_label_classification\"` for 3 epochs with a learning rate of 2e-5 and weight decay of 0.01.\n\n#### Inference\n\nThere are multiple ways to use this model in Huggingface Transformers. Possibly the simplest is using a pipeline:\n\n```python\nfrom transformers import pipeline\n\nclassifier = pipeline(task=\"text-classification\", model=\"SamLowe/roberta-base-go_emotions\", top_k=None)\n\nsentences = [\"I am not having a great day\"]\n\nmodel_outputs = classifier(sentences)\nprint(model_outputs[0])\n# produces a list of dicts for each of the labels\n```\n\n#### Evaluation / metrics\n\nEvaluation of the model is available at\n\n- https://github.com/samlowe/go_emotions-dataset/blob/main/eval-roberta-base-go_emotions.ipynb\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/samlowe/go_emotions-dataset/blob/main/eval-roberta-base-go_emotions.ipynb)\n\n##### Summary\n\nAs provided in the above notebook, evaluation of the multi-label output (of the 28 dim output via a threshold of 0.5 to binarize each) using the dataset test split gives:\n\n- Accuracy: 0.474\n- Precision: 0.575\n- Recall: 0.396\n- F1: 0.450\n\nBut the metrics are more meaningful when measured per label given the multi-label nature (each label is effectively an independent binary classification) and the fact that there is drastically different representations of the labels in the dataset.\n\nWith a threshold of 0.5 applied to binarize the model outputs, as per the above notebook, the metrics per label are:\n\n|                | accuracy | precision | recall | f1    | mcc   | support | threshold |\n| -------------- | -------- | --------- | ------ | ----- | ----- | ------- | --------- |\n| admiration     | 0.946    | 0.725     | 0.675  | 0.699 | 0.670 | 504     | 0.5       |\n| amusement      | 0.982    | 0.790     | 0.871  | 0.829 | 0.821 | 264     | 0.5       |\n| anger          | 0.970    | 0.652     | 0.379  | 0.479 | 0.483 | 198     | 0.5       |\n| annoyance      | 0.940    | 0.472     | 0.159  | 0.238 | 0.250 | 320     | 0.5       |\n| approval       | 0.942    | 0.609     | 0.302  | 0.404 | 0.403 | 351     | 0.5       |\n| caring         | 0.973    | 0.448     | 0.319  | 0.372 | 0.364 | 135     | 0.5       |\n| confusion      | 0.972    | 0.500     | 0.431  | 0.463 | 0.450 | 153     | 0.5       |\n| curiosity      | 0.950    | 0.537     | 0.356  | 0.428 | 0.412 | 284     | 0.5       |\n| desire         | 0.987    | 0.630     | 0.410  | 0.496 | 0.502 | 83      | 0.5       |\n| disappointment | 0.974    | 0.625     | 0.199  | 0.302 | 0.343 | 151     | 0.5       |\n| disapproval    | 0.950    | 0.494     | 0.307  | 0.379 | 0.365 | 267     | 0.5       |\n| disgust        | 0.982    | 0.707     | 0.333  | 0.453 | 0.478 | 123     | 0.5       |\n| embarrassment  | 0.994    | 0.750     | 0.243  | 0.367 | 0.425 | 37      | 0.5       |\n| excitement     | 0.983    | 0.603     | 0.340  | 0.435 | 0.445 | 103     | 0.5       |\n| fear           | 0.992    | 0.758     | 0.603  | 0.671 | 0.672 | 78      | 0.5       |\n| gratitude      | 0.990    | 0.960     | 0.881  | 0.919 | 0.914 | 352     | 0.5       |\n| grief          | 0.999    | 0.000     | 0.000  | 0.000 | 0.000 | 6       | 0.5       |\n| joy            | 0.978    | 0.647     | 0.559  | 0.600 | 0.590 | 161     | 0.5       |\n| love           | 0.982    | 0.773     | 0.832  | 0.802 | 0.793 | 238     | 0.5       |\n| nervousness    | 0.996    | 0.600     | 0.130  | 0.214 | 0.278 | 23      | 0.5       |\n| optimism       | 0.972    | 0.667     | 0.376  | 0.481 | 0.488 | 186     | 0.5       |\n| pride          | 0.997    | 0.000     | 0.000  | 0.000 | 0.000 | 16      | 0.5       |\n| realization    | 0.974    | 0.541     | 0.138  | 0.220 | 0.264 | 145     | 0.5       |\n| relief         | 0.998    | 0.000     | 0.000  | 0.000 | 0.000 | 11      | 0.5       |\n| remorse        | 0.991    | 0.553     | 0.750  | 0.636 | 0.640 | 56      | 0.5       |\n| sadness        | 0.977    | 0.621     | 0.494  | 0.550 | 0.542 | 156     | 0.5       |\n| surprise       | 0.981    | 0.750     | 0.404  | 0.525 | 0.542 | 141     | 0.5       |\n| neutral        | 0.782    | 0.694     | 0.604  | 0.646 | 0.492 | 1787    | 0.5       |\n\nOptimizing the threshold per label for the one that gives the optimum F1 metrics gives slightly better metrics - sacrificing some precision for a greater gain in recall, hence to the benefit of F1 (how this was done is shown in the above notebook):\n\n|                | accuracy | precision | recall | f1    | mcc   | support | threshold |\n| -------------- | -------- | --------- | ------ | ----- | ----- | ------- | --------- |\n| admiration     | 0.940    | 0.651     | 0.776  | 0.708 | 0.678 | 504     | 0.25      |\n| amusement      | 0.982    | 0.781     | 0.890  | 0.832 | 0.825 | 264     | 0.45      |\n| anger          | 0.959    | 0.454     | 0.601  | 0.517 | 0.502 | 198     | 0.15      |\n| annoyance      | 0.864    | 0.243     | 0.619  | 0.349 | 0.328 | 320     | 0.10      |\n| approval       | 0.926    | 0.432     | 0.442  | 0.437 | 0.397 | 351     | 0.30      |\n| caring         | 0.972    | 0.426     | 0.385  | 0.405 | 0.391 | 135     | 0.40      |\n| confusion      | 0.974    | 0.548     | 0.412  | 0.470 | 0.462 | 153     | 0.55      |\n| curiosity      | 0.943    | 0.473     | 0.711  | 0.568 | 0.552 | 284     | 0.25      |\n| desire         | 0.985    | 0.518     | 0.530  | 0.524 | 0.516 | 83      | 0.25      |\n| disappointment | 0.974    | 0.562     | 0.298  | 0.390 | 0.398 | 151     | 0.40      |\n| disapproval    | 0.941    | 0.414     | 0.468  | 0.439 | 0.409 | 267     | 0.30      |\n| disgust        | 0.978    | 0.523     | 0.463  | 0.491 | 0.481 | 123     | 0.20      |\n| embarrassment  | 0.994    | 0.567     | 0.459  | 0.507 | 0.507 | 37      | 0.10      |\n| excitement     | 0.981    | 0.500     | 0.417  | 0.455 | 0.447 | 103     | 0.35      |\n| fear           | 0.991    | 0.712     | 0.667  | 0.689 | 0.685 | 78      | 0.40      |\n| gratitude      | 0.990    | 0.957     | 0.889  | 0.922 | 0.917 | 352     | 0.45      |\n| grief          | 0.999    | 0.333     | 0.333  | 0.333 | 0.333 | 6       | 0.05      |\n| joy            | 0.978    | 0.623     | 0.646  | 0.634 | 0.623 | 161     | 0.40      |\n| love           | 0.982    | 0.740     | 0.899  | 0.812 | 0.807 | 238     | 0.25      |\n| nervousness    | 0.996    | 0.571     | 0.348  | 0.432 | 0.444 | 23      | 0.25      |\n| optimism       | 0.971    | 0.580     | 0.565  | 0.572 | 0.557 | 186     | 0.20      |\n| pride          | 0.998    | 0.875     | 0.438  | 0.583 | 0.618 | 16      | 0.10      |\n| realization    | 0.961    | 0.270     | 0.262  | 0.266 | 0.246 | 145     | 0.15      |\n| relief         | 0.992    | 0.152     | 0.636  | 0.246 | 0.309 | 11      | 0.05      |\n| remorse        | 0.991    | 0.541     | 0.946  | 0.688 | 0.712 | 56      | 0.10      |\n| sadness        | 0.977    | 0.599     | 0.583  | 0.591 | 0.579 | 156     | 0.40      |\n| surprise       | 0.977    | 0.543     | 0.674  | 0.601 | 0.593 | 141     | 0.15      |\n| neutral        | 0.758    | 0.598     | 0.810  | 0.688 | 0.513 | 1787    | 0.25      |\n\nThis improves the overall metrics:\n\n- Precision: 0.542\n- Recall: 0.577\n- F1: 0.541\n\nOr if calculated weighted by the relative size of the support of each label:\n\n- Precision: 0.572\n- Recall: 0.677\n- F1: 0.611\n\n#### Commentary on the dataset\n\nSome labels (E.g. gratitude) when considered independently perform very strongly with F1 exceeding 0.9, whilst others (E.g. relief) perform very poorly.\n\nThis is a challenging dataset. Labels such as relief do have much fewer examples in the training data (less than 100 out of the 40k+, and only 11 in the test split).\n\nBut there is also some ambiguity and/or labelling errors visible in the training data of go_emotions that is suspected to constrain the performance. Data cleaning on the dataset to reduce some of the mistakes, ambiguity, conflicts and duplication in the labelling would produce a higher performing model.",
    "card_content": "---\nlanguage: en\ntags:\n- text-classification\n- pytorch\n- roberta\n- emotions\n- multi-class-classification\n- multi-label-classification\ndatasets:\n- go_emotions\nlicense: mit\nwidget:\n- text: I am not having a great day.\n---\n\n#### Overview\n\nModel trained from [roberta-base](https://huggingface.co/roberta-base) on the [go_emotions](https://huggingface.co/datasets/go_emotions) dataset for multi-label classification.\n\n##### ONNX version also available\n\nA version of this model in ONNX format (including an INT8 quantized ONNX version) is now available at [https://huggingface.co/SamLowe/roberta-base-go_emotions-onnx](https://huggingface.co/SamLowe/roberta-base-go_emotions-onnx). These are faster for inference, esp for smaller batch sizes, massively reduce the size of the dependencies required for inference, make inference of the model more multi-platform, and in the case of the quantized version reduce the model file/download size by 75% whilst retaining almost all the accuracy if you only need inference.\n\n#### Dataset used for the model\n\n[go_emotions](https://huggingface.co/datasets/go_emotions) is based on Reddit data and has 28 labels. It is a multi-label dataset where one or multiple labels may apply for any given input text, hence this model is a multi-label classification model with 28 'probability' float outputs for any given input text. Typically a threshold of 0.5 is applied to the probabilities for the prediction for each label.\n\n#### How the model was created\n\nThe model was trained using `AutoModelForSequenceClassification.from_pretrained` with `problem_type=\"multi_label_classification\"` for 3 epochs with a learning rate of 2e-5 and weight decay of 0.01.\n\n#### Inference\n\nThere are multiple ways to use this model in Huggingface Transformers. Possibly the simplest is using a pipeline:\n\n```python\nfrom transformers import pipeline\n\nclassifier = pipeline(task=\"text-classification\", model=\"SamLowe/roberta-base-go_emotions\", top_k=None)\n\nsentences = [\"I am not having a great day\"]\n\nmodel_outputs = classifier(sentences)\nprint(model_outputs[0])\n# produces a list of dicts for each of the labels\n```\n\n#### Evaluation / metrics\n\nEvaluation of the model is available at\n\n- https://github.com/samlowe/go_emotions-dataset/blob/main/eval-roberta-base-go_emotions.ipynb\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/samlowe/go_emotions-dataset/blob/main/eval-roberta-base-go_emotions.ipynb)\n\n##### Summary\n\nAs provided in the above notebook, evaluation of the multi-label output (of the 28 dim output via a threshold of 0.5 to binarize each) using the dataset test split gives:\n\n- Accuracy: 0.474\n- Precision: 0.575\n- Recall: 0.396\n- F1: 0.450\n\nBut the metrics are more meaningful when measured per label given the multi-label nature (each label is effectively an independent binary classification) and the fact that there is drastically different representations of the labels in the dataset.\n\nWith a threshold of 0.5 applied to binarize the model outputs, as per the above notebook, the metrics per label are:\n\n|                | accuracy | precision | recall | f1    | mcc   | support | threshold |\n| -------------- | -------- | --------- | ------ | ----- | ----- | ------- | --------- |\n| admiration     | 0.946    | 0.725     | 0.675  | 0.699 | 0.670 | 504     | 0.5       |\n| amusement      | 0.982    | 0.790     | 0.871  | 0.829 | 0.821 | 264     | 0.5       |\n| anger          | 0.970    | 0.652     | 0.379  | 0.479 | 0.483 | 198     | 0.5       |\n| annoyance      | 0.940    | 0.472     | 0.159  | 0.238 | 0.250 | 320     | 0.5       |\n| approval       | 0.942    | 0.609     | 0.302  | 0.404 | 0.403 | 351     | 0.5       |\n| caring         | 0.973    | 0.448     | 0.319  | 0.372 | 0.364 | 135     | 0.5       |\n| confusion      | 0.972    | 0.500     | 0.431  | 0.463 | 0.450 | 153     | 0.5       |\n| curiosity      | 0.950    | 0.537     | 0.356  | 0.428 | 0.412 | 284     | 0.5       |\n| desire         | 0.987    | 0.630     | 0.410  | 0.496 | 0.502 | 83      | 0.5       |\n| disappointment | 0.974    | 0.625     | 0.199  | 0.302 | 0.343 | 151     | 0.5       |\n| disapproval    | 0.950    | 0.494     | 0.307  | 0.379 | 0.365 | 267     | 0.5       |\n| disgust        | 0.982    | 0.707     | 0.333  | 0.453 | 0.478 | 123     | 0.5       |\n| embarrassment  | 0.994    | 0.750     | 0.243  | 0.367 | 0.425 | 37      | 0.5       |\n| excitement     | 0.983    | 0.603     | 0.340  | 0.435 | 0.445 | 103     | 0.5       |\n| fear           | 0.992    | 0.758     | 0.603  | 0.671 | 0.672 | 78      | 0.5       |\n| gratitude      | 0.990    | 0.960     | 0.881  | 0.919 | 0.914 | 352     | 0.5       |\n| grief          | 0.999    | 0.000     | 0.000  | 0.000 | 0.000 | 6       | 0.5       |\n| joy            | 0.978    | 0.647     | 0.559  | 0.600 | 0.590 | 161     | 0.5       |\n| love           | 0.982    | 0.773     | 0.832  | 0.802 | 0.793 | 238     | 0.5       |\n| nervousness    | 0.996    | 0.600     | 0.130  | 0.214 | 0.278 | 23      | 0.5       |\n| optimism       | 0.972    | 0.667     | 0.376  | 0.481 | 0.488 | 186     | 0.5       |\n| pride          | 0.997    | 0.000     | 0.000  | 0.000 | 0.000 | 16      | 0.5       |\n| realization    | 0.974    | 0.541     | 0.138  | 0.220 | 0.264 | 145     | 0.5       |\n| relief         | 0.998    | 0.000     | 0.000  | 0.000 | 0.000 | 11      | 0.5       |\n| remorse        | 0.991    | 0.553     | 0.750  | 0.636 | 0.640 | 56      | 0.5       |\n| sadness        | 0.977    | 0.621     | 0.494  | 0.550 | 0.542 | 156     | 0.5       |\n| surprise       | 0.981    | 0.750     | 0.404  | 0.525 | 0.542 | 141     | 0.5       |\n| neutral        | 0.782    | 0.694     | 0.604  | 0.646 | 0.492 | 1787    | 0.5       |\n\nOptimizing the threshold per label for the one that gives the optimum F1 metrics gives slightly better metrics - sacrificing some precision for a greater gain in recall, hence to the benefit of F1 (how this was done is shown in the above notebook):\n\n|                | accuracy | precision | recall | f1    | mcc   | support | threshold |\n| -------------- | -------- | --------- | ------ | ----- | ----- | ------- | --------- |\n| admiration     | 0.940    | 0.651     | 0.776  | 0.708 | 0.678 | 504     | 0.25      |\n| amusement      | 0.982    | 0.781     | 0.890  | 0.832 | 0.825 | 264     | 0.45      |\n| anger          | 0.959    | 0.454     | 0.601  | 0.517 | 0.502 | 198     | 0.15      |\n| annoyance      | 0.864    | 0.243     | 0.619  | 0.349 | 0.328 | 320     | 0.10      |\n| approval       | 0.926    | 0.432     | 0.442  | 0.437 | 0.397 | 351     | 0.30      |\n| caring         | 0.972    | 0.426     | 0.385  | 0.405 | 0.391 | 135     | 0.40      |\n| confusion      | 0.974    | 0.548     | 0.412  | 0.470 | 0.462 | 153     | 0.55      |\n| curiosity      | 0.943    | 0.473     | 0.711  | 0.568 | 0.552 | 284     | 0.25      |\n| desire         | 0.985    | 0.518     | 0.530  | 0.524 | 0.516 | 83      | 0.25      |\n| disappointment | 0.974    | 0.562     | 0.298  | 0.390 | 0.398 | 151     | 0.40      |\n| disapproval    | 0.941    | 0.414     | 0.468  | 0.439 | 0.409 | 267     | 0.30      |\n| disgust        | 0.978    | 0.523     | 0.463  | 0.491 | 0.481 | 123     | 0.20      |\n| embarrassment  | 0.994    | 0.567     | 0.459  | 0.507 | 0.507 | 37      | 0.10      |\n| excitement     | 0.981    | 0.500     | 0.417  | 0.455 | 0.447 | 103     | 0.35      |\n| fear           | 0.991    | 0.712     | 0.667  | 0.689 | 0.685 | 78      | 0.40      |\n| gratitude      | 0.990    | 0.957     | 0.889  | 0.922 | 0.917 | 352     | 0.45      |\n| grief          | 0.999    | 0.333     | 0.333  | 0.333 | 0.333 | 6       | 0.05      |\n| joy            | 0.978    | 0.623     | 0.646  | 0.634 | 0.623 | 161     | 0.40      |\n| love           | 0.982    | 0.740     | 0.899  | 0.812 | 0.807 | 238     | 0.25      |\n| nervousness    | 0.996    | 0.571     | 0.348  | 0.432 | 0.444 | 23      | 0.25      |\n| optimism       | 0.971    | 0.580     | 0.565  | 0.572 | 0.557 | 186     | 0.20      |\n| pride          | 0.998    | 0.875     | 0.438  | 0.583 | 0.618 | 16      | 0.10      |\n| realization    | 0.961    | 0.270     | 0.262  | 0.266 | 0.246 | 145     | 0.15      |\n| relief         | 0.992    | 0.152     | 0.636  | 0.246 | 0.309 | 11      | 0.05      |\n| remorse        | 0.991    | 0.541     | 0.946  | 0.688 | 0.712 | 56      | 0.10      |\n| sadness        | 0.977    | 0.599     | 0.583  | 0.591 | 0.579 | 156     | 0.40      |\n| surprise       | 0.977    | 0.543     | 0.674  | 0.601 | 0.593 | 141     | 0.15      |\n| neutral        | 0.758    | 0.598     | 0.810  | 0.688 | 0.513 | 1787    | 0.25      |\n\nThis improves the overall metrics:\n\n- Precision: 0.542\n- Recall: 0.577\n- F1: 0.541\n\nOr if calculated weighted by the relative size of the support of each label:\n\n- Precision: 0.572\n- Recall: 0.677\n- F1: 0.611\n\n#### Commentary on the dataset\n\nSome labels (E.g. gratitude) when considered independently perform very strongly with F1 exceeding 0.9, whilst others (E.g. relief) perform very poorly.\n\nThis is a challenging dataset. Labels such as relief do have much fewer examples in the training data (less than 100 out of the 40k+, and only 11 in the test split).\n\nBut there is also some ambiguity and/or labelling errors visible in the training data of go_emotions that is suspected to constrain the performance. Data cleaning on the dataset to reduce some of the mistakes, ambiguity, conflicts and duplication in the labelling would produce a higher performing model.",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModelForSequenceClassification",
      "custom_class": null,
      "pipeline_tag": "text-classification",
      "processor": "AutoTokenizer"
    },
    "safetensors": {
      "parameters": {
        "I64": 514,
        "F32": 124667164
      },
      "total": 124667678
    },
    "model_index": null,
    "trending_score": null
  },
  {
    "model_id": "tals/albert-xlarge-vitaminc-mnli",
    "model_name": "tals/albert-xlarge-vitaminc-mnli",
    "author": "tals",
    "downloads": 475708,
    "downloads_all_time": null,
    "likes": 6,
    "tags": [
      "transformers",
      "pytorch",
      "tf",
      "safetensors",
      "albert",
      "text-classification",
      "dataset:glue",
      "dataset:multi_nli",
      "dataset:tals/vitaminc",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/tals/albert-xlarge-vitaminc-mnli",
    "dependencies": [
      [
        "datasets",
        null
      ]
    ],
    "last_modified": "2023-03-17T05:27:53+00:00",
    "created_at": "2022-03-02T23:29:05+00:00",
    "analysis_date": "2025-03-22T00:55:27.350359",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "albert",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "datasets": [
        "glue",
        "multi_nli",
        "tals/vitaminc"
      ]
    },
    "card_text": "# Details\nModel used in [Get Your Vitamin C! Robust Fact Verification with Contrastive Evidence](https://aclanthology.org/2021.naacl-main.52/) (Schuster et al., NAACL 21`).\n\nFor more details see: https://github.com/TalSchuster/VitaminC\n\nWhen using this model, please cite the paper.\n\n# BibTeX entry and citation info\n\n```bibtex\n@inproceedings{schuster-etal-2021-get,\n    title = \"Get Your Vitamin {C}! Robust Fact Verification with Contrastive Evidence\",\n    author = \"Schuster, Tal  and\n      Fisch, Adam  and\n      Barzilay, Regina\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.52\",\n    doi = \"10.18653/v1/2021.naacl-main.52\",\n    pages = \"624--643\",\n    abstract = \"Typical fact verification models use retrieved written evidence to verify claims. Evidence sources, however, often change over time as more information is gathered and revised. In order to adapt, models must be sensitive to subtle differences in supporting evidence. We present VitaminC, a benchmark infused with challenging cases that require fact verification models to discern and adjust to slight factual changes. We collect over 100,000 Wikipedia revisions that modify an underlying fact, and leverage these revisions, together with additional synthetically constructed ones, to create a total of over 400,000 claim-evidence pairs. Unlike previous resources, the examples in VitaminC are contrastive, i.e., they contain evidence pairs that are nearly identical in language and content, with the exception that one supports a given claim while the other does not. We show that training using this design increases robustness{---}improving accuracy by 10{\\%} on adversarial fact verification and 6{\\%} on adversarial natural language inference (NLI). Moreover, the structure of VitaminC leads us to define additional tasks for fact-checking resources: tagging relevant words in the evidence for verifying the claim, identifying factual revisions, and providing automatic edits via factually consistent text generation.\",\n}\n\n```\n",
    "card_content": "---\ndatasets:\n- glue\n- multi_nli\n- tals/vitaminc\n---\n# Details\nModel used in [Get Your Vitamin C! Robust Fact Verification with Contrastive Evidence](https://aclanthology.org/2021.naacl-main.52/) (Schuster et al., NAACL 21`).\n\nFor more details see: https://github.com/TalSchuster/VitaminC\n\nWhen using this model, please cite the paper.\n\n# BibTeX entry and citation info\n\n```bibtex\n@inproceedings{schuster-etal-2021-get,\n    title = \"Get Your Vitamin {C}! Robust Fact Verification with Contrastive Evidence\",\n    author = \"Schuster, Tal  and\n      Fisch, Adam  and\n      Barzilay, Regina\",\n    booktitle = \"Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\",\n    month = jun,\n    year = \"2021\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2021.naacl-main.52\",\n    doi = \"10.18653/v1/2021.naacl-main.52\",\n    pages = \"624--643\",\n    abstract = \"Typical fact verification models use retrieved written evidence to verify claims. Evidence sources, however, often change over time as more information is gathered and revised. In order to adapt, models must be sensitive to subtle differences in supporting evidence. We present VitaminC, a benchmark infused with challenging cases that require fact verification models to discern and adjust to slight factual changes. We collect over 100,000 Wikipedia revisions that modify an underlying fact, and leverage these revisions, together with additional synthetically constructed ones, to create a total of over 400,000 claim-evidence pairs. Unlike previous resources, the examples in VitaminC are contrastive, i.e., they contain evidence pairs that are nearly identical in language and content, with the exception that one supports a given claim while the other does not. We show that training using this design increases robustness{---}improving accuracy by 10{\\%} on adversarial fact verification and 6{\\%} on adversarial natural language inference (NLI). Moreover, the structure of VitaminC leads us to define additional tasks for fact-checking resources: tagging relevant words in the evidence for verifying the claim, identifying factual revisions, and providing automatic edits via factually consistent text generation.\",\n}\n\n```\n",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModelForSequenceClassification",
      "custom_class": null,
      "pipeline_tag": "text-classification",
      "processor": "AutoTokenizer"
    },
    "safetensors": {
      "parameters": {
        "I64": 512,
        "F32": 58731011
      },
      "total": 58731523
    },
    "model_index": null,
    "trending_score": null
  },
  {
    "model_id": "mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis",
    "model_name": "mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis",
    "author": "mrm8488",
    "downloads": 470572,
    "downloads_all_time": null,
    "likes": 379,
    "tags": [
      "transformers",
      "pytorch",
      "tensorboard",
      "safetensors",
      "roberta",
      "text-classification",
      "generated_from_trainer",
      "financial",
      "stocks",
      "sentiment",
      "dataset:financial_phrasebank",
      "license:apache-2.0",
      "model-index",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis",
    "dependencies": [
      [
        "transformers",
        "4.10.2"
      ],
      [
        "pytorch",
        "1.9.0+cu102"
      ],
      [
        "datasets",
        "1.12.1"
      ],
      [
        "tokenizers",
        "0.10.3"
      ]
    ],
    "last_modified": "2024-01-21T15:17:58+00:00",
    "created_at": "2022-03-02T23:29:05+00:00",
    "analysis_date": "2025-03-22T00:55:28.745964",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "roberta",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "license": "apache-2.0",
      "thumbnail": "https://huggingface.co/mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis/resolve/main/logo_no_bg.png",
      "tags": [
        "generated_from_trainer",
        "financial",
        "stocks",
        "sentiment"
      ],
      "widget": [
        {
          "text": "Operating profit totaled EUR 9.4 mn , down from EUR 11.7 mn in 2004 ."
        }
      ],
      "datasets": [
        "financial_phrasebank"
      ],
      "metrics": [
        "accuracy"
      ],
      "model-index": [
        {
          "name": "distilRoberta-financial-sentiment",
          "results": [
            {
              "task": {
                "type": "text-classification",
                "name": "Text Classification"
              },
              "dataset": {
                "name": "financial_phrasebank",
                "type": "financial_phrasebank",
                "args": "sentences_allagree"
              },
              "metrics": [
                {
                  "type": "accuracy",
                  "value": 0.9823008849557522,
                  "name": "Accuracy"
                }
              ]
            }
          ]
        }
      ]
    },
    "card_text": "\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n\n<div style=\"text-align:center;width:250px;height:250px;\">\n    <img src=\"https://huggingface.co/mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis/resolve/main/logo_no_bg.png\" alt=\"logo\">\n</div>\n\n\n# DistilRoberta-financial-sentiment\n\n\nThis model is a fine-tuned version of [distilroberta-base](https://huggingface.co/distilroberta-base) on the financial_phrasebank dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.1116\n- Accuracy: **0.98**23\n\n## Base Model description\n\nThis model is a distilled version of the [RoBERTa-base model](https://huggingface.co/roberta-base). It follows the same training procedure as [DistilBERT](https://huggingface.co/distilbert-base-uncased).\nThe code for the distillation process can be found [here](https://github.com/huggingface/transformers/tree/master/examples/distillation).\nThis model is case-sensitive: it makes a difference between English and English.\n\nThe model has 6 layers, 768 dimension and 12 heads, totalizing 82M parameters (compared to 125M parameters for RoBERTa-base).\nOn average DistilRoBERTa is twice as fast as Roberta-base.\n\n## Training Data\n\nPolar sentiment dataset of sentences from financial news. The dataset consists of 4840 sentences from English language financial news categorised by sentiment. The dataset is divided by agreement rate of 5-8 annotators.\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 5\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss | Accuracy |\n|:-------------:|:-----:|:----:|:---------------:|:--------:|\n| No log        | 1.0   | 255  | 0.1670          | 0.9646   |\n| 0.209         | 2.0   | 510  | 0.2290          | 0.9558   |\n| 0.209         | 3.0   | 765  | 0.2044          | 0.9558   |\n| 0.0326        | 4.0   | 1020 | 0.1116          | 0.9823   |\n| 0.0326        | 5.0   | 1275 | 0.1127          | 0.9779   |\n\n\n### Framework versions\n\n- Transformers 4.10.2\n- Pytorch 1.9.0+cu102\n- Datasets 1.12.1\n- Tokenizers 0.10.3\n",
    "card_content": "---\nlicense: apache-2.0\nthumbnail: https://huggingface.co/mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis/resolve/main/logo_no_bg.png\ntags:\n- generated_from_trainer\n- financial\n- stocks\n- sentiment\nwidget:\n- text: Operating profit totaled EUR 9.4 mn , down from EUR 11.7 mn in 2004 .\ndatasets:\n- financial_phrasebank\nmetrics:\n- accuracy\nmodel-index:\n- name: distilRoberta-financial-sentiment\n  results:\n  - task:\n      type: text-classification\n      name: Text Classification\n    dataset:\n      name: financial_phrasebank\n      type: financial_phrasebank\n      args: sentences_allagree\n    metrics:\n    - type: accuracy\n      value: 0.9823008849557522\n      name: Accuracy\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n\n<div style=\"text-align:center;width:250px;height:250px;\">\n    <img src=\"https://huggingface.co/mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis/resolve/main/logo_no_bg.png\" alt=\"logo\">\n</div>\n\n\n# DistilRoberta-financial-sentiment\n\n\nThis model is a fine-tuned version of [distilroberta-base](https://huggingface.co/distilroberta-base) on the financial_phrasebank dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.1116\n- Accuracy: **0.98**23\n\n## Base Model description\n\nThis model is a distilled version of the [RoBERTa-base model](https://huggingface.co/roberta-base). It follows the same training procedure as [DistilBERT](https://huggingface.co/distilbert-base-uncased).\nThe code for the distillation process can be found [here](https://github.com/huggingface/transformers/tree/master/examples/distillation).\nThis model is case-sensitive: it makes a difference between English and English.\n\nThe model has 6 layers, 768 dimension and 12 heads, totalizing 82M parameters (compared to 125M parameters for RoBERTa-base).\nOn average DistilRoBERTa is twice as fast as Roberta-base.\n\n## Training Data\n\nPolar sentiment dataset of sentences from financial news. The dataset consists of 4840 sentences from English language financial news categorised by sentiment. The dataset is divided by agreement rate of 5-8 annotators.\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 5\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss | Accuracy |\n|:-------------:|:-----:|:----:|:---------------:|:--------:|\n| No log        | 1.0   | 255  | 0.1670          | 0.9646   |\n| 0.209         | 2.0   | 510  | 0.2290          | 0.9558   |\n| 0.209         | 3.0   | 765  | 0.2044          | 0.9558   |\n| 0.0326        | 4.0   | 1020 | 0.1116          | 0.9823   |\n| 0.0326        | 5.0   | 1275 | 0.1127          | 0.9779   |\n\n\n### Framework versions\n\n- Transformers 4.10.2\n- Pytorch 1.9.0+cu102\n- Datasets 1.12.1\n- Tokenizers 0.10.3\n",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModelForSequenceClassification",
      "custom_class": null,
      "pipeline_tag": "text-classification",
      "processor": "AutoTokenizer"
    },
    "safetensors": {
      "parameters": {
        "I64": 514,
        "F32": 82120707
      },
      "total": 82121221
    },
    "model_index": [
      {
        "name": "distilRoberta-financial-sentiment",
        "results": [
          {
            "task": {
              "name": "Text Classification",
              "type": "text-classification"
            },
            "dataset": {
              "name": "financial_phrasebank",
              "type": "financial_phrasebank",
              "args": "sentences_allagree"
            },
            "metrics": [
              {
                "name": "Accuracy",
                "type": "accuracy",
                "value": 0.9823008849557522,
                "verified": false
              }
            ]
          }
        ]
      }
    ],
    "trending_score": null
  },
  {
    "model_id": "cross-encoder/ms-marco-electra-base",
    "model_name": "cross-encoder/ms-marco-electra-base",
    "author": "cross-encoder",
    "downloads": 460397,
    "downloads_all_time": null,
    "likes": 5,
    "tags": [
      "transformers",
      "pytorch",
      "safetensors",
      "electra",
      "text-classification",
      "license:apache-2.0",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/cross-encoder/ms-marco-electra-base",
    "dependencies": [
      [
        "sentence_transformers",
        null
      ],
      [
        "transformers",
        null
      ],
      [
        "torch",
        null
      ]
    ],
    "last_modified": "2025-03-07T14:59:40+00:00",
    "created_at": "2022-03-02T23:29:05+00:00",
    "analysis_date": "2025-03-22T00:55:30.097237",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "electra",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "license": "apache-2.0"
    },
    "card_text": "# Cross-Encoder for MS Marco\n\nThis model was trained on the [MS Marco Passage Ranking](https://github.com/microsoft/MSMARCO-Passage-Ranking) task.\n\nThe model can be used for Information Retrieval: Given a query, encode the query will all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. See [SBERT.net Retrieve & Re-rank](https://www.sbert.net/examples/applications/retrieve_rerank/README.html) for more details. The training code is available here: [SBERT.net Training MS Marco](https://github.com/UKPLab/sentence-transformers/tree/master/examples/training/ms_marco)\n\n\n## Usage with SentenceTransformers\n\nThe usage is easy when you have [SentenceTransformers](https://www.sbert.net/) installed. Then you can use the pre-trained models like this:\n```python\nfrom sentence_transformers import CrossEncoder\n\nmodel = CrossEncoder('cross-encoder/ms-marco-electra-base')\nscores = model.predict([\n    (\"How many people live in Berlin?\", \"Berlin had a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\"),\n    (\"How many people live in Berlin?\", \"Berlin is well known for its museums.\"),\n])\nprint(scores)\n# [9.9227107e-01 2.0136760e-05]\n```\n\n## Usage with Transformers\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\n\nmodel = AutoModelForSequenceClassification.from_pretrained('model_name')\ntokenizer = AutoTokenizer.from_pretrained('model_name')\n\nfeatures = tokenizer(['How many people live in Berlin?', 'How many people live in Berlin?'], ['Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.', 'New York City is famous for the Metropolitan Museum of Art.'],  padding=True, truncation=True, return_tensors=\"pt\")\n\nmodel.eval()\nwith torch.no_grad():\n    scores = model(**features).logits\n    print(scores)\n```\n\n\n## Performance\nIn the following table, we provide various pre-trained Cross-Encoders together with their performance on the [TREC Deep Learning 2019](https://microsoft.github.io/TREC-2019-Deep-Learning/) and the [MS Marco Passage Reranking](https://github.com/microsoft/MSMARCO-Passage-Ranking/) dataset. \n\n\n| Model-Name        | NDCG@10 (TREC DL 19) | MRR@10 (MS Marco Dev)  | Docs / Sec |\n| ------------- |:-------------| -----| --- | \n| **Version 2 models** | | | \n| cross-encoder/ms-marco-TinyBERT-L-2-v2 | 69.84 | 32.56 | 9000\n| cross-encoder/ms-marco-MiniLM-L-2-v2 | 71.01 | 34.85 | 4100\n| cross-encoder/ms-marco-MiniLM-L-4-v2 | 73.04 | 37.70 | 2500\n| cross-encoder/ms-marco-MiniLM-L-6-v2 | 74.30 | 39.01 | 1800\n| cross-encoder/ms-marco-MiniLM-L-12-v2 | 74.31 | 39.02 | 960\n| **Version 1 models** | | | \n| cross-encoder/ms-marco-TinyBERT-L-2  | 67.43 | 30.15  | 9000\n| cross-encoder/ms-marco-TinyBERT-L-4  | 68.09 | 34.50  | 2900\n| cross-encoder/ms-marco-TinyBERT-L-6 |  69.57 | 36.13  | 680\n| cross-encoder/ms-marco-electra-base | 71.99 | 36.41 | 340\n| **Other models** | | | \n| nboost/pt-tinybert-msmarco | 63.63 | 28.80 | 2900 \n| nboost/pt-bert-base-uncased-msmarco | 70.94 | 34.75 | 340 \n| nboost/pt-bert-large-msmarco | 73.36 | 36.48 | 100 \n| Capreolus/electra-base-msmarco | 71.23 | 36.89 | 340 \n| amberoad/bert-multilingual-passage-reranking-msmarco | 68.40 | 35.54 | 330 \n| sebastian-hofstaetter/distilbert-cat-margin_mse-T2-msmarco | 72.82 | 37.88 | 720\n \n Note: Runtime was computed on a V100 GPU.\n",
    "card_content": "---\nlicense: apache-2.0\n---\n# Cross-Encoder for MS Marco\n\nThis model was trained on the [MS Marco Passage Ranking](https://github.com/microsoft/MSMARCO-Passage-Ranking) task.\n\nThe model can be used for Information Retrieval: Given a query, encode the query will all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. See [SBERT.net Retrieve & Re-rank](https://www.sbert.net/examples/applications/retrieve_rerank/README.html) for more details. The training code is available here: [SBERT.net Training MS Marco](https://github.com/UKPLab/sentence-transformers/tree/master/examples/training/ms_marco)\n\n\n## Usage with SentenceTransformers\n\nThe usage is easy when you have [SentenceTransformers](https://www.sbert.net/) installed. Then you can use the pre-trained models like this:\n```python\nfrom sentence_transformers import CrossEncoder\n\nmodel = CrossEncoder('cross-encoder/ms-marco-electra-base')\nscores = model.predict([\n    (\"How many people live in Berlin?\", \"Berlin had a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\"),\n    (\"How many people live in Berlin?\", \"Berlin is well known for its museums.\"),\n])\nprint(scores)\n# [9.9227107e-01 2.0136760e-05]\n```\n\n## Usage with Transformers\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\n\nmodel = AutoModelForSequenceClassification.from_pretrained('model_name')\ntokenizer = AutoTokenizer.from_pretrained('model_name')\n\nfeatures = tokenizer(['How many people live in Berlin?', 'How many people live in Berlin?'], ['Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.', 'New York City is famous for the Metropolitan Museum of Art.'],  padding=True, truncation=True, return_tensors=\"pt\")\n\nmodel.eval()\nwith torch.no_grad():\n    scores = model(**features).logits\n    print(scores)\n```\n\n\n## Performance\nIn the following table, we provide various pre-trained Cross-Encoders together with their performance on the [TREC Deep Learning 2019](https://microsoft.github.io/TREC-2019-Deep-Learning/) and the [MS Marco Passage Reranking](https://github.com/microsoft/MSMARCO-Passage-Ranking/) dataset. \n\n\n| Model-Name        | NDCG@10 (TREC DL 19) | MRR@10 (MS Marco Dev)  | Docs / Sec |\n| ------------- |:-------------| -----| --- | \n| **Version 2 models** | | | \n| cross-encoder/ms-marco-TinyBERT-L-2-v2 | 69.84 | 32.56 | 9000\n| cross-encoder/ms-marco-MiniLM-L-2-v2 | 71.01 | 34.85 | 4100\n| cross-encoder/ms-marco-MiniLM-L-4-v2 | 73.04 | 37.70 | 2500\n| cross-encoder/ms-marco-MiniLM-L-6-v2 | 74.30 | 39.01 | 1800\n| cross-encoder/ms-marco-MiniLM-L-12-v2 | 74.31 | 39.02 | 960\n| **Version 1 models** | | | \n| cross-encoder/ms-marco-TinyBERT-L-2  | 67.43 | 30.15  | 9000\n| cross-encoder/ms-marco-TinyBERT-L-4  | 68.09 | 34.50  | 2900\n| cross-encoder/ms-marco-TinyBERT-L-6 |  69.57 | 36.13  | 680\n| cross-encoder/ms-marco-electra-base | 71.99 | 36.41 | 340\n| **Other models** | | | \n| nboost/pt-tinybert-msmarco | 63.63 | 28.80 | 2900 \n| nboost/pt-bert-base-uncased-msmarco | 70.94 | 34.75 | 340 \n| nboost/pt-bert-large-msmarco | 73.36 | 36.48 | 100 \n| Capreolus/electra-base-msmarco | 71.23 | 36.89 | 340 \n| amberoad/bert-multilingual-passage-reranking-msmarco | 68.40 | 35.54 | 330 \n| sebastian-hofstaetter/distilbert-cat-margin_mse-T2-msmarco | 72.82 | 37.88 | 720\n \n Note: Runtime was computed on a V100 GPU.\n",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModelForSequenceClassification",
      "custom_class": null,
      "pipeline_tag": "text-classification",
      "processor": "AutoTokenizer"
    },
    "safetensors": {
      "parameters": {
        "I64": 512,
        "F32": 109483009
      },
      "total": 109483521
    },
    "model_index": null,
    "trending_score": null
  },
  {
    "model_id": "nbroad/ESG-BERT",
    "model_name": "nbroad/ESG-BERT",
    "author": "nbroad",
    "downloads": 414796,
    "downloads_all_time": null,
    "likes": 63,
    "tags": [
      "transformers",
      "pytorch",
      "safetensors",
      "bert",
      "text-classification",
      "en",
      "arxiv:1910.09700",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/nbroad/ESG-BERT",
    "dependencies": [
      [
        "torchserve",
        null
      ],
      [
        "torch-model-archiver",
        null
      ],
      [
        "torchvision",
        null
      ],
      [
        "transformers",
        null
      ]
    ],
    "last_modified": "2023-04-26T04:50:33+00:00",
    "created_at": "2022-03-02T23:29:05+00:00",
    "analysis_date": "2025-03-22T00:55:31.963968",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "bert",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": [
        "en"
      ],
      "widget": [
        {
          "text": "In fiscal year 2019, we reduced our comprehensive carbon footprint for the fourth consecutive year—down 35 percent compared to 2015, when Apple’s carbon emissions peaked, even as net revenue increased by 11 percent over that same period. In the past year, we avoided over 10 million metric tons from our emissions reduction initiatives—like our Supplier Clean Energy Program, which lowered our footprint by 4.4 million metric tons. ",
          "example_title": "Reduced carbon footprint"
        },
        {
          "text": "We believe it is essential to establish validated conflict-free sources of 3TG within the Democratic Republic of the Congo (the “DRC”) and adjoining countries (together, with the DRC, the “Covered Countries”), so that these minerals can be procured in a way that contributes to economic growth and development in the region. To aid in this effort, we have established a conflict minerals policy and an internal team to implement the policy.",
          "example_title": "Conflict minerals policy"
        }
      ]
    },
    "card_text": "# Model Card for ESG-BERT\nDomain Specific BERT Model for Text Mining in Sustainable Investing\n \n \n \n# Model Details\n \n## Model Description\n \n \n \n- **Developed by:** [Mukut Mukherjee](https://www.linkedin.com/in/mukutm/), [Charan Pothireddi](https://www.linkedin.com/in/sree-charan-pothireddi-6a0a3587/) and [Parabole.ai](https://www.linkedin.com/in/sree-charan-pothireddi-6a0a3587/)\n- **Shared by [Optional]:** HuggingFace\n- **Model type:** Language model\n- **Language(s) (NLP):** en\n- **License:** More information needed\n- **Related Models:** \n  - **Parent Model:** BERT\n- **Resources for more information:** \n - [GitHub Repo](https://github.com/mukut03/ESG-BERT)\n - [Blog Post](https://towardsdatascience.com/nlp-meets-sustainable-investing-d0542b3c264b?source=friends_link&sk=1f7e6641c3378aaff319a81decf387bf)\n \n# Uses\n \n \n## Direct Use\n \nText Mining in Sustainable Investing\n \n## Downstream Use [Optional]\n \nThe applications of ESG-BERT can be expanded way beyond just text classification. It can be fine-tuned to perform various other downstream NLP tasks in the domain of Sustainable Investing.\n \n## Out-of-Scope Use\n \nThe model should not be used to intentionally create hostile or alienating environments for people. \n# Bias, Risks, and Limitations\n \n \nSignificant research has explored bias and fairness issues with language models (see, e.g., [Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)). Predictions generated by the model may include disturbing and harmful stereotypes across protected classes; identity characteristics; and sensitive, social, and occupational groups.\n \n \n## Recommendations\n \n \nUsers (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recomendations.\n \n \n# Training Details\n \n## Training Data\n \nMore information needed\n \n## Training Procedure\n \n<!-- This relates heavily to the Technical Specifications. Content here should link to that section when it is relevant to the training procedure. -->\n \n### Preprocessing\n \nMore information needed\n \n### Speeds, Sizes, Times\n \nMore information needed\n \n# Evaluation\n \n \n \n## Testing Data, Factors & Metrics\n \n### Testing Data\n \nThe fine-tuned model for text classification is also available [here](https://drive.google.com/drive/folders/1Qz4HP3xkjLfJ6DGCFNeJ7GmcPq65_HVe?usp=sharing). It can be used directly to make predictions using just a few steps.  First, download the fine-tuned pytorch_model.bin, config.json, and vocab.txt\n \n### Factors\n \nMore information needed\n \n### Metrics\n \nMore information needed\n \n## Results \n \nESG-BERT was further trained on unstructured text data with accuracies of 100% and 98% for Next Sentence Prediction and Masked Language Modelling tasks. Fine-tuning ESG-BERT for text classification yielded an F-1 score of 0.90. For comparison, the general BERT (BERT-base) model scored 0.79 after fine-tuning, and the sci-kit learn approach scored 0.67.\n \n# Model Examination\n \nMore information needed\n \n# Environmental Impact\n \n \nCarbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).\n \n- **Hardware Type:** More information needed\n- **Hours used:** More information needed\n- **Cloud Provider:**  information needed\n- **Compute Region:** More information needed\n- **Carbon Emitted:** More information needed\n \n# Technical Specifications [optional]\n \n## Model Architecture and Objective\n \nMore information needed\n \n## Compute Infrastructure\n \nMore information needed\n \n### Hardware\n \nMore information needed\n \n### Software\n \nJDK 11 is needed to serve the model\n \n# Citation\n \n<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->\n \n**BibTeX:**\n \nMore information needed\n \n**APA:**\n \nMore information needed\n \n# Glossary [optional]\n \n<!-- If relevant, include terms and calculations in this section that can help readers understand the model or model card. -->\n \nMore information needed\n \n# More Information [optional]\n \nMore information needed\n \n# Model Card Authors [optional]\n[Mukut Mukherjee](https://www.linkedin.com/in/mukutm/), [Charan Pothireddi](https://www.linkedin.com/in/sree-charan-pothireddi-6a0a3587/) and [Parabole.ai](https://www.linkedin.com/in/sree-charan-pothireddi-6a0a3587/), in collaboration with the Ezi Ozoani and the HuggingFace Team\n \n \n# Model Card Contact\n \nMore information needed\n \n# How to Get Started with the Model\n \nUse the code below to get started with the model.\n \n<details>\n <summary> Click to expand </summary>\n \n```\npip install torchserve torch-model-archiver\n \npip install torchvision\n \npip install transformers\n \n```\n \nNext up, we'll set up the handler script. It is a basic handler for text classification that can be improved upon. Save this script as \"handler.py\" in your directory. [1]\n \n```\n \nfrom abc import ABC\n \nimport json\n \nimport logging\n \nimport os\n \nimport torch\n \nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n \nfrom ts.torch_handler.base_handler import BaseHandler\n \nlogger = logging.getLogger(__name__)\n \nclass TransformersClassifierHandler(BaseHandler, ABC):\n \n   \"\"\"\n \n   Transformers text classifier handler class. This handler takes a text (string) and\n \n   as input and returns the classification text based on the serialized transformers checkpoint.\n \n   \"\"\"\n \n   def __init__(self):\n \n       super(TransformersClassifierHandler, self).__init__()\n \n       self.initialized = False\n \ndef initialize(self, ctx):\n \n       self.manifest = ctx.manifest\n \nproperties = ctx.system_properties\n \n       model_dir = properties.get(\"model_dir\")\n \n       self.device = torch.device(\"cuda:\" + str(properties.get(\"gpu_id\")) if torch.cuda.is_available() else \"cpu\")\n \n# Read model serialize/pt file\n \n       self.model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n \n       self.tokenizer = AutoTokenizer.from_pretrained(model_dir)\n \nself.model.to(self.device)\n \n       self.model.eval()\n \nlogger.debug('Transformer model from path {0} loaded successfully'.format(model_dir))\n \n# Read the mapping file, index to object name\n \n       mapping_file_path = os.path.join(model_dir, \"index_to_name.json\")\n \nif os.path.isfile(mapping_file_path):\n \n           with open(mapping_file_path) as f:\n \n               self.mapping = json.load(f)\n \n       else:\n \n           logger.warning('Missing the index_to_name.json file. Inference output will not include class name.')\n \nself.initialized = True\n \ndef preprocess(self, data):\n \n       \"\"\" Very basic preprocessing code - only tokenizes.\n \n           Extend with your own preprocessing steps as needed.\n \n       \"\"\"\n \n       text = data[0].get(\"data\")\n \n       if text is None:\n \n           text = data[0].get(\"body\")\n \n       sentences = text.decode('utf-8')\n \n       logger.info(\"Received text: '%s'\", sentences)\n \ninputs = self.tokenizer.encode_plus(\n \n           sentences,\n \n           add_special_tokens=True,\n \n           return_tensors=\"pt\"\n \n       )\n \n       return inputs\n \ndef inference(self, inputs):\n \n       \"\"\"\n \n       Predict the class of a text using a trained transformer model.\n \n       \"\"\"\n \n       # NOTE: This makes the assumption that your model expects text to be tokenized \n \n       # with \"input_ids\" and \"token_type_ids\" - which is true for some popular transformer models, e.g. bert.\n \n       # If your transformer model expects different tokenization, adapt this code to suit\n \n       # its expected input format.\n \n       prediction = self.model(\n \n           inputs['input_ids'].to(self.device),\n \n           token_type_ids=inputs['token_type_ids'].to(self.device)\n \n       )[0].argmax().item()\n \n       logger.info(\"Model predicted: '%s'\", prediction)\n \nif self.mapping:\n \n           prediction = self.mapping[str(prediction)]\n \nreturn [prediction]\n \ndef postprocess(self, inference_output):\n \n       # TODO: Add any needed post-processing of the model predictions here\n \n       return inference_output\n \n_service = TransformersClassifierHandler()\n \ndef handle(data, context):\n \n   try:\n \n       if not _service.initialized:\n \n           _service.initialize(context)\n \nif data is None:\n \n           return None\n \ndata = _service.preprocess(data)\n \n       data = _service.inference(data)\n \n       data = _service.postprocess(data)\n \nreturn data\n \n   except Exception as e:\n \n       raise e\n \n \n \n```\n \nTorcheServe uses a format called MAR (Model Archive). We can convert our PyTorch model to a .mar file using this command:\n \n```\n \ntorch-model-archiver --model-name \"bert\" --version 1.0 --serialized-file ./bert_model/pytorch_model.bin --extra-files \"./bert_model/config.json,./bert_model/vocab.txt\" --handler \"./handler.py\"\n \n```\n \nMove the .mar file into a new directory: \n \n```\n \nmkdir model_store && mv bert.mar model_store\n \n```\n \nFinally, we can start TorchServe using the command: \n \n```\n \ntorchserve --start --model-store model_store --models bert=bert.mar\n \n```\n \nWe can now query the model from another terminal window using the Inference API. We pass a text file containing text that the model will try to classify. \n \n\n \n \n```\n \ncurl -X POST http://127.0.0.1:8080/predictions/bert -T predict.txt\n \n```\n \nThis returns a label number which correlates to a textual label. This is stored in the label_dict.txt dictionary file. \n \n```\n \n__label__Business_Ethics :  0\n \n__label__Data_Security :  1\n \n__label__Access_And_Affordability :  2\n \n__label__Business_Model_Resilience :  3\n \n__label__Competitive_Behavior :  4\n \n__label__Critical_Incident_Risk_Management :  5\n \n__label__Customer_Welfare :  6\n \n__label__Director_Removal :  7\n \n__label__Employee_Engagement_Inclusion_And_Diversity :  8\n \n__label__Employee_Health_And_Safety :  9\n \n__label__Human_Rights_And_Community_Relations :  10\n \n__label__Labor_Practices :  11\n \n__label__Management_Of_Legal_And_Regulatory_Framework :  12\n \n__label__Physical_Impacts_Of_Climate_Change :  13\n \n__label__Product_Quality_And_Safety :  14\n \n__label__Product_Design_And_Lifecycle_Management :  15\n \n__label__Selling_Practices_And_Product_Labeling :  16\n \n__label__Supply_Chain_Management :  17\n \n__label__Systemic_Risk_Management :  18\n \n__label__Waste_And_Hazardous_Materials_Management :  19\n \n__label__Water_And_Wastewater_Management :  20\n \n__label__Air_Quality :  21\n \n__label__Customer_Privacy :  22\n \n__label__Ecological_Impacts :  23\n \n__label__Energy_Management :  24\n \n__label__GHG_Emissions :  25\n \n```\n\n<\\details>\n",
    "card_content": "---\nlanguage:\n- en\nwidget:\n- text: 'In fiscal year 2019, we reduced our comprehensive carbon footprint for the\n    fourth consecutive year—down 35 percent compared to 2015, when Apple’s carbon\n    emissions peaked, even as net revenue increased by 11 percent over that same period.\n    In the past year, we avoided over 10 million metric tons from our emissions reduction\n    initiatives—like our Supplier Clean Energy Program, which lowered our footprint\n    by 4.4 million metric tons. '\n  example_title: Reduced carbon footprint\n- text: We believe it is essential to establish validated conflict-free sources of\n    3TG within the Democratic Republic of the Congo (the “DRC”) and adjoining countries\n    (together, with the DRC, the “Covered Countries”), so that these minerals can\n    be procured in a way that contributes to economic growth and development in the\n    region. To aid in this effort, we have established a conflict minerals policy\n    and an internal team to implement the policy.\n  example_title: Conflict minerals policy\n---\n# Model Card for ESG-BERT\nDomain Specific BERT Model for Text Mining in Sustainable Investing\n \n \n \n# Model Details\n \n## Model Description\n \n \n \n- **Developed by:** [Mukut Mukherjee](https://www.linkedin.com/in/mukutm/), [Charan Pothireddi](https://www.linkedin.com/in/sree-charan-pothireddi-6a0a3587/) and [Parabole.ai](https://www.linkedin.com/in/sree-charan-pothireddi-6a0a3587/)\n- **Shared by [Optional]:** HuggingFace\n- **Model type:** Language model\n- **Language(s) (NLP):** en\n- **License:** More information needed\n- **Related Models:** \n  - **Parent Model:** BERT\n- **Resources for more information:** \n - [GitHub Repo](https://github.com/mukut03/ESG-BERT)\n - [Blog Post](https://towardsdatascience.com/nlp-meets-sustainable-investing-d0542b3c264b?source=friends_link&sk=1f7e6641c3378aaff319a81decf387bf)\n \n# Uses\n \n \n## Direct Use\n \nText Mining in Sustainable Investing\n \n## Downstream Use [Optional]\n \nThe applications of ESG-BERT can be expanded way beyond just text classification. It can be fine-tuned to perform various other downstream NLP tasks in the domain of Sustainable Investing.\n \n## Out-of-Scope Use\n \nThe model should not be used to intentionally create hostile or alienating environments for people. \n# Bias, Risks, and Limitations\n \n \nSignificant research has explored bias and fairness issues with language models (see, e.g., [Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)). Predictions generated by the model may include disturbing and harmful stereotypes across protected classes; identity characteristics; and sensitive, social, and occupational groups.\n \n \n## Recommendations\n \n \nUsers (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recomendations.\n \n \n# Training Details\n \n## Training Data\n \nMore information needed\n \n## Training Procedure\n \n<!-- This relates heavily to the Technical Specifications. Content here should link to that section when it is relevant to the training procedure. -->\n \n### Preprocessing\n \nMore information needed\n \n### Speeds, Sizes, Times\n \nMore information needed\n \n# Evaluation\n \n \n \n## Testing Data, Factors & Metrics\n \n### Testing Data\n \nThe fine-tuned model for text classification is also available [here](https://drive.google.com/drive/folders/1Qz4HP3xkjLfJ6DGCFNeJ7GmcPq65_HVe?usp=sharing). It can be used directly to make predictions using just a few steps.  First, download the fine-tuned pytorch_model.bin, config.json, and vocab.txt\n \n### Factors\n \nMore information needed\n \n### Metrics\n \nMore information needed\n \n## Results \n \nESG-BERT was further trained on unstructured text data with accuracies of 100% and 98% for Next Sentence Prediction and Masked Language Modelling tasks. Fine-tuning ESG-BERT for text classification yielded an F-1 score of 0.90. For comparison, the general BERT (BERT-base) model scored 0.79 after fine-tuning, and the sci-kit learn approach scored 0.67.\n \n# Model Examination\n \nMore information needed\n \n# Environmental Impact\n \n \nCarbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).\n \n- **Hardware Type:** More information needed\n- **Hours used:** More information needed\n- **Cloud Provider:**  information needed\n- **Compute Region:** More information needed\n- **Carbon Emitted:** More information needed\n \n# Technical Specifications [optional]\n \n## Model Architecture and Objective\n \nMore information needed\n \n## Compute Infrastructure\n \nMore information needed\n \n### Hardware\n \nMore information needed\n \n### Software\n \nJDK 11 is needed to serve the model\n \n# Citation\n \n<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->\n \n**BibTeX:**\n \nMore information needed\n \n**APA:**\n \nMore information needed\n \n# Glossary [optional]\n \n<!-- If relevant, include terms and calculations in this section that can help readers understand the model or model card. -->\n \nMore information needed\n \n# More Information [optional]\n \nMore information needed\n \n# Model Card Authors [optional]\n[Mukut Mukherjee](https://www.linkedin.com/in/mukutm/), [Charan Pothireddi](https://www.linkedin.com/in/sree-charan-pothireddi-6a0a3587/) and [Parabole.ai](https://www.linkedin.com/in/sree-charan-pothireddi-6a0a3587/), in collaboration with the Ezi Ozoani and the HuggingFace Team\n \n \n# Model Card Contact\n \nMore information needed\n \n# How to Get Started with the Model\n \nUse the code below to get started with the model.\n \n<details>\n <summary> Click to expand </summary>\n \n```\npip install torchserve torch-model-archiver\n \npip install torchvision\n \npip install transformers\n \n```\n \nNext up, we'll set up the handler script. It is a basic handler for text classification that can be improved upon. Save this script as \"handler.py\" in your directory. [1]\n \n```\n \nfrom abc import ABC\n \nimport json\n \nimport logging\n \nimport os\n \nimport torch\n \nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n \nfrom ts.torch_handler.base_handler import BaseHandler\n \nlogger = logging.getLogger(__name__)\n \nclass TransformersClassifierHandler(BaseHandler, ABC):\n \n   \"\"\"\n \n   Transformers text classifier handler class. This handler takes a text (string) and\n \n   as input and returns the classification text based on the serialized transformers checkpoint.\n \n   \"\"\"\n \n   def __init__(self):\n \n       super(TransformersClassifierHandler, self).__init__()\n \n       self.initialized = False\n \ndef initialize(self, ctx):\n \n       self.manifest = ctx.manifest\n \nproperties = ctx.system_properties\n \n       model_dir = properties.get(\"model_dir\")\n \n       self.device = torch.device(\"cuda:\" + str(properties.get(\"gpu_id\")) if torch.cuda.is_available() else \"cpu\")\n \n# Read model serialize/pt file\n \n       self.model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n \n       self.tokenizer = AutoTokenizer.from_pretrained(model_dir)\n \nself.model.to(self.device)\n \n       self.model.eval()\n \nlogger.debug('Transformer model from path {0} loaded successfully'.format(model_dir))\n \n# Read the mapping file, index to object name\n \n       mapping_file_path = os.path.join(model_dir, \"index_to_name.json\")\n \nif os.path.isfile(mapping_file_path):\n \n           with open(mapping_file_path) as f:\n \n               self.mapping = json.load(f)\n \n       else:\n \n           logger.warning('Missing the index_to_name.json file. Inference output will not include class name.')\n \nself.initialized = True\n \ndef preprocess(self, data):\n \n       \"\"\" Very basic preprocessing code - only tokenizes.\n \n           Extend with your own preprocessing steps as needed.\n \n       \"\"\"\n \n       text = data[0].get(\"data\")\n \n       if text is None:\n \n           text = data[0].get(\"body\")\n \n       sentences = text.decode('utf-8')\n \n       logger.info(\"Received text: '%s'\", sentences)\n \ninputs = self.tokenizer.encode_plus(\n \n           sentences,\n \n           add_special_tokens=True,\n \n           return_tensors=\"pt\"\n \n       )\n \n       return inputs\n \ndef inference(self, inputs):\n \n       \"\"\"\n \n       Predict the class of a text using a trained transformer model.\n \n       \"\"\"\n \n       # NOTE: This makes the assumption that your model expects text to be tokenized \n \n       # with \"input_ids\" and \"token_type_ids\" - which is true for some popular transformer models, e.g. bert.\n \n       # If your transformer model expects different tokenization, adapt this code to suit\n \n       # its expected input format.\n \n       prediction = self.model(\n \n           inputs['input_ids'].to(self.device),\n \n           token_type_ids=inputs['token_type_ids'].to(self.device)\n \n       )[0].argmax().item()\n \n       logger.info(\"Model predicted: '%s'\", prediction)\n \nif self.mapping:\n \n           prediction = self.mapping[str(prediction)]\n \nreturn [prediction]\n \ndef postprocess(self, inference_output):\n \n       # TODO: Add any needed post-processing of the model predictions here\n \n       return inference_output\n \n_service = TransformersClassifierHandler()\n \ndef handle(data, context):\n \n   try:\n \n       if not _service.initialized:\n \n           _service.initialize(context)\n \nif data is None:\n \n           return None\n \ndata = _service.preprocess(data)\n \n       data = _service.inference(data)\n \n       data = _service.postprocess(data)\n \nreturn data\n \n   except Exception as e:\n \n       raise e\n \n \n \n```\n \nTorcheServe uses a format called MAR (Model Archive). We can convert our PyTorch model to a .mar file using this command:\n \n```\n \ntorch-model-archiver --model-name \"bert\" --version 1.0 --serialized-file ./bert_model/pytorch_model.bin --extra-files \"./bert_model/config.json,./bert_model/vocab.txt\" --handler \"./handler.py\"\n \n```\n \nMove the .mar file into a new directory: \n \n```\n \nmkdir model_store && mv bert.mar model_store\n \n```\n \nFinally, we can start TorchServe using the command: \n \n```\n \ntorchserve --start --model-store model_store --models bert=bert.mar\n \n```\n \nWe can now query the model from another terminal window using the Inference API. We pass a text file containing text that the model will try to classify. \n \n\n \n \n```\n \ncurl -X POST http://127.0.0.1:8080/predictions/bert -T predict.txt\n \n```\n \nThis returns a label number which correlates to a textual label. This is stored in the label_dict.txt dictionary file. \n \n```\n \n__label__Business_Ethics :  0\n \n__label__Data_Security :  1\n \n__label__Access_And_Affordability :  2\n \n__label__Business_Model_Resilience :  3\n \n__label__Competitive_Behavior :  4\n \n__label__Critical_Incident_Risk_Management :  5\n \n__label__Customer_Welfare :  6\n \n__label__Director_Removal :  7\n \n__label__Employee_Engagement_Inclusion_And_Diversity :  8\n \n__label__Employee_Health_And_Safety :  9\n \n__label__Human_Rights_And_Community_Relations :  10\n \n__label__Labor_Practices :  11\n \n__label__Management_Of_Legal_And_Regulatory_Framework :  12\n \n__label__Physical_Impacts_Of_Climate_Change :  13\n \n__label__Product_Quality_And_Safety :  14\n \n__label__Product_Design_And_Lifecycle_Management :  15\n \n__label__Selling_Practices_And_Product_Labeling :  16\n \n__label__Supply_Chain_Management :  17\n \n__label__Systemic_Risk_Management :  18\n \n__label__Waste_And_Hazardous_Materials_Management :  19\n \n__label__Water_And_Wastewater_Management :  20\n \n__label__Air_Quality :  21\n \n__label__Customer_Privacy :  22\n \n__label__Ecological_Impacts :  23\n \n__label__Energy_Management :  24\n \n__label__GHG_Emissions :  25\n \n```\n\n<\\details>\n",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModelForSequenceClassification",
      "custom_class": null,
      "pipeline_tag": "text-classification",
      "processor": "AutoTokenizer"
    },
    "safetensors": {
      "parameters": {
        "I64": 512,
        "F32": 109502234
      },
      "total": 109502746
    },
    "model_index": null,
    "trending_score": null
  },
  {
    "model_id": "oliverguhr/german-sentiment-bert",
    "model_name": "oliverguhr/german-sentiment-bert",
    "author": "oliverguhr",
    "downloads": 393811,
    "downloads_all_time": null,
    "likes": 60,
    "tags": [
      "transformers",
      "pytorch",
      "tf",
      "jax",
      "onnx",
      "safetensors",
      "bert",
      "text-classification",
      "sentiment",
      "de",
      "license:mit",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/oliverguhr/german-sentiment-bert",
    "dependencies": [
      [
        "germansentiment",
        null
      ]
    ],
    "last_modified": "2025-03-03T16:52:02+00:00",
    "created_at": "2022-03-02T23:29:05+00:00",
    "analysis_date": "2025-03-22T00:55:33.008857",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "bert",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": [
        "de"
      ],
      "tags": [
        "sentiment",
        "bert"
      ],
      "license": "mit",
      "widget": [
        {
          "text": "Das ist gar nicht mal so schlecht"
        }
      ],
      "metrics": [
        "f1"
      ]
    },
    "card_text": "\n\n# German Sentiment Classification with Bert\n\nThis model was trained for sentiment classification of German language texts. To achieve the best results all model inputs needs to be preprocessed with the same procedure, that was applied during the training. To simplify the usage of the model, \nwe provide a Python package that bundles the code need for the preprocessing and inferencing. \n\nThe model uses the Googles Bert architecture and was trained on 1.834 million German-language samples. The training data contains texts from various domains like Twitter, Facebook and movie, app and hotel reviews. \nYou can find more information about the dataset and the training process in the [paper](http://www.lrec-conf.org/proceedings/lrec2020/pdf/2020.lrec-1.202.pdf).\n\n## Using the Python package\n\nTo get started install the package from [pypi](https://pypi.org/project/germansentiment/):\n\n```bash\npip install germansentiment\n```\n\n```python\nfrom germansentiment import SentimentModel\n\nmodel = SentimentModel()\n\ntexts = [\n    \"Mit keinem guten Ergebniss\",\"Das ist gar nicht mal so gut\",\n    \"Total awesome!\",\"nicht so schlecht wie erwartet\",\n    \"Der Test verlief positiv.\",\"Sie fährt ein grünes Auto.\"]\n       \nresult = model.predict_sentiment(texts)\nprint(result)\n```\n\nThe code above will output following list:\n\n```python\n[\"negative\",\"negative\",\"positive\",\"positive\",\"neutral\", \"neutral\"]\n```\n\n### Output class probabilities\n\n```python\nfrom germansentiment import SentimentModel\n\nmodel = SentimentModel()\n\nclasses, probabilities = model.predict_sentiment([\"das ist super\"], output_probabilities = True) \nprint(classes, probabilities)\n```\n```python\n['positive'] [[['positive', 0.9761366844177246], ['negative', 0.023540444672107697], ['neutral', 0.00032294404809363186]]]\n```\n\n\n\n## Model and Data\n\nIf you are interested in code and data that was used to train this model please have a look at [this repository](https://github.com/oliverguhr/german-sentiment) and our [paper](http://www.lrec-conf.org/proceedings/lrec2020/pdf/2020.lrec-1.202.pdf). Here is a table of the F1 scores that this model achieves on different datasets. Since we trained this model with a newer version of the transformer library, the results are slightly better than reported in the paper.\n\n| Dataset                                                      | F1 micro Score |\n| :----------------------------------------------------------- | -------------: |\n| [holidaycheck](https://github.com/oliverguhr/german-sentiment) |         0.9568 |\n| [scare](https://www.romanklinger.de/scare/)                  |         0.9418 |\n| [filmstarts](https://github.com/oliverguhr/german-sentiment) |         0.9021 |\n| [germeval](https://sites.google.com/view/germeval2017-absa/home) |         0.7536 |\n| [PotTS](https://www.aclweb.org/anthology/L16-1181/)          |         0.6780 |\n| [emotions](https://github.com/oliverguhr/german-sentiment)  |         0.9649 |\n| [sb10k](https://www.spinningbytes.com/resources/germansentiment/) |         0.7376 |\n| [Leipzig Wikipedia Corpus 2016](https://wortschatz.uni-leipzig.de/de/download/german) |         0.9967 |\n| all                                                          |         0.9639 |\n\n## Cite\n\nFor feedback and questions contact me view mail or Twitter [@oliverguhr](https://twitter.com/oliverguhr). Please cite us if you found this useful:\n\n```\n@InProceedings{guhr-EtAl:2020:LREC,\n  author    = {Guhr, Oliver  and  Schumann, Anne-Kathrin  and  Bahrmann, Frank  and  Böhme, Hans Joachim},\n  title     = {Training a Broad-Coverage German Sentiment Classification Model for Dialog Systems},\n  booktitle      = {Proceedings of The 12th Language Resources and Evaluation Conference},\n  month          = {May},\n  year           = {2020},\n  address        = {Marseille, France},\n  publisher      = {European Language Resources Association},\n  pages     = {1620--1625},\n  url       = {https://www.aclweb.org/anthology/2020.lrec-1.202}\n}\n```\n\n\n",
    "card_content": "---\nlanguage:\n- de\ntags:\n- sentiment\n- bert\nlicense: mit\nwidget:\n- text: Das ist gar nicht mal so schlecht\nmetrics:\n- f1\n---\n\n\n# German Sentiment Classification with Bert\n\nThis model was trained for sentiment classification of German language texts. To achieve the best results all model inputs needs to be preprocessed with the same procedure, that was applied during the training. To simplify the usage of the model, \nwe provide a Python package that bundles the code need for the preprocessing and inferencing. \n\nThe model uses the Googles Bert architecture and was trained on 1.834 million German-language samples. The training data contains texts from various domains like Twitter, Facebook and movie, app and hotel reviews. \nYou can find more information about the dataset and the training process in the [paper](http://www.lrec-conf.org/proceedings/lrec2020/pdf/2020.lrec-1.202.pdf).\n\n## Using the Python package\n\nTo get started install the package from [pypi](https://pypi.org/project/germansentiment/):\n\n```bash\npip install germansentiment\n```\n\n```python\nfrom germansentiment import SentimentModel\n\nmodel = SentimentModel()\n\ntexts = [\n    \"Mit keinem guten Ergebniss\",\"Das ist gar nicht mal so gut\",\n    \"Total awesome!\",\"nicht so schlecht wie erwartet\",\n    \"Der Test verlief positiv.\",\"Sie fährt ein grünes Auto.\"]\n       \nresult = model.predict_sentiment(texts)\nprint(result)\n```\n\nThe code above will output following list:\n\n```python\n[\"negative\",\"negative\",\"positive\",\"positive\",\"neutral\", \"neutral\"]\n```\n\n### Output class probabilities\n\n```python\nfrom germansentiment import SentimentModel\n\nmodel = SentimentModel()\n\nclasses, probabilities = model.predict_sentiment([\"das ist super\"], output_probabilities = True) \nprint(classes, probabilities)\n```\n```python\n['positive'] [[['positive', 0.9761366844177246], ['negative', 0.023540444672107697], ['neutral', 0.00032294404809363186]]]\n```\n\n\n\n## Model and Data\n\nIf you are interested in code and data that was used to train this model please have a look at [this repository](https://github.com/oliverguhr/german-sentiment) and our [paper](http://www.lrec-conf.org/proceedings/lrec2020/pdf/2020.lrec-1.202.pdf). Here is a table of the F1 scores that this model achieves on different datasets. Since we trained this model with a newer version of the transformer library, the results are slightly better than reported in the paper.\n\n| Dataset                                                      | F1 micro Score |\n| :----------------------------------------------------------- | -------------: |\n| [holidaycheck](https://github.com/oliverguhr/german-sentiment) |         0.9568 |\n| [scare](https://www.romanklinger.de/scare/)                  |         0.9418 |\n| [filmstarts](https://github.com/oliverguhr/german-sentiment) |         0.9021 |\n| [germeval](https://sites.google.com/view/germeval2017-absa/home) |         0.7536 |\n| [PotTS](https://www.aclweb.org/anthology/L16-1181/)          |         0.6780 |\n| [emotions](https://github.com/oliverguhr/german-sentiment)  |         0.9649 |\n| [sb10k](https://www.spinningbytes.com/resources/germansentiment/) |         0.7376 |\n| [Leipzig Wikipedia Corpus 2016](https://wortschatz.uni-leipzig.de/de/download/german) |         0.9967 |\n| all                                                          |         0.9639 |\n\n## Cite\n\nFor feedback and questions contact me view mail or Twitter [@oliverguhr](https://twitter.com/oliverguhr). Please cite us if you found this useful:\n\n```\n@InProceedings{guhr-EtAl:2020:LREC,\n  author    = {Guhr, Oliver  and  Schumann, Anne-Kathrin  and  Bahrmann, Frank  and  Böhme, Hans Joachim},\n  title     = {Training a Broad-Coverage German Sentiment Classification Model for Dialog Systems},\n  booktitle      = {Proceedings of The 12th Language Resources and Evaluation Conference},\n  month          = {May},\n  year           = {2020},\n  address        = {Marseille, France},\n  publisher      = {European Language Resources Association},\n  pages     = {1620--1625},\n  url       = {https://www.aclweb.org/anthology/2020.lrec-1.202}\n}\n```\n\n\n",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModelForSequenceClassification",
      "custom_class": null,
      "pipeline_tag": "text-classification",
      "processor": "AutoTokenizer"
    },
    "safetensors": {
      "parameters": {
        "F32": 109083651
      },
      "total": 109083651
    },
    "model_index": null,
    "trending_score": null
  },
  {
    "model_id": "mixedbread-ai/mxbai-rerank-xsmall-v1",
    "model_name": "mixedbread-ai/mxbai-rerank-xsmall-v1",
    "author": "mixedbread-ai",
    "downloads": 389799,
    "downloads_all_time": null,
    "likes": 38,
    "tags": [
      "transformers",
      "onnx",
      "safetensors",
      "deberta-v2",
      "text-classification",
      "reranker",
      "transformers.js",
      "en",
      "license:apache-2.0",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/mixedbread-ai/mxbai-rerank-xsmall-v1",
    "dependencies": null,
    "last_modified": "2025-03-13T04:17:27+00:00",
    "created_at": "2024-02-29T10:31:57+00:00",
    "analysis_date": "2025-03-22T00:55:37.450687",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "deberta-v2",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "library_name": "transformers",
      "tags": [
        "reranker",
        "transformers.js"
      ],
      "license": "apache-2.0",
      "language": [
        "en"
      ]
    },
    "card_text": "<br><br>\n\n<p align=\"center\">\n<svg xmlns=\"http://www.w3.org/2000/svg\" xml:space=\"preserve\" viewBox=\"0 0 2020 1130\" width=\"150\" height=\"150\" aria-hidden=\"true\"><path fill=\"#e95a0f\" d=\"M398.167 621.992c-1.387-20.362-4.092-40.739-3.851-61.081.355-30.085 6.873-59.139 21.253-85.976 10.487-19.573 24.09-36.822 40.662-51.515 16.394-14.535 34.338-27.046 54.336-36.182 15.224-6.955 31.006-12.609 47.829-14.168 11.809-1.094 23.753-2.514 35.524-1.836 23.033 1.327 45.131 7.255 66.255 16.75 16.24 7.3 31.497 16.165 45.651 26.969 12.997 9.921 24.412 21.37 34.158 34.509 11.733 15.817 20.849 33.037 25.987 52.018 3.468 12.81 6.438 25.928 7.779 39.097 1.722 16.908 1.642 34.003 2.235 51.021.427 12.253.224 24.547 1.117 36.762 1.677 22.93 4.062 45.764 11.8 67.7 5.376 15.239 12.499 29.55 20.846 43.681l-18.282 20.328c-1.536 1.71-2.795 3.665-4.254 5.448l-19.323 23.533c-13.859-5.449-27.446-11.803-41.657-16.086-13.622-4.106-27.793-6.765-41.905-8.775-15.256-2.173-30.701-3.475-46.105-4.049-23.571-.879-47.178-1.056-70.769-1.029-10.858.013-21.723 1.116-32.57 1.926-5.362.4-10.69 1.255-16.464 1.477-2.758-7.675-5.284-14.865-7.367-22.181-3.108-10.92-4.325-22.554-13.16-31.095-2.598-2.512-5.069-5.341-6.883-8.443-6.366-10.884-12.48-21.917-18.571-32.959-4.178-7.573-8.411-14.375-17.016-18.559-10.34-5.028-19.538-12.387-29.311-18.611-3.173-2.021-6.414-4.312-9.952-5.297-5.857-1.63-11.98-2.301-17.991-3.376z\"></path><path fill=\"#ed6d7b\" d=\"M1478.998 758.842c-12.025.042-24.05.085-36.537-.373-.14-8.536.231-16.569.453-24.607.033-1.179-.315-2.986-1.081-3.4-.805-.434-2.376.338-3.518.81-.856.354-1.562 1.069-3.589 2.521-.239-3.308-.664-5.586-.519-7.827.488-7.544 2.212-15.166 1.554-22.589-1.016-11.451 1.397-14.592-12.332-14.419-3.793.048-3.617-2.803-3.332-5.331.499-4.422 1.45-8.803 1.77-13.233.311-4.316.068-8.672.068-12.861-2.554-.464-4.326-.86-6.12-1.098-4.415-.586-6.051-2.251-5.065-7.31 1.224-6.279.848-12.862 1.276-19.306.19-2.86-.971-4.473-3.794-4.753-4.113-.407-8.242-1.057-12.352-.975-4.663.093-5.192-2.272-4.751-6.012.733-6.229 1.252-12.483 1.875-18.726l1.102-10.495c-5.905-.309-11.146-.805-16.385-.778-3.32.017-5.174-1.4-5.566-4.4-1.172-8.968-2.479-17.944-3.001-26.96-.26-4.484-1.936-5.705-6.005-5.774-9.284-.158-18.563-.594-27.843-.953-7.241-.28-10.137-2.764-11.3-9.899-.746-4.576-2.715-7.801-7.777-8.207-7.739-.621-15.511-.992-23.207-1.961-7.327-.923-14.587-2.415-21.853-3.777-5.021-.941-10.003-2.086-15.003-3.14 4.515-22.952 13.122-44.382 26.284-63.587 18.054-26.344 41.439-47.239 69.102-63.294 15.847-9.197 32.541-16.277 50.376-20.599 16.655-4.036 33.617-5.715 50.622-4.385 33.334 2.606 63.836 13.955 92.415 31.15 15.864 9.545 30.241 20.86 42.269 34.758 8.113 9.374 15.201 19.78 21.718 30.359 10.772 17.484 16.846 36.922 20.611 56.991 1.783 9.503 2.815 19.214 3.318 28.876.758 14.578.755 29.196.65 44.311l-51.545 20.013c-7.779 3.059-15.847 5.376-21.753 12.365-4.73 5.598-10.658 10.316-16.547 14.774-9.9 7.496-18.437 15.988-25.083 26.631-3.333 5.337-7.901 10.381-12.999 14.038-11.355 8.144-17.397 18.973-19.615 32.423l-6.988 41.011z\"></path><path fill=\"#ec663e\" d=\"M318.11 923.047c-.702 17.693-.832 35.433-2.255 53.068-1.699 21.052-6.293 41.512-14.793 61.072-9.001 20.711-21.692 38.693-38.496 53.583-16.077 14.245-34.602 24.163-55.333 30.438-21.691 6.565-43.814 8.127-66.013 6.532-22.771-1.636-43.88-9.318-62.74-22.705-20.223-14.355-35.542-32.917-48.075-54.096-9.588-16.203-16.104-33.55-19.201-52.015-2.339-13.944-2.307-28.011-.403-42.182 2.627-19.545 9.021-37.699 17.963-55.067 11.617-22.564 27.317-41.817 48.382-56.118 15.819-10.74 33.452-17.679 52.444-20.455 8.77-1.282 17.696-1.646 26.568-2.055 11.755-.542 23.534-.562 35.289-1.11 8.545-.399 17.067-1.291 26.193-1.675 1.349 1.77 2.24 3.199 2.835 4.742 4.727 12.261 10.575 23.865 18.636 34.358 7.747 10.084 14.83 20.684 22.699 30.666 3.919 4.972 8.37 9.96 13.609 13.352 7.711 4.994 16.238 8.792 24.617 12.668 5.852 2.707 12.037 4.691 18.074 6.998z\"></path><path fill=\"#ea580e\" d=\"M1285.167 162.995c3.796-29.75 13.825-56.841 32.74-80.577 16.339-20.505 36.013-36.502 59.696-47.614 14.666-6.881 29.971-11.669 46.208-12.749 10.068-.669 20.239-1.582 30.255-.863 16.6 1.191 32.646 5.412 47.9 12.273 19.39 8.722 36.44 20.771 50.582 36.655 15.281 17.162 25.313 37.179 31.49 59.286 5.405 19.343 6.31 39.161 4.705 58.825-2.37 29.045-11.836 55.923-30.451 78.885-10.511 12.965-22.483 24.486-37.181 33.649-5.272-5.613-10.008-11.148-14.539-16.846-5.661-7.118-10.958-14.533-16.78-21.513-4.569-5.478-9.548-10.639-14.624-15.658-3.589-3.549-7.411-6.963-11.551-9.827-5.038-3.485-10.565-6.254-15.798-9.468-8.459-5.195-17.011-9.669-26.988-11.898-12.173-2.72-24.838-4.579-35.622-11.834-1.437-.967-3.433-1.192-5.213-1.542-12.871-2.529-25.454-5.639-36.968-12.471-5.21-3.091-11.564-4.195-17.011-6.965-4.808-2.445-8.775-6.605-13.646-8.851-8.859-4.085-18.114-7.311-27.204-10.896z\"></path><path fill=\"#f8ab00\" d=\"M524.963 311.12c-9.461-5.684-19.513-10.592-28.243-17.236-12.877-9.801-24.031-21.578-32.711-35.412-11.272-17.965-19.605-37.147-21.902-58.403-1.291-11.951-2.434-24.073-1.87-36.034.823-17.452 4.909-34.363 11.581-50.703 8.82-21.603 22.25-39.792 39.568-55.065 18.022-15.894 39.162-26.07 62.351-32.332 19.22-5.19 38.842-6.177 58.37-4.674 23.803 1.831 45.56 10.663 65.062 24.496 17.193 12.195 31.688 27.086 42.894 45.622-11.403 8.296-22.633 16.117-34.092 23.586-17.094 11.142-34.262 22.106-48.036 37.528-8.796 9.848-17.201 20.246-27.131 28.837-16.859 14.585-27.745 33.801-41.054 51.019-11.865 15.349-20.663 33.117-30.354 50.08-5.303 9.283-9.654 19.11-14.434 28.692z\"></path><path fill=\"#ea5227\" d=\"M1060.11 1122.049c-7.377 1.649-14.683 4.093-22.147 4.763-11.519 1.033-23.166 1.441-34.723 1.054-19.343-.647-38.002-4.7-55.839-12.65-15.078-6.72-28.606-15.471-40.571-26.836-24.013-22.81-42.053-49.217-49.518-81.936-1.446-6.337-1.958-12.958-2.235-19.477-.591-13.926-.219-27.909-1.237-41.795-.916-12.5-3.16-24.904-4.408-37.805 1.555-1.381 3.134-2.074 3.778-3.27 4.729-8.79 12.141-15.159 19.083-22.03 5.879-5.818 10.688-12.76 16.796-18.293 6.993-6.335 11.86-13.596 14.364-22.612l8.542-29.993c8.015 1.785 15.984 3.821 24.057 5.286 8.145 1.478 16.371 2.59 24.602 3.493 8.453.927 16.956 1.408 25.891 2.609 1.119 16.09 1.569 31.667 2.521 47.214.676 11.045 1.396 22.154 3.234 33.043 2.418 14.329 5.708 28.527 9.075 42.674 3.499 14.705 4.028 29.929 10.415 44.188 10.157 22.674 18.29 46.25 28.281 69.004 7.175 16.341 12.491 32.973 15.078 50.615.645 4.4 3.256 8.511 4.963 12.755z\"></path><path fill=\"#ea5330\" d=\"M1060.512 1122.031c-2.109-4.226-4.72-8.337-5.365-12.737-2.587-17.642-7.904-34.274-15.078-50.615-9.991-22.755-18.124-46.33-28.281-69.004-6.387-14.259-6.916-29.482-10.415-44.188-3.366-14.147-6.656-28.346-9.075-42.674-1.838-10.889-2.558-21.999-3.234-33.043-.951-15.547-1.401-31.124-2.068-47.146 8.568-.18 17.146.487 25.704.286l41.868-1.4c.907 3.746 1.245 7.04 1.881 10.276l8.651 42.704c.903 4.108 2.334 8.422 4.696 11.829 7.165 10.338 14.809 20.351 22.456 30.345 4.218 5.512 8.291 11.304 13.361 15.955 8.641 7.927 18.065 14.995 27.071 22.532 12.011 10.052 24.452 19.302 40.151 22.854-1.656 11.102-2.391 22.44-5.172 33.253-4.792 18.637-12.38 36.209-23.412 52.216-13.053 18.94-29.086 34.662-49.627 45.055-10.757 5.443-22.443 9.048-34.111 13.501z\"></path><path fill=\"#f8aa05\" d=\"M1989.106 883.951c5.198 8.794 11.46 17.148 15.337 26.491 5.325 12.833 9.744 26.207 12.873 39.737 2.95 12.757 3.224 25.908 1.987 39.219-1.391 14.973-4.643 29.268-10.349 43.034-5.775 13.932-13.477 26.707-23.149 38.405-14.141 17.104-31.215 30.458-50.807 40.488-14.361 7.352-29.574 12.797-45.741 14.594-10.297 1.144-20.732 2.361-31.031 1.894-24.275-1.1-47.248-7.445-68.132-20.263-6.096-3.741-11.925-7.917-17.731-12.342 5.319-5.579 10.361-10.852 15.694-15.811l37.072-34.009c.975-.892 2.113-1.606 3.08-2.505 6.936-6.448 14.765-12.2 20.553-19.556 8.88-11.285 20.064-19.639 31.144-28.292 4.306-3.363 9.06-6.353 12.673-10.358 5.868-6.504 10.832-13.814 16.422-20.582 6.826-8.264 13.727-16.481 20.943-24.401 4.065-4.461 8.995-8.121 13.249-12.424 14.802-14.975 28.77-30.825 45.913-43.317z\"></path><path fill=\"#ed6876\" d=\"M1256.099 523.419c5.065.642 10.047 1.787 15.068 2.728 7.267 1.362 14.526 2.854 21.853 3.777 7.696.97 15.468 1.34 23.207 1.961 5.062.406 7.031 3.631 7.777 8.207 1.163 7.135 4.059 9.62 11.3 9.899l27.843.953c4.069.069 5.745 1.291 6.005 5.774.522 9.016 1.829 17.992 3.001 26.96.392 3 2.246 4.417 5.566 4.4 5.239-.026 10.48.469 16.385.778l-1.102 10.495-1.875 18.726c-.44 3.74.088 6.105 4.751 6.012 4.11-.082 8.239.568 12.352.975 2.823.28 3.984 1.892 3.794 4.753-.428 6.444-.052 13.028-1.276 19.306-.986 5.059.651 6.724 5.065 7.31 1.793.238 3.566.634 6.12 1.098 0 4.189.243 8.545-.068 12.861-.319 4.43-1.27 8.811-1.77 13.233-.285 2.528-.461 5.379 3.332 5.331 13.729-.173 11.316 2.968 12.332 14.419.658 7.423-1.066 15.045-1.554 22.589-.145 2.241.28 4.519.519 7.827 2.026-1.452 2.733-2.167 3.589-2.521 1.142-.472 2.713-1.244 3.518-.81.767.414 1.114 2.221 1.081 3.4l-.917 24.539c-11.215.82-22.45.899-33.636 1.674l-43.952 3.436c-1.086-3.01-2.319-5.571-2.296-8.121.084-9.297-4.468-16.583-9.091-24.116-3.872-6.308-8.764-13.052-9.479-19.987-1.071-10.392-5.716-15.936-14.889-18.979-1.097-.364-2.16-.844-3.214-1.327-7.478-3.428-15.548-5.918-19.059-14.735-.904-2.27-3.657-3.775-5.461-5.723-2.437-2.632-4.615-5.525-7.207-7.987-2.648-2.515-5.352-5.346-8.589-6.777-4.799-2.121-10.074-3.185-15.175-4.596l-15.785-4.155c.274-12.896 1.722-25.901.54-38.662-1.647-17.783-3.457-35.526-2.554-53.352.528-10.426 2.539-20.777 3.948-31.574z\"></path><path fill=\"#f6a200\" d=\"M525.146 311.436c4.597-9.898 8.947-19.725 14.251-29.008 9.691-16.963 18.49-34.73 30.354-50.08 13.309-17.218 24.195-36.434 41.054-51.019 9.93-8.591 18.335-18.989 27.131-28.837 13.774-15.422 30.943-26.386 48.036-37.528 11.459-7.469 22.688-15.29 34.243-23.286 11.705 16.744 19.716 35.424 22.534 55.717 2.231 16.066 2.236 32.441 2.753 49.143-4.756 1.62-9.284 2.234-13.259 4.056-6.43 2.948-12.193 7.513-18.774 9.942-19.863 7.331-33.806 22.349-47.926 36.784-7.86 8.035-13.511 18.275-19.886 27.705-4.434 6.558-9.345 13.037-12.358 20.254-4.249 10.177-6.94 21.004-10.296 31.553-12.33.053-24.741 1.027-36.971-.049-20.259-1.783-40.227-5.567-58.755-14.69-.568-.28-1.295-.235-2.132-.658z\"></path><path fill=\"#f7a80d\" d=\"M1989.057 883.598c-17.093 12.845-31.061 28.695-45.863 43.67-4.254 4.304-9.184 7.963-13.249 12.424-7.216 7.92-14.117 16.137-20.943 24.401-5.59 6.768-10.554 14.078-16.422 20.582-3.614 4.005-8.367 6.995-12.673 10.358-11.08 8.653-22.264 17.007-31.144 28.292-5.788 7.356-13.617 13.108-20.553 19.556-.967.899-2.105 1.614-3.08 2.505l-37.072 34.009c-5.333 4.96-10.375 10.232-15.859 15.505-21.401-17.218-37.461-38.439-48.623-63.592 3.503-1.781 7.117-2.604 9.823-4.637 8.696-6.536 20.392-8.406 27.297-17.714.933-1.258 2.646-1.973 4.065-2.828 17.878-10.784 36.338-20.728 53.441-32.624 10.304-7.167 18.637-17.23 27.583-26.261 3.819-3.855 7.436-8.091 10.3-12.681 12.283-19.68 24.43-39.446 40.382-56.471 12.224-13.047 17.258-29.524 22.539-45.927 15.85 4.193 29.819 12.129 42.632 22.08 10.583 8.219 19.782 17.883 27.42 29.351z\"></path><path fill=\"#ef7a72\" d=\"M1479.461 758.907c1.872-13.734 4.268-27.394 6.525-41.076 2.218-13.45 8.26-24.279 19.615-32.423 5.099-3.657 9.667-8.701 12.999-14.038 6.646-10.643 15.183-19.135 25.083-26.631 5.888-4.459 11.817-9.176 16.547-14.774 5.906-6.99 13.974-9.306 21.753-12.365l51.48-19.549c.753 11.848.658 23.787 1.641 35.637 1.771 21.353 4.075 42.672 11.748 62.955.17.449.107.985-.019 2.158-6.945 4.134-13.865 7.337-20.437 11.143-3.935 2.279-7.752 5.096-10.869 8.384-6.011 6.343-11.063 13.624-17.286 19.727-9.096 8.92-12.791 20.684-18.181 31.587-.202.409-.072.984-.096 1.481-8.488-1.72-16.937-3.682-25.476-5.094-9.689-1.602-19.426-3.084-29.201-3.949-15.095-1.335-30.241-2.1-45.828-3.172z\"></path><path fill=\"#e94e3b\" d=\"M957.995 766.838c-20.337-5.467-38.791-14.947-55.703-27.254-8.2-5.967-15.451-13.238-22.958-20.37 2.969-3.504 5.564-6.772 8.598-9.563 7.085-6.518 11.283-14.914 15.8-23.153 4.933-8.996 10.345-17.743 14.966-26.892 2.642-5.231 5.547-11.01 5.691-16.611.12-4.651.194-8.932 2.577-12.742 8.52-13.621 15.483-28.026 18.775-43.704 2.11-10.049 7.888-18.774 7.81-29.825-.064-9.089 4.291-18.215 6.73-27.313 3.212-11.983 7.369-23.797 9.492-35.968 3.202-18.358 5.133-36.945 7.346-55.466l4.879-45.8c6.693.288 13.386.575 20.54 1.365.13 3.458-.41 6.407-.496 9.37l-1.136 42.595c-.597 11.552-2.067 23.058-3.084 34.59l-3.845 44.478c-.939 10.202-1.779 20.432-3.283 30.557-.96 6.464-4.46 12.646-1.136 19.383.348.706-.426 1.894-.448 2.864-.224 9.918-5.99 19.428-2.196 29.646.103.279-.033.657-.092.983l-8.446 46.205c-1.231 6.469-2.936 12.846-4.364 19.279-1.5 6.757-2.602 13.621-4.456 20.277-3.601 12.93-10.657 25.3-5.627 39.47.368 1.036.234 2.352.017 3.476l-5.949 30.123z\"></path><path fill=\"#ea5043\" d=\"M958.343 767.017c1.645-10.218 3.659-20.253 5.602-30.302.217-1.124.351-2.44-.017-3.476-5.03-14.17 2.026-26.539 5.627-39.47 1.854-6.656 2.956-13.52 4.456-20.277 1.428-6.433 3.133-12.81 4.364-19.279l8.446-46.205c.059-.326.196-.705.092-.983-3.794-10.218 1.972-19.728 2.196-29.646.022-.97.796-2.158.448-2.864-3.324-6.737.176-12.919 1.136-19.383 1.504-10.125 2.344-20.355 3.283-30.557l3.845-44.478c1.017-11.532 2.488-23.038 3.084-34.59.733-14.18.722-28.397 1.136-42.595.086-2.963.626-5.912.956-9.301 5.356-.48 10.714-.527 16.536-.081 2.224 15.098 1.855 29.734 1.625 44.408-.157 10.064 1.439 20.142 1.768 30.23.334 10.235-.035 20.49.116 30.733.084 5.713.789 11.418.861 17.13.054 4.289-.469 8.585-.702 12.879-.072 1.323-.138 2.659-.031 3.975l2.534 34.405-1.707 36.293-1.908 48.69c-.182 8.103.993 16.237.811 24.34-.271 12.076-1.275 24.133-1.787 36.207-.102 2.414-.101 5.283 1.06 7.219 4.327 7.22 4.463 15.215 4.736 23.103.365 10.553.088 21.128.086 31.693-11.44 2.602-22.84.688-34.106-.916-11.486-1.635-22.806-4.434-34.546-6.903z\"></path><path fill=\"#eb5d19\" d=\"M398.091 622.45c6.086.617 12.21 1.288 18.067 2.918 3.539.985 6.779 3.277 9.952 5.297 9.773 6.224 18.971 13.583 29.311 18.611 8.606 4.184 12.839 10.986 17.016 18.559l18.571 32.959c1.814 3.102 4.285 5.931 6.883 8.443 8.835 8.542 10.052 20.175 13.16 31.095 2.082 7.317 4.609 14.507 6.946 22.127-29.472 3.021-58.969 5.582-87.584 15.222-1.185-2.302-1.795-4.362-2.769-6.233-4.398-8.449-6.703-18.174-14.942-24.299-2.511-1.866-5.103-3.814-7.047-6.218-8.358-10.332-17.028-20.276-28.772-26.973 4.423-11.478 9.299-22.806 13.151-34.473 4.406-13.348 6.724-27.18 6.998-41.313.098-5.093.643-10.176 1.06-15.722z\"></path><path fill=\"#e94c32\" d=\"M981.557 392.109c-1.172 15.337-2.617 30.625-4.438 45.869-2.213 18.521-4.144 37.108-7.346 55.466-2.123 12.171-6.28 23.985-9.492 35.968-2.439 9.098-6.794 18.224-6.73 27.313.078 11.051-5.7 19.776-7.81 29.825-3.292 15.677-10.255 30.082-18.775 43.704-2.383 3.81-2.458 8.091-2.577 12.742-.144 5.6-3.049 11.38-5.691 16.611-4.621 9.149-10.033 17.896-14.966 26.892-4.517 8.239-8.715 16.635-15.8 23.153-3.034 2.791-5.629 6.06-8.735 9.255-12.197-10.595-21.071-23.644-29.301-37.24-7.608-12.569-13.282-25.962-17.637-40.37 13.303-6.889 25.873-13.878 35.311-25.315.717-.869 1.934-1.312 2.71-2.147 5.025-5.405 10.515-10.481 14.854-16.397 6.141-8.374 10.861-17.813 17.206-26.008 8.22-10.618 13.657-22.643 20.024-34.466 4.448-.626 6.729-3.21 8.114-6.89 1.455-3.866 2.644-7.895 4.609-11.492 4.397-8.05 9.641-15.659 13.708-23.86 3.354-6.761 5.511-14.116 8.203-21.206 5.727-15.082 7.277-31.248 12.521-46.578 3.704-10.828 3.138-23.116 4.478-34.753l7.56-.073z\"></path><path fill=\"#f7a617\" d=\"M1918.661 831.99c-4.937 16.58-9.971 33.057-22.196 46.104-15.952 17.025-28.099 36.791-40.382 56.471-2.864 4.59-6.481 8.825-10.3 12.681-8.947 9.031-17.279 19.094-27.583 26.261-17.103 11.896-35.564 21.84-53.441 32.624-1.419.856-3.132 1.571-4.065 2.828-6.904 9.308-18.6 11.178-27.297 17.714-2.705 2.033-6.319 2.856-9.874 4.281-3.413-9.821-6.916-19.583-9.36-29.602-1.533-6.284-1.474-12.957-1.665-19.913 1.913-.78 3.374-1.057 4.81-1.431 15.822-4.121 31.491-8.029 43.818-20.323 9.452-9.426 20.371-17.372 30.534-26.097 6.146-5.277 13.024-10.052 17.954-16.326 14.812-18.848 28.876-38.285 43.112-57.581 2.624-3.557 5.506-7.264 6.83-11.367 2.681-8.311 4.375-16.94 6.476-25.438 17.89.279 35.333 3.179 52.629 9.113z\"></path><path fill=\"#ea553a\" d=\"M1172.91 977.582c-15.775-3.127-28.215-12.377-40.227-22.43-9.005-7.537-18.43-14.605-27.071-22.532-5.07-4.651-9.143-10.443-13.361-15.955-7.647-9.994-15.291-20.007-22.456-30.345-2.361-3.407-3.792-7.72-4.696-11.829-3.119-14.183-5.848-28.453-8.651-42.704-.636-3.236-.974-6.53-1.452-10.209 15.234-2.19 30.471-3.969 46.408-5.622 2.692 5.705 4.882 11.222 6.63 16.876 2.9 9.381 7.776 17.194 15.035 24.049 7.056 6.662 13.305 14.311 19.146 22.099 9.509 12.677 23.01 19.061 36.907 25.054-1.048 7.441-2.425 14.854-3.066 22.33-.956 11.162-1.393 22.369-2.052 33.557l-1.096 17.661z\"></path><path fill=\"#ea5453\" d=\"M1163.123 704.036c-4.005 5.116-7.685 10.531-12.075 15.293-12.842 13.933-27.653 25.447-44.902 34.538-3.166-5.708-5.656-11.287-8.189-17.251-3.321-12.857-6.259-25.431-9.963-37.775-4.6-15.329-10.6-30.188-11.349-46.562-.314-6.871-1.275-14.287-7.114-19.644-1.047-.961-1.292-3.053-1.465-4.67l-4.092-39.927c-.554-5.245-.383-10.829-2.21-15.623-3.622-9.503-4.546-19.253-4.688-29.163-.088-6.111 1.068-12.256.782-18.344-.67-14.281-1.76-28.546-2.9-42.8-.657-8.222-1.951-16.395-2.564-24.62-.458-6.137-.285-12.322-.104-18.21.959 5.831 1.076 11.525 2.429 16.909 2.007 7.986 5.225 15.664 7.324 23.632 3.222 12.23 1.547 25.219 6.728 37.355 4.311 10.099 6.389 21.136 9.732 31.669 2.228 7.02 6.167 13.722 7.121 20.863 1.119 8.376 6.1 13.974 10.376 20.716l2.026 10.576c1.711 9.216 3.149 18.283 8.494 26.599 6.393 9.946 11.348 20.815 16.943 31.276 4.021 7.519 6.199 16.075 12.925 22.065l24.462 22.26c.556.503 1.507.571 2.274.841z\"></path><path fill=\"#ea5b15\" d=\"M1285.092 163.432c9.165 3.148 18.419 6.374 27.279 10.459 4.871 2.246 8.838 6.406 13.646 8.851 5.446 2.77 11.801 3.874 17.011 6.965 11.514 6.831 24.097 9.942 36.968 12.471 1.78.35 3.777.576 5.213 1.542 10.784 7.255 23.448 9.114 35.622 11.834 9.977 2.23 18.529 6.703 26.988 11.898 5.233 3.214 10.76 5.983 15.798 9.468 4.14 2.864 7.962 6.279 11.551 9.827 5.076 5.02 10.056 10.181 14.624 15.658 5.822 6.98 11.119 14.395 16.78 21.513 4.531 5.698 9.267 11.233 14.222 16.987-10.005 5.806-20.07 12.004-30.719 16.943-7.694 3.569-16.163 5.464-24.688 7.669-2.878-7.088-5.352-13.741-7.833-20.392-.802-2.15-1.244-4.55-2.498-6.396-4.548-6.7-9.712-12.999-14.011-19.847-6.672-10.627-15.34-18.93-26.063-25.376-9.357-5.625-18.367-11.824-27.644-17.587-6.436-3.997-12.902-8.006-19.659-11.405-5.123-2.577-11.107-3.536-16.046-6.37-17.187-9.863-35.13-17.887-54.031-23.767-4.403-1.37-8.953-2.267-13.436-3.382l.926-27.565z\"></path><path fill=\"#ea504b\" d=\"M1098 737l7.789 16.893c-15.04 9.272-31.679 15.004-49.184 17.995-9.464 1.617-19.122 2.097-29.151 3.019-.457-10.636-.18-21.211-.544-31.764-.273-7.888-.409-15.883-4.736-23.103-1.16-1.936-1.162-4.805-1.06-7.219l1.787-36.207c.182-8.103-.993-16.237-.811-24.34.365-16.236 1.253-32.461 1.908-48.69.484-12 .942-24.001 1.98-36.069 5.57 10.19 10.632 20.42 15.528 30.728 1.122 2.362 2.587 5.09 2.339 7.488-1.536 14.819 5.881 26.839 12.962 38.33 10.008 16.241 16.417 33.54 20.331 51.964 2.285 10.756 4.729 21.394 11.958 30.165L1098 737z\"></path><path fill=\"#f6a320\" d=\"M1865.78 822.529c-1.849 8.846-3.544 17.475-6.224 25.786-1.323 4.102-4.206 7.81-6.83 11.367l-43.112 57.581c-4.93 6.273-11.808 11.049-17.954 16.326-10.162 8.725-21.082 16.671-30.534 26.097-12.327 12.294-27.997 16.202-43.818 20.323-1.436.374-2.897.651-4.744.986-1.107-17.032-1.816-34.076-2.079-51.556 1.265-.535 2.183-.428 2.888-.766 10.596-5.072 20.8-11.059 32.586-13.273 1.69-.317 3.307-1.558 4.732-2.662l26.908-21.114c4.992-4.003 11.214-7.393 14.381-12.585 11.286-18.5 22.363-37.263 27.027-58.87l36.046 1.811c3.487.165 6.983.14 10.727.549z\"></path><path fill=\"#ec6333\" d=\"M318.448 922.814c-6.374-2.074-12.56-4.058-18.412-6.765-8.379-3.876-16.906-7.675-24.617-12.668-5.239-3.392-9.69-8.381-13.609-13.352-7.87-9.983-14.953-20.582-22.699-30.666-8.061-10.493-13.909-22.097-18.636-34.358-.595-1.543-1.486-2.972-2.382-4.783 6.84-1.598 13.797-3.023 20.807-4.106 18.852-2.912 36.433-9.493 53.737-17.819.697.888.889 1.555 1.292 2.051l17.921 21.896c4.14 4.939 8.06 10.191 12.862 14.412 5.67 4.984 12.185 9.007 18.334 13.447-8.937 16.282-16.422 33.178-20.696 51.31-1.638 6.951-2.402 14.107-3.903 21.403z\"></path><path fill=\"#f49700\" d=\"M623.467 326.903c2.893-10.618 5.584-21.446 9.833-31.623 3.013-7.217 7.924-13.696 12.358-20.254 6.375-9.43 12.026-19.67 19.886-27.705 14.12-14.434 28.063-29.453 47.926-36.784 6.581-2.429 12.344-6.994 18.774-9.942 3.975-1.822 8.503-2.436 13.186-3.592 1.947 18.557 3.248 37.15 8.307 55.686-15.453 7.931-28.853 18.092-40.46 29.996-10.417 10.683-19.109 23.111-28.013 35.175-3.238 4.388-4.888 9.948-7.262 14.973-17.803-3.987-35.767-6.498-54.535-5.931z\"></path><path fill=\"#ea544c\" d=\"M1097.956 736.615c-2.925-3.218-5.893-6.822-8.862-10.425-7.229-8.771-9.672-19.409-11.958-30.165-3.914-18.424-10.323-35.722-20.331-51.964-7.081-11.491-14.498-23.511-12.962-38.33.249-2.398-1.217-5.126-2.339-7.488l-15.232-31.019-3.103-34.338c-.107-1.316-.041-2.653.031-3.975.233-4.294.756-8.59.702-12.879-.072-5.713-.776-11.417-.861-17.13l-.116-30.733c-.329-10.088-1.926-20.166-1.768-30.23.23-14.674.599-29.31-1.162-44.341 9.369-.803 18.741-1.179 28.558-1.074 1.446 15.814 2.446 31.146 3.446 46.478.108 6.163-.064 12.348.393 18.485.613 8.225 1.907 16.397 2.564 24.62l2.9 42.8c.286 6.088-.869 12.234-.782 18.344.142 9.91 1.066 19.661 4.688 29.163 1.827 4.794 1.657 10.377 2.21 15.623l4.092 39.927c.172 1.617.417 3.71 1.465 4.67 5.839 5.357 6.8 12.773 7.114 19.644.749 16.374 6.749 31.233 11.349 46.562 3.704 12.344 6.642 24.918 9.963 37.775z\"></path><path fill=\"#ec5c61\" d=\"M1204.835 568.008c1.254 25.351-1.675 50.16-10.168 74.61-8.598-4.883-18.177-8.709-24.354-15.59-7.44-8.289-13.929-17.442-21.675-25.711-8.498-9.072-16.731-18.928-21.084-31.113-.54-1.513-1.691-2.807-2.594-4.564-4.605-9.247-7.706-18.544-7.96-29.09-.835-7.149-1.214-13.944-2.609-20.523-2.215-10.454-5.626-20.496-7.101-31.302-2.513-18.419-7.207-36.512-5.347-55.352.24-2.43-.17-4.949-.477-7.402l-4.468-34.792c2.723-.379 5.446-.757 8.585-.667 1.749 8.781 2.952 17.116 4.448 25.399 1.813 10.037 3.64 20.084 5.934 30.017 1.036 4.482 3.953 8.573 4.73 13.064 1.794 10.377 4.73 20.253 9.272 29.771 2.914 6.105 4.761 12.711 7.496 18.912 2.865 6.496 6.264 12.755 9.35 19.156 3.764 7.805 7.667 15.013 16.1 19.441 7.527 3.952 13.713 10.376 20.983 14.924 6.636 4.152 13.932 7.25 20.937 10.813z\"></path><path fill=\"#ed676f\" d=\"M1140.75 379.231c18.38-4.858 36.222-11.21 53.979-18.971 3.222 3.368 5.693 6.744 8.719 9.512 2.333 2.134 5.451 5.07 8.067 4.923 7.623-.429 12.363 2.688 17.309 8.215 5.531 6.18 12.744 10.854 19.224 16.184-5.121 7.193-10.461 14.241-15.323 21.606-13.691 20.739-22.99 43.255-26.782 67.926-.543 3.536-1.281 7.043-2.366 10.925-14.258-6.419-26.411-14.959-32.731-29.803-1.087-2.553-2.596-4.93-3.969-7.355-1.694-2.993-3.569-5.89-5.143-8.943-1.578-3.062-2.922-6.249-4.295-9.413-1.57-3.621-3.505-7.163-4.47-10.946-1.257-4.93-.636-10.572-2.725-15.013-5.831-12.397-7.467-25.628-9.497-38.847z\"></path><path fill=\"#ed656e\" d=\"M1254.103 647.439c5.325.947 10.603 2.272 15.847 3.722 5.101 1.41 10.376 2.475 15.175 4.596 3.237 1.431 5.942 4.262 8.589 6.777 2.592 2.462 4.77 5.355 7.207 7.987 1.804 1.948 4.557 3.453 5.461 5.723 3.51 8.817 11.581 11.307 19.059 14.735 1.053.483 2.116.963 3.214 1.327 9.172 3.043 13.818 8.587 14.889 18.979.715 6.935 5.607 13.679 9.479 19.987 4.623 7.533 9.175 14.819 9.091 24.116-.023 2.55 1.21 5.111 1.874 8.055-19.861 2.555-39.795 4.296-59.597 9.09l-11.596-23.203c-1.107-2.169-2.526-4.353-4.307-5.975-7.349-6.694-14.863-13.209-22.373-19.723l-17.313-14.669c-2.776-2.245-5.935-4.017-8.92-6.003l11.609-38.185c1.508-5.453 1.739-11.258 2.613-17.336z\"></path><path fill=\"#ec6168\" d=\"M1140.315 379.223c2.464 13.227 4.101 26.459 9.931 38.856 2.089 4.441 1.468 10.083 2.725 15.013.965 3.783 2.9 7.325 4.47 10.946 1.372 3.164 2.716 6.351 4.295 9.413 1.574 3.053 3.449 5.95 5.143 8.943 1.372 2.425 2.882 4.803 3.969 7.355 6.319 14.844 18.473 23.384 32.641 30.212.067 5.121-.501 10.201-.435 15.271l.985 38.117c.151 4.586.616 9.162.868 14.201-7.075-3.104-14.371-6.202-21.007-10.354-7.269-4.548-13.456-10.972-20.983-14.924-8.434-4.428-12.337-11.637-16.1-19.441-3.087-6.401-6.485-12.66-9.35-19.156-2.735-6.201-4.583-12.807-7.496-18.912-4.542-9.518-7.477-19.394-9.272-29.771-.777-4.491-3.694-8.581-4.73-13.064-2.294-9.933-4.121-19.98-5.934-30.017-1.496-8.283-2.699-16.618-4.036-25.335 10.349-2.461 20.704-4.511 31.054-6.582.957-.191 1.887-.515 3.264-.769z\"></path><path fill=\"#e94c28\" d=\"M922 537c-6.003 11.784-11.44 23.81-19.66 34.428-6.345 8.196-11.065 17.635-17.206 26.008-4.339 5.916-9.828 10.992-14.854 16.397-.776.835-1.993 1.279-2.71 2.147-9.439 11.437-22.008 18.427-35.357 24.929-4.219-10.885-6.942-22.155-7.205-33.905l-.514-49.542c7.441-2.893 14.452-5.197 21.334-7.841 1.749-.672 3.101-2.401 4.604-3.681 6.749-5.745 12.845-12.627 20.407-16.944 7.719-4.406 14.391-9.101 18.741-16.889.626-1.122 1.689-2.077 2.729-2.877 7.197-5.533 12.583-12.51 16.906-20.439.68-1.247 2.495-1.876 4.105-2.651 2.835 1.408 5.267 2.892 7.884 3.892 3.904 1.491 4.392 3.922 2.833 7.439-1.47 3.318-2.668 6.756-4.069 10.106-1.247 2.981-.435 5.242 2.413 6.544 2.805 1.282 3.125 3.14 1.813 5.601l-6.907 12.799L922 537z\"></path><path fill=\"#eb5659\" d=\"M1124.995 566c.868 1.396 2.018 2.691 2.559 4.203 4.353 12.185 12.586 22.041 21.084 31.113 7.746 8.269 14.235 17.422 21.675 25.711 6.176 6.881 15.756 10.707 24.174 15.932-6.073 22.316-16.675 42.446-31.058 60.937-1.074-.131-2.025-.199-2.581-.702l-24.462-22.26c-6.726-5.99-8.904-14.546-12.925-22.065-5.594-10.461-10.55-21.33-16.943-31.276-5.345-8.315-6.783-17.383-8.494-26.599-.63-3.394-1.348-6.772-1.738-10.848-.371-6.313-1.029-11.934-1.745-18.052l6.34 4.04 1.288-.675-2.143-15.385 9.454 1.208v-8.545L1124.995 566z\"></path><path fill=\"#f5a02d\" d=\"M1818.568 820.096c-4.224 21.679-15.302 40.442-26.587 58.942-3.167 5.192-9.389 8.582-14.381 12.585l-26.908 21.114c-1.425 1.104-3.042 2.345-4.732 2.662-11.786 2.214-21.99 8.201-32.586 13.273-.705.338-1.624.231-2.824.334a824.35 824.35 0 0 1-8.262-42.708c4.646-2.14 9.353-3.139 13.269-5.47 5.582-3.323 11.318-6.942 15.671-11.652 7.949-8.6 14.423-18.572 22.456-27.081 8.539-9.046 13.867-19.641 18.325-30.922l46.559 8.922z\"></path><path fill=\"#eb5a57\" d=\"M1124.96 565.639c-5.086-4.017-10.208-8.395-15.478-12.901v8.545l-9.454-1.208 2.143 15.385-1.288.675-6.34-4.04c.716 6.118 1.375 11.74 1.745 17.633-4.564-6.051-9.544-11.649-10.663-20.025-.954-7.141-4.892-13.843-7.121-20.863-3.344-10.533-5.421-21.57-9.732-31.669-5.181-12.135-3.506-25.125-6.728-37.355-2.099-7.968-5.317-15.646-7.324-23.632-1.353-5.384-1.47-11.078-2.429-16.909l-3.294-46.689a278.63 278.63 0 0 1 27.57-2.084c2.114 12.378 3.647 24.309 5.479 36.195 1.25 8.111 2.832 16.175 4.422 24.23 1.402 7.103 2.991 14.169 4.55 21.241 1.478 6.706.273 14.002 4.6 20.088 5.401 7.597 7.176 16.518 9.467 25.337 1.953 7.515 5.804 14.253 11.917 19.406.254 10.095 3.355 19.392 7.96 28.639z\"></path><path fill=\"#ea541c\" d=\"M911.651 810.999c-2.511 10.165-5.419 20.146-8.2 30.162-2.503 9.015-7.37 16.277-14.364 22.612-6.108 5.533-10.917 12.475-16.796 18.293-6.942 6.871-14.354 13.24-19.083 22.03-.644 1.196-2.222 1.889-3.705 2.857-2.39-7.921-4.101-15.991-6.566-23.823-5.451-17.323-12.404-33.976-23.414-48.835l21.627-21.095c3.182-3.29 5.532-7.382 8.295-11.083l10.663-14.163c9.528 4.78 18.925 9.848 28.625 14.247 7.324 3.321 15.036 5.785 22.917 8.799z\"></path><path fill=\"#eb5d19\" d=\"M1284.092 191.421c4.557.69 9.107 1.587 13.51 2.957 18.901 5.881 36.844 13.904 54.031 23.767 4.938 2.834 10.923 3.792 16.046 6.37 6.757 3.399 13.224 7.408 19.659 11.405l27.644 17.587c10.723 6.446 19.392 14.748 26.063 25.376 4.299 6.848 9.463 13.147 14.011 19.847 1.254 1.847 1.696 4.246 2.498 6.396l7.441 20.332c-11.685 1.754-23.379 3.133-35.533 4.037-.737-2.093-.995-3.716-1.294-5.33-3.157-17.057-14.048-30.161-23.034-44.146-3.027-4.71-7.786-8.529-12.334-11.993-9.346-7.116-19.004-13.834-28.688-20.491-6.653-4.573-13.311-9.251-20.431-13.002-8.048-4.24-16.479-7.85-24.989-11.091-11.722-4.465-23.673-8.328-35.527-12.449l.927-19.572z\"></path><path fill=\"#eb5e24\" d=\"M1283.09 211.415c11.928 3.699 23.88 7.562 35.602 12.027 8.509 3.241 16.941 6.852 24.989 11.091 7.12 3.751 13.778 8.429 20.431 13.002 9.684 6.657 19.342 13.375 28.688 20.491 4.548 3.463 9.307 7.283 12.334 11.993 8.986 13.985 19.877 27.089 23.034 44.146.299 1.615.557 3.237.836 5.263-13.373-.216-26.749-.839-40.564-1.923-2.935-9.681-4.597-18.92-12.286-26.152-15.577-14.651-30.4-30.102-45.564-45.193-.686-.683-1.626-1.156-2.516-1.584l-47.187-22.615 2.203-20.546z\"></path><path fill=\"#e9511f\" d=\"M913 486.001c-1.29.915-3.105 1.543-3.785 2.791-4.323 7.929-9.709 14.906-16.906 20.439-1.04.8-2.103 1.755-2.729 2.877-4.35 7.788-11.022 12.482-18.741 16.889-7.562 4.317-13.658 11.199-20.407 16.944-1.503 1.28-2.856 3.009-4.604 3.681-6.881 2.643-13.893 4.948-21.262 7.377-.128-11.151.202-22.302.378-33.454.03-1.892-.6-3.795-.456-6.12 13.727-1.755 23.588-9.527 33.278-17.663 2.784-2.337 6.074-4.161 8.529-6.784l29.057-31.86c1.545-1.71 3.418-3.401 4.221-5.459 5.665-14.509 11.49-28.977 16.436-43.736 2.817-8.407 4.074-17.338 6.033-26.032 5.039.714 10.078 1.427 15.536 2.629-.909 8.969-2.31 17.438-3.546 25.931-2.41 16.551-5.84 32.839-11.991 48.461L913 486.001z\"></path><path fill=\"#ea5741\" d=\"M1179.451 903.828c-14.224-5.787-27.726-12.171-37.235-24.849-5.841-7.787-12.09-15.436-19.146-22.099-7.259-6.854-12.136-14.667-15.035-24.049-1.748-5.654-3.938-11.171-6.254-17.033 15.099-4.009 30.213-8.629 44.958-15.533l28.367 36.36c6.09 8.015 13.124 14.75 22.72 18.375-7.404 14.472-13.599 29.412-17.48 45.244-.271 1.106-.382 2.25-.895 3.583z\"></path><path fill=\"#ea522a\" d=\"M913.32 486.141c2.693-7.837 5.694-15.539 8.722-23.231 6.151-15.622 9.581-31.91 11.991-48.461l3.963-25.861c7.582.317 15.168 1.031 22.748 1.797 4.171.421 8.333.928 12.877 1.596-.963 11.836-.398 24.125-4.102 34.953-5.244 15.33-6.794 31.496-12.521 46.578-2.692 7.09-4.849 14.445-8.203 21.206-4.068 8.201-9.311 15.81-13.708 23.86-1.965 3.597-3.154 7.627-4.609 11.492-1.385 3.68-3.666 6.265-8.114 6.89-1.994-1.511-3.624-3.059-5.077-4.44l6.907-12.799c1.313-2.461.993-4.318-1.813-5.601-2.849-1.302-3.66-3.563-2.413-6.544 1.401-3.35 2.599-6.788 4.069-10.106 1.558-3.517 1.071-5.948-2.833-7.439-2.617-1-5.049-2.484-7.884-3.892z\"></path><path fill=\"#eb5e24\" d=\"M376.574 714.118c12.053 6.538 20.723 16.481 29.081 26.814 1.945 2.404 4.537 4.352 7.047 6.218 8.24 6.125 10.544 15.85 14.942 24.299.974 1.871 1.584 3.931 2.376 6.29-7.145 3.719-14.633 6.501-21.386 10.517-9.606 5.713-18.673 12.334-28.425 18.399-3.407-3.73-6.231-7.409-9.335-10.834l-30.989-33.862c11.858-11.593 22.368-24.28 31.055-38.431 1.86-3.031 3.553-6.164 5.632-9.409z\"></path><path fill=\"#e95514\" d=\"M859.962 787.636c-3.409 5.037-6.981 9.745-10.516 14.481-2.763 3.701-5.113 7.792-8.295 11.083-6.885 7.118-14.186 13.834-21.65 20.755-13.222-17.677-29.417-31.711-48.178-42.878-.969-.576-2.068-.934-3.27-1.709 6.28-8.159 12.733-15.993 19.16-23.849 1.459-1.783 2.718-3.738 4.254-5.448l18.336-19.969c4.909 5.34 9.619 10.738 14.081 16.333 9.72 12.19 21.813 21.566 34.847 29.867.411.262.725.674 1.231 1.334z\"></path><path fill=\"#eb5f2d\" d=\"M339.582 762.088l31.293 33.733c3.104 3.425 5.928 7.104 9.024 10.979-12.885 11.619-24.548 24.139-33.899 38.704-.872 1.359-1.56 2.837-2.644 4.428-6.459-4.271-12.974-8.294-18.644-13.278-4.802-4.221-8.722-9.473-12.862-14.412l-17.921-21.896c-.403-.496-.595-1.163-.926-2.105 16.738-10.504 32.58-21.87 46.578-36.154z\"></path><path fill=\"#f28d00\" d=\"M678.388 332.912c1.989-5.104 3.638-10.664 6.876-15.051 8.903-12.064 17.596-24.492 28.013-35.175 11.607-11.904 25.007-22.064 40.507-29.592 4.873 11.636 9.419 23.412 13.67 35.592-5.759 4.084-11.517 7.403-16.594 11.553-4.413 3.607-8.124 8.092-12.023 12.301-5.346 5.772-10.82 11.454-15.782 17.547-3.929 4.824-7.17 10.208-10.716 15.344l-33.95-12.518z\"></path><path fill=\"#f08369\" d=\"M1580.181 771.427c-.191-.803-.322-1.377-.119-1.786 5.389-10.903 9.084-22.666 18.181-31.587 6.223-6.103 11.276-13.385 17.286-19.727 3.117-3.289 6.933-6.105 10.869-8.384 6.572-3.806 13.492-7.009 20.461-10.752 1.773 3.23 3.236 6.803 4.951 10.251l12.234 24.993c-1.367 1.966-2.596 3.293-3.935 4.499-7.845 7.07-16.315 13.564-23.407 21.32-6.971 7.623-12.552 16.517-18.743 24.854l-37.777-13.68z\"></path><path fill=\"#f18b5e\" d=\"M1618.142 785.4c6.007-8.63 11.588-17.524 18.559-25.147 7.092-7.755 15.562-14.249 23.407-21.32 1.338-1.206 2.568-2.534 3.997-4.162l28.996 33.733c1.896 2.205 4.424 3.867 6.66 6.394-6.471 7.492-12.967 14.346-19.403 21.255l-18.407 19.953c-12.958-12.409-27.485-22.567-43.809-30.706z\"></path><path fill=\"#f49c3a\" d=\"M1771.617 811.1c-4.066 11.354-9.394 21.949-17.933 30.995-8.032 8.509-14.507 18.481-22.456 27.081-4.353 4.71-10.089 8.329-15.671 11.652-3.915 2.331-8.623 3.331-13.318 5.069-4.298-9.927-8.255-19.998-12.1-30.743 4.741-4.381 9.924-7.582 13.882-11.904 7.345-8.021 14.094-16.603 20.864-25.131 4.897-6.168 9.428-12.626 14.123-18.955l32.61 11.936z\"></path><path fill=\"#f08000\" d=\"M712.601 345.675c3.283-5.381 6.524-10.765 10.453-15.589 4.962-6.093 10.435-11.774 15.782-17.547 3.899-4.21 7.61-8.695 12.023-12.301 5.078-4.15 10.836-7.469 16.636-11.19a934.12 934.12 0 0 1 23.286 35.848c-4.873 6.234-9.676 11.895-14.63 17.421l-25.195 27.801c-11.713-9.615-24.433-17.645-38.355-24.443z\"></path><path fill=\"#ed6e04\" d=\"M751.11 370.42c8.249-9.565 16.693-18.791 25.041-28.103 4.954-5.526 9.757-11.187 14.765-17.106 7.129 6.226 13.892 13.041 21.189 19.225 5.389 4.567 11.475 8.312 17.53 12.92-5.51 7.863-10.622 15.919-17.254 22.427-8.881 8.716-18.938 16.233-28.49 24.264-5.703-6.587-11.146-13.427-17.193-19.682-4.758-4.921-10.261-9.121-15.587-13.944z\"></path><path fill=\"#ea541c\" d=\"M921.823 385.544c-1.739 9.04-2.995 17.971-5.813 26.378-4.946 14.759-10.771 29.227-16.436 43.736-.804 2.058-2.676 3.749-4.221 5.459l-29.057 31.86c-2.455 2.623-5.745 4.447-8.529 6.784-9.69 8.135-19.551 15.908-33.208 17.237-1.773-9.728-3.147-19.457-4.091-29.6l36.13-16.763c.581-.267 1.046-.812 1.525-1.269 8.033-7.688 16.258-15.19 24.011-23.152 4.35-4.467 9.202-9.144 11.588-14.69 6.638-15.425 15.047-30.299 17.274-47.358 3.536.344 7.072.688 10.829 1.377z\"></path><path fill=\"#f3944d\" d=\"M1738.688 798.998c-4.375 6.495-8.906 12.953-13.803 19.121-6.771 8.528-13.519 17.11-20.864 25.131-3.958 4.322-9.141 7.523-13.925 11.54-8.036-13.464-16.465-26.844-27.999-38.387 5.988-6.951 12.094-13.629 18.261-20.25l19.547-20.95 38.783 23.794z\"></path><path fill=\"#ec6168\" d=\"M1239.583 703.142c3.282 1.805 6.441 3.576 9.217 5.821 5.88 4.755 11.599 9.713 17.313 14.669l22.373 19.723c1.781 1.622 3.2 3.806 4.307 5.975 3.843 7.532 7.477 15.171 11.194 23.136-10.764 4.67-21.532 8.973-32.69 12.982l-22.733-27.366c-2.003-2.416-4.096-4.758-6.194-7.093-3.539-3.94-6.927-8.044-10.74-11.701-2.57-2.465-5.762-4.283-8.675-6.39l16.627-29.755z\"></path><path fill=\"#ec663e\" d=\"M1351.006 332.839l-28.499 10.33c-.294.107-.533.367-1.194.264-11.067-19.018-27.026-32.559-44.225-44.855-4.267-3.051-8.753-5.796-13.138-8.682l9.505-24.505c10.055 4.069 19.821 8.227 29.211 13.108 3.998 2.078 7.299 5.565 10.753 8.598 3.077 2.701 5.743 5.891 8.926 8.447 4.116 3.304 9.787 5.345 12.62 9.432 6.083 8.777 10.778 18.517 16.041 27.863z\"></path><path fill=\"#eb5e5b\" d=\"M1222.647 733.051c3.223 1.954 6.415 3.771 8.985 6.237 3.813 3.658 7.201 7.761 10.74 11.701l6.194 7.093 22.384 27.409c-13.056 6.836-25.309 14.613-36.736 24.161l-39.323-44.7 24.494-27.846c1.072-1.224 1.974-2.598 3.264-4.056z\"></path><path fill=\"#ea580e\" d=\"M876.001 376.171c5.874 1.347 11.748 2.694 17.812 4.789-.81 5.265-2.687 9.791-2.639 14.296.124 11.469-4.458 20.383-12.73 27.863-2.075 1.877-3.659 4.286-5.668 6.248l-22.808 21.967c-.442.422-1.212.488-1.813.757l-23.113 10.389-9.875 4.514c-2.305-6.09-4.609-12.181-6.614-18.676 7.64-4.837 15.567-8.54 22.18-13.873 9.697-7.821 18.931-16.361 27.443-25.455 5.613-5.998 12.679-11.331 14.201-20.475.699-4.2 2.384-8.235 3.623-12.345z\"></path><path fill=\"#e95514\" d=\"M815.103 467.384c3.356-1.894 6.641-3.415 9.94-4.903l23.113-10.389c.6-.269 1.371-.335 1.813-.757l22.808-21.967c2.008-1.962 3.593-4.371 5.668-6.248 8.272-7.48 12.854-16.394 12.73-27.863-.049-4.505 1.828-9.031 2.847-13.956 5.427.559 10.836 1.526 16.609 2.68-1.863 17.245-10.272 32.119-16.91 47.544-2.387 5.546-7.239 10.223-11.588 14.69-7.753 7.962-15.978 15.464-24.011 23.152-.478.458-.944 1.002-1.525 1.269l-36.069 16.355c-2.076-6.402-3.783-12.81-5.425-19.607z\"></path><path fill=\"#eb620b\" d=\"M783.944 404.402c9.499-8.388 19.556-15.905 28.437-24.621 6.631-6.508 11.744-14.564 17.575-22.273 9.271 4.016 18.501 8.375 27.893 13.43-4.134 7.07-8.017 13.778-12.833 19.731-5.785 7.15-12.109 13.917-18.666 20.376-7.99 7.869-16.466 15.244-24.731 22.832l-17.674-29.475z\"></path><path fill=\"#ea544c\" d=\"M1197.986 854.686c-9.756-3.309-16.79-10.044-22.88-18.059l-28.001-36.417c8.601-5.939 17.348-11.563 26.758-17.075 1.615 1.026 2.639 1.876 3.505 2.865l26.664 30.44c3.723 4.139 7.995 7.785 12.017 11.656l-18.064 26.591z\"></path><path fill=\"#ec6333\" d=\"M1351.41 332.903c-5.667-9.409-10.361-19.149-16.445-27.926-2.833-4.087-8.504-6.128-12.62-9.432-3.184-2.555-5.849-5.745-8.926-8.447-3.454-3.033-6.756-6.52-10.753-8.598-9.391-4.88-19.157-9.039-29.138-13.499 1.18-5.441 2.727-10.873 4.81-16.607 11.918 4.674 24.209 8.261 34.464 14.962 14.239 9.304 29.011 18.453 39.595 32.464 2.386 3.159 5.121 6.077 7.884 8.923 6.564 6.764 10.148 14.927 11.723 24.093l-20.594 4.067z\"></path><path fill=\"#eb5e5b\" d=\"M1117 536.549c-6.113-4.702-9.965-11.44-11.917-18.955-2.292-8.819-4.066-17.74-9.467-25.337-4.327-6.085-3.122-13.382-4.6-20.088l-4.55-21.241c-1.59-8.054-3.172-16.118-4.422-24.23l-5.037-36.129c6.382-1.43 12.777-2.462 19.582-3.443 1.906 11.646 3.426 23.24 4.878 34.842.307 2.453.717 4.973.477 7.402-1.86 18.84 2.834 36.934 5.347 55.352 1.474 10.806 4.885 20.848 7.101 31.302 1.394 6.579 1.774 13.374 2.609 20.523z\"></path><path fill=\"#ec644b\" d=\"M1263.638 290.071c4.697 2.713 9.183 5.458 13.45 8.509 17.199 12.295 33.158 25.836 43.873 44.907-8.026 4.725-16.095 9.106-24.83 13.372-11.633-15.937-25.648-28.515-41.888-38.689-1.609-1.008-3.555-1.48-5.344-2.2 2.329-3.852 4.766-7.645 6.959-11.573l7.78-14.326z\"></path><path fill=\"#eb5f2d\" d=\"M1372.453 328.903c-2.025-9.233-5.608-17.396-12.172-24.16-2.762-2.846-5.498-5.764-7.884-8.923-10.584-14.01-25.356-23.16-39.595-32.464-10.256-6.701-22.546-10.289-34.284-15.312.325-5.246 1.005-10.444 2.027-15.863l47.529 22.394c.89.428 1.83.901 2.516 1.584l45.564 45.193c7.69 7.233 9.352 16.472 11.849 26.084-5.032.773-10.066 1.154-15.55 1.466z\"></path><path fill=\"#e95a0f\" d=\"M801.776 434.171c8.108-7.882 16.584-15.257 24.573-23.126 6.558-6.459 12.881-13.226 18.666-20.376 4.817-5.953 8.7-12.661 13.011-19.409 5.739 1.338 11.463 3.051 17.581 4.838-.845 4.183-2.53 8.219-3.229 12.418-1.522 9.144-8.588 14.477-14.201 20.475-8.512 9.094-17.745 17.635-27.443 25.455-6.613 5.333-14.54 9.036-22.223 13.51-2.422-4.469-4.499-8.98-6.735-13.786z\"></path><path fill=\"#eb5e5b\" d=\"M1248.533 316.002c2.155.688 4.101 1.159 5.71 2.168 16.24 10.174 30.255 22.752 41.532 38.727-7.166 5.736-14.641 11.319-22.562 16.731-1.16-1.277-1.684-2.585-2.615-3.46l-38.694-36.2 14.203-15.029c.803-.86 1.38-1.93 2.427-2.936z\"></path><path fill=\"#eb5a57\" d=\"M1216.359 827.958c-4.331-3.733-8.603-7.379-12.326-11.518l-26.664-30.44c-.866-.989-1.89-1.839-3.152-2.902 6.483-6.054 13.276-11.959 20.371-18.005l39.315 44.704c-5.648 6.216-11.441 12.12-17.544 18.161z\"></path><path fill=\"#ec6168\" d=\"M1231.598 334.101l38.999 36.066c.931.876 1.456 2.183 2.303 3.608-4.283 4.279-8.7 8.24-13.769 12.091-4.2-3.051-7.512-6.349-11.338-8.867-12.36-8.136-22.893-18.27-32.841-29.093l16.646-13.805z\"></path><path fill=\"#ed656e\" d=\"M1214.597 347.955c10.303 10.775 20.836 20.908 33.196 29.044 3.825 2.518 7.137 5.816 10.992 8.903-3.171 4.397-6.65 8.648-10.432 13.046-6.785-5.184-13.998-9.858-19.529-16.038-4.946-5.527-9.687-8.644-17.309-8.215-2.616.147-5.734-2.788-8.067-4.923-3.026-2.769-5.497-6.144-8.35-9.568 6.286-4.273 12.715-8.237 19.499-12.25z\"></path></svg>\n</p>\n\n<p align=\"center\">\n<b>The crispy rerank family from <a href=\"https://mixedbread.com\"><b>Mixedbread</b></a>.</b>\n</p>\n\n<p align=\"center\">\n<sup> 🍞 Looking for a simple end-to-end retrieval solution? Meet Omni, our multimodal and multilingual model. <a href=\"https://mixedbread.com\"><b>Get in touch for access.</a> </sup>\n</p>\n  \n# mxbai-rerank-xsmall-v1\n\nThis is the smallest model in our family of powerful reranker models. You can learn more about the models in our [blog post](https://www.mixedbread.ai/blog/mxbai-rerank-v1).\n\nWe have three models:\n\n- [mxbai-rerank-xsmall-v1](https://huggingface.co/mixedbread-ai/mxbai-rerank-xsmall-v1) (🍞)\n- [mxbai-rerank-base-v1](https://huggingface.co/mixedbread-ai/mxbai-rerank-base-v1)\n- [mxbai-rerank-large-v1](https://huggingface.co/mixedbread-ai/mxbai-rerank-large-v1)\n\n## Quickstart\n\nCurrently, the best way to use our models is with the most recent version of sentence-transformers.\n\n`pip install -U sentence-transformers`\n\nLet's say you have a query, and you want to rerank a set of documents. You can do that with only one line of code:\n\n```python\nfrom sentence_transformers import CrossEncoder\n\n# Load the model, here we use our base sized model\nmodel = CrossEncoder(\"mixedbread-ai/mxbai-rerank-xsmall-v1\")\n\n\n# Example query and documents\nquery = \"Who wrote 'To Kill a Mockingbird'?\"\ndocuments = [\n    \"'To Kill a Mockingbird' is a novel by Harper Lee published in 1960. It was immediately successful, winning the Pulitzer Prize, and has become a classic of modern American literature.\",\n    \"The novel 'Moby-Dick' was written by Herman Melville and first published in 1851. It is considered a masterpiece of American literature and deals with complex themes of obsession, revenge, and the conflict between good and evil.\",\n    \"Harper Lee, an American novelist widely known for her novel 'To Kill a Mockingbird', was born in 1926 in Monroeville, Alabama. She received the Pulitzer Prize for Fiction in 1961.\",\n    \"Jane Austen was an English novelist known primarily for her six major novels, which interpret, critique and comment upon the British landed gentry at the end of the 18th century.\",\n    \"The 'Harry Potter' series, which consists of seven fantasy novels written by British author J.K. Rowling, is among the most popular and critically acclaimed books of the modern era.\",\n    \"'The Great Gatsby', a novel written by American author F. Scott Fitzgerald, was published in 1925. The story is set in the Jazz Age and follows the life of millionaire Jay Gatsby and his pursuit of Daisy Buchanan.\"\n]\n\n# Lets get the scores\nresults = model.rank(query, documents, return_documents=True, top_k=3)\n```\n\n<details>\n  <summary>JavaScript Example</summary>\n\nInstall [transformers.js](https://github.com/xenova/transformers.js)\n\n`npm i @xenova/transformers`\n\nLet's say you have a query, and you want to rerank a set of documents. In JavaScript, you need to add a function:\n\n```javascript\nimport { AutoTokenizer, AutoModelForSequenceClassification } from '@xenova/transformers';\n\nconst model_id = 'mixedbread-ai/mxbai-rerank-xsmall-v1';\nconst model = await AutoModelForSequenceClassification.from_pretrained(model_id);\nconst tokenizer = await AutoTokenizer.from_pretrained(model_id);\n\n/**\n * Performs ranking with the CrossEncoder on the given query and documents. Returns a sorted list with the document indices and scores.\n * @param {string} query A single query\n * @param {string[]} documents A list of documents\n * @param {Object} options Options for ranking\n * @param {number} [options.top_k=undefined] Return the top-k documents. If undefined, all documents are returned.\n * @param {number} [options.return_documents=false] If true, also returns the documents. If false, only returns the indices and scores.\n */\nasync function rank(query, documents, {\n    top_k = undefined,\n    return_documents = false,\n} = {}) {\n    const inputs = tokenizer(\n        new Array(documents.length).fill(query),\n        {\n            text_pair: documents,\n            padding: true,\n            truncation: true,\n        }\n    )\n    const { logits } = await model(inputs);\n    return logits\n        .sigmoid()\n        .tolist()\n        .map(([score], i) => ({\n            corpus_id: i,\n            score,\n            ...(return_documents ? { text: documents[i] } : {})\n        }))\n        .sort((a, b) => b.score - a.score)\n        .slice(0, top_k);\n}\n\n// Example usage:\nconst query = \"Who wrote 'To Kill a Mockingbird'?\"\nconst documents = [\n    \"'To Kill a Mockingbird' is a novel by Harper Lee published in 1960. It was immediately successful, winning the Pulitzer Prize, and has become a classic of modern American literature.\",\n    \"The novel 'Moby-Dick' was written by Herman Melville and first published in 1851. It is considered a masterpiece of American literature and deals with complex themes of obsession, revenge, and the conflict between good and evil.\",\n    \"Harper Lee, an American novelist widely known for her novel 'To Kill a Mockingbird', was born in 1926 in Monroeville, Alabama. She received the Pulitzer Prize for Fiction in 1961.\",\n    \"Jane Austen was an English novelist known primarily for her six major novels, which interpret, critique and comment upon the British landed gentry at the end of the 18th century.\",\n    \"The 'Harry Potter' series, which consists of seven fantasy novels written by British author J.K. Rowling, is among the most popular and critically acclaimed books of the modern era.\",\n    \"'The Great Gatsby', a novel written by American author F. Scott Fitzgerald, was published in 1925. The story is set in the Jazz Age and follows the life of millionaire Jay Gatsby and his pursuit of Daisy Buchanan.\"\n]\n\nconst results = await rank(query, documents, { return_documents: true, top_k: 3 });\nconsole.log(results);\n```\n</details>\n\n## Using API\n\nYou can use the large model via our API as follows:\n\n```python\nfrom mixedbread_ai.client import MixedbreadAI\n\nmxbai = MixedbreadAI(api_key=\"{MIXEDBREAD_API_KEY}\")\n\nres = mxbai.reranking(\n  model=\"mixedbread-ai/mxbai-rerank-large-v1\",\n  query=\"Who is the author of To Kill a Mockingbird?\",\n  input=[\n    \"To Kill a Mockingbird is a novel by Harper Lee published in 1960. It was immediately successful, winning the Pulitzer Prize, and has become a classic of modern American literature.\",\n    \"The novel Moby-Dick was written by Herman Melville and first published in 1851. It is considered a masterpiece of American literature and deals with complex themes of obsession, revenge, and the conflict between good and evil.\",\n    \"Harper Lee, an American novelist widely known for her novel To Kill a Mockingbird, was born in 1926 in Monroeville, Alabama. She received the Pulitzer Prize for Fiction in 1961.\",\n    \"Jane Austen was an English novelist known primarily for her six major novels, which interpret, critique and comment upon the British landed gentry at the end of the 18th century.\",\n    \"The Harry Potter series, which consists of seven fantasy novels written by British author J.K. Rowling, is among the most popular and critically acclaimed books of the modern era.\",\n    \"The Great Gatsby, a novel written by American author F. Scott Fitzgerald, was published in 1925. The story is set in the Jazz Age and follows the life of millionaire Jay Gatsby and his pursuit of Daisy Buchanan.\"\n  ],\n  top_k=3,\n  return_input=false\n)\n\nprint(res.data)\n```\n\nThe API comes with additional features, such as a continous trained reranker! Check out the [docs](https://www.mixedbread.ai/docs) for more information.\n\n\n## Evaluation\n\nOur reranker models are designed to elevate your search. They work extremely well in combination with keyword search and can even outperform semantic search systems in many cases.\n\n| Model                                                                                 | NDCG@10  | Accuracy@3 |\n| ------------------------------------------------------------------------------------- | -------- | ---------- |\n| Lexical Search (Lucene)                                                               | 38.0     | 66.4       |\n| [BAAI/bge-reranker-base](https://huggingface.co/BAAI/bge-reranker-base)               | 41.6     | 66.9       |\n| [BAAI/bge-reranker-large](https://huggingface.co/BAAI/bge-reranker-large)             | 45.2     | 70.6       |\n| cohere-embed-v3 (semantic search)                                                     | 47.5     | 70.9       |\n| [mxbai-rerank-xsmall-v1](https://huggingface.co/mixedbread-ai/mxbai-rerank-xsmall-v1) | **43.9** | **70.0**   |\n| [mxbai-rerank-base-v1](https://huggingface.co/mixedbread-ai/mxbai-rerank-base-v1)     | **46.9** | **72.3**   |\n| [mxbai-rerank-large-v1](https://huggingface.co/mixedbread-ai/mxbai-rerank-large-v1)   | **48.8** | **74.9**   |\n\nThe reported results are aggregated from 11 datasets of BEIR. We used [Pyserini](https://github.com/castorini/pyserini/) to evaluate the models. Find more in our [blog-post](https://www.mixedbread.ai/blog/mxbai-rerank-v1) and on this [spreadsheet](https://docs.google.com/spreadsheets/d/15ELkSMFv-oHa5TRiIjDvhIstH9dlc3pnZeO-iGz4Ld4/edit?usp=sharing).\n\n## Community\nPlease join our [Discord Community](https://discord.gg/jDfMHzAVfU) and share your feedback and thoughts! We are here to help and also always happy to chat.\n\n## Citation\n\n```bibtex\n@online{rerank2024mxbai,\n  title={Boost Your Search With The Crispy Mixedbread Rerank Models},\n  author={Aamir Shakir and Darius Koenig and Julius Lipp and Sean Lee},\n  year={2024},\n  url={https://www.mixedbread.ai/blog/mxbai-rerank-v1},\n}\n```\n\n## License\nApache 2.0",
    "card_content": "---\nlibrary_name: transformers\ntags:\n- reranker\n- transformers.js\nlicense: apache-2.0\nlanguage:\n- en\n---\n<br><br>\n\n<p align=\"center\">\n<svg xmlns=\"http://www.w3.org/2000/svg\" xml:space=\"preserve\" viewBox=\"0 0 2020 1130\" width=\"150\" height=\"150\" aria-hidden=\"true\"><path fill=\"#e95a0f\" d=\"M398.167 621.992c-1.387-20.362-4.092-40.739-3.851-61.081.355-30.085 6.873-59.139 21.253-85.976 10.487-19.573 24.09-36.822 40.662-51.515 16.394-14.535 34.338-27.046 54.336-36.182 15.224-6.955 31.006-12.609 47.829-14.168 11.809-1.094 23.753-2.514 35.524-1.836 23.033 1.327 45.131 7.255 66.255 16.75 16.24 7.3 31.497 16.165 45.651 26.969 12.997 9.921 24.412 21.37 34.158 34.509 11.733 15.817 20.849 33.037 25.987 52.018 3.468 12.81 6.438 25.928 7.779 39.097 1.722 16.908 1.642 34.003 2.235 51.021.427 12.253.224 24.547 1.117 36.762 1.677 22.93 4.062 45.764 11.8 67.7 5.376 15.239 12.499 29.55 20.846 43.681l-18.282 20.328c-1.536 1.71-2.795 3.665-4.254 5.448l-19.323 23.533c-13.859-5.449-27.446-11.803-41.657-16.086-13.622-4.106-27.793-6.765-41.905-8.775-15.256-2.173-30.701-3.475-46.105-4.049-23.571-.879-47.178-1.056-70.769-1.029-10.858.013-21.723 1.116-32.57 1.926-5.362.4-10.69 1.255-16.464 1.477-2.758-7.675-5.284-14.865-7.367-22.181-3.108-10.92-4.325-22.554-13.16-31.095-2.598-2.512-5.069-5.341-6.883-8.443-6.366-10.884-12.48-21.917-18.571-32.959-4.178-7.573-8.411-14.375-17.016-18.559-10.34-5.028-19.538-12.387-29.311-18.611-3.173-2.021-6.414-4.312-9.952-5.297-5.857-1.63-11.98-2.301-17.991-3.376z\"></path><path fill=\"#ed6d7b\" d=\"M1478.998 758.842c-12.025.042-24.05.085-36.537-.373-.14-8.536.231-16.569.453-24.607.033-1.179-.315-2.986-1.081-3.4-.805-.434-2.376.338-3.518.81-.856.354-1.562 1.069-3.589 2.521-.239-3.308-.664-5.586-.519-7.827.488-7.544 2.212-15.166 1.554-22.589-1.016-11.451 1.397-14.592-12.332-14.419-3.793.048-3.617-2.803-3.332-5.331.499-4.422 1.45-8.803 1.77-13.233.311-4.316.068-8.672.068-12.861-2.554-.464-4.326-.86-6.12-1.098-4.415-.586-6.051-2.251-5.065-7.31 1.224-6.279.848-12.862 1.276-19.306.19-2.86-.971-4.473-3.794-4.753-4.113-.407-8.242-1.057-12.352-.975-4.663.093-5.192-2.272-4.751-6.012.733-6.229 1.252-12.483 1.875-18.726l1.102-10.495c-5.905-.309-11.146-.805-16.385-.778-3.32.017-5.174-1.4-5.566-4.4-1.172-8.968-2.479-17.944-3.001-26.96-.26-4.484-1.936-5.705-6.005-5.774-9.284-.158-18.563-.594-27.843-.953-7.241-.28-10.137-2.764-11.3-9.899-.746-4.576-2.715-7.801-7.777-8.207-7.739-.621-15.511-.992-23.207-1.961-7.327-.923-14.587-2.415-21.853-3.777-5.021-.941-10.003-2.086-15.003-3.14 4.515-22.952 13.122-44.382 26.284-63.587 18.054-26.344 41.439-47.239 69.102-63.294 15.847-9.197 32.541-16.277 50.376-20.599 16.655-4.036 33.617-5.715 50.622-4.385 33.334 2.606 63.836 13.955 92.415 31.15 15.864 9.545 30.241 20.86 42.269 34.758 8.113 9.374 15.201 19.78 21.718 30.359 10.772 17.484 16.846 36.922 20.611 56.991 1.783 9.503 2.815 19.214 3.318 28.876.758 14.578.755 29.196.65 44.311l-51.545 20.013c-7.779 3.059-15.847 5.376-21.753 12.365-4.73 5.598-10.658 10.316-16.547 14.774-9.9 7.496-18.437 15.988-25.083 26.631-3.333 5.337-7.901 10.381-12.999 14.038-11.355 8.144-17.397 18.973-19.615 32.423l-6.988 41.011z\"></path><path fill=\"#ec663e\" d=\"M318.11 923.047c-.702 17.693-.832 35.433-2.255 53.068-1.699 21.052-6.293 41.512-14.793 61.072-9.001 20.711-21.692 38.693-38.496 53.583-16.077 14.245-34.602 24.163-55.333 30.438-21.691 6.565-43.814 8.127-66.013 6.532-22.771-1.636-43.88-9.318-62.74-22.705-20.223-14.355-35.542-32.917-48.075-54.096-9.588-16.203-16.104-33.55-19.201-52.015-2.339-13.944-2.307-28.011-.403-42.182 2.627-19.545 9.021-37.699 17.963-55.067 11.617-22.564 27.317-41.817 48.382-56.118 15.819-10.74 33.452-17.679 52.444-20.455 8.77-1.282 17.696-1.646 26.568-2.055 11.755-.542 23.534-.562 35.289-1.11 8.545-.399 17.067-1.291 26.193-1.675 1.349 1.77 2.24 3.199 2.835 4.742 4.727 12.261 10.575 23.865 18.636 34.358 7.747 10.084 14.83 20.684 22.699 30.666 3.919 4.972 8.37 9.96 13.609 13.352 7.711 4.994 16.238 8.792 24.617 12.668 5.852 2.707 12.037 4.691 18.074 6.998z\"></path><path fill=\"#ea580e\" d=\"M1285.167 162.995c3.796-29.75 13.825-56.841 32.74-80.577 16.339-20.505 36.013-36.502 59.696-47.614 14.666-6.881 29.971-11.669 46.208-12.749 10.068-.669 20.239-1.582 30.255-.863 16.6 1.191 32.646 5.412 47.9 12.273 19.39 8.722 36.44 20.771 50.582 36.655 15.281 17.162 25.313 37.179 31.49 59.286 5.405 19.343 6.31 39.161 4.705 58.825-2.37 29.045-11.836 55.923-30.451 78.885-10.511 12.965-22.483 24.486-37.181 33.649-5.272-5.613-10.008-11.148-14.539-16.846-5.661-7.118-10.958-14.533-16.78-21.513-4.569-5.478-9.548-10.639-14.624-15.658-3.589-3.549-7.411-6.963-11.551-9.827-5.038-3.485-10.565-6.254-15.798-9.468-8.459-5.195-17.011-9.669-26.988-11.898-12.173-2.72-24.838-4.579-35.622-11.834-1.437-.967-3.433-1.192-5.213-1.542-12.871-2.529-25.454-5.639-36.968-12.471-5.21-3.091-11.564-4.195-17.011-6.965-4.808-2.445-8.775-6.605-13.646-8.851-8.859-4.085-18.114-7.311-27.204-10.896z\"></path><path fill=\"#f8ab00\" d=\"M524.963 311.12c-9.461-5.684-19.513-10.592-28.243-17.236-12.877-9.801-24.031-21.578-32.711-35.412-11.272-17.965-19.605-37.147-21.902-58.403-1.291-11.951-2.434-24.073-1.87-36.034.823-17.452 4.909-34.363 11.581-50.703 8.82-21.603 22.25-39.792 39.568-55.065 18.022-15.894 39.162-26.07 62.351-32.332 19.22-5.19 38.842-6.177 58.37-4.674 23.803 1.831 45.56 10.663 65.062 24.496 17.193 12.195 31.688 27.086 42.894 45.622-11.403 8.296-22.633 16.117-34.092 23.586-17.094 11.142-34.262 22.106-48.036 37.528-8.796 9.848-17.201 20.246-27.131 28.837-16.859 14.585-27.745 33.801-41.054 51.019-11.865 15.349-20.663 33.117-30.354 50.08-5.303 9.283-9.654 19.11-14.434 28.692z\"></path><path fill=\"#ea5227\" d=\"M1060.11 1122.049c-7.377 1.649-14.683 4.093-22.147 4.763-11.519 1.033-23.166 1.441-34.723 1.054-19.343-.647-38.002-4.7-55.839-12.65-15.078-6.72-28.606-15.471-40.571-26.836-24.013-22.81-42.053-49.217-49.518-81.936-1.446-6.337-1.958-12.958-2.235-19.477-.591-13.926-.219-27.909-1.237-41.795-.916-12.5-3.16-24.904-4.408-37.805 1.555-1.381 3.134-2.074 3.778-3.27 4.729-8.79 12.141-15.159 19.083-22.03 5.879-5.818 10.688-12.76 16.796-18.293 6.993-6.335 11.86-13.596 14.364-22.612l8.542-29.993c8.015 1.785 15.984 3.821 24.057 5.286 8.145 1.478 16.371 2.59 24.602 3.493 8.453.927 16.956 1.408 25.891 2.609 1.119 16.09 1.569 31.667 2.521 47.214.676 11.045 1.396 22.154 3.234 33.043 2.418 14.329 5.708 28.527 9.075 42.674 3.499 14.705 4.028 29.929 10.415 44.188 10.157 22.674 18.29 46.25 28.281 69.004 7.175 16.341 12.491 32.973 15.078 50.615.645 4.4 3.256 8.511 4.963 12.755z\"></path><path fill=\"#ea5330\" d=\"M1060.512 1122.031c-2.109-4.226-4.72-8.337-5.365-12.737-2.587-17.642-7.904-34.274-15.078-50.615-9.991-22.755-18.124-46.33-28.281-69.004-6.387-14.259-6.916-29.482-10.415-44.188-3.366-14.147-6.656-28.346-9.075-42.674-1.838-10.889-2.558-21.999-3.234-33.043-.951-15.547-1.401-31.124-2.068-47.146 8.568-.18 17.146.487 25.704.286l41.868-1.4c.907 3.746 1.245 7.04 1.881 10.276l8.651 42.704c.903 4.108 2.334 8.422 4.696 11.829 7.165 10.338 14.809 20.351 22.456 30.345 4.218 5.512 8.291 11.304 13.361 15.955 8.641 7.927 18.065 14.995 27.071 22.532 12.011 10.052 24.452 19.302 40.151 22.854-1.656 11.102-2.391 22.44-5.172 33.253-4.792 18.637-12.38 36.209-23.412 52.216-13.053 18.94-29.086 34.662-49.627 45.055-10.757 5.443-22.443 9.048-34.111 13.501z\"></path><path fill=\"#f8aa05\" d=\"M1989.106 883.951c5.198 8.794 11.46 17.148 15.337 26.491 5.325 12.833 9.744 26.207 12.873 39.737 2.95 12.757 3.224 25.908 1.987 39.219-1.391 14.973-4.643 29.268-10.349 43.034-5.775 13.932-13.477 26.707-23.149 38.405-14.141 17.104-31.215 30.458-50.807 40.488-14.361 7.352-29.574 12.797-45.741 14.594-10.297 1.144-20.732 2.361-31.031 1.894-24.275-1.1-47.248-7.445-68.132-20.263-6.096-3.741-11.925-7.917-17.731-12.342 5.319-5.579 10.361-10.852 15.694-15.811l37.072-34.009c.975-.892 2.113-1.606 3.08-2.505 6.936-6.448 14.765-12.2 20.553-19.556 8.88-11.285 20.064-19.639 31.144-28.292 4.306-3.363 9.06-6.353 12.673-10.358 5.868-6.504 10.832-13.814 16.422-20.582 6.826-8.264 13.727-16.481 20.943-24.401 4.065-4.461 8.995-8.121 13.249-12.424 14.802-14.975 28.77-30.825 45.913-43.317z\"></path><path fill=\"#ed6876\" d=\"M1256.099 523.419c5.065.642 10.047 1.787 15.068 2.728 7.267 1.362 14.526 2.854 21.853 3.777 7.696.97 15.468 1.34 23.207 1.961 5.062.406 7.031 3.631 7.777 8.207 1.163 7.135 4.059 9.62 11.3 9.899l27.843.953c4.069.069 5.745 1.291 6.005 5.774.522 9.016 1.829 17.992 3.001 26.96.392 3 2.246 4.417 5.566 4.4 5.239-.026 10.48.469 16.385.778l-1.102 10.495-1.875 18.726c-.44 3.74.088 6.105 4.751 6.012 4.11-.082 8.239.568 12.352.975 2.823.28 3.984 1.892 3.794 4.753-.428 6.444-.052 13.028-1.276 19.306-.986 5.059.651 6.724 5.065 7.31 1.793.238 3.566.634 6.12 1.098 0 4.189.243 8.545-.068 12.861-.319 4.43-1.27 8.811-1.77 13.233-.285 2.528-.461 5.379 3.332 5.331 13.729-.173 11.316 2.968 12.332 14.419.658 7.423-1.066 15.045-1.554 22.589-.145 2.241.28 4.519.519 7.827 2.026-1.452 2.733-2.167 3.589-2.521 1.142-.472 2.713-1.244 3.518-.81.767.414 1.114 2.221 1.081 3.4l-.917 24.539c-11.215.82-22.45.899-33.636 1.674l-43.952 3.436c-1.086-3.01-2.319-5.571-2.296-8.121.084-9.297-4.468-16.583-9.091-24.116-3.872-6.308-8.764-13.052-9.479-19.987-1.071-10.392-5.716-15.936-14.889-18.979-1.097-.364-2.16-.844-3.214-1.327-7.478-3.428-15.548-5.918-19.059-14.735-.904-2.27-3.657-3.775-5.461-5.723-2.437-2.632-4.615-5.525-7.207-7.987-2.648-2.515-5.352-5.346-8.589-6.777-4.799-2.121-10.074-3.185-15.175-4.596l-15.785-4.155c.274-12.896 1.722-25.901.54-38.662-1.647-17.783-3.457-35.526-2.554-53.352.528-10.426 2.539-20.777 3.948-31.574z\"></path><path fill=\"#f6a200\" d=\"M525.146 311.436c4.597-9.898 8.947-19.725 14.251-29.008 9.691-16.963 18.49-34.73 30.354-50.08 13.309-17.218 24.195-36.434 41.054-51.019 9.93-8.591 18.335-18.989 27.131-28.837 13.774-15.422 30.943-26.386 48.036-37.528 11.459-7.469 22.688-15.29 34.243-23.286 11.705 16.744 19.716 35.424 22.534 55.717 2.231 16.066 2.236 32.441 2.753 49.143-4.756 1.62-9.284 2.234-13.259 4.056-6.43 2.948-12.193 7.513-18.774 9.942-19.863 7.331-33.806 22.349-47.926 36.784-7.86 8.035-13.511 18.275-19.886 27.705-4.434 6.558-9.345 13.037-12.358 20.254-4.249 10.177-6.94 21.004-10.296 31.553-12.33.053-24.741 1.027-36.971-.049-20.259-1.783-40.227-5.567-58.755-14.69-.568-.28-1.295-.235-2.132-.658z\"></path><path fill=\"#f7a80d\" d=\"M1989.057 883.598c-17.093 12.845-31.061 28.695-45.863 43.67-4.254 4.304-9.184 7.963-13.249 12.424-7.216 7.92-14.117 16.137-20.943 24.401-5.59 6.768-10.554 14.078-16.422 20.582-3.614 4.005-8.367 6.995-12.673 10.358-11.08 8.653-22.264 17.007-31.144 28.292-5.788 7.356-13.617 13.108-20.553 19.556-.967.899-2.105 1.614-3.08 2.505l-37.072 34.009c-5.333 4.96-10.375 10.232-15.859 15.505-21.401-17.218-37.461-38.439-48.623-63.592 3.503-1.781 7.117-2.604 9.823-4.637 8.696-6.536 20.392-8.406 27.297-17.714.933-1.258 2.646-1.973 4.065-2.828 17.878-10.784 36.338-20.728 53.441-32.624 10.304-7.167 18.637-17.23 27.583-26.261 3.819-3.855 7.436-8.091 10.3-12.681 12.283-19.68 24.43-39.446 40.382-56.471 12.224-13.047 17.258-29.524 22.539-45.927 15.85 4.193 29.819 12.129 42.632 22.08 10.583 8.219 19.782 17.883 27.42 29.351z\"></path><path fill=\"#ef7a72\" d=\"M1479.461 758.907c1.872-13.734 4.268-27.394 6.525-41.076 2.218-13.45 8.26-24.279 19.615-32.423 5.099-3.657 9.667-8.701 12.999-14.038 6.646-10.643 15.183-19.135 25.083-26.631 5.888-4.459 11.817-9.176 16.547-14.774 5.906-6.99 13.974-9.306 21.753-12.365l51.48-19.549c.753 11.848.658 23.787 1.641 35.637 1.771 21.353 4.075 42.672 11.748 62.955.17.449.107.985-.019 2.158-6.945 4.134-13.865 7.337-20.437 11.143-3.935 2.279-7.752 5.096-10.869 8.384-6.011 6.343-11.063 13.624-17.286 19.727-9.096 8.92-12.791 20.684-18.181 31.587-.202.409-.072.984-.096 1.481-8.488-1.72-16.937-3.682-25.476-5.094-9.689-1.602-19.426-3.084-29.201-3.949-15.095-1.335-30.241-2.1-45.828-3.172z\"></path><path fill=\"#e94e3b\" d=\"M957.995 766.838c-20.337-5.467-38.791-14.947-55.703-27.254-8.2-5.967-15.451-13.238-22.958-20.37 2.969-3.504 5.564-6.772 8.598-9.563 7.085-6.518 11.283-14.914 15.8-23.153 4.933-8.996 10.345-17.743 14.966-26.892 2.642-5.231 5.547-11.01 5.691-16.611.12-4.651.194-8.932 2.577-12.742 8.52-13.621 15.483-28.026 18.775-43.704 2.11-10.049 7.888-18.774 7.81-29.825-.064-9.089 4.291-18.215 6.73-27.313 3.212-11.983 7.369-23.797 9.492-35.968 3.202-18.358 5.133-36.945 7.346-55.466l4.879-45.8c6.693.288 13.386.575 20.54 1.365.13 3.458-.41 6.407-.496 9.37l-1.136 42.595c-.597 11.552-2.067 23.058-3.084 34.59l-3.845 44.478c-.939 10.202-1.779 20.432-3.283 30.557-.96 6.464-4.46 12.646-1.136 19.383.348.706-.426 1.894-.448 2.864-.224 9.918-5.99 19.428-2.196 29.646.103.279-.033.657-.092.983l-8.446 46.205c-1.231 6.469-2.936 12.846-4.364 19.279-1.5 6.757-2.602 13.621-4.456 20.277-3.601 12.93-10.657 25.3-5.627 39.47.368 1.036.234 2.352.017 3.476l-5.949 30.123z\"></path><path fill=\"#ea5043\" d=\"M958.343 767.017c1.645-10.218 3.659-20.253 5.602-30.302.217-1.124.351-2.44-.017-3.476-5.03-14.17 2.026-26.539 5.627-39.47 1.854-6.656 2.956-13.52 4.456-20.277 1.428-6.433 3.133-12.81 4.364-19.279l8.446-46.205c.059-.326.196-.705.092-.983-3.794-10.218 1.972-19.728 2.196-29.646.022-.97.796-2.158.448-2.864-3.324-6.737.176-12.919 1.136-19.383 1.504-10.125 2.344-20.355 3.283-30.557l3.845-44.478c1.017-11.532 2.488-23.038 3.084-34.59.733-14.18.722-28.397 1.136-42.595.086-2.963.626-5.912.956-9.301 5.356-.48 10.714-.527 16.536-.081 2.224 15.098 1.855 29.734 1.625 44.408-.157 10.064 1.439 20.142 1.768 30.23.334 10.235-.035 20.49.116 30.733.084 5.713.789 11.418.861 17.13.054 4.289-.469 8.585-.702 12.879-.072 1.323-.138 2.659-.031 3.975l2.534 34.405-1.707 36.293-1.908 48.69c-.182 8.103.993 16.237.811 24.34-.271 12.076-1.275 24.133-1.787 36.207-.102 2.414-.101 5.283 1.06 7.219 4.327 7.22 4.463 15.215 4.736 23.103.365 10.553.088 21.128.086 31.693-11.44 2.602-22.84.688-34.106-.916-11.486-1.635-22.806-4.434-34.546-6.903z\"></path><path fill=\"#eb5d19\" d=\"M398.091 622.45c6.086.617 12.21 1.288 18.067 2.918 3.539.985 6.779 3.277 9.952 5.297 9.773 6.224 18.971 13.583 29.311 18.611 8.606 4.184 12.839 10.986 17.016 18.559l18.571 32.959c1.814 3.102 4.285 5.931 6.883 8.443 8.835 8.542 10.052 20.175 13.16 31.095 2.082 7.317 4.609 14.507 6.946 22.127-29.472 3.021-58.969 5.582-87.584 15.222-1.185-2.302-1.795-4.362-2.769-6.233-4.398-8.449-6.703-18.174-14.942-24.299-2.511-1.866-5.103-3.814-7.047-6.218-8.358-10.332-17.028-20.276-28.772-26.973 4.423-11.478 9.299-22.806 13.151-34.473 4.406-13.348 6.724-27.18 6.998-41.313.098-5.093.643-10.176 1.06-15.722z\"></path><path fill=\"#e94c32\" d=\"M981.557 392.109c-1.172 15.337-2.617 30.625-4.438 45.869-2.213 18.521-4.144 37.108-7.346 55.466-2.123 12.171-6.28 23.985-9.492 35.968-2.439 9.098-6.794 18.224-6.73 27.313.078 11.051-5.7 19.776-7.81 29.825-3.292 15.677-10.255 30.082-18.775 43.704-2.383 3.81-2.458 8.091-2.577 12.742-.144 5.6-3.049 11.38-5.691 16.611-4.621 9.149-10.033 17.896-14.966 26.892-4.517 8.239-8.715 16.635-15.8 23.153-3.034 2.791-5.629 6.06-8.735 9.255-12.197-10.595-21.071-23.644-29.301-37.24-7.608-12.569-13.282-25.962-17.637-40.37 13.303-6.889 25.873-13.878 35.311-25.315.717-.869 1.934-1.312 2.71-2.147 5.025-5.405 10.515-10.481 14.854-16.397 6.141-8.374 10.861-17.813 17.206-26.008 8.22-10.618 13.657-22.643 20.024-34.466 4.448-.626 6.729-3.21 8.114-6.89 1.455-3.866 2.644-7.895 4.609-11.492 4.397-8.05 9.641-15.659 13.708-23.86 3.354-6.761 5.511-14.116 8.203-21.206 5.727-15.082 7.277-31.248 12.521-46.578 3.704-10.828 3.138-23.116 4.478-34.753l7.56-.073z\"></path><path fill=\"#f7a617\" d=\"M1918.661 831.99c-4.937 16.58-9.971 33.057-22.196 46.104-15.952 17.025-28.099 36.791-40.382 56.471-2.864 4.59-6.481 8.825-10.3 12.681-8.947 9.031-17.279 19.094-27.583 26.261-17.103 11.896-35.564 21.84-53.441 32.624-1.419.856-3.132 1.571-4.065 2.828-6.904 9.308-18.6 11.178-27.297 17.714-2.705 2.033-6.319 2.856-9.874 4.281-3.413-9.821-6.916-19.583-9.36-29.602-1.533-6.284-1.474-12.957-1.665-19.913 1.913-.78 3.374-1.057 4.81-1.431 15.822-4.121 31.491-8.029 43.818-20.323 9.452-9.426 20.371-17.372 30.534-26.097 6.146-5.277 13.024-10.052 17.954-16.326 14.812-18.848 28.876-38.285 43.112-57.581 2.624-3.557 5.506-7.264 6.83-11.367 2.681-8.311 4.375-16.94 6.476-25.438 17.89.279 35.333 3.179 52.629 9.113z\"></path><path fill=\"#ea553a\" d=\"M1172.91 977.582c-15.775-3.127-28.215-12.377-40.227-22.43-9.005-7.537-18.43-14.605-27.071-22.532-5.07-4.651-9.143-10.443-13.361-15.955-7.647-9.994-15.291-20.007-22.456-30.345-2.361-3.407-3.792-7.72-4.696-11.829-3.119-14.183-5.848-28.453-8.651-42.704-.636-3.236-.974-6.53-1.452-10.209 15.234-2.19 30.471-3.969 46.408-5.622 2.692 5.705 4.882 11.222 6.63 16.876 2.9 9.381 7.776 17.194 15.035 24.049 7.056 6.662 13.305 14.311 19.146 22.099 9.509 12.677 23.01 19.061 36.907 25.054-1.048 7.441-2.425 14.854-3.066 22.33-.956 11.162-1.393 22.369-2.052 33.557l-1.096 17.661z\"></path><path fill=\"#ea5453\" d=\"M1163.123 704.036c-4.005 5.116-7.685 10.531-12.075 15.293-12.842 13.933-27.653 25.447-44.902 34.538-3.166-5.708-5.656-11.287-8.189-17.251-3.321-12.857-6.259-25.431-9.963-37.775-4.6-15.329-10.6-30.188-11.349-46.562-.314-6.871-1.275-14.287-7.114-19.644-1.047-.961-1.292-3.053-1.465-4.67l-4.092-39.927c-.554-5.245-.383-10.829-2.21-15.623-3.622-9.503-4.546-19.253-4.688-29.163-.088-6.111 1.068-12.256.782-18.344-.67-14.281-1.76-28.546-2.9-42.8-.657-8.222-1.951-16.395-2.564-24.62-.458-6.137-.285-12.322-.104-18.21.959 5.831 1.076 11.525 2.429 16.909 2.007 7.986 5.225 15.664 7.324 23.632 3.222 12.23 1.547 25.219 6.728 37.355 4.311 10.099 6.389 21.136 9.732 31.669 2.228 7.02 6.167 13.722 7.121 20.863 1.119 8.376 6.1 13.974 10.376 20.716l2.026 10.576c1.711 9.216 3.149 18.283 8.494 26.599 6.393 9.946 11.348 20.815 16.943 31.276 4.021 7.519 6.199 16.075 12.925 22.065l24.462 22.26c.556.503 1.507.571 2.274.841z\"></path><path fill=\"#ea5b15\" d=\"M1285.092 163.432c9.165 3.148 18.419 6.374 27.279 10.459 4.871 2.246 8.838 6.406 13.646 8.851 5.446 2.77 11.801 3.874 17.011 6.965 11.514 6.831 24.097 9.942 36.968 12.471 1.78.35 3.777.576 5.213 1.542 10.784 7.255 23.448 9.114 35.622 11.834 9.977 2.23 18.529 6.703 26.988 11.898 5.233 3.214 10.76 5.983 15.798 9.468 4.14 2.864 7.962 6.279 11.551 9.827 5.076 5.02 10.056 10.181 14.624 15.658 5.822 6.98 11.119 14.395 16.78 21.513 4.531 5.698 9.267 11.233 14.222 16.987-10.005 5.806-20.07 12.004-30.719 16.943-7.694 3.569-16.163 5.464-24.688 7.669-2.878-7.088-5.352-13.741-7.833-20.392-.802-2.15-1.244-4.55-2.498-6.396-4.548-6.7-9.712-12.999-14.011-19.847-6.672-10.627-15.34-18.93-26.063-25.376-9.357-5.625-18.367-11.824-27.644-17.587-6.436-3.997-12.902-8.006-19.659-11.405-5.123-2.577-11.107-3.536-16.046-6.37-17.187-9.863-35.13-17.887-54.031-23.767-4.403-1.37-8.953-2.267-13.436-3.382l.926-27.565z\"></path><path fill=\"#ea504b\" d=\"M1098 737l7.789 16.893c-15.04 9.272-31.679 15.004-49.184 17.995-9.464 1.617-19.122 2.097-29.151 3.019-.457-10.636-.18-21.211-.544-31.764-.273-7.888-.409-15.883-4.736-23.103-1.16-1.936-1.162-4.805-1.06-7.219l1.787-36.207c.182-8.103-.993-16.237-.811-24.34.365-16.236 1.253-32.461 1.908-48.69.484-12 .942-24.001 1.98-36.069 5.57 10.19 10.632 20.42 15.528 30.728 1.122 2.362 2.587 5.09 2.339 7.488-1.536 14.819 5.881 26.839 12.962 38.33 10.008 16.241 16.417 33.54 20.331 51.964 2.285 10.756 4.729 21.394 11.958 30.165L1098 737z\"></path><path fill=\"#f6a320\" d=\"M1865.78 822.529c-1.849 8.846-3.544 17.475-6.224 25.786-1.323 4.102-4.206 7.81-6.83 11.367l-43.112 57.581c-4.93 6.273-11.808 11.049-17.954 16.326-10.162 8.725-21.082 16.671-30.534 26.097-12.327 12.294-27.997 16.202-43.818 20.323-1.436.374-2.897.651-4.744.986-1.107-17.032-1.816-34.076-2.079-51.556 1.265-.535 2.183-.428 2.888-.766 10.596-5.072 20.8-11.059 32.586-13.273 1.69-.317 3.307-1.558 4.732-2.662l26.908-21.114c4.992-4.003 11.214-7.393 14.381-12.585 11.286-18.5 22.363-37.263 27.027-58.87l36.046 1.811c3.487.165 6.983.14 10.727.549z\"></path><path fill=\"#ec6333\" d=\"M318.448 922.814c-6.374-2.074-12.56-4.058-18.412-6.765-8.379-3.876-16.906-7.675-24.617-12.668-5.239-3.392-9.69-8.381-13.609-13.352-7.87-9.983-14.953-20.582-22.699-30.666-8.061-10.493-13.909-22.097-18.636-34.358-.595-1.543-1.486-2.972-2.382-4.783 6.84-1.598 13.797-3.023 20.807-4.106 18.852-2.912 36.433-9.493 53.737-17.819.697.888.889 1.555 1.292 2.051l17.921 21.896c4.14 4.939 8.06 10.191 12.862 14.412 5.67 4.984 12.185 9.007 18.334 13.447-8.937 16.282-16.422 33.178-20.696 51.31-1.638 6.951-2.402 14.107-3.903 21.403z\"></path><path fill=\"#f49700\" d=\"M623.467 326.903c2.893-10.618 5.584-21.446 9.833-31.623 3.013-7.217 7.924-13.696 12.358-20.254 6.375-9.43 12.026-19.67 19.886-27.705 14.12-14.434 28.063-29.453 47.926-36.784 6.581-2.429 12.344-6.994 18.774-9.942 3.975-1.822 8.503-2.436 13.186-3.592 1.947 18.557 3.248 37.15 8.307 55.686-15.453 7.931-28.853 18.092-40.46 29.996-10.417 10.683-19.109 23.111-28.013 35.175-3.238 4.388-4.888 9.948-7.262 14.973-17.803-3.987-35.767-6.498-54.535-5.931z\"></path><path fill=\"#ea544c\" d=\"M1097.956 736.615c-2.925-3.218-5.893-6.822-8.862-10.425-7.229-8.771-9.672-19.409-11.958-30.165-3.914-18.424-10.323-35.722-20.331-51.964-7.081-11.491-14.498-23.511-12.962-38.33.249-2.398-1.217-5.126-2.339-7.488l-15.232-31.019-3.103-34.338c-.107-1.316-.041-2.653.031-3.975.233-4.294.756-8.59.702-12.879-.072-5.713-.776-11.417-.861-17.13l-.116-30.733c-.329-10.088-1.926-20.166-1.768-30.23.23-14.674.599-29.31-1.162-44.341 9.369-.803 18.741-1.179 28.558-1.074 1.446 15.814 2.446 31.146 3.446 46.478.108 6.163-.064 12.348.393 18.485.613 8.225 1.907 16.397 2.564 24.62l2.9 42.8c.286 6.088-.869 12.234-.782 18.344.142 9.91 1.066 19.661 4.688 29.163 1.827 4.794 1.657 10.377 2.21 15.623l4.092 39.927c.172 1.617.417 3.71 1.465 4.67 5.839 5.357 6.8 12.773 7.114 19.644.749 16.374 6.749 31.233 11.349 46.562 3.704 12.344 6.642 24.918 9.963 37.775z\"></path><path fill=\"#ec5c61\" d=\"M1204.835 568.008c1.254 25.351-1.675 50.16-10.168 74.61-8.598-4.883-18.177-8.709-24.354-15.59-7.44-8.289-13.929-17.442-21.675-25.711-8.498-9.072-16.731-18.928-21.084-31.113-.54-1.513-1.691-2.807-2.594-4.564-4.605-9.247-7.706-18.544-7.96-29.09-.835-7.149-1.214-13.944-2.609-20.523-2.215-10.454-5.626-20.496-7.101-31.302-2.513-18.419-7.207-36.512-5.347-55.352.24-2.43-.17-4.949-.477-7.402l-4.468-34.792c2.723-.379 5.446-.757 8.585-.667 1.749 8.781 2.952 17.116 4.448 25.399 1.813 10.037 3.64 20.084 5.934 30.017 1.036 4.482 3.953 8.573 4.73 13.064 1.794 10.377 4.73 20.253 9.272 29.771 2.914 6.105 4.761 12.711 7.496 18.912 2.865 6.496 6.264 12.755 9.35 19.156 3.764 7.805 7.667 15.013 16.1 19.441 7.527 3.952 13.713 10.376 20.983 14.924 6.636 4.152 13.932 7.25 20.937 10.813z\"></path><path fill=\"#ed676f\" d=\"M1140.75 379.231c18.38-4.858 36.222-11.21 53.979-18.971 3.222 3.368 5.693 6.744 8.719 9.512 2.333 2.134 5.451 5.07 8.067 4.923 7.623-.429 12.363 2.688 17.309 8.215 5.531 6.18 12.744 10.854 19.224 16.184-5.121 7.193-10.461 14.241-15.323 21.606-13.691 20.739-22.99 43.255-26.782 67.926-.543 3.536-1.281 7.043-2.366 10.925-14.258-6.419-26.411-14.959-32.731-29.803-1.087-2.553-2.596-4.93-3.969-7.355-1.694-2.993-3.569-5.89-5.143-8.943-1.578-3.062-2.922-6.249-4.295-9.413-1.57-3.621-3.505-7.163-4.47-10.946-1.257-4.93-.636-10.572-2.725-15.013-5.831-12.397-7.467-25.628-9.497-38.847z\"></path><path fill=\"#ed656e\" d=\"M1254.103 647.439c5.325.947 10.603 2.272 15.847 3.722 5.101 1.41 10.376 2.475 15.175 4.596 3.237 1.431 5.942 4.262 8.589 6.777 2.592 2.462 4.77 5.355 7.207 7.987 1.804 1.948 4.557 3.453 5.461 5.723 3.51 8.817 11.581 11.307 19.059 14.735 1.053.483 2.116.963 3.214 1.327 9.172 3.043 13.818 8.587 14.889 18.979.715 6.935 5.607 13.679 9.479 19.987 4.623 7.533 9.175 14.819 9.091 24.116-.023 2.55 1.21 5.111 1.874 8.055-19.861 2.555-39.795 4.296-59.597 9.09l-11.596-23.203c-1.107-2.169-2.526-4.353-4.307-5.975-7.349-6.694-14.863-13.209-22.373-19.723l-17.313-14.669c-2.776-2.245-5.935-4.017-8.92-6.003l11.609-38.185c1.508-5.453 1.739-11.258 2.613-17.336z\"></path><path fill=\"#ec6168\" d=\"M1140.315 379.223c2.464 13.227 4.101 26.459 9.931 38.856 2.089 4.441 1.468 10.083 2.725 15.013.965 3.783 2.9 7.325 4.47 10.946 1.372 3.164 2.716 6.351 4.295 9.413 1.574 3.053 3.449 5.95 5.143 8.943 1.372 2.425 2.882 4.803 3.969 7.355 6.319 14.844 18.473 23.384 32.641 30.212.067 5.121-.501 10.201-.435 15.271l.985 38.117c.151 4.586.616 9.162.868 14.201-7.075-3.104-14.371-6.202-21.007-10.354-7.269-4.548-13.456-10.972-20.983-14.924-8.434-4.428-12.337-11.637-16.1-19.441-3.087-6.401-6.485-12.66-9.35-19.156-2.735-6.201-4.583-12.807-7.496-18.912-4.542-9.518-7.477-19.394-9.272-29.771-.777-4.491-3.694-8.581-4.73-13.064-2.294-9.933-4.121-19.98-5.934-30.017-1.496-8.283-2.699-16.618-4.036-25.335 10.349-2.461 20.704-4.511 31.054-6.582.957-.191 1.887-.515 3.264-.769z\"></path><path fill=\"#e94c28\" d=\"M922 537c-6.003 11.784-11.44 23.81-19.66 34.428-6.345 8.196-11.065 17.635-17.206 26.008-4.339 5.916-9.828 10.992-14.854 16.397-.776.835-1.993 1.279-2.71 2.147-9.439 11.437-22.008 18.427-35.357 24.929-4.219-10.885-6.942-22.155-7.205-33.905l-.514-49.542c7.441-2.893 14.452-5.197 21.334-7.841 1.749-.672 3.101-2.401 4.604-3.681 6.749-5.745 12.845-12.627 20.407-16.944 7.719-4.406 14.391-9.101 18.741-16.889.626-1.122 1.689-2.077 2.729-2.877 7.197-5.533 12.583-12.51 16.906-20.439.68-1.247 2.495-1.876 4.105-2.651 2.835 1.408 5.267 2.892 7.884 3.892 3.904 1.491 4.392 3.922 2.833 7.439-1.47 3.318-2.668 6.756-4.069 10.106-1.247 2.981-.435 5.242 2.413 6.544 2.805 1.282 3.125 3.14 1.813 5.601l-6.907 12.799L922 537z\"></path><path fill=\"#eb5659\" d=\"M1124.995 566c.868 1.396 2.018 2.691 2.559 4.203 4.353 12.185 12.586 22.041 21.084 31.113 7.746 8.269 14.235 17.422 21.675 25.711 6.176 6.881 15.756 10.707 24.174 15.932-6.073 22.316-16.675 42.446-31.058 60.937-1.074-.131-2.025-.199-2.581-.702l-24.462-22.26c-6.726-5.99-8.904-14.546-12.925-22.065-5.594-10.461-10.55-21.33-16.943-31.276-5.345-8.315-6.783-17.383-8.494-26.599-.63-3.394-1.348-6.772-1.738-10.848-.371-6.313-1.029-11.934-1.745-18.052l6.34 4.04 1.288-.675-2.143-15.385 9.454 1.208v-8.545L1124.995 566z\"></path><path fill=\"#f5a02d\" d=\"M1818.568 820.096c-4.224 21.679-15.302 40.442-26.587 58.942-3.167 5.192-9.389 8.582-14.381 12.585l-26.908 21.114c-1.425 1.104-3.042 2.345-4.732 2.662-11.786 2.214-21.99 8.201-32.586 13.273-.705.338-1.624.231-2.824.334a824.35 824.35 0 0 1-8.262-42.708c4.646-2.14 9.353-3.139 13.269-5.47 5.582-3.323 11.318-6.942 15.671-11.652 7.949-8.6 14.423-18.572 22.456-27.081 8.539-9.046 13.867-19.641 18.325-30.922l46.559 8.922z\"></path><path fill=\"#eb5a57\" d=\"M1124.96 565.639c-5.086-4.017-10.208-8.395-15.478-12.901v8.545l-9.454-1.208 2.143 15.385-1.288.675-6.34-4.04c.716 6.118 1.375 11.74 1.745 17.633-4.564-6.051-9.544-11.649-10.663-20.025-.954-7.141-4.892-13.843-7.121-20.863-3.344-10.533-5.421-21.57-9.732-31.669-5.181-12.135-3.506-25.125-6.728-37.355-2.099-7.968-5.317-15.646-7.324-23.632-1.353-5.384-1.47-11.078-2.429-16.909l-3.294-46.689a278.63 278.63 0 0 1 27.57-2.084c2.114 12.378 3.647 24.309 5.479 36.195 1.25 8.111 2.832 16.175 4.422 24.23 1.402 7.103 2.991 14.169 4.55 21.241 1.478 6.706.273 14.002 4.6 20.088 5.401 7.597 7.176 16.518 9.467 25.337 1.953 7.515 5.804 14.253 11.917 19.406.254 10.095 3.355 19.392 7.96 28.639z\"></path><path fill=\"#ea541c\" d=\"M911.651 810.999c-2.511 10.165-5.419 20.146-8.2 30.162-2.503 9.015-7.37 16.277-14.364 22.612-6.108 5.533-10.917 12.475-16.796 18.293-6.942 6.871-14.354 13.24-19.083 22.03-.644 1.196-2.222 1.889-3.705 2.857-2.39-7.921-4.101-15.991-6.566-23.823-5.451-17.323-12.404-33.976-23.414-48.835l21.627-21.095c3.182-3.29 5.532-7.382 8.295-11.083l10.663-14.163c9.528 4.78 18.925 9.848 28.625 14.247 7.324 3.321 15.036 5.785 22.917 8.799z\"></path><path fill=\"#eb5d19\" d=\"M1284.092 191.421c4.557.69 9.107 1.587 13.51 2.957 18.901 5.881 36.844 13.904 54.031 23.767 4.938 2.834 10.923 3.792 16.046 6.37 6.757 3.399 13.224 7.408 19.659 11.405l27.644 17.587c10.723 6.446 19.392 14.748 26.063 25.376 4.299 6.848 9.463 13.147 14.011 19.847 1.254 1.847 1.696 4.246 2.498 6.396l7.441 20.332c-11.685 1.754-23.379 3.133-35.533 4.037-.737-2.093-.995-3.716-1.294-5.33-3.157-17.057-14.048-30.161-23.034-44.146-3.027-4.71-7.786-8.529-12.334-11.993-9.346-7.116-19.004-13.834-28.688-20.491-6.653-4.573-13.311-9.251-20.431-13.002-8.048-4.24-16.479-7.85-24.989-11.091-11.722-4.465-23.673-8.328-35.527-12.449l.927-19.572z\"></path><path fill=\"#eb5e24\" d=\"M1283.09 211.415c11.928 3.699 23.88 7.562 35.602 12.027 8.509 3.241 16.941 6.852 24.989 11.091 7.12 3.751 13.778 8.429 20.431 13.002 9.684 6.657 19.342 13.375 28.688 20.491 4.548 3.463 9.307 7.283 12.334 11.993 8.986 13.985 19.877 27.089 23.034 44.146.299 1.615.557 3.237.836 5.263-13.373-.216-26.749-.839-40.564-1.923-2.935-9.681-4.597-18.92-12.286-26.152-15.577-14.651-30.4-30.102-45.564-45.193-.686-.683-1.626-1.156-2.516-1.584l-47.187-22.615 2.203-20.546z\"></path><path fill=\"#e9511f\" d=\"M913 486.001c-1.29.915-3.105 1.543-3.785 2.791-4.323 7.929-9.709 14.906-16.906 20.439-1.04.8-2.103 1.755-2.729 2.877-4.35 7.788-11.022 12.482-18.741 16.889-7.562 4.317-13.658 11.199-20.407 16.944-1.503 1.28-2.856 3.009-4.604 3.681-6.881 2.643-13.893 4.948-21.262 7.377-.128-11.151.202-22.302.378-33.454.03-1.892-.6-3.795-.456-6.12 13.727-1.755 23.588-9.527 33.278-17.663 2.784-2.337 6.074-4.161 8.529-6.784l29.057-31.86c1.545-1.71 3.418-3.401 4.221-5.459 5.665-14.509 11.49-28.977 16.436-43.736 2.817-8.407 4.074-17.338 6.033-26.032 5.039.714 10.078 1.427 15.536 2.629-.909 8.969-2.31 17.438-3.546 25.931-2.41 16.551-5.84 32.839-11.991 48.461L913 486.001z\"></path><path fill=\"#ea5741\" d=\"M1179.451 903.828c-14.224-5.787-27.726-12.171-37.235-24.849-5.841-7.787-12.09-15.436-19.146-22.099-7.259-6.854-12.136-14.667-15.035-24.049-1.748-5.654-3.938-11.171-6.254-17.033 15.099-4.009 30.213-8.629 44.958-15.533l28.367 36.36c6.09 8.015 13.124 14.75 22.72 18.375-7.404 14.472-13.599 29.412-17.48 45.244-.271 1.106-.382 2.25-.895 3.583z\"></path><path fill=\"#ea522a\" d=\"M913.32 486.141c2.693-7.837 5.694-15.539 8.722-23.231 6.151-15.622 9.581-31.91 11.991-48.461l3.963-25.861c7.582.317 15.168 1.031 22.748 1.797 4.171.421 8.333.928 12.877 1.596-.963 11.836-.398 24.125-4.102 34.953-5.244 15.33-6.794 31.496-12.521 46.578-2.692 7.09-4.849 14.445-8.203 21.206-4.068 8.201-9.311 15.81-13.708 23.86-1.965 3.597-3.154 7.627-4.609 11.492-1.385 3.68-3.666 6.265-8.114 6.89-1.994-1.511-3.624-3.059-5.077-4.44l6.907-12.799c1.313-2.461.993-4.318-1.813-5.601-2.849-1.302-3.66-3.563-2.413-6.544 1.401-3.35 2.599-6.788 4.069-10.106 1.558-3.517 1.071-5.948-2.833-7.439-2.617-1-5.049-2.484-7.884-3.892z\"></path><path fill=\"#eb5e24\" d=\"M376.574 714.118c12.053 6.538 20.723 16.481 29.081 26.814 1.945 2.404 4.537 4.352 7.047 6.218 8.24 6.125 10.544 15.85 14.942 24.299.974 1.871 1.584 3.931 2.376 6.29-7.145 3.719-14.633 6.501-21.386 10.517-9.606 5.713-18.673 12.334-28.425 18.399-3.407-3.73-6.231-7.409-9.335-10.834l-30.989-33.862c11.858-11.593 22.368-24.28 31.055-38.431 1.86-3.031 3.553-6.164 5.632-9.409z\"></path><path fill=\"#e95514\" d=\"M859.962 787.636c-3.409 5.037-6.981 9.745-10.516 14.481-2.763 3.701-5.113 7.792-8.295 11.083-6.885 7.118-14.186 13.834-21.65 20.755-13.222-17.677-29.417-31.711-48.178-42.878-.969-.576-2.068-.934-3.27-1.709 6.28-8.159 12.733-15.993 19.16-23.849 1.459-1.783 2.718-3.738 4.254-5.448l18.336-19.969c4.909 5.34 9.619 10.738 14.081 16.333 9.72 12.19 21.813 21.566 34.847 29.867.411.262.725.674 1.231 1.334z\"></path><path fill=\"#eb5f2d\" d=\"M339.582 762.088l31.293 33.733c3.104 3.425 5.928 7.104 9.024 10.979-12.885 11.619-24.548 24.139-33.899 38.704-.872 1.359-1.56 2.837-2.644 4.428-6.459-4.271-12.974-8.294-18.644-13.278-4.802-4.221-8.722-9.473-12.862-14.412l-17.921-21.896c-.403-.496-.595-1.163-.926-2.105 16.738-10.504 32.58-21.87 46.578-36.154z\"></path><path fill=\"#f28d00\" d=\"M678.388 332.912c1.989-5.104 3.638-10.664 6.876-15.051 8.903-12.064 17.596-24.492 28.013-35.175 11.607-11.904 25.007-22.064 40.507-29.592 4.873 11.636 9.419 23.412 13.67 35.592-5.759 4.084-11.517 7.403-16.594 11.553-4.413 3.607-8.124 8.092-12.023 12.301-5.346 5.772-10.82 11.454-15.782 17.547-3.929 4.824-7.17 10.208-10.716 15.344l-33.95-12.518z\"></path><path fill=\"#f08369\" d=\"M1580.181 771.427c-.191-.803-.322-1.377-.119-1.786 5.389-10.903 9.084-22.666 18.181-31.587 6.223-6.103 11.276-13.385 17.286-19.727 3.117-3.289 6.933-6.105 10.869-8.384 6.572-3.806 13.492-7.009 20.461-10.752 1.773 3.23 3.236 6.803 4.951 10.251l12.234 24.993c-1.367 1.966-2.596 3.293-3.935 4.499-7.845 7.07-16.315 13.564-23.407 21.32-6.971 7.623-12.552 16.517-18.743 24.854l-37.777-13.68z\"></path><path fill=\"#f18b5e\" d=\"M1618.142 785.4c6.007-8.63 11.588-17.524 18.559-25.147 7.092-7.755 15.562-14.249 23.407-21.32 1.338-1.206 2.568-2.534 3.997-4.162l28.996 33.733c1.896 2.205 4.424 3.867 6.66 6.394-6.471 7.492-12.967 14.346-19.403 21.255l-18.407 19.953c-12.958-12.409-27.485-22.567-43.809-30.706z\"></path><path fill=\"#f49c3a\" d=\"M1771.617 811.1c-4.066 11.354-9.394 21.949-17.933 30.995-8.032 8.509-14.507 18.481-22.456 27.081-4.353 4.71-10.089 8.329-15.671 11.652-3.915 2.331-8.623 3.331-13.318 5.069-4.298-9.927-8.255-19.998-12.1-30.743 4.741-4.381 9.924-7.582 13.882-11.904 7.345-8.021 14.094-16.603 20.864-25.131 4.897-6.168 9.428-12.626 14.123-18.955l32.61 11.936z\"></path><path fill=\"#f08000\" d=\"M712.601 345.675c3.283-5.381 6.524-10.765 10.453-15.589 4.962-6.093 10.435-11.774 15.782-17.547 3.899-4.21 7.61-8.695 12.023-12.301 5.078-4.15 10.836-7.469 16.636-11.19a934.12 934.12 0 0 1 23.286 35.848c-4.873 6.234-9.676 11.895-14.63 17.421l-25.195 27.801c-11.713-9.615-24.433-17.645-38.355-24.443z\"></path><path fill=\"#ed6e04\" d=\"M751.11 370.42c8.249-9.565 16.693-18.791 25.041-28.103 4.954-5.526 9.757-11.187 14.765-17.106 7.129 6.226 13.892 13.041 21.189 19.225 5.389 4.567 11.475 8.312 17.53 12.92-5.51 7.863-10.622 15.919-17.254 22.427-8.881 8.716-18.938 16.233-28.49 24.264-5.703-6.587-11.146-13.427-17.193-19.682-4.758-4.921-10.261-9.121-15.587-13.944z\"></path><path fill=\"#ea541c\" d=\"M921.823 385.544c-1.739 9.04-2.995 17.971-5.813 26.378-4.946 14.759-10.771 29.227-16.436 43.736-.804 2.058-2.676 3.749-4.221 5.459l-29.057 31.86c-2.455 2.623-5.745 4.447-8.529 6.784-9.69 8.135-19.551 15.908-33.208 17.237-1.773-9.728-3.147-19.457-4.091-29.6l36.13-16.763c.581-.267 1.046-.812 1.525-1.269 8.033-7.688 16.258-15.19 24.011-23.152 4.35-4.467 9.202-9.144 11.588-14.69 6.638-15.425 15.047-30.299 17.274-47.358 3.536.344 7.072.688 10.829 1.377z\"></path><path fill=\"#f3944d\" d=\"M1738.688 798.998c-4.375 6.495-8.906 12.953-13.803 19.121-6.771 8.528-13.519 17.11-20.864 25.131-3.958 4.322-9.141 7.523-13.925 11.54-8.036-13.464-16.465-26.844-27.999-38.387 5.988-6.951 12.094-13.629 18.261-20.25l19.547-20.95 38.783 23.794z\"></path><path fill=\"#ec6168\" d=\"M1239.583 703.142c3.282 1.805 6.441 3.576 9.217 5.821 5.88 4.755 11.599 9.713 17.313 14.669l22.373 19.723c1.781 1.622 3.2 3.806 4.307 5.975 3.843 7.532 7.477 15.171 11.194 23.136-10.764 4.67-21.532 8.973-32.69 12.982l-22.733-27.366c-2.003-2.416-4.096-4.758-6.194-7.093-3.539-3.94-6.927-8.044-10.74-11.701-2.57-2.465-5.762-4.283-8.675-6.39l16.627-29.755z\"></path><path fill=\"#ec663e\" d=\"M1351.006 332.839l-28.499 10.33c-.294.107-.533.367-1.194.264-11.067-19.018-27.026-32.559-44.225-44.855-4.267-3.051-8.753-5.796-13.138-8.682l9.505-24.505c10.055 4.069 19.821 8.227 29.211 13.108 3.998 2.078 7.299 5.565 10.753 8.598 3.077 2.701 5.743 5.891 8.926 8.447 4.116 3.304 9.787 5.345 12.62 9.432 6.083 8.777 10.778 18.517 16.041 27.863z\"></path><path fill=\"#eb5e5b\" d=\"M1222.647 733.051c3.223 1.954 6.415 3.771 8.985 6.237 3.813 3.658 7.201 7.761 10.74 11.701l6.194 7.093 22.384 27.409c-13.056 6.836-25.309 14.613-36.736 24.161l-39.323-44.7 24.494-27.846c1.072-1.224 1.974-2.598 3.264-4.056z\"></path><path fill=\"#ea580e\" d=\"M876.001 376.171c5.874 1.347 11.748 2.694 17.812 4.789-.81 5.265-2.687 9.791-2.639 14.296.124 11.469-4.458 20.383-12.73 27.863-2.075 1.877-3.659 4.286-5.668 6.248l-22.808 21.967c-.442.422-1.212.488-1.813.757l-23.113 10.389-9.875 4.514c-2.305-6.09-4.609-12.181-6.614-18.676 7.64-4.837 15.567-8.54 22.18-13.873 9.697-7.821 18.931-16.361 27.443-25.455 5.613-5.998 12.679-11.331 14.201-20.475.699-4.2 2.384-8.235 3.623-12.345z\"></path><path fill=\"#e95514\" d=\"M815.103 467.384c3.356-1.894 6.641-3.415 9.94-4.903l23.113-10.389c.6-.269 1.371-.335 1.813-.757l22.808-21.967c2.008-1.962 3.593-4.371 5.668-6.248 8.272-7.48 12.854-16.394 12.73-27.863-.049-4.505 1.828-9.031 2.847-13.956 5.427.559 10.836 1.526 16.609 2.68-1.863 17.245-10.272 32.119-16.91 47.544-2.387 5.546-7.239 10.223-11.588 14.69-7.753 7.962-15.978 15.464-24.011 23.152-.478.458-.944 1.002-1.525 1.269l-36.069 16.355c-2.076-6.402-3.783-12.81-5.425-19.607z\"></path><path fill=\"#eb620b\" d=\"M783.944 404.402c9.499-8.388 19.556-15.905 28.437-24.621 6.631-6.508 11.744-14.564 17.575-22.273 9.271 4.016 18.501 8.375 27.893 13.43-4.134 7.07-8.017 13.778-12.833 19.731-5.785 7.15-12.109 13.917-18.666 20.376-7.99 7.869-16.466 15.244-24.731 22.832l-17.674-29.475z\"></path><path fill=\"#ea544c\" d=\"M1197.986 854.686c-9.756-3.309-16.79-10.044-22.88-18.059l-28.001-36.417c8.601-5.939 17.348-11.563 26.758-17.075 1.615 1.026 2.639 1.876 3.505 2.865l26.664 30.44c3.723 4.139 7.995 7.785 12.017 11.656l-18.064 26.591z\"></path><path fill=\"#ec6333\" d=\"M1351.41 332.903c-5.667-9.409-10.361-19.149-16.445-27.926-2.833-4.087-8.504-6.128-12.62-9.432-3.184-2.555-5.849-5.745-8.926-8.447-3.454-3.033-6.756-6.52-10.753-8.598-9.391-4.88-19.157-9.039-29.138-13.499 1.18-5.441 2.727-10.873 4.81-16.607 11.918 4.674 24.209 8.261 34.464 14.962 14.239 9.304 29.011 18.453 39.595 32.464 2.386 3.159 5.121 6.077 7.884 8.923 6.564 6.764 10.148 14.927 11.723 24.093l-20.594 4.067z\"></path><path fill=\"#eb5e5b\" d=\"M1117 536.549c-6.113-4.702-9.965-11.44-11.917-18.955-2.292-8.819-4.066-17.74-9.467-25.337-4.327-6.085-3.122-13.382-4.6-20.088l-4.55-21.241c-1.59-8.054-3.172-16.118-4.422-24.23l-5.037-36.129c6.382-1.43 12.777-2.462 19.582-3.443 1.906 11.646 3.426 23.24 4.878 34.842.307 2.453.717 4.973.477 7.402-1.86 18.84 2.834 36.934 5.347 55.352 1.474 10.806 4.885 20.848 7.101 31.302 1.394 6.579 1.774 13.374 2.609 20.523z\"></path><path fill=\"#ec644b\" d=\"M1263.638 290.071c4.697 2.713 9.183 5.458 13.45 8.509 17.199 12.295 33.158 25.836 43.873 44.907-8.026 4.725-16.095 9.106-24.83 13.372-11.633-15.937-25.648-28.515-41.888-38.689-1.609-1.008-3.555-1.48-5.344-2.2 2.329-3.852 4.766-7.645 6.959-11.573l7.78-14.326z\"></path><path fill=\"#eb5f2d\" d=\"M1372.453 328.903c-2.025-9.233-5.608-17.396-12.172-24.16-2.762-2.846-5.498-5.764-7.884-8.923-10.584-14.01-25.356-23.16-39.595-32.464-10.256-6.701-22.546-10.289-34.284-15.312.325-5.246 1.005-10.444 2.027-15.863l47.529 22.394c.89.428 1.83.901 2.516 1.584l45.564 45.193c7.69 7.233 9.352 16.472 11.849 26.084-5.032.773-10.066 1.154-15.55 1.466z\"></path><path fill=\"#e95a0f\" d=\"M801.776 434.171c8.108-7.882 16.584-15.257 24.573-23.126 6.558-6.459 12.881-13.226 18.666-20.376 4.817-5.953 8.7-12.661 13.011-19.409 5.739 1.338 11.463 3.051 17.581 4.838-.845 4.183-2.53 8.219-3.229 12.418-1.522 9.144-8.588 14.477-14.201 20.475-8.512 9.094-17.745 17.635-27.443 25.455-6.613 5.333-14.54 9.036-22.223 13.51-2.422-4.469-4.499-8.98-6.735-13.786z\"></path><path fill=\"#eb5e5b\" d=\"M1248.533 316.002c2.155.688 4.101 1.159 5.71 2.168 16.24 10.174 30.255 22.752 41.532 38.727-7.166 5.736-14.641 11.319-22.562 16.731-1.16-1.277-1.684-2.585-2.615-3.46l-38.694-36.2 14.203-15.029c.803-.86 1.38-1.93 2.427-2.936z\"></path><path fill=\"#eb5a57\" d=\"M1216.359 827.958c-4.331-3.733-8.603-7.379-12.326-11.518l-26.664-30.44c-.866-.989-1.89-1.839-3.152-2.902 6.483-6.054 13.276-11.959 20.371-18.005l39.315 44.704c-5.648 6.216-11.441 12.12-17.544 18.161z\"></path><path fill=\"#ec6168\" d=\"M1231.598 334.101l38.999 36.066c.931.876 1.456 2.183 2.303 3.608-4.283 4.279-8.7 8.24-13.769 12.091-4.2-3.051-7.512-6.349-11.338-8.867-12.36-8.136-22.893-18.27-32.841-29.093l16.646-13.805z\"></path><path fill=\"#ed656e\" d=\"M1214.597 347.955c10.303 10.775 20.836 20.908 33.196 29.044 3.825 2.518 7.137 5.816 10.992 8.903-3.171 4.397-6.65 8.648-10.432 13.046-6.785-5.184-13.998-9.858-19.529-16.038-4.946-5.527-9.687-8.644-17.309-8.215-2.616.147-5.734-2.788-8.067-4.923-3.026-2.769-5.497-6.144-8.35-9.568 6.286-4.273 12.715-8.237 19.499-12.25z\"></path></svg>\n</p>\n\n<p align=\"center\">\n<b>The crispy rerank family from <a href=\"https://mixedbread.com\"><b>Mixedbread</b></a>.</b>\n</p>\n\n<p align=\"center\">\n<sup> 🍞 Looking for a simple end-to-end retrieval solution? Meet Omni, our multimodal and multilingual model. <a href=\"https://mixedbread.com\"><b>Get in touch for access.</a> </sup>\n</p>\n  \n# mxbai-rerank-xsmall-v1\n\nThis is the smallest model in our family of powerful reranker models. You can learn more about the models in our [blog post](https://www.mixedbread.ai/blog/mxbai-rerank-v1).\n\nWe have three models:\n\n- [mxbai-rerank-xsmall-v1](https://huggingface.co/mixedbread-ai/mxbai-rerank-xsmall-v1) (🍞)\n- [mxbai-rerank-base-v1](https://huggingface.co/mixedbread-ai/mxbai-rerank-base-v1)\n- [mxbai-rerank-large-v1](https://huggingface.co/mixedbread-ai/mxbai-rerank-large-v1)\n\n## Quickstart\n\nCurrently, the best way to use our models is with the most recent version of sentence-transformers.\n\n`pip install -U sentence-transformers`\n\nLet's say you have a query, and you want to rerank a set of documents. You can do that with only one line of code:\n\n```python\nfrom sentence_transformers import CrossEncoder\n\n# Load the model, here we use our base sized model\nmodel = CrossEncoder(\"mixedbread-ai/mxbai-rerank-xsmall-v1\")\n\n\n# Example query and documents\nquery = \"Who wrote 'To Kill a Mockingbird'?\"\ndocuments = [\n    \"'To Kill a Mockingbird' is a novel by Harper Lee published in 1960. It was immediately successful, winning the Pulitzer Prize, and has become a classic of modern American literature.\",\n    \"The novel 'Moby-Dick' was written by Herman Melville and first published in 1851. It is considered a masterpiece of American literature and deals with complex themes of obsession, revenge, and the conflict between good and evil.\",\n    \"Harper Lee, an American novelist widely known for her novel 'To Kill a Mockingbird', was born in 1926 in Monroeville, Alabama. She received the Pulitzer Prize for Fiction in 1961.\",\n    \"Jane Austen was an English novelist known primarily for her six major novels, which interpret, critique and comment upon the British landed gentry at the end of the 18th century.\",\n    \"The 'Harry Potter' series, which consists of seven fantasy novels written by British author J.K. Rowling, is among the most popular and critically acclaimed books of the modern era.\",\n    \"'The Great Gatsby', a novel written by American author F. Scott Fitzgerald, was published in 1925. The story is set in the Jazz Age and follows the life of millionaire Jay Gatsby and his pursuit of Daisy Buchanan.\"\n]\n\n# Lets get the scores\nresults = model.rank(query, documents, return_documents=True, top_k=3)\n```\n\n<details>\n  <summary>JavaScript Example</summary>\n\nInstall [transformers.js](https://github.com/xenova/transformers.js)\n\n`npm i @xenova/transformers`\n\nLet's say you have a query, and you want to rerank a set of documents. In JavaScript, you need to add a function:\n\n```javascript\nimport { AutoTokenizer, AutoModelForSequenceClassification } from '@xenova/transformers';\n\nconst model_id = 'mixedbread-ai/mxbai-rerank-xsmall-v1';\nconst model = await AutoModelForSequenceClassification.from_pretrained(model_id);\nconst tokenizer = await AutoTokenizer.from_pretrained(model_id);\n\n/**\n * Performs ranking with the CrossEncoder on the given query and documents. Returns a sorted list with the document indices and scores.\n * @param {string} query A single query\n * @param {string[]} documents A list of documents\n * @param {Object} options Options for ranking\n * @param {number} [options.top_k=undefined] Return the top-k documents. If undefined, all documents are returned.\n * @param {number} [options.return_documents=false] If true, also returns the documents. If false, only returns the indices and scores.\n */\nasync function rank(query, documents, {\n    top_k = undefined,\n    return_documents = false,\n} = {}) {\n    const inputs = tokenizer(\n        new Array(documents.length).fill(query),\n        {\n            text_pair: documents,\n            padding: true,\n            truncation: true,\n        }\n    )\n    const { logits } = await model(inputs);\n    return logits\n        .sigmoid()\n        .tolist()\n        .map(([score], i) => ({\n            corpus_id: i,\n            score,\n            ...(return_documents ? { text: documents[i] } : {})\n        }))\n        .sort((a, b) => b.score - a.score)\n        .slice(0, top_k);\n}\n\n// Example usage:\nconst query = \"Who wrote 'To Kill a Mockingbird'?\"\nconst documents = [\n    \"'To Kill a Mockingbird' is a novel by Harper Lee published in 1960. It was immediately successful, winning the Pulitzer Prize, and has become a classic of modern American literature.\",\n    \"The novel 'Moby-Dick' was written by Herman Melville and first published in 1851. It is considered a masterpiece of American literature and deals with complex themes of obsession, revenge, and the conflict between good and evil.\",\n    \"Harper Lee, an American novelist widely known for her novel 'To Kill a Mockingbird', was born in 1926 in Monroeville, Alabama. She received the Pulitzer Prize for Fiction in 1961.\",\n    \"Jane Austen was an English novelist known primarily for her six major novels, which interpret, critique and comment upon the British landed gentry at the end of the 18th century.\",\n    \"The 'Harry Potter' series, which consists of seven fantasy novels written by British author J.K. Rowling, is among the most popular and critically acclaimed books of the modern era.\",\n    \"'The Great Gatsby', a novel written by American author F. Scott Fitzgerald, was published in 1925. The story is set in the Jazz Age and follows the life of millionaire Jay Gatsby and his pursuit of Daisy Buchanan.\"\n]\n\nconst results = await rank(query, documents, { return_documents: true, top_k: 3 });\nconsole.log(results);\n```\n</details>\n\n## Using API\n\nYou can use the large model via our API as follows:\n\n```python\nfrom mixedbread_ai.client import MixedbreadAI\n\nmxbai = MixedbreadAI(api_key=\"{MIXEDBREAD_API_KEY}\")\n\nres = mxbai.reranking(\n  model=\"mixedbread-ai/mxbai-rerank-large-v1\",\n  query=\"Who is the author of To Kill a Mockingbird?\",\n  input=[\n    \"To Kill a Mockingbird is a novel by Harper Lee published in 1960. It was immediately successful, winning the Pulitzer Prize, and has become a classic of modern American literature.\",\n    \"The novel Moby-Dick was written by Herman Melville and first published in 1851. It is considered a masterpiece of American literature and deals with complex themes of obsession, revenge, and the conflict between good and evil.\",\n    \"Harper Lee, an American novelist widely known for her novel To Kill a Mockingbird, was born in 1926 in Monroeville, Alabama. She received the Pulitzer Prize for Fiction in 1961.\",\n    \"Jane Austen was an English novelist known primarily for her six major novels, which interpret, critique and comment upon the British landed gentry at the end of the 18th century.\",\n    \"The Harry Potter series, which consists of seven fantasy novels written by British author J.K. Rowling, is among the most popular and critically acclaimed books of the modern era.\",\n    \"The Great Gatsby, a novel written by American author F. Scott Fitzgerald, was published in 1925. The story is set in the Jazz Age and follows the life of millionaire Jay Gatsby and his pursuit of Daisy Buchanan.\"\n  ],\n  top_k=3,\n  return_input=false\n)\n\nprint(res.data)\n```\n\nThe API comes with additional features, such as a continous trained reranker! Check out the [docs](https://www.mixedbread.ai/docs) for more information.\n\n\n## Evaluation\n\nOur reranker models are designed to elevate your search. They work extremely well in combination with keyword search and can even outperform semantic search systems in many cases.\n\n| Model                                                                                 | NDCG@10  | Accuracy@3 |\n| ------------------------------------------------------------------------------------- | -------- | ---------- |\n| Lexical Search (Lucene)                                                               | 38.0     | 66.4       |\n| [BAAI/bge-reranker-base](https://huggingface.co/BAAI/bge-reranker-base)               | 41.6     | 66.9       |\n| [BAAI/bge-reranker-large](https://huggingface.co/BAAI/bge-reranker-large)             | 45.2     | 70.6       |\n| cohere-embed-v3 (semantic search)                                                     | 47.5     | 70.9       |\n| [mxbai-rerank-xsmall-v1](https://huggingface.co/mixedbread-ai/mxbai-rerank-xsmall-v1) | **43.9** | **70.0**   |\n| [mxbai-rerank-base-v1](https://huggingface.co/mixedbread-ai/mxbai-rerank-base-v1)     | **46.9** | **72.3**   |\n| [mxbai-rerank-large-v1](https://huggingface.co/mixedbread-ai/mxbai-rerank-large-v1)   | **48.8** | **74.9**   |\n\nThe reported results are aggregated from 11 datasets of BEIR. We used [Pyserini](https://github.com/castorini/pyserini/) to evaluate the models. Find more in our [blog-post](https://www.mixedbread.ai/blog/mxbai-rerank-v1) and on this [spreadsheet](https://docs.google.com/spreadsheets/d/15ELkSMFv-oHa5TRiIjDvhIstH9dlc3pnZeO-iGz4Ld4/edit?usp=sharing).\n\n## Community\nPlease join our [Discord Community](https://discord.gg/jDfMHzAVfU) and share your feedback and thoughts! We are here to help and also always happy to chat.\n\n## Citation\n\n```bibtex\n@online{rerank2024mxbai,\n  title={Boost Your Search With The Crispy Mixedbread Rerank Models},\n  author={Aamir Shakir and Darius Koenig and Julius Lipp and Sean Lee},\n  year={2024},\n  url={https://www.mixedbread.ai/blog/mxbai-rerank-v1},\n}\n```\n\n## License\nApache 2.0",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModelForSequenceClassification",
      "custom_class": null,
      "pipeline_tag": "text-classification",
      "processor": "AutoTokenizer"
    },
    "safetensors": {
      "parameters": {
        "F16": 70830337
      },
      "total": 70830337
    },
    "model_index": null,
    "trending_score": null
  },
  {
    "model_id": "madhurjindal/autonlp-Gibberish-Detector-492513457",
    "model_name": "madhurjindal/autonlp-Gibberish-Detector-492513457",
    "author": "madhurjindal",
    "downloads": 371090,
    "downloads_all_time": null,
    "likes": 57,
    "tags": [
      "transformers",
      "pytorch",
      "onnx",
      "safetensors",
      "distilbert",
      "text-classification",
      "autonlp",
      "en",
      "dataset:madhurjindal/autonlp-data-Gibberish-Detector",
      "doi:10.57967/hf/2664",
      "license:mit",
      "co2_eq_emissions",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/madhurjindal/autonlp-Gibberish-Detector-492513457",
    "dependencies": [
      [
        "torch",
        null
      ],
      [
        "transformers",
        null
      ]
    ],
    "last_modified": "2024-06-17T06:31:08+00:00",
    "created_at": "2022-03-02T23:29:05+00:00",
    "analysis_date": "2025-03-22T00:55:39.245642",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "distilbert",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "tags": [
        "autonlp"
      ],
      "language": "en",
      "widget": [
        {
          "text": "I love Machine Learning!"
        }
      ],
      "datasets": [
        "madhurjindal/autonlp-data-Gibberish-Detector"
      ],
      "co2_eq_emissions": 5.527544460835904,
      "license": "mit"
    },
    "card_text": "\n# Problem Description\nThe ability to process and understand user input is crucial for various applications, such as chatbots or downstream tasks. However, a common challenge faced in such systems is the presence of gibberish or nonsensical input. To address this problem, we present a project focused on developing a gibberish detector for the English language.\nThe primary goal of this project is to classify user input as either **gibberish** or **non-gibberish**, enabling more accurate and meaningful interactions with the system. We also aim to enhance the overall performance and user experience of chatbots and other systems that rely on user input.\n\n>## What is Gibberish?\nGibberish refers to **nonsensical or meaningless language or text** that lacks coherence or any discernible meaning. It can be characterized by a combination of random words, nonsensical phrases, grammatical errors, or syntactical abnormalities that prevent the communication from conveying a clear and understandable message. Gibberish can vary in intensity, ranging from simple noise with no meaningful words to sentences that may appear superficially correct but lack coherence or logical structure when examined closely. Detecting and identifying gibberish is essential in various contexts, such as **natural language processing**, **chatbot systems**, **spam filtering**, and **language-based security measures**, to ensure effective communication and accurate processing of user inputs.\n\n## Label Description\nThus, we break down the problem into 4 categories:\n\n1. **Noise:** Gibberish at the zero level where even the different constituents of the input phrase (words) do not hold any meaning independently.  \n   *For example: `dfdfer fgerfow2e0d qsqskdsd djksdnfkff swq.`*\n   \n2. **Word Salad:** Gibberish at level 1 where words make sense independently, but when looked at the bigger picture (the phrase) any meaning is not depicted.  \n   *For example: `22 madhur old punjab pickle chennai`*\n\n3. **Mild gibberish:** Gibberish at level 2 where there is a part of the sentence that has grammatical errors, word sense errors, or any syntactical abnormalities, which leads the sentence to miss out on a coherent meaning.  \n   *For example: `Madhur study in a teacher`*\n\n4. **Clean:** This category represents a set of words that form a complete and meaningful sentence on its own.  \n   *For example: `I love this website`*\n\n> **Tip:** To facilitate gibberish detection, you can combine the labels based on the desired level of detection. For instance, if you need to detect gibberish at level 1, you can group Noise and Word Salad together as \"Gibberish,\" while considering Mild gibberish and Clean separately as \"NotGibberish.\" This approach allows for flexibility in detecting and categorizing different levels of gibberish based on specific requirements.\n\n\n# Model Trained Using AutoNLP\n\n- Problem type: Multi-class Classification\n- Model ID: 492513457\n- CO2 Emissions (in grams): 5.527544460835904\n\n## Validation Metrics\n\n- Loss: 0.07609463483095169\n- Accuracy: 0.9735624586913417\n- Macro F1: 0.9736173135739408\n- Micro F1: 0.9735624586913417\n- Weighted F1: 0.9736173135739408\n- Macro Precision: 0.9737771415197378\n- Micro Precision: 0.9735624586913417\n- Weighted Precision: 0.9737771415197378\n- Macro Recall: 0.9735624586913417\n- Micro Recall: 0.9735624586913417\n- Weighted Recall: 0.9735624586913417\n\n\n## Usage\n\nYou can use cURL to access this model:\n\n```\n$ curl -X POST -H \"Authorization: Bearer YOUR_API_KEY\" -H \"Content-Type: application/json\" -d '{\"inputs\": \"I love Machine Learning!\"}' https://api-inference.huggingface.co/models/madhurjindal/autonlp-Gibberish-Detector-492513457\n```\n\nOr Python API:\n\n```\nimport torch\nimport torch.nn.functional as F\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\"madhurjindal/autonlp-Gibberish-Detector-492513457\", use_auth_token=True)\n\ntokenizer = AutoTokenizer.from_pretrained(\"madhurjindal/autonlp-Gibberish-Detector-492513457\", use_auth_token=True)\n\ninputs = tokenizer(\"I love Machine Learning!\", return_tensors=\"pt\")\n\noutputs = model(**inputs)\n\nprobs = F.softmax(outputs.logits, dim=-1)\n\npredicted_index = torch.argmax(probs, dim=1).item()\n\npredicted_prob = probs[0][predicted_index].item()\n\nlabels = model.config.id2label\n\npredicted_label = labels[predicted_index]\n\nfor i, prob in enumerate(probs[0]):\n    print(f\"Class: {labels[i]}, Probability: {prob:.4f}\")\n```\n\nAnother simplifed solution with transformers pipline:\n\n```\nfrom transformers import pipeline\nselected_model = \"madhurjindal/autonlp-Gibberish-Detector-492513457\"\nclassifier = pipeline(\"text-classification\", model=selected_model)\nclassifier(\"I love Machine Learning!\")\n```",
    "card_content": "---\ntags:\n- autonlp\nlanguage: en\nwidget:\n- text: I love Machine Learning!\ndatasets:\n- madhurjindal/autonlp-data-Gibberish-Detector\nco2_eq_emissions: 5.527544460835904\nlicense: mit\n---\n\n# Problem Description\nThe ability to process and understand user input is crucial for various applications, such as chatbots or downstream tasks. However, a common challenge faced in such systems is the presence of gibberish or nonsensical input. To address this problem, we present a project focused on developing a gibberish detector for the English language.\nThe primary goal of this project is to classify user input as either **gibberish** or **non-gibberish**, enabling more accurate and meaningful interactions with the system. We also aim to enhance the overall performance and user experience of chatbots and other systems that rely on user input.\n\n>## What is Gibberish?\nGibberish refers to **nonsensical or meaningless language or text** that lacks coherence or any discernible meaning. It can be characterized by a combination of random words, nonsensical phrases, grammatical errors, or syntactical abnormalities that prevent the communication from conveying a clear and understandable message. Gibberish can vary in intensity, ranging from simple noise with no meaningful words to sentences that may appear superficially correct but lack coherence or logical structure when examined closely. Detecting and identifying gibberish is essential in various contexts, such as **natural language processing**, **chatbot systems**, **spam filtering**, and **language-based security measures**, to ensure effective communication and accurate processing of user inputs.\n\n## Label Description\nThus, we break down the problem into 4 categories:\n\n1. **Noise:** Gibberish at the zero level where even the different constituents of the input phrase (words) do not hold any meaning independently.  \n   *For example: `dfdfer fgerfow2e0d qsqskdsd djksdnfkff swq.`*\n   \n2. **Word Salad:** Gibberish at level 1 where words make sense independently, but when looked at the bigger picture (the phrase) any meaning is not depicted.  \n   *For example: `22 madhur old punjab pickle chennai`*\n\n3. **Mild gibberish:** Gibberish at level 2 where there is a part of the sentence that has grammatical errors, word sense errors, or any syntactical abnormalities, which leads the sentence to miss out on a coherent meaning.  \n   *For example: `Madhur study in a teacher`*\n\n4. **Clean:** This category represents a set of words that form a complete and meaningful sentence on its own.  \n   *For example: `I love this website`*\n\n> **Tip:** To facilitate gibberish detection, you can combine the labels based on the desired level of detection. For instance, if you need to detect gibberish at level 1, you can group Noise and Word Salad together as \"Gibberish,\" while considering Mild gibberish and Clean separately as \"NotGibberish.\" This approach allows for flexibility in detecting and categorizing different levels of gibberish based on specific requirements.\n\n\n# Model Trained Using AutoNLP\n\n- Problem type: Multi-class Classification\n- Model ID: 492513457\n- CO2 Emissions (in grams): 5.527544460835904\n\n## Validation Metrics\n\n- Loss: 0.07609463483095169\n- Accuracy: 0.9735624586913417\n- Macro F1: 0.9736173135739408\n- Micro F1: 0.9735624586913417\n- Weighted F1: 0.9736173135739408\n- Macro Precision: 0.9737771415197378\n- Micro Precision: 0.9735624586913417\n- Weighted Precision: 0.9737771415197378\n- Macro Recall: 0.9735624586913417\n- Micro Recall: 0.9735624586913417\n- Weighted Recall: 0.9735624586913417\n\n\n## Usage\n\nYou can use cURL to access this model:\n\n```\n$ curl -X POST -H \"Authorization: Bearer YOUR_API_KEY\" -H \"Content-Type: application/json\" -d '{\"inputs\": \"I love Machine Learning!\"}' https://api-inference.huggingface.co/models/madhurjindal/autonlp-Gibberish-Detector-492513457\n```\n\nOr Python API:\n\n```\nimport torch\nimport torch.nn.functional as F\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\"madhurjindal/autonlp-Gibberish-Detector-492513457\", use_auth_token=True)\n\ntokenizer = AutoTokenizer.from_pretrained(\"madhurjindal/autonlp-Gibberish-Detector-492513457\", use_auth_token=True)\n\ninputs = tokenizer(\"I love Machine Learning!\", return_tensors=\"pt\")\n\noutputs = model(**inputs)\n\nprobs = F.softmax(outputs.logits, dim=-1)\n\npredicted_index = torch.argmax(probs, dim=1).item()\n\npredicted_prob = probs[0][predicted_index].item()\n\nlabels = model.config.id2label\n\npredicted_label = labels[predicted_index]\n\nfor i, prob in enumerate(probs[0]):\n    print(f\"Class: {labels[i]}, Probability: {prob:.4f}\")\n```\n\nAnother simplifed solution with transformers pipline:\n\n```\nfrom transformers import pipeline\nselected_model = \"madhurjindal/autonlp-Gibberish-Detector-492513457\"\nclassifier = pipeline(\"text-classification\", model=selected_model)\nclassifier(\"I love Machine Learning!\")\n```",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModelForSequenceClassification",
      "custom_class": null,
      "pipeline_tag": "text-classification",
      "processor": "AutoTokenizer"
    },
    "safetensors": {
      "parameters": {
        "F32": 66956548
      },
      "total": 66956548
    },
    "model_index": null,
    "trending_score": null
  },
  {
    "model_id": "cross-encoder/ms-marco-MiniLM-L2-v2",
    "model_name": "cross-encoder/ms-marco-MiniLM-L2-v2",
    "author": "cross-encoder",
    "downloads": 369890,
    "downloads_all_time": null,
    "likes": 10,
    "tags": [
      "transformers",
      "pytorch",
      "jax",
      "safetensors",
      "bert",
      "text-classification",
      "license:apache-2.0",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/cross-encoder/ms-marco-MiniLM-L2-v2",
    "dependencies": [
      [
        "sentence_transformers",
        null
      ],
      [
        "transformers",
        null
      ],
      [
        "torch",
        null
      ]
    ],
    "last_modified": "2025-03-07T14:56:04+00:00",
    "created_at": "2022-03-02T23:29:05+00:00",
    "analysis_date": "2025-03-22T00:55:40.798520",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "bert",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "license": "apache-2.0"
    },
    "card_text": "# Cross-Encoder for MS Marco\n\nThis model was trained on the [MS Marco Passage Ranking](https://github.com/microsoft/MSMARCO-Passage-Ranking) task.\n\nThe model can be used for Information Retrieval: Given a query, encode the query will all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. See [SBERT.net Retrieve & Re-rank](https://www.sbert.net/examples/applications/retrieve_rerank/README.html) for more details. The training code is available here: [SBERT.net Training MS Marco](https://github.com/UKPLab/sentence-transformers/tree/master/examples/training/ms_marco)\n\n\n## Usage with SentenceTransformers\n\nThe usage is easy when you have [SentenceTransformers](https://www.sbert.net/) installed. Then you can use the pre-trained models like this:\n```python\nfrom sentence_transformers import CrossEncoder\n\nmodel = CrossEncoder('cross-encoder/ms-marco-MiniLM-L2-v2')\nscores = model.predict([\n    (\"How many people live in Berlin?\", \"Berlin had a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\"),\n    (\"How many people live in Berlin?\", \"Berlin is well known for its museums.\"),\n])\nprint(scores)\n# [ 8.510401 -4.860082]\n```\n\n\n## Usage with Transformers\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\n\nmodel = AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-MiniLM-L2-v2')\ntokenizer = AutoTokenizer.from_pretrained('cross-encoder/ms-marco-MiniLM-L2-v2')\n\nfeatures = tokenizer(['How many people live in Berlin?', 'How many people live in Berlin?'], ['Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.', 'New York City is famous for the Metropolitan Museum of Art.'],  padding=True, truncation=True, return_tensors=\"pt\")\n\nmodel.eval()\nwith torch.no_grad():\n    scores = model(**features).logits\n    print(scores)\n```\n\n\n## Performance\nIn the following table, we provide various pre-trained Cross-Encoders together with their performance on the [TREC Deep Learning 2019](https://microsoft.github.io/TREC-2019-Deep-Learning/) and the [MS Marco Passage Reranking](https://github.com/microsoft/MSMARCO-Passage-Ranking/) dataset. \n\n\n| Model-Name        | NDCG@10 (TREC DL 19) | MRR@10 (MS Marco Dev)  | Docs / Sec |\n| ------------- |:-------------| -----| --- | \n| **Version 2 models** | | | \n| cross-encoder/ms-marco-TinyBERT-L2-v2 | 69.84 | 32.56 | 9000\n| cross-encoder/ms-marco-MiniLM-L2-v2 | 71.01 | 34.85 | 4100\n| cross-encoder/ms-marco-MiniLM-L4-v2 | 73.04 | 37.70 | 2500\n| cross-encoder/ms-marco-MiniLM-L6-v2 | 74.30 | 39.01 | 1800\n| cross-encoder/ms-marco-MiniLM-L12-v2 | 74.31 | 39.02 | 960\n| **Version 1 models** | | | \n| cross-encoder/ms-marco-TinyBERT-L2  | 67.43 | 30.15  | 9000\n| cross-encoder/ms-marco-TinyBERT-L4  | 68.09 | 34.50  | 2900\n| cross-encoder/ms-marco-TinyBERT-L6 |  69.57 | 36.13  | 680\n| cross-encoder/ms-marco-electra-base | 71.99 | 36.41 | 340\n| **Other models** | | | \n| nboost/pt-tinybert-msmarco | 63.63 | 28.80 | 2900 \n| nboost/pt-bert-base-uncased-msmarco | 70.94 | 34.75 | 340 \n| nboost/pt-bert-large-msmarco | 73.36 | 36.48 | 100 \n| Capreolus/electra-base-msmarco | 71.23 | 36.89 | 340 \n| amberoad/bert-multilingual-passage-reranking-msmarco | 68.40 | 35.54 | 330 \n| sebastian-hofstaetter/distilbert-cat-margin_mse-T2-msmarco | 72.82 | 37.88 | 720\n \n Note: Runtime was computed on a V100 GPU.\n",
    "card_content": "---\nlicense: apache-2.0\n---\n# Cross-Encoder for MS Marco\n\nThis model was trained on the [MS Marco Passage Ranking](https://github.com/microsoft/MSMARCO-Passage-Ranking) task.\n\nThe model can be used for Information Retrieval: Given a query, encode the query will all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. See [SBERT.net Retrieve & Re-rank](https://www.sbert.net/examples/applications/retrieve_rerank/README.html) for more details. The training code is available here: [SBERT.net Training MS Marco](https://github.com/UKPLab/sentence-transformers/tree/master/examples/training/ms_marco)\n\n\n## Usage with SentenceTransformers\n\nThe usage is easy when you have [SentenceTransformers](https://www.sbert.net/) installed. Then you can use the pre-trained models like this:\n```python\nfrom sentence_transformers import CrossEncoder\n\nmodel = CrossEncoder('cross-encoder/ms-marco-MiniLM-L2-v2')\nscores = model.predict([\n    (\"How many people live in Berlin?\", \"Berlin had a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\"),\n    (\"How many people live in Berlin?\", \"Berlin is well known for its museums.\"),\n])\nprint(scores)\n# [ 8.510401 -4.860082]\n```\n\n\n## Usage with Transformers\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\n\nmodel = AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-MiniLM-L2-v2')\ntokenizer = AutoTokenizer.from_pretrained('cross-encoder/ms-marco-MiniLM-L2-v2')\n\nfeatures = tokenizer(['How many people live in Berlin?', 'How many people live in Berlin?'], ['Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.', 'New York City is famous for the Metropolitan Museum of Art.'],  padding=True, truncation=True, return_tensors=\"pt\")\n\nmodel.eval()\nwith torch.no_grad():\n    scores = model(**features).logits\n    print(scores)\n```\n\n\n## Performance\nIn the following table, we provide various pre-trained Cross-Encoders together with their performance on the [TREC Deep Learning 2019](https://microsoft.github.io/TREC-2019-Deep-Learning/) and the [MS Marco Passage Reranking](https://github.com/microsoft/MSMARCO-Passage-Ranking/) dataset. \n\n\n| Model-Name        | NDCG@10 (TREC DL 19) | MRR@10 (MS Marco Dev)  | Docs / Sec |\n| ------------- |:-------------| -----| --- | \n| **Version 2 models** | | | \n| cross-encoder/ms-marco-TinyBERT-L2-v2 | 69.84 | 32.56 | 9000\n| cross-encoder/ms-marco-MiniLM-L2-v2 | 71.01 | 34.85 | 4100\n| cross-encoder/ms-marco-MiniLM-L4-v2 | 73.04 | 37.70 | 2500\n| cross-encoder/ms-marco-MiniLM-L6-v2 | 74.30 | 39.01 | 1800\n| cross-encoder/ms-marco-MiniLM-L12-v2 | 74.31 | 39.02 | 960\n| **Version 1 models** | | | \n| cross-encoder/ms-marco-TinyBERT-L2  | 67.43 | 30.15  | 9000\n| cross-encoder/ms-marco-TinyBERT-L4  | 68.09 | 34.50  | 2900\n| cross-encoder/ms-marco-TinyBERT-L6 |  69.57 | 36.13  | 680\n| cross-encoder/ms-marco-electra-base | 71.99 | 36.41 | 340\n| **Other models** | | | \n| nboost/pt-tinybert-msmarco | 63.63 | 28.80 | 2900 \n| nboost/pt-bert-base-uncased-msmarco | 70.94 | 34.75 | 340 \n| nboost/pt-bert-large-msmarco | 73.36 | 36.48 | 100 \n| Capreolus/electra-base-msmarco | 71.23 | 36.89 | 340 \n| amberoad/bert-multilingual-passage-reranking-msmarco | 68.40 | 35.54 | 330 \n| sebastian-hofstaetter/distilbert-cat-margin_mse-T2-msmarco | 72.82 | 37.88 | 720\n \n Note: Runtime was computed on a V100 GPU.\n",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModelForSequenceClassification",
      "custom_class": null,
      "pipeline_tag": "text-classification",
      "processor": "AutoTokenizer"
    },
    "safetensors": {
      "parameters": {
        "I64": 512,
        "F32": 15615745
      },
      "total": 15616257
    },
    "model_index": null,
    "trending_score": null
  },
  {
    "model_id": "jinaai/jina-reranker-v2-base-multilingual",
    "model_name": "jinaai/jina-reranker-v2-base-multilingual",
    "author": "jinaai",
    "downloads": 363385,
    "downloads_all_time": null,
    "likes": 251,
    "tags": [
      "transformers",
      "pytorch",
      "onnx",
      "safetensors",
      "text-classification",
      "reranker",
      "cross-encoder",
      "transformers.js",
      "custom_code",
      "multilingual",
      "license:cc-by-nc-4.0",
      "autotrain_compatible",
      "region:eu"
    ],
    "card_url": "https://huggingface.co/jinaai/jina-reranker-v2-base-multilingual",
    "dependencies": [
      [
        "transformers",
        null
      ],
      [
        "einops",
        null
      ],
      [
        "sentence-transformers",
        null
      ]
    ],
    "last_modified": "2025-01-06T16:21:54+00:00",
    "created_at": "2024-06-19T09:37:19+00:00",
    "analysis_date": "2025-03-22T00:55:42.539472",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "unknown",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "pipeline_tag": "text-classification",
      "tags": [
        "transformers",
        "reranker",
        "cross-encoder",
        "transformers.js"
      ],
      "language": [
        "multilingual"
      ],
      "inference": false,
      "license": "cc-by-nc-4.0",
      "library_name": "transformers"
    },
    "card_text": "\n<br><br>\n\n<p align=\"center\">\n<img src=\"https://huggingface.co/datasets/jinaai/documentation-images/resolve/main/logo.webp\" alt=\"Jina AI: Your Search Foundation, Supercharged!\" width=\"150px\">\n</p>\n\n<p align=\"center\">\n<b>Trained by <a href=\"https://jina.ai/\"><b>Jina AI</b></a>.</b>\n</p>\n\n# jina-reranker-v2-base-multilingual\n\n## Intended Usage & Model Info\n\nThe **Jina Reranker v2** (`jina-reranker-v2-base-multilingual`) is a transformer-based model that has been fine-tuned for text reranking task, which is a crucial component in many information retrieval systems. It is a cross-encoder model that takes a query and a document pair as input and outputs a score indicating the relevance of the document to the query. The model is trained on a large dataset of query-document pairs and is capable of reranking documents in multiple languages with high accuracy.\n\nCompared with the state-of-the-art reranker models, including the previous released `jina-reranker-v1-base-en`, the **Jina Reranker v2** model has demonstrated competitiveness across a series of benchmarks targeting for text retrieval, multilingual capability, function-calling-aware and text-to-SQL-aware reranking, and code retrieval tasks.\n\nThe `jina-reranker-v2-base-multilingual` model is capable of handling long texts with a context length of up to `1024` tokens, enabling the processing of extensive inputs. To enable the model to handle long texts that exceed 1024 tokens, the model uses a sliding window approach to chunk the input text into smaller pieces and rerank each chunk separately.\n\nThe model is also equipped with a flash attention mechanism, which significantly improves the model's performance.\n\n\n# Usage\n\n_This model repository is licenced for research and evaluation purposes under CC-BY-NC-4.0. For commercial usage, please refer to Jina AI's APIs, AWS Sagemaker or Azure Marketplace offerings. Please [contact us](https://jina.ai/contact-sales) for any further clarifications._\n1. The easiest way to use `jina-reranker-v2-base-multilingual` is to call Jina AI's [Reranker API](https://jina.ai/reranker/).\n\n```bash\ncurl https://api.jina.ai/v1/rerank \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer YOUR_API_KEY\" \\\n  -d '{\n  \"model\": \"jina-reranker-v2-base-multilingual\",\n  \"query\": \"Organic skincare products for sensitive skin\",\n  \"documents\": [\n    \"Organic skincare for sensitive skin with aloe vera and chamomile.\",\n    \"New makeup trends focus on bold colors and innovative techniques\",\n    \"Bio-Hautpflege für empfindliche Haut mit Aloe Vera und Kamille\",\n    \"Neue Make-up-Trends setzen auf kräftige Farben und innovative Techniken\",\n    \"Cuidado de la piel orgánico para piel sensible con aloe vera y manzanilla\",\n    \"Las nuevas tendencias de maquillaje se centran en colores vivos y técnicas innovadoras\",\n    \"针对敏感肌专门设计的天然有机护肤产品\",\n    \"新的化妆趋势注重鲜艳的颜色和创新的技巧\",\n    \"敏感肌のために特別に設計された天然有機スキンケア製品\",\n    \"新しいメイクのトレンドは鮮やかな色と革新的な技術に焦点を当てています\"\n  ],\n  \"top_n\": 3\n}'\n```\n\n2. You can also use the `transformers` library to interact with the model programmatically.\n\nBefore you start, install the `transformers` and `einops` libraries:\n\n```bash\npip install transformers einops\n```\n\nAnd then:\n```python\nfrom transformers import AutoModelForSequenceClassification\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    'jinaai/jina-reranker-v2-base-multilingual',\n    torch_dtype=\"auto\",\n    trust_remote_code=True,\n)\n\nmodel.to('cuda') # or 'cpu' if no GPU is available\nmodel.eval()\n\n# Example query and documents\nquery = \"Organic skincare products for sensitive skin\"\ndocuments = [\n    \"Organic skincare for sensitive skin with aloe vera and chamomile.\",\n    \"New makeup trends focus on bold colors and innovative techniques\",\n    \"Bio-Hautpflege für empfindliche Haut mit Aloe Vera und Kamille\",\n    \"Neue Make-up-Trends setzen auf kräftige Farben und innovative Techniken\",\n    \"Cuidado de la piel orgánico para piel sensible con aloe vera y manzanilla\",\n    \"Las nuevas tendencias de maquillaje se centran en colores vivos y técnicas innovadoras\",\n    \"针对敏感肌专门设计的天然有机护肤产品\",\n    \"新的化妆趋势注重鲜艳的颜色和创新的技巧\",\n    \"敏感肌のために特別に設計された天然有機スキンケア製品\",\n    \"新しいメイクのトレンドは鮮やかな色と革新的な技術に焦点を当てています\",\n]\n\n# construct sentence pairs\nsentence_pairs = [[query, doc] for doc in documents]\n\nscores = model.compute_score(sentence_pairs, max_length=1024)\n```\n\nThe scores will be a list of floats, where each float represents the relevance score of the corresponding document to the query. Higher scores indicate higher relevance.\nFor instance the returning scores in this case will be:\n```bash\n[0.8311430811882019, 0.09401018172502518,\n 0.6334102749824524, 0.08269733935594559,\n 0.7620701193809509, 0.09947021305561066,\n 0.9263036847114563, 0.05834583938121796,\n 0.8418256044387817, 0.11124119907617569]\n```\n\nThe model gives high relevance scores to the documents that are most relevant to the query regardless of the language of the document.\n\nNote that by default, the `jina-reranker-v2-base-multilingual` model uses [flash attention](https://github.com/Dao-AILab/flash-attention), which requires certain types of GPU hardware to run.\nIf you encounter any issues, you can try call `AutoModelForSequenceClassification.from_pretrained()` with `use_flash_attn=False`.\nThis will use the standard attention mechanism instead of flash attention.\n\nIf you want to use flash attention for fast inference, you need to install the following packages:\n```bash\npip install ninja # required for flash attention\npip install flash-attn --no-build-isolation\n```\nEnjoy the 3x-6x speedup with flash attention! ⚡️⚡️⚡️\n\n\n3. You can also use the `transformers.js` library to run the model directly in JavaScript (in-browser, Node.js, Deno, etc.)!\n\nIf you haven't already, you can install the [Transformers.js](https://huggingface.co/docs/transformers.js) JavaScript library (v3) using:\n```bash\nnpm i xenova/transformers.js#v3\n```\n\nThen, you can use the following code to interact with the model:\n```js\nimport { AutoTokenizer, XLMRobertaModel } from '@xenova/transformers';\n\nconst model_id = 'jinaai/jina-reranker-v2-base-multilingual';\nconst model = await XLMRobertaModel.from_pretrained(model_id, { dtype: 'fp32' });\nconst tokenizer = await AutoTokenizer.from_pretrained(model_id);\n\n/**\n * Performs ranking with the CrossEncoder on the given query and documents. Returns a sorted list with the document indices and scores.\n * @param {string} query A single query\n * @param {string[]} documents A list of documents\n * @param {Object} options Options for ranking\n * @param {number} [options.top_k=undefined] Return the top-k documents. If undefined, all documents are returned.\n * @param {number} [options.return_documents=false] If true, also returns the documents. If false, only returns the indices and scores.\n */\nasync function rank(query, documents, {\n    top_k = undefined,\n    return_documents = false,\n} = {}) {\n    const inputs = tokenizer(\n        new Array(documents.length).fill(query),\n        { text_pair: documents, padding: true, truncation: true }\n    )\n    const { logits } = await model(inputs);\n    return logits.sigmoid().tolist()\n        .map(([score], i) => ({\n            corpus_id: i,\n            score,\n            ...(return_documents ? { text: documents[i] } : {})\n        })).sort((a, b) => b.score - a.score).slice(0, top_k);\n}\n\n// Example usage:\nconst query = \"Organic skincare products for sensitive skin\"\nconst documents = [\n    \"Organic skincare for sensitive skin with aloe vera and chamomile.\",\n    \"New makeup trends focus on bold colors and innovative techniques\",\n    \"Bio-Hautpflege für empfindliche Haut mit Aloe Vera und Kamille\",\n    \"Neue Make-up-Trends setzen auf kräftige Farben und innovative Techniken\",\n    \"Cuidado de la piel orgánico para piel sensible con aloe vera y manzanilla\",\n    \"Las nuevas tendencias de maquillaje se centran en colores vivos y técnicas innovadoras\",\n    \"针对敏感肌专门设计的天然有机护肤产品\",\n    \"新的化妆趋势注重鲜艳的颜色和创新的技巧\",\n    \"敏感肌のために特別に設計された天然有機スキンケア製品\",\n    \"新しいメイクのトレンドは鮮やかな色と革新的な技術に焦点を当てています\",\n]\n\nconst results = await rank(query, documents, { return_documents: true, top_k: 3 });\nconsole.log(results);\n```\n\n\nThat's it! You can now use the `jina-reranker-v2-base-multilingual` model in your projects.\n\n\nIn addition to the `compute_score()` function, the `jina-reranker-v2-base-multilingual` model also provides a `model.rerank()` function that can be used to rerank documents based on a query. You can use it as follows:\n\n```python\nresult = model.rerank(\n    query,\n    documents,\n    max_query_length=512,\n    max_length=1024,\n    top_n=3\n)\n```\n\nInside the `result` object, you will find the reranked documents along with their scores. You can use this information to further process the documents as needed.\n\nThe `rerank()` function will automatically chunk the input documents into smaller pieces if they exceed the model's maximum input length. This allows you to rerank long documents without running into memory issues.\nSpecifically, the `rerank()` function will split the documents into chunks of size `max_length` and rerank each chunk separately. The scores from all the chunks are then combined to produce the final reranking results. You can control the query length and document length in each chunk by setting the `max_query_length` and `max_length` parameters. The `rerank()` function also supports the `overlap` parameter (default is `80`) which determines how much overlap there is between adjacent chunks. This can be useful when reranking long documents to ensure that the model has enough context to make accurate predictions.\n\n3. Alternatively, `jina-reranker-v2-base-multilingual` has been integrated with `CrossEncoder` from the `sentence-transformers` library.\n\nBefore you start, install the `sentence-transformers` libraries:\n\n```bash\npip install sentence-transformers\n```\n\nThe [`CrossEncoder`](https://sbert.net/docs/package_reference/cross_encoder/cross_encoder.html) class supports a [`predict`](https://sbert.net/docs/package_reference/cross_encoder/cross_encoder.html#sentence_transformers.cross_encoder.CrossEncoder.predict) method to get query-document relevance scores, and a [`rank`](https://sbert.net/docs/package_reference/cross_encoder/cross_encoder.html#sentence_transformers.cross_encoder.CrossEncoder.rank) method to rank all documents given your query.\n\n```python\nfrom sentence_transformers import CrossEncoder\n\nmodel = CrossEncoder(\n    \"jinaai/jina-reranker-v2-base-multilingual\",\n    automodel_args={\"torch_dtype\": \"auto\"},\n    trust_remote_code=True,\n)\n\n# Example query and documents\nquery = \"Organic skincare products for sensitive skin\"\ndocuments = [\n    \"Organic skincare for sensitive skin with aloe vera and chamomile.\",\n    \"New makeup trends focus on bold colors and innovative techniques\",\n    \"Bio-Hautpflege für empfindliche Haut mit Aloe Vera und Kamille\",\n    \"Neue Make-up-Trends setzen auf kräftige Farben und innovative Techniken\",\n    \"Cuidado de la piel orgánico para piel sensible con aloe vera y manzanilla\",\n    \"Las nuevas tendencias de maquillaje se centran en colores vivos y técnicas innovadoras\",\n    \"针对敏感肌专门设计的天然有机护肤产品\",\n    \"新的化妆趋势注重鲜艳的颜色和创新的技巧\",\n    \"敏感肌のために特別に設計された天然有機スキンケア製品\",\n    \"新しいメイクのトレンドは鮮やかな色と革新的な技術に焦点を当てています\",\n]\n\n# construct sentence pairs\nsentence_pairs = [[query, doc] for doc in documents]\n\nscores = model.predict(sentence_pairs, convert_to_tensor=True).tolist()\n\"\"\"\n[0.828125, 0.0927734375, 0.6328125, 0.08251953125, 0.76171875, 0.099609375, 0.92578125, 0.058349609375, 0.84375, 0.111328125]\n\"\"\"\n\nrankings = model.rank(query, documents, return_documents=True, convert_to_tensor=True)\nprint(f\"Query: {query}\")\nfor ranking in rankings:\n    print(f\"ID: {ranking['corpus_id']}, Score: {ranking['score']:.4f}, Text: {ranking['text']}\")\n\"\"\"\nQuery: Organic skincare products for sensitive skin\nID: 6, Score: 0.9258, Text: 针对敏感肌专门设计的天然有机护肤产品\nID: 8, Score: 0.8438, Text: 敏感肌のために特別に設計された天然有機スキンケア製品\nID: 0, Score: 0.8281, Text: Organic skincare for sensitive skin with aloe vera and chamomile.\nID: 4, Score: 0.7617, Text: Cuidado de la piel orgánico para piel sensible con aloe vera y manzanilla\nID: 2, Score: 0.6328, Text: Bio-Hautpflege für empfindliche Haut mit Aloe Vera und Kamille\nID: 9, Score: 0.1113, Text: 新しいメイクのトレンドは鮮やかな色と革新的な技術に焦点を当てています\nID: 5, Score: 0.0996, Text: Las nuevas tendencias de maquillaje se centran en colores vivos y técnicas innovadoras\nID: 1, Score: 0.0928, Text: New makeup trends focus on bold colors and innovative techniques\nID: 3, Score: 0.0825, Text: Neue Make-up-Trends setzen auf kräftige Farben und innovative Techniken\nID: 7, Score: 0.0583, Text: 新的化妆趋势注重鲜艳的颜色和创新的技巧\n\"\"\"\n```\n\n# Evaluation\n\nWe evaluated Jina Reranker v2 on multiple benchmarks to ensure top-tier performance and search relevance.\n\n|           Model Name            |   Model Size | MKQA(nDCG@10, 26 langs) \t| BEIR(nDCG@10, 17 datasets) \t| MLDR(recall@10, 13 langs) | CodeSearchNet (MRR@10, 3 tasks) \t| AirBench (nDCG@10, zh/en) \t| ToolBench (recall@3, 3 tasks) \t| TableSearch (recall@3) \t|\n| :-----------------------------: | :----------: | ------------------------- | ---------------------------- | --------------------------- | --------------------------------- | --------------------------- | ------------------------------- | ------------------------ |\n| jina-reranker-v2-multilingual \t|    278M    \t|          54.83          \t|            53.17           \t|           68.95           \t|              71.36              \t|           61.33           \t|             77.75             \t|          93.31         \t|\n|       bge-reranker-v2-m3      \t|    568M    \t|          54.17          \t|            53.65           \t|           59.73           \t|              62.86              \t|           61.28           \t|             78.46             \t|          74.86         \t|\n|  mmarco-mMiniLMv2-L12-H384-v1 \t|    118M    \t|          53.37          \t|            45.40           \t|           28.91           \t|              51.78              \t|           56.46           \t|             58.39             \t|          53.60         \t|\n|    jina-reranker-v1-base-en   \t|    137M    \t|            -            \t|            52.45           \t|             -             \t|                -                \t|             -             \t|             74.13             \t|          72.89         \t|\n\nNote:\n- NDCG@10 and MRR@10 measure ranking quality, with higher scores indicating better search results\n- recall@3 measures the proportion of relevant documents retrieved, with higher scores indicating better search results",
    "card_content": "---\npipeline_tag: text-classification\ntags:\n- transformers\n- reranker\n- cross-encoder\n- transformers.js\nlanguage:\n- multilingual\ninference: false\nlicense: cc-by-nc-4.0\nlibrary_name: transformers\n---\n\n<br><br>\n\n<p align=\"center\">\n<img src=\"https://huggingface.co/datasets/jinaai/documentation-images/resolve/main/logo.webp\" alt=\"Jina AI: Your Search Foundation, Supercharged!\" width=\"150px\">\n</p>\n\n<p align=\"center\">\n<b>Trained by <a href=\"https://jina.ai/\"><b>Jina AI</b></a>.</b>\n</p>\n\n# jina-reranker-v2-base-multilingual\n\n## Intended Usage & Model Info\n\nThe **Jina Reranker v2** (`jina-reranker-v2-base-multilingual`) is a transformer-based model that has been fine-tuned for text reranking task, which is a crucial component in many information retrieval systems. It is a cross-encoder model that takes a query and a document pair as input and outputs a score indicating the relevance of the document to the query. The model is trained on a large dataset of query-document pairs and is capable of reranking documents in multiple languages with high accuracy.\n\nCompared with the state-of-the-art reranker models, including the previous released `jina-reranker-v1-base-en`, the **Jina Reranker v2** model has demonstrated competitiveness across a series of benchmarks targeting for text retrieval, multilingual capability, function-calling-aware and text-to-SQL-aware reranking, and code retrieval tasks.\n\nThe `jina-reranker-v2-base-multilingual` model is capable of handling long texts with a context length of up to `1024` tokens, enabling the processing of extensive inputs. To enable the model to handle long texts that exceed 1024 tokens, the model uses a sliding window approach to chunk the input text into smaller pieces and rerank each chunk separately.\n\nThe model is also equipped with a flash attention mechanism, which significantly improves the model's performance.\n\n\n# Usage\n\n_This model repository is licenced for research and evaluation purposes under CC-BY-NC-4.0. For commercial usage, please refer to Jina AI's APIs, AWS Sagemaker or Azure Marketplace offerings. Please [contact us](https://jina.ai/contact-sales) for any further clarifications._\n1. The easiest way to use `jina-reranker-v2-base-multilingual` is to call Jina AI's [Reranker API](https://jina.ai/reranker/).\n\n```bash\ncurl https://api.jina.ai/v1/rerank \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer YOUR_API_KEY\" \\\n  -d '{\n  \"model\": \"jina-reranker-v2-base-multilingual\",\n  \"query\": \"Organic skincare products for sensitive skin\",\n  \"documents\": [\n    \"Organic skincare for sensitive skin with aloe vera and chamomile.\",\n    \"New makeup trends focus on bold colors and innovative techniques\",\n    \"Bio-Hautpflege für empfindliche Haut mit Aloe Vera und Kamille\",\n    \"Neue Make-up-Trends setzen auf kräftige Farben und innovative Techniken\",\n    \"Cuidado de la piel orgánico para piel sensible con aloe vera y manzanilla\",\n    \"Las nuevas tendencias de maquillaje se centran en colores vivos y técnicas innovadoras\",\n    \"针对敏感肌专门设计的天然有机护肤产品\",\n    \"新的化妆趋势注重鲜艳的颜色和创新的技巧\",\n    \"敏感肌のために特別に設計された天然有機スキンケア製品\",\n    \"新しいメイクのトレンドは鮮やかな色と革新的な技術に焦点を当てています\"\n  ],\n  \"top_n\": 3\n}'\n```\n\n2. You can also use the `transformers` library to interact with the model programmatically.\n\nBefore you start, install the `transformers` and `einops` libraries:\n\n```bash\npip install transformers einops\n```\n\nAnd then:\n```python\nfrom transformers import AutoModelForSequenceClassification\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    'jinaai/jina-reranker-v2-base-multilingual',\n    torch_dtype=\"auto\",\n    trust_remote_code=True,\n)\n\nmodel.to('cuda') # or 'cpu' if no GPU is available\nmodel.eval()\n\n# Example query and documents\nquery = \"Organic skincare products for sensitive skin\"\ndocuments = [\n    \"Organic skincare for sensitive skin with aloe vera and chamomile.\",\n    \"New makeup trends focus on bold colors and innovative techniques\",\n    \"Bio-Hautpflege für empfindliche Haut mit Aloe Vera und Kamille\",\n    \"Neue Make-up-Trends setzen auf kräftige Farben und innovative Techniken\",\n    \"Cuidado de la piel orgánico para piel sensible con aloe vera y manzanilla\",\n    \"Las nuevas tendencias de maquillaje se centran en colores vivos y técnicas innovadoras\",\n    \"针对敏感肌专门设计的天然有机护肤产品\",\n    \"新的化妆趋势注重鲜艳的颜色和创新的技巧\",\n    \"敏感肌のために特別に設計された天然有機スキンケア製品\",\n    \"新しいメイクのトレンドは鮮やかな色と革新的な技術に焦点を当てています\",\n]\n\n# construct sentence pairs\nsentence_pairs = [[query, doc] for doc in documents]\n\nscores = model.compute_score(sentence_pairs, max_length=1024)\n```\n\nThe scores will be a list of floats, where each float represents the relevance score of the corresponding document to the query. Higher scores indicate higher relevance.\nFor instance the returning scores in this case will be:\n```bash\n[0.8311430811882019, 0.09401018172502518,\n 0.6334102749824524, 0.08269733935594559,\n 0.7620701193809509, 0.09947021305561066,\n 0.9263036847114563, 0.05834583938121796,\n 0.8418256044387817, 0.11124119907617569]\n```\n\nThe model gives high relevance scores to the documents that are most relevant to the query regardless of the language of the document.\n\nNote that by default, the `jina-reranker-v2-base-multilingual` model uses [flash attention](https://github.com/Dao-AILab/flash-attention), which requires certain types of GPU hardware to run.\nIf you encounter any issues, you can try call `AutoModelForSequenceClassification.from_pretrained()` with `use_flash_attn=False`.\nThis will use the standard attention mechanism instead of flash attention.\n\nIf you want to use flash attention for fast inference, you need to install the following packages:\n```bash\npip install ninja # required for flash attention\npip install flash-attn --no-build-isolation\n```\nEnjoy the 3x-6x speedup with flash attention! ⚡️⚡️⚡️\n\n\n3. You can also use the `transformers.js` library to run the model directly in JavaScript (in-browser, Node.js, Deno, etc.)!\n\nIf you haven't already, you can install the [Transformers.js](https://huggingface.co/docs/transformers.js) JavaScript library (v3) using:\n```bash\nnpm i xenova/transformers.js#v3\n```\n\nThen, you can use the following code to interact with the model:\n```js\nimport { AutoTokenizer, XLMRobertaModel } from '@xenova/transformers';\n\nconst model_id = 'jinaai/jina-reranker-v2-base-multilingual';\nconst model = await XLMRobertaModel.from_pretrained(model_id, { dtype: 'fp32' });\nconst tokenizer = await AutoTokenizer.from_pretrained(model_id);\n\n/**\n * Performs ranking with the CrossEncoder on the given query and documents. Returns a sorted list with the document indices and scores.\n * @param {string} query A single query\n * @param {string[]} documents A list of documents\n * @param {Object} options Options for ranking\n * @param {number} [options.top_k=undefined] Return the top-k documents. If undefined, all documents are returned.\n * @param {number} [options.return_documents=false] If true, also returns the documents. If false, only returns the indices and scores.\n */\nasync function rank(query, documents, {\n    top_k = undefined,\n    return_documents = false,\n} = {}) {\n    const inputs = tokenizer(\n        new Array(documents.length).fill(query),\n        { text_pair: documents, padding: true, truncation: true }\n    )\n    const { logits } = await model(inputs);\n    return logits.sigmoid().tolist()\n        .map(([score], i) => ({\n            corpus_id: i,\n            score,\n            ...(return_documents ? { text: documents[i] } : {})\n        })).sort((a, b) => b.score - a.score).slice(0, top_k);\n}\n\n// Example usage:\nconst query = \"Organic skincare products for sensitive skin\"\nconst documents = [\n    \"Organic skincare for sensitive skin with aloe vera and chamomile.\",\n    \"New makeup trends focus on bold colors and innovative techniques\",\n    \"Bio-Hautpflege für empfindliche Haut mit Aloe Vera und Kamille\",\n    \"Neue Make-up-Trends setzen auf kräftige Farben und innovative Techniken\",\n    \"Cuidado de la piel orgánico para piel sensible con aloe vera y manzanilla\",\n    \"Las nuevas tendencias de maquillaje se centran en colores vivos y técnicas innovadoras\",\n    \"针对敏感肌专门设计的天然有机护肤产品\",\n    \"新的化妆趋势注重鲜艳的颜色和创新的技巧\",\n    \"敏感肌のために特別に設計された天然有機スキンケア製品\",\n    \"新しいメイクのトレンドは鮮やかな色と革新的な技術に焦点を当てています\",\n]\n\nconst results = await rank(query, documents, { return_documents: true, top_k: 3 });\nconsole.log(results);\n```\n\n\nThat's it! You can now use the `jina-reranker-v2-base-multilingual` model in your projects.\n\n\nIn addition to the `compute_score()` function, the `jina-reranker-v2-base-multilingual` model also provides a `model.rerank()` function that can be used to rerank documents based on a query. You can use it as follows:\n\n```python\nresult = model.rerank(\n    query,\n    documents,\n    max_query_length=512,\n    max_length=1024,\n    top_n=3\n)\n```\n\nInside the `result` object, you will find the reranked documents along with their scores. You can use this information to further process the documents as needed.\n\nThe `rerank()` function will automatically chunk the input documents into smaller pieces if they exceed the model's maximum input length. This allows you to rerank long documents without running into memory issues.\nSpecifically, the `rerank()` function will split the documents into chunks of size `max_length` and rerank each chunk separately. The scores from all the chunks are then combined to produce the final reranking results. You can control the query length and document length in each chunk by setting the `max_query_length` and `max_length` parameters. The `rerank()` function also supports the `overlap` parameter (default is `80`) which determines how much overlap there is between adjacent chunks. This can be useful when reranking long documents to ensure that the model has enough context to make accurate predictions.\n\n3. Alternatively, `jina-reranker-v2-base-multilingual` has been integrated with `CrossEncoder` from the `sentence-transformers` library.\n\nBefore you start, install the `sentence-transformers` libraries:\n\n```bash\npip install sentence-transformers\n```\n\nThe [`CrossEncoder`](https://sbert.net/docs/package_reference/cross_encoder/cross_encoder.html) class supports a [`predict`](https://sbert.net/docs/package_reference/cross_encoder/cross_encoder.html#sentence_transformers.cross_encoder.CrossEncoder.predict) method to get query-document relevance scores, and a [`rank`](https://sbert.net/docs/package_reference/cross_encoder/cross_encoder.html#sentence_transformers.cross_encoder.CrossEncoder.rank) method to rank all documents given your query.\n\n```python\nfrom sentence_transformers import CrossEncoder\n\nmodel = CrossEncoder(\n    \"jinaai/jina-reranker-v2-base-multilingual\",\n    automodel_args={\"torch_dtype\": \"auto\"},\n    trust_remote_code=True,\n)\n\n# Example query and documents\nquery = \"Organic skincare products for sensitive skin\"\ndocuments = [\n    \"Organic skincare for sensitive skin with aloe vera and chamomile.\",\n    \"New makeup trends focus on bold colors and innovative techniques\",\n    \"Bio-Hautpflege für empfindliche Haut mit Aloe Vera und Kamille\",\n    \"Neue Make-up-Trends setzen auf kräftige Farben und innovative Techniken\",\n    \"Cuidado de la piel orgánico para piel sensible con aloe vera y manzanilla\",\n    \"Las nuevas tendencias de maquillaje se centran en colores vivos y técnicas innovadoras\",\n    \"针对敏感肌专门设计的天然有机护肤产品\",\n    \"新的化妆趋势注重鲜艳的颜色和创新的技巧\",\n    \"敏感肌のために特別に設計された天然有機スキンケア製品\",\n    \"新しいメイクのトレンドは鮮やかな色と革新的な技術に焦点を当てています\",\n]\n\n# construct sentence pairs\nsentence_pairs = [[query, doc] for doc in documents]\n\nscores = model.predict(sentence_pairs, convert_to_tensor=True).tolist()\n\"\"\"\n[0.828125, 0.0927734375, 0.6328125, 0.08251953125, 0.76171875, 0.099609375, 0.92578125, 0.058349609375, 0.84375, 0.111328125]\n\"\"\"\n\nrankings = model.rank(query, documents, return_documents=True, convert_to_tensor=True)\nprint(f\"Query: {query}\")\nfor ranking in rankings:\n    print(f\"ID: {ranking['corpus_id']}, Score: {ranking['score']:.4f}, Text: {ranking['text']}\")\n\"\"\"\nQuery: Organic skincare products for sensitive skin\nID: 6, Score: 0.9258, Text: 针对敏感肌专门设计的天然有机护肤产品\nID: 8, Score: 0.8438, Text: 敏感肌のために特別に設計された天然有機スキンケア製品\nID: 0, Score: 0.8281, Text: Organic skincare for sensitive skin with aloe vera and chamomile.\nID: 4, Score: 0.7617, Text: Cuidado de la piel orgánico para piel sensible con aloe vera y manzanilla\nID: 2, Score: 0.6328, Text: Bio-Hautpflege für empfindliche Haut mit Aloe Vera und Kamille\nID: 9, Score: 0.1113, Text: 新しいメイクのトレンドは鮮やかな色と革新的な技術に焦点を当てています\nID: 5, Score: 0.0996, Text: Las nuevas tendencias de maquillaje se centran en colores vivos y técnicas innovadoras\nID: 1, Score: 0.0928, Text: New makeup trends focus on bold colors and innovative techniques\nID: 3, Score: 0.0825, Text: Neue Make-up-Trends setzen auf kräftige Farben und innovative Techniken\nID: 7, Score: 0.0583, Text: 新的化妆趋势注重鲜艳的颜色和创新的技巧\n\"\"\"\n```\n\n# Evaluation\n\nWe evaluated Jina Reranker v2 on multiple benchmarks to ensure top-tier performance and search relevance.\n\n|           Model Name            |   Model Size | MKQA(nDCG@10, 26 langs) \t| BEIR(nDCG@10, 17 datasets) \t| MLDR(recall@10, 13 langs) | CodeSearchNet (MRR@10, 3 tasks) \t| AirBench (nDCG@10, zh/en) \t| ToolBench (recall@3, 3 tasks) \t| TableSearch (recall@3) \t|\n| :-----------------------------: | :----------: | ------------------------- | ---------------------------- | --------------------------- | --------------------------------- | --------------------------- | ------------------------------- | ------------------------ |\n| jina-reranker-v2-multilingual \t|    278M    \t|          54.83          \t|            53.17           \t|           68.95           \t|              71.36              \t|           61.33           \t|             77.75             \t|          93.31         \t|\n|       bge-reranker-v2-m3      \t|    568M    \t|          54.17          \t|            53.65           \t|           59.73           \t|              62.86              \t|           61.28           \t|             78.46             \t|          74.86         \t|\n|  mmarco-mMiniLMv2-L12-H384-v1 \t|    118M    \t|          53.37          \t|            45.40           \t|           28.91           \t|              51.78              \t|           56.46           \t|             58.39             \t|          53.60         \t|\n|    jina-reranker-v1-base-en   \t|    137M    \t|            -            \t|            52.45           \t|             -             \t|                -                \t|             -             \t|             74.13             \t|          72.89         \t|\n\nNote:\n- NDCG@10 and MRR@10 measure ranking quality, with higher scores indicating better search results\n- recall@3 measures the proportion of relevant documents retrieved, with higher scores indicating better search results",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModelForSequenceClassification",
      "custom_class": null,
      "pipeline_tag": "text-classification",
      "processor": null
    },
    "safetensors": {
      "parameters": {
        "BF16": 278437633
      },
      "total": 278437633
    },
    "model_index": null,
    "trending_score": null
  },
  {
    "model_id": "WebOrganizer/TopicClassifier-NoURL",
    "model_name": "WebOrganizer/TopicClassifier-NoURL",
    "author": "WebOrganizer",
    "downloads": 357603,
    "downloads_all_time": null,
    "likes": 4,
    "tags": [
      "transformers",
      "safetensors",
      "new",
      "text-classification",
      "custom_code",
      "dataset:WebOrganizer/TopicAnnotations-Llama-3.1-8B",
      "dataset:WebOrganizer/TopicAnnotations-Llama-3.1-405B-FP8",
      "arxiv:2502.10341",
      "base_model:Alibaba-NLP/gte-base-en-v1.5",
      "base_model:finetune:Alibaba-NLP/gte-base-en-v1.5",
      "autotrain_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/WebOrganizer/TopicClassifier-NoURL",
    "dependencies": [
      [
        "transformers",
        null
      ],
      [
        "xformers",
        null
      ],
      [
        "torch",
        null
      ]
    ],
    "last_modified": "2025-02-19T17:22:19+00:00",
    "created_at": "2025-02-10T20:43:29+00:00",
    "analysis_date": "2025-03-22T00:55:44.071855",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "new",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "library_name": "transformers",
      "datasets": [
        "WebOrganizer/TopicAnnotations-Llama-3.1-8B",
        "WebOrganizer/TopicAnnotations-Llama-3.1-405B-FP8"
      ],
      "base_model": [
        "Alibaba-NLP/gte-base-en-v1.5"
      ]
    },
    "card_text": "# WebOrganizer/TopicClassifier-NoURL\n\n[[Paper](https://arxiv.org/abs/2502.10341)] [[Website](https://weborganizer.allenai.org)] [[GitHub](https://github.com/CodeCreator/WebOrganizer)]\n\nThe TopicClassifier-NoURL organizes web content into 17 categories based on the text contents of web pages (without using URL information).\nThe model is a [gte-base-en-v1.5](https://huggingface.co/Alibaba-NLP/gte-base-en-v1.5) with 140M parameters fine-tuned on the following training data:\n1. [WebOrganizer/TopicAnnotations-Llama-3.1-8B](https://huggingface.co/datasets/WebOrganizer/TopicAnnotations-Llama-3.1-8B): 1M documents annotated by Llama-3.1-8B (first-stage training)\n2. [WebOrganizer/TopicAnnotations-Llama-3.1-405B-FP8](https://huggingface.co/datasets/WebOrganizer/TopicAnnotations-Llama-3.1-405B-FP8): 100K documents annotated by Llama-3.1-405B-FP8 (second-stage training)\n\n#### All Domain Classifiers\n- [WebOrganizer/FormatClassifier](https://huggingface.co/WebOrganizer/FormatClassifier)\n- [WebOrganizer/FormatClassifier-NoURL](https://huggingface.co/WebOrganizer/FormatClassifier-NoURL)\n- [WebOrganizer/TopicClassifier](https://huggingface.co/WebOrganizer/TopicClassifier)\n- [WebOrganizer/TopicClassifier-NoURL](https://huggingface.co/WebOrganizer/TopicClassifier-NoURL) *← you are here!*\n\n## Usage\n\nThis classifier expects input in the following format:\n```\n{text}\n```\n\nExample:\n```python\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\ntokenizer = AutoTokenizer.from_pretrained(\"WebOrganizer/TopicClassifier-NoURL\")\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"WebOrganizer/TopicClassifier-NoURL\",\n    trust_remote_code=True,\n    use_memory_efficient_attention=False)\n\nweb_page = \"\"\"How to build a computer from scratch? Here are the components you need...\"\"\"\n\ninputs = tokenizer([web_page], return_tensors=\"pt\")\noutputs = model(**inputs)\n\nprobs = outputs.logits.softmax(dim=-1)\nprint(probs.argmax(dim=-1))\n# -> 5 (\"Hardware\" topic)\n```\n\nYou can convert the `logits` of the model with a softmax to obtain a probability distribution over the following 24 categories (in order of labels, also see `id2label` and `label2id` in the model config):\n1. Adult\n2. Art & Design\n3. Software Dev.\n4. Crime & Law\n5. Education & Jobs\n6. Hardware\n7. Entertainment\n8. Social Life\n9. Fashion & Beauty\n10. Finance & Business\n11. Food & Dining\n12. Games\n13. Health\n14. History\n15. Home & Hobbies\n16. Industrial\n17. Literature\n18. Politics\n19. Religion\n20. Science & Tech.\n21. Software\n22. Sports & Fitness\n23. Transportation\n24. Travel\n\nThe full definitions of the categories can be found in the [taxonomy config](https://github.com/CodeCreator/WebOrganizer/blob/main/define_domains/taxonomies/topics.yaml).\n\n#### Efficient Inference\nWe recommend that you use the efficient gte-base-en-v1.5 implementation by enabling unpadding and memory efficient attention. This __requires installing `xformers`__ (see more [here](https://huggingface.co/Alibaba-NLP/new-impl#recommendation-enable-unpadding-and-acceleration-with-xformers)) and loading the model like:\n```python\nAutoModelForSequenceClassification.from_pretrained(\n    \"WebOrganizer/TopicClassifier-NoURL\",\n    trust_remote_code=True,\n    unpad_inputs=True,\n    use_memory_efficient_attention=True,\n    torch_dtype=torch.bfloat16\n)\n```\n\n## Citation\n```bibtex\n@article{wettig2025organize,\n  title={Organize the Web: Constructing Domains Enhances Pre-Training Data Curation},\n  author={Alexander Wettig and Kyle Lo and Sewon Min and Hannaneh Hajishirzi and Danqi Chen and Luca Soldaini},\n  journal={arXiv preprint arXiv:2502.10341},\n  year={2025}\n}\n```",
    "card_content": "---\nlibrary_name: transformers\ndatasets:\n- WebOrganizer/TopicAnnotations-Llama-3.1-8B\n- WebOrganizer/TopicAnnotations-Llama-3.1-405B-FP8\nbase_model:\n- Alibaba-NLP/gte-base-en-v1.5\n---\n# WebOrganizer/TopicClassifier-NoURL\n\n[[Paper](https://arxiv.org/abs/2502.10341)] [[Website](https://weborganizer.allenai.org)] [[GitHub](https://github.com/CodeCreator/WebOrganizer)]\n\nThe TopicClassifier-NoURL organizes web content into 17 categories based on the text contents of web pages (without using URL information).\nThe model is a [gte-base-en-v1.5](https://huggingface.co/Alibaba-NLP/gte-base-en-v1.5) with 140M parameters fine-tuned on the following training data:\n1. [WebOrganizer/TopicAnnotations-Llama-3.1-8B](https://huggingface.co/datasets/WebOrganizer/TopicAnnotations-Llama-3.1-8B): 1M documents annotated by Llama-3.1-8B (first-stage training)\n2. [WebOrganizer/TopicAnnotations-Llama-3.1-405B-FP8](https://huggingface.co/datasets/WebOrganizer/TopicAnnotations-Llama-3.1-405B-FP8): 100K documents annotated by Llama-3.1-405B-FP8 (second-stage training)\n\n#### All Domain Classifiers\n- [WebOrganizer/FormatClassifier](https://huggingface.co/WebOrganizer/FormatClassifier)\n- [WebOrganizer/FormatClassifier-NoURL](https://huggingface.co/WebOrganizer/FormatClassifier-NoURL)\n- [WebOrganizer/TopicClassifier](https://huggingface.co/WebOrganizer/TopicClassifier)\n- [WebOrganizer/TopicClassifier-NoURL](https://huggingface.co/WebOrganizer/TopicClassifier-NoURL) *← you are here!*\n\n## Usage\n\nThis classifier expects input in the following format:\n```\n{text}\n```\n\nExample:\n```python\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\ntokenizer = AutoTokenizer.from_pretrained(\"WebOrganizer/TopicClassifier-NoURL\")\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"WebOrganizer/TopicClassifier-NoURL\",\n    trust_remote_code=True,\n    use_memory_efficient_attention=False)\n\nweb_page = \"\"\"How to build a computer from scratch? Here are the components you need...\"\"\"\n\ninputs = tokenizer([web_page], return_tensors=\"pt\")\noutputs = model(**inputs)\n\nprobs = outputs.logits.softmax(dim=-1)\nprint(probs.argmax(dim=-1))\n# -> 5 (\"Hardware\" topic)\n```\n\nYou can convert the `logits` of the model with a softmax to obtain a probability distribution over the following 24 categories (in order of labels, also see `id2label` and `label2id` in the model config):\n1. Adult\n2. Art & Design\n3. Software Dev.\n4. Crime & Law\n5. Education & Jobs\n6. Hardware\n7. Entertainment\n8. Social Life\n9. Fashion & Beauty\n10. Finance & Business\n11. Food & Dining\n12. Games\n13. Health\n14. History\n15. Home & Hobbies\n16. Industrial\n17. Literature\n18. Politics\n19. Religion\n20. Science & Tech.\n21. Software\n22. Sports & Fitness\n23. Transportation\n24. Travel\n\nThe full definitions of the categories can be found in the [taxonomy config](https://github.com/CodeCreator/WebOrganizer/blob/main/define_domains/taxonomies/topics.yaml).\n\n#### Efficient Inference\nWe recommend that you use the efficient gte-base-en-v1.5 implementation by enabling unpadding and memory efficient attention. This __requires installing `xformers`__ (see more [here](https://huggingface.co/Alibaba-NLP/new-impl#recommendation-enable-unpadding-and-acceleration-with-xformers)) and loading the model like:\n```python\nAutoModelForSequenceClassification.from_pretrained(\n    \"WebOrganizer/TopicClassifier-NoURL\",\n    trust_remote_code=True,\n    unpad_inputs=True,\n    use_memory_efficient_attention=True,\n    torch_dtype=torch.bfloat16\n)\n```\n\n## Citation\n```bibtex\n@article{wettig2025organize,\n  title={Organize the Web: Constructing Domains Enhances Pre-Training Data Curation},\n  author={Alexander Wettig and Kyle Lo and Sewon Min and Hannaneh Hajishirzi and Danqi Chen and Luca Soldaini},\n  journal={arXiv preprint arXiv:2502.10341},\n  year={2025}\n}\n```",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModelForSequenceClassification",
      "custom_class": "modeling.NewForSequenceClassification",
      "pipeline_tag": "text-classification",
      "processor": null
    },
    "safetensors": {
      "parameters": {
        "F32": 137385240
      },
      "total": 137385240
    },
    "model_index": null,
    "trending_score": null
  },
  {
    "model_id": "cross-encoder/ms-marco-TinyBERT-L2-v2",
    "model_name": "cross-encoder/ms-marco-TinyBERT-L2-v2",
    "author": "cross-encoder",
    "downloads": 346454,
    "downloads_all_time": null,
    "likes": 19,
    "tags": [
      "transformers",
      "pytorch",
      "jax",
      "safetensors",
      "bert",
      "text-classification",
      "license:apache-2.0",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/cross-encoder/ms-marco-TinyBERT-L2-v2",
    "dependencies": [
      [
        "transformers",
        null
      ],
      [
        "torch",
        null
      ],
      [
        "sentence_transformers",
        null
      ]
    ],
    "last_modified": "2025-03-06T12:21:46+00:00",
    "created_at": "2022-03-02T23:29:05+00:00",
    "analysis_date": "2025-03-22T00:55:45.403403",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "bert",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "license": "apache-2.0"
    },
    "card_text": "# Cross-Encoder for MS Marco\n\nThis model was trained on the [MS Marco Passage Ranking](https://github.com/microsoft/MSMARCO-Passage-Ranking) task.\n\nThe model can be used for Information Retrieval: Given a query, encode the query will all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. See [SBERT.net Retrieve & Re-rank](https://www.sbert.net/examples/applications/retrieve_rerank/README.html) for more details. The training code is available here: [SBERT.net Training MS Marco](https://github.com/UKPLab/sentence-transformers/tree/master/examples/training/ms_marco)\n\n\n## Usage with Transformers\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\n\nmodel = AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-TinyBERT-L2-v2')\ntokenizer = AutoTokenizer.from_pretrained('cross-encoder/ms-marco-TinyBERT-L2-v2')\n\nfeatures = tokenizer(['How many people live in Berlin?', 'How many people live in Berlin?'], ['Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.', 'New York City is famous for the Metropolitan Museum of Art.'],  padding=True, truncation=True, return_tensors=\"pt\")\n\nmodel.eval()\nwith torch.no_grad():\n    scores = model(**features).logits\n    print(scores)\n```\n\n\n## Usage with SentenceTransformers\n\nThe usage becomes easier when you have [SentenceTransformers](https://www.sbert.net/) installed. Then, you can use the pre-trained models like this:\n```python\nfrom sentence_transformers import CrossEncoder\n\nmodel = CrossEncoder('cross-encoder/ms-marco-TinyBERT-L2-v2', max_length=512)\nscores = model.predict([('Query', 'Paragraph1'), ('Query', 'Paragraph2') , ('Query', 'Paragraph3')])\n```\n\n\n## Performance\nIn the following table, we provide various pre-trained Cross-Encoders together with their performance on the [TREC Deep Learning 2019](https://microsoft.github.io/TREC-2019-Deep-Learning/) and the [MS Marco Passage Reranking](https://github.com/microsoft/MSMARCO-Passage-Ranking/) dataset. \n\n\n| Model-Name        | NDCG@10 (TREC DL 19) | MRR@10 (MS Marco Dev)  | Docs / Sec |\n| ------------- |:-------------| -----| --- | \n| **Version 2 models** | | | \n| cross-encoder/ms-marco-TinyBERT-L2-v2 | 69.84 | 32.56 | 9000\n| cross-encoder/ms-marco-MiniLM-L2-v2 | 71.01 | 34.85 | 4100\n| cross-encoder/ms-marco-MiniLM-L4-v2 | 73.04 | 37.70 | 2500\n| cross-encoder/ms-marco-MiniLM-L6-v2 | 74.30 | 39.01 | 1800\n| cross-encoder/ms-marco-MiniLM-L12-v2 | 74.31 | 39.02 | 960\n| **Version 1 models** | | | \n| cross-encoder/ms-marco-TinyBERT-L2  | 67.43 | 30.15  | 9000\n| cross-encoder/ms-marco-TinyBERT-L4  | 68.09 | 34.50  | 2900\n| cross-encoder/ms-marco-TinyBERT-L6 |  69.57 | 36.13  | 680\n| cross-encoder/ms-marco-electra-base | 71.99 | 36.41 | 340\n| **Other models** | | | \n| nboost/pt-tinybert-msmarco | 63.63 | 28.80 | 2900 \n| nboost/pt-bert-base-uncased-msmarco | 70.94 | 34.75 | 340 \n| nboost/pt-bert-large-msmarco | 73.36 | 36.48 | 100 \n| Capreolus/electra-base-msmarco | 71.23 | 36.89 | 340 \n| amberoad/bert-multilingual-passage-reranking-msmarco | 68.40 | 35.54 | 330 \n| sebastian-hofstaetter/distilbert-cat-margin_mse-T2-msmarco | 72.82 | 37.88 | 720\n \n Note: Runtime was computed on a V100 GPU.\n",
    "card_content": "---\nlicense: apache-2.0\n---\n# Cross-Encoder for MS Marco\n\nThis model was trained on the [MS Marco Passage Ranking](https://github.com/microsoft/MSMARCO-Passage-Ranking) task.\n\nThe model can be used for Information Retrieval: Given a query, encode the query will all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. See [SBERT.net Retrieve & Re-rank](https://www.sbert.net/examples/applications/retrieve_rerank/README.html) for more details. The training code is available here: [SBERT.net Training MS Marco](https://github.com/UKPLab/sentence-transformers/tree/master/examples/training/ms_marco)\n\n\n## Usage with Transformers\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\n\nmodel = AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-TinyBERT-L2-v2')\ntokenizer = AutoTokenizer.from_pretrained('cross-encoder/ms-marco-TinyBERT-L2-v2')\n\nfeatures = tokenizer(['How many people live in Berlin?', 'How many people live in Berlin?'], ['Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.', 'New York City is famous for the Metropolitan Museum of Art.'],  padding=True, truncation=True, return_tensors=\"pt\")\n\nmodel.eval()\nwith torch.no_grad():\n    scores = model(**features).logits\n    print(scores)\n```\n\n\n## Usage with SentenceTransformers\n\nThe usage becomes easier when you have [SentenceTransformers](https://www.sbert.net/) installed. Then, you can use the pre-trained models like this:\n```python\nfrom sentence_transformers import CrossEncoder\n\nmodel = CrossEncoder('cross-encoder/ms-marco-TinyBERT-L2-v2', max_length=512)\nscores = model.predict([('Query', 'Paragraph1'), ('Query', 'Paragraph2') , ('Query', 'Paragraph3')])\n```\n\n\n## Performance\nIn the following table, we provide various pre-trained Cross-Encoders together with their performance on the [TREC Deep Learning 2019](https://microsoft.github.io/TREC-2019-Deep-Learning/) and the [MS Marco Passage Reranking](https://github.com/microsoft/MSMARCO-Passage-Ranking/) dataset. \n\n\n| Model-Name        | NDCG@10 (TREC DL 19) | MRR@10 (MS Marco Dev)  | Docs / Sec |\n| ------------- |:-------------| -----| --- | \n| **Version 2 models** | | | \n| cross-encoder/ms-marco-TinyBERT-L2-v2 | 69.84 | 32.56 | 9000\n| cross-encoder/ms-marco-MiniLM-L2-v2 | 71.01 | 34.85 | 4100\n| cross-encoder/ms-marco-MiniLM-L4-v2 | 73.04 | 37.70 | 2500\n| cross-encoder/ms-marco-MiniLM-L6-v2 | 74.30 | 39.01 | 1800\n| cross-encoder/ms-marco-MiniLM-L12-v2 | 74.31 | 39.02 | 960\n| **Version 1 models** | | | \n| cross-encoder/ms-marco-TinyBERT-L2  | 67.43 | 30.15  | 9000\n| cross-encoder/ms-marco-TinyBERT-L4  | 68.09 | 34.50  | 2900\n| cross-encoder/ms-marco-TinyBERT-L6 |  69.57 | 36.13  | 680\n| cross-encoder/ms-marco-electra-base | 71.99 | 36.41 | 340\n| **Other models** | | | \n| nboost/pt-tinybert-msmarco | 63.63 | 28.80 | 2900 \n| nboost/pt-bert-base-uncased-msmarco | 70.94 | 34.75 | 340 \n| nboost/pt-bert-large-msmarco | 73.36 | 36.48 | 100 \n| Capreolus/electra-base-msmarco | 71.23 | 36.89 | 340 \n| amberoad/bert-multilingual-passage-reranking-msmarco | 68.40 | 35.54 | 330 \n| sebastian-hofstaetter/distilbert-cat-margin_mse-T2-msmarco | 72.82 | 37.88 | 720\n \n Note: Runtime was computed on a V100 GPU.\n",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModelForSequenceClassification",
      "custom_class": null,
      "pipeline_tag": "text-classification",
      "processor": "AutoTokenizer"
    },
    "safetensors": {
      "parameters": {
        "I64": 512,
        "F32": 4386049
      },
      "total": 4386561
    },
    "model_index": null,
    "trending_score": null
  },
  {
    "model_id": "Alibaba-NLP/gte-multilingual-reranker-base",
    "model_name": "Alibaba-NLP/gte-multilingual-reranker-base",
    "author": "Alibaba-NLP",
    "downloads": 309474,
    "downloads_all_time": null,
    "likes": 111,
    "tags": [
      "sentence-transformers",
      "safetensors",
      "new",
      "text-classification",
      "transformers",
      "text-embeddings-inference",
      "custom_code",
      "af",
      "ar",
      "az",
      "be",
      "bg",
      "bn",
      "ca",
      "ceb",
      "cs",
      "cy",
      "da",
      "de",
      "el",
      "en",
      "es",
      "et",
      "eu",
      "fa",
      "fi",
      "fr",
      "gl",
      "gu",
      "he",
      "hi",
      "hr",
      "ht",
      "hu",
      "hy",
      "id",
      "is",
      "it",
      "ja",
      "jv",
      "ka",
      "kk",
      "km",
      "kn",
      "ko",
      "ky",
      "lo",
      "lt",
      "lv",
      "mk",
      "ml",
      "mn",
      "mr",
      "ms",
      "my",
      "ne",
      "nl",
      "no",
      "pa",
      "pl",
      "pt",
      "qu",
      "ro",
      "ru",
      "si",
      "sk",
      "sl",
      "so",
      "sq",
      "sr",
      "sv",
      "sw",
      "ta",
      "te",
      "th",
      "tl",
      "tr",
      "uk",
      "ur",
      "vi",
      "yo",
      "zh",
      "arxiv:2407.19669",
      "license:apache-2.0",
      "region:us"
    ],
    "card_url": "https://huggingface.co/Alibaba-NLP/gte-multilingual-reranker-base",
    "dependencies": [
      [
        "transformers",
        "4.36.0"
      ]
    ],
    "last_modified": "2025-01-09T05:57:35+00:00",
    "created_at": "2024-07-20T08:38:18+00:00",
    "analysis_date": "2025-03-22T00:55:46.636946",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "new",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "license": "apache-2.0",
      "pipeline_tag": "text-classification",
      "tags": [
        "transformers",
        "sentence-transformers",
        "text-embeddings-inference"
      ],
      "language": [
        "af",
        "ar",
        "az",
        "be",
        "bg",
        "bn",
        "ca",
        "ceb",
        "cs",
        "cy",
        "da",
        "de",
        "el",
        "en",
        "es",
        "et",
        "eu",
        "fa",
        "fi",
        "fr",
        "gl",
        "gu",
        "he",
        "hi",
        "hr",
        "ht",
        "hu",
        "hy",
        "id",
        "is",
        "it",
        "ja",
        "jv",
        "ka",
        "kk",
        "km",
        "kn",
        "ko",
        "ky",
        "lo",
        "lt",
        "lv",
        "mk",
        "ml",
        "mn",
        "mr",
        "ms",
        "my",
        "ne",
        "nl",
        "no",
        "pa",
        "pl",
        "pt",
        "qu",
        "ro",
        "ru",
        "si",
        "sk",
        "sl",
        "so",
        "sq",
        "sr",
        "sv",
        "sw",
        "ta",
        "te",
        "th",
        "tl",
        "tr",
        "uk",
        "ur",
        "vi",
        "yo",
        "zh"
      ]
    },
    "card_text": "\n## gte-multilingual-reranker-base\n\nThe **gte-multilingual-reranker-base** model is the first reranker model in the [GTE](https://huggingface.co/collections/Alibaba-NLP/gte-models-6680f0b13f885cb431e6d469) family of models, featuring several key attributes:\n- **High Performance**: Achieves state-of-the-art (SOTA) results in multilingual retrieval tasks and multi-task representation model evaluations when compared to reranker models of similar size.\n- **Training Architecture**: Trained using an encoder-only transformers architecture, resulting in a smaller model size. Unlike previous models based on decode-only LLM architecture (e.g., gte-qwen2-1.5b-instruct), this model has lower hardware requirements for inference, offering a 10x increase in inference speed.\n- **Long Context**: Supports text lengths up to **8192** tokens.\n- **Multilingual Capability**: Supports over **70** languages.\n\n\n## Model Information\n- Model Size: 306M\n- Max Input Tokens: 8192\n\n\n### Usage\n- **It is recommended to install xformers and enable unpadding for acceleration,\nrefer to [enable-unpadding-and-xformers](https://huggingface.co/Alibaba-NLP/new-impl#recommendation-enable-unpadding-and-acceleration-with-xformers).**\n- **How to use it offline: [new-impl/discussions/2](https://huggingface.co/Alibaba-NLP/new-impl/discussions/2#662b08d04d8c3d0a09c88fa3)**\n\n\nUsing Huggingface transformers (transformers>=4.36.0)\n```\nimport torch\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\nmodel_name_or_path = \"Alibaba-NLP/gte-multilingual-reranker-base\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_name_or_path, trust_remote_code=True,\n    torch_dtype=torch.float16\n)\nmodel.eval()\n\npairs = [[\"中国的首都在哪儿\",\"北京\"], [\"what is the capital of China?\", \"北京\"], [\"how to implement quick sort in python?\",\"Introduction of quick sort\"]]\nwith torch.no_grad():\n    inputs = tokenizer(pairs, padding=True, truncation=True, return_tensors='pt', max_length=512)\n    scores = model(**inputs, return_dict=True).logits.view(-1, ).float()\n    print(scores)\n\n# tensor([1.2315, 0.5923, 0.3041])\n```\n\nUsage with infinity:\n\n[Infinity](https://github.com/michaelfeil/infinity), a MIT Licensed Inference RestAPI Server.\n```\ndocker run --gpus all -v $PWD/data:/app/.cache -p \"7997\":\"7997\" \\\nmichaelf34/infinity:0.0.68 \\\nv2 --model-id Alibaba-NLP/gte-multilingual-reranker-base --revision \"main\" --dtype bfloat16 --batch-size 32 --device cuda --engine torch --port 7997\n```\n\n## Evaluation\n\nResults of reranking based on multiple text retreival datasets\n\n![image](./images/mgte-reranker.png)\n\n**More detailed experimental results can be found in the [paper](https://arxiv.org/pdf/2407.19669)**.\n\n## Cloud API Services\n\nIn addition to the open-source [GTE](https://huggingface.co/collections/Alibaba-NLP/gte-models-6680f0b13f885cb431e6d469) series models, GTE series models are also available as commercial API services on Alibaba Cloud.\n\n- [Embedding Models](https://help.aliyun.com/zh/model-studio/developer-reference/general-text-embedding/): Three versions of the text embedding models are available: text-embedding-v1/v2/v3, with v3 being the latest API service.\n- [ReRank Models](https://help.aliyun.com/zh/model-studio/developer-reference/general-text-sorting-model/): The gte-rerank model service is available.\n\nNote that the models behind the commercial APIs are not entirely identical to the open-source models.\n\n\n## Citation\n\nIf you find our paper or models helpful, please consider cite:\n\n```\n@inproceedings{zhang2024mgte,\n  title={mGTE: Generalized Long-Context Text Representation and Reranking Models for Multilingual Text Retrieval},\n  author={Zhang, Xin and Zhang, Yanzhao and Long, Dingkun and Xie, Wen and Dai, Ziqi and Tang, Jialong and Lin, Huan and Yang, Baosong and Xie, Pengjun and Huang, Fei and others},\n  booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track},\n  pages={1393--1412},\n  year={2024}\n}\n```",
    "card_content": "---\nlicense: apache-2.0\npipeline_tag: text-classification\ntags:\n- transformers\n- sentence-transformers\n- text-embeddings-inference\nlanguage:\n- af\n- ar\n- az\n- be\n- bg\n- bn\n- ca\n- ceb\n- cs\n- cy\n- da\n- de\n- el\n- en\n- es\n- et\n- eu\n- fa\n- fi\n- fr\n- gl\n- gu\n- he\n- hi\n- hr\n- ht\n- hu\n- hy\n- id\n- is\n- it\n- ja\n- jv\n- ka\n- kk\n- km\n- kn\n- ko\n- ky\n- lo\n- lt\n- lv\n- mk\n- ml\n- mn\n- mr\n- ms\n- my\n- ne\n- nl\n- 'no'\n- pa\n- pl\n- pt\n- qu\n- ro\n- ru\n- si\n- sk\n- sl\n- so\n- sq\n- sr\n- sv\n- sw\n- ta\n- te\n- th\n- tl\n- tr\n- uk\n- ur\n- vi\n- yo\n- zh\n---\n\n## gte-multilingual-reranker-base\n\nThe **gte-multilingual-reranker-base** model is the first reranker model in the [GTE](https://huggingface.co/collections/Alibaba-NLP/gte-models-6680f0b13f885cb431e6d469) family of models, featuring several key attributes:\n- **High Performance**: Achieves state-of-the-art (SOTA) results in multilingual retrieval tasks and multi-task representation model evaluations when compared to reranker models of similar size.\n- **Training Architecture**: Trained using an encoder-only transformers architecture, resulting in a smaller model size. Unlike previous models based on decode-only LLM architecture (e.g., gte-qwen2-1.5b-instruct), this model has lower hardware requirements for inference, offering a 10x increase in inference speed.\n- **Long Context**: Supports text lengths up to **8192** tokens.\n- **Multilingual Capability**: Supports over **70** languages.\n\n\n## Model Information\n- Model Size: 306M\n- Max Input Tokens: 8192\n\n\n### Usage\n- **It is recommended to install xformers and enable unpadding for acceleration,\nrefer to [enable-unpadding-and-xformers](https://huggingface.co/Alibaba-NLP/new-impl#recommendation-enable-unpadding-and-acceleration-with-xformers).**\n- **How to use it offline: [new-impl/discussions/2](https://huggingface.co/Alibaba-NLP/new-impl/discussions/2#662b08d04d8c3d0a09c88fa3)**\n\n\nUsing Huggingface transformers (transformers>=4.36.0)\n```\nimport torch\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\nmodel_name_or_path = \"Alibaba-NLP/gte-multilingual-reranker-base\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_name_or_path, trust_remote_code=True,\n    torch_dtype=torch.float16\n)\nmodel.eval()\n\npairs = [[\"中国的首都在哪儿\",\"北京\"], [\"what is the capital of China?\", \"北京\"], [\"how to implement quick sort in python?\",\"Introduction of quick sort\"]]\nwith torch.no_grad():\n    inputs = tokenizer(pairs, padding=True, truncation=True, return_tensors='pt', max_length=512)\n    scores = model(**inputs, return_dict=True).logits.view(-1, ).float()\n    print(scores)\n\n# tensor([1.2315, 0.5923, 0.3041])\n```\n\nUsage with infinity:\n\n[Infinity](https://github.com/michaelfeil/infinity), a MIT Licensed Inference RestAPI Server.\n```\ndocker run --gpus all -v $PWD/data:/app/.cache -p \"7997\":\"7997\" \\\nmichaelf34/infinity:0.0.68 \\\nv2 --model-id Alibaba-NLP/gte-multilingual-reranker-base --revision \"main\" --dtype bfloat16 --batch-size 32 --device cuda --engine torch --port 7997\n```\n\n## Evaluation\n\nResults of reranking based on multiple text retreival datasets\n\n![image](./images/mgte-reranker.png)\n\n**More detailed experimental results can be found in the [paper](https://arxiv.org/pdf/2407.19669)**.\n\n## Cloud API Services\n\nIn addition to the open-source [GTE](https://huggingface.co/collections/Alibaba-NLP/gte-models-6680f0b13f885cb431e6d469) series models, GTE series models are also available as commercial API services on Alibaba Cloud.\n\n- [Embedding Models](https://help.aliyun.com/zh/model-studio/developer-reference/general-text-embedding/): Three versions of the text embedding models are available: text-embedding-v1/v2/v3, with v3 being the latest API service.\n- [ReRank Models](https://help.aliyun.com/zh/model-studio/developer-reference/general-text-sorting-model/): The gte-rerank model service is available.\n\nNote that the models behind the commercial APIs are not entirely identical to the open-source models.\n\n\n## Citation\n\nIf you find our paper or models helpful, please consider cite:\n\n```\n@inproceedings{zhang2024mgte,\n  title={mGTE: Generalized Long-Context Text Representation and Reranking Models for Multilingual Text Retrieval},\n  author={Zhang, Xin and Zhang, Yanzhao and Long, Dingkun and Xie, Wen and Dai, Ziqi and Tang, Jialong and Lin, Huan and Yang, Baosong and Xie, Pengjun and Huang, Fei and others},\n  booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track},\n  pages={1393--1412},\n  year={2024}\n}\n```",
    "library_name": "sentence-transformers",
    "transformers_info": {
      "auto_model": "AutoModelForSequenceClassification",
      "custom_class": "Alibaba-NLP/new-impl--modeling.NewForSequenceClassification",
      "pipeline_tag": "text-classification",
      "processor": null
    },
    "safetensors": {
      "parameters": {
        "F16": 305959681
      },
      "total": 305959681
    },
    "model_index": null,
    "trending_score": null
  },
  {
    "model_id": "maidalun1020/bce-reranker-base_v1",
    "model_name": "maidalun1020/bce-reranker-base_v1",
    "author": "maidalun1020",
    "downloads": 285525,
    "downloads_all_time": null,
    "likes": 185,
    "tags": [
      "sentence-transformers",
      "pytorch",
      "xlm-roberta",
      "text-classification",
      "transformers",
      "en",
      "zh",
      "ja",
      "ko",
      "license:apache-2.0",
      "region:us"
    ],
    "card_url": "https://huggingface.co/maidalun1020/bce-reranker-base_v1",
    "dependencies": null,
    "last_modified": "2024-11-25T02:16:09+00:00",
    "created_at": "2023-12-29T07:37:26+00:00",
    "analysis_date": "2025-03-22T00:55:50.828599",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "xlm-roberta",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "license": "apache-2.0",
      "pipeline_tag": "text-classification",
      "tags": [
        "transformers",
        "sentence-transformers"
      ],
      "language": [
        "en",
        "zh",
        "ja",
        "ko"
      ]
    },
    "card_text": "<!--\n * @Description: \n * @Author: shenlei\n * @Date: 2023-12-19 10:31:41\n * @LastEditTime: 2024-01-10 00:17:02\n * @LastEditors: shenlei\n-->\n<h1 align=\"center\">BCEmbedding: Bilingual and Crosslingual Embedding for RAG</h1>\n\n<p align=\"center\">\n  <a href=\"https://github.com/netease-youdao/BCEmbedding/blob/master/LICENSE\">\n    <img src=\"https://img.shields.io/badge/license-Apache--2.0-yellow\">\n  </a>\n  <a href=\"https://twitter.com/YDopensource\">\n    <img src=\"https://img.shields.io/badge/follow-%40YDOpenSource-1DA1F2?logo=twitter&style={style}\">\n  </a>\n</p>\n\n最新、最详细bce-reranker-base_v1相关信息，请移步（The latest \"Updates\" should be checked in）：\n  \n<p align=\"left\">\n  <a href=\"https://github.com/netease-youdao/BCEmbedding\">GitHub</a>\n</p>\n\n## 主要特点(Key Features)：\n- 中英日韩四个语种，以及中英日韩四个语种的跨语种能力(Multilingual and Crosslingual capability in English, Chinese, Japanese and Korean)；\n- RAG优化，适配更多真实业务场景(RAG adaptation for more domains, including Education, Law, Finance, Medical, Literature, FAQ, Textbook, Wikipedia, etc.)；\n- <a href=\"https://github.com/netease-youdao/BCEmbedding\">BCEmbedding</a>适配长文本做rerank(Handle long passages reranking more than 512 limit in <a href=\"https://github.com/netease-youdao/BCEmbedding\">BCEmbedding</a>)；\n- RerankerModel可以提供 **“绝对”分数**，低质量passage过滤阈值推荐0.35或0.4。（RerankerModel provides **\"meaningful\" (for filtering bad passages with a threshold of 0.35 or 0.4) similarity score**）\n- **最佳实践（Best practice）** ：embedding召回top50-100片段，reranker对这50-100片段精排，最后取top5-10片段。（1. Get top 50-100 passages with [bce-embedding-base_v1](https://huggingface.co/maidalun1020/bce-embedding-base_v1) for \"`recall`\"；    2. Rerank passages with [bce-reranker-base_v1](https://huggingface.co/maidalun1020/bce-reranker-base_v1) and get top 5-10 for \"`precision`\" finally. ）\n\n## News:\n- `BCEmbedding`技术博客（ **Technical Blog** ）: [为RAG而生-BCEmbedding技术报告](https://zhuanlan.zhihu.com/p/681370855)\n- Related link for **EmbeddingModel** : [bce-embedding-base_v1](https://huggingface.co/maidalun1020/bce-embedding-base_v1)\n\n## Third-party Examples:\n- RAG applications: [QAnything](https://github.com/netease-youdao/qanything), [HuixiangDou](https://github.com/InternLM/HuixiangDou), [ChatPDF](https://github.com/shibing624/ChatPDF).\n- Efficient inference framework: [ChatLLM.cpp](https://github.com/foldl/chatllm.cpp), [Xinference](https://github.com/xorbitsai/inference), [mindnlp (Huawei GPU, 华为GPU)](https://github.com/mindspore-lab/mindnlp/tree/master/llm/inference/bce).\n\n![image/jpeg](assets/rag_eval_multiple_domains_summary.jpg)\n\n![image/jpeg](assets/Wechat.jpg)\n\n-----------------------------------------\n<details open=\"open\">\n<summary>Click to Open Contents</summary>\n\n- <a href=\"#-bilingual-and-crosslingual-superiority\" target=\"_Self\">🌐 Bilingual and Crosslingual Superiority</a>\n- <a href=\"#-key-features\" target=\"_Self\">💡 Key Features</a>\n- <a href=\"#-latest-updates\" target=\"_Self\">🚀 Latest Updates</a>\n- <a href=\"#-model-list\" target=\"_Self\">🍎 Model List</a>\n- <a href=\"#-manual\" target=\"_Self\">📖 Manual</a>\n  - <a href=\"#installation\" target=\"_Self\">Installation</a>\n  - <a href=\"#quick-start\" target=\"_Self\">Quick Start (`transformers`, `sentence-transformers`)</a>\n  - <a href=\"#integrations-for-rag-frameworks\" target=\"_Self\">Integrations for RAG Frameworks (`langchain`, `llama_index`)</a>\n- <a href=\"#%EF%B8%8F-evaluation\" target=\"_Self\">⚙️ Evaluation</a>\n  - <a href=\"#evaluate-semantic-representation-by-mteb\" target=\"_Self\">Evaluate Semantic Representation by MTEB</a>\n  - <a href=\"#evaluate-rag-by-llamaindex\" target=\"_Self\">Evaluate RAG by LlamaIndex</a>\n- <a href=\"#-leaderboard\" target=\"_Self\">📈 Leaderboard</a>\n  - <a href=\"#semantic-representation-evaluations-in-mteb\" target=\"_Self\">Semantic Representation Evaluations in MTEB</a>\n  - <a href=\"#rag-evaluations-in-llamaindex\" target=\"_Self\">RAG Evaluations in LlamaIndex</a>\n- <a href=\"#-youdaos-bcembedding-api\" target=\"_Self\">🛠 Youdao's BCEmbedding API</a>\n- <a href=\"#-wechat-group\" target=\"_Self\">🧲 WeChat Group</a>\n- <a href=\"#%EF%B8%8F-citation\" target=\"_Self\">✏️ Citation</a>\n- <a href=\"#-license\" target=\"_Self\">🔐 License</a>\n- <a href=\"#-related-links\" target=\"_Self\">🔗 Related Links</a>\n\n</details>\n<br>\n\n**B**ilingual and **C**rosslingual **Embedding** (`BCEmbedding`), developed by NetEase Youdao, encompasses `EmbeddingModel` and `RerankerModel`. The `EmbeddingModel` specializes in generating semantic vectors, playing a crucial role in semantic search and question-answering, and the `RerankerModel` excels at refining search results and ranking tasks. \n\n`BCEmbedding` serves as the cornerstone of Youdao's Retrieval Augmented Generation (RAG) implmentation, notably [QAnything](http://qanything.ai) [[github](https://github.com/netease-youdao/qanything)], an open-source implementation widely integrated in various Youdao products like [Youdao Speed Reading](https://read.youdao.com/#/home) and [Youdao Translation](https://fanyi.youdao.com/download-Mac?keyfrom=fanyiweb_navigation). \n\nDistinguished for its bilingual and crosslingual proficiency, `BCEmbedding` excels in bridging Chinese and English linguistic gaps, which achieves\n- **A high performence on <a href=\"#semantic-representation-evaluations-in-mteb\">Semantic Representation Evaluations in MTEB</a>**;\n- **A new benchmark in the realm of <a href=\"#rag-evaluations-in-llamaindex\">RAG Evaluations in LlamaIndex</a>**.\n\n  `BCEmbedding`是由网易有道开发的双语和跨语种语义表征算法模型库，其中包含`EmbeddingModel`和`RerankerModel`两类基础模型。`EmbeddingModel`专门用于生成语义向量，在语义搜索和问答中起着关键作用，而`RerankerModel`擅长优化语义搜索结果和语义相关顺序精排。\n  \n  `BCEmbedding`作为有道的检索增强生成式应用（RAG）的基石，特别是在[QAnything](http://qanything.ai) [[github](https://github.com/netease-youdao/qanything)]中发挥着重要作用。QAnything作为一个网易有道开源项目，在有道许多产品中有很好的应用实践，比如[有道速读](https://read.youdao.com/#/home)和[有道翻译](https://fanyi.youdao.com/download-Mac?keyfrom=fanyiweb_navigation)\n  \n  `BCEmbedding`以其出色的双语和跨语种能力而著称，在语义检索中消除中英语言之间的差异，从而实现：\n  - **强大的双语和跨语种语义表征能力【<a href=\"#semantic-representation-evaluations-in-mteb\">基于MTEB的语义表征评测指标</a>】。**\n  - **基于LlamaIndex的RAG评测，表现SOTA【<a href=\"#rag-evaluations-in-llamaindex\">基于LlamaIndex的RAG评测指标</a>】。**\n\n## 🌐 Bilingual and Crosslingual Superiority\n\nExisting embedding models often encounter performance challenges in bilingual and crosslingual scenarios, particularly in Chinese, English and their crosslingual tasks. `BCEmbedding`, leveraging the strength of Youdao's translation engine, excels in delivering superior performance across monolingual, bilingual, and crosslingual settings.\n\n`EmbeddingModel` supports ***Chinese (ch) and English (en)*** (more languages support will come soon), while `RerankerModel` supports ***Chinese (ch), English (en), Japanese (ja) and Korean (ko)***.\n\n  现有的单个语义表征模型在双语和跨语种场景中常常表现不佳，特别是在中文、英文及其跨语种任务中。`BCEmbedding`充分利用有道翻译引擎的优势，实现只需一个模型就可以在单语、双语和跨语种场景中表现出卓越的性能。\n  \n  `EmbeddingModel`支持***中文和英文***（之后会支持更多语种）；`RerankerModel`支持***中文，英文，日文和韩文***。\n\n## 💡 Key Features\n\n- **Bilingual and Crosslingual Proficiency**: Powered by Youdao's translation engine, excelling in Chinese, English and their crosslingual retrieval task, with upcoming support for additional languages.\n\n- **RAG-Optimized**: Tailored for diverse RAG tasks including **translation, summarization, and question answering**, ensuring accurate **query understanding**. See <a href=#rag-evaluations-in-llamaindex>RAG Evaluations in LlamaIndex</a>.\n\n- **Efficient and Precise Retrieval**: Dual-encoder for efficient retrieval of `EmbeddingModel` in first stage, and cross-encoder of `RerankerModel` for enhanced precision and deeper semantic analysis in second stage.\n\n- **Broad Domain Adaptability**: Trained on diverse datasets for superior performance across various fields.\n\n- **User-Friendly Design**: Instruction-free, versatile use for multiple tasks without specifying query instruction for each task.\n\n- **Meaningful Reranking Scores**: `RerankerModel` provides relevant scores to improve result quality and optimize large language model performance.\n\n- **Proven in Production**: Successfully implemented and validated in Youdao's products.\n\n  - **双语和跨语种能力**：基于有道翻译引擎的强大能力，我们的`BCEmbedding`具备强大的中英双语和跨语种语义表征能力。\n  \n  - **RAG适配**：面向RAG做了针对性优化，可以适配大多数相关任务，比如**翻译，摘要，问答**等。此外，针对**问题理解**（query understanding）也做了针对优化，详见 <a href=\"#rag-evaluations-in-llamaindex\">基于LlamaIndex的RAG评测指标</a>。\n  \n  - **高效且精确的语义检索**：`EmbeddingModel`采用双编码器，可以在第一阶段实现高效的语义检索。`RerankerModel`采用交叉编码器，可以在第二阶段实现更高精度的语义顺序精排。\n  \n  - **更好的领域泛化性**：为了在更多场景实现更好的效果，我们收集了多种多样的领域数据。\n  \n  - **用户友好**：语义检索时不需要特殊指令前缀。也就是，你不需要为各种任务绞尽脑汁设计指令前缀。\n  \n  - **有意义的重排序分数**：`RerankerModel`可以提供有意义的语义相关性分数（不仅仅是排序），可以用于过滤无意义文本片段，提高大模型生成效果。\n  \n  - **产品化检验**：`BCEmbedding`已经被有道众多真实产品检验。\n\n## 🚀 Latest Updates\n\n- ***2024-01-03***: **Model Releases** - [bce-embedding-base_v1](https://huggingface.co/maidalun1020/bce-embedding-base_v1) and [bce-reranker-base_v1](https://huggingface.co/maidalun1020/bce-reranker-base_v1) are available.\n- ***2024-01-03***: **Eval Datasets** [[CrosslingualMultiDomainsDataset](https://huggingface.co/datasets/maidalun1020/CrosslingualMultiDomainsDataset)] - Evaluate the performence of RAG, using [LlamaIndex](https://github.com/run-llama/llama_index).\n- ***2024-01-03***: **Eval Datasets** [[Details](https://github.com/netease-youdao/BCEmbedding/blob/master/BCEmbedding/evaluation/c_mteb/Retrieval.py)] - Evaluate the performence of crosslingual semantic representation, using [MTEB](https://github.com/embeddings-benchmark/mteb).\n\n  - ***2024-01-03***: **模型发布** - [bce-embedding-base_v1](https://huggingface.co/maidalun1020/bce-embedding-base_v1)和[bce-reranker-base_v1](https://huggingface.co/maidalun1020/bce-reranker-base_v1)已发布.\n  - ***2024-01-03***: **RAG评测数据** [[CrosslingualMultiDomainsDataset](https://huggingface.co/datasets/maidalun1020/CrosslingualMultiDomainsDataset)] - 基于[LlamaIndex](https://github.com/run-llama/llama_index)的RAG评测数据已发布。\n  - ***2024-01-03***: **跨语种语义表征评测数据** [[详情](https://github.com/netease-youdao/BCEmbedding/blob/master/BCEmbedding/evaluation/c_mteb/Retrieval.py)] - 基于[MTEB](https://github.com/embeddings-benchmark/mteb)的跨语种评测数据已发布.\n\n## 🍎 Model List\n\n| Model Name | Model Type | Languages | Parameters | Weights |  \n|:-------------------------------|:--------:|:--------:|:--------:|:--------:|  \n| bce-embedding-base_v1 | `EmbeddingModel` | ch, en | 279M | [download](https://huggingface.co/maidalun1020/bce-embedding-base_v1) |  \n| bce-reranker-base_v1 | `RerankerModel` | ch, en, ja, ko | 279M | [download](https://huggingface.co/maidalun1020/bce-reranker-base_v1) |  \n\n## 📖 Manual\n\n### Installation\n\nFirst, create a conda environment and activate it.\n\n```bash\nconda create --name bce python=3.10 -y\nconda activate bce\n```\n\nThen install `BCEmbedding` for minimal installation:\n\n```bash\npip install BCEmbedding==0.1.1\n```\n\nOr install from source:\n\n```bash\ngit clone git@github.com:netease-youdao/BCEmbedding.git\ncd BCEmbedding\npip install -v -e .\n```\n\n### Quick Start\n\n#### 1. Based on `BCEmbedding`\n\nUse `EmbeddingModel`, and `cls` [pooler](./BCEmbedding/models/embedding.py#L24) is default.\n\n```python\nfrom BCEmbedding import EmbeddingModel\n\n# list of sentences\nsentences = ['sentence_0', 'sentence_1', ...]\n\n# init embedding model\nmodel = EmbeddingModel(model_name_or_path=\"maidalun1020/bce-embedding-base_v1\")\n\n# extract embeddings\nembeddings = model.encode(sentences)\n```\n\nUse `RerankerModel` to calculate relevant scores and rerank:\n\n```python\nfrom BCEmbedding import RerankerModel\n\n# your query and corresponding passages\nquery = 'input_query'\npassages = ['passage_0', 'passage_1', ...]\n\n# construct sentence pairs\nsentence_pairs = [[query, passage] for passage in passages]\n\n# init reranker model\nmodel = RerankerModel(model_name_or_path=\"maidalun1020/bce-reranker-base_v1\")\n\n# method 0: calculate scores of sentence pairs\nscores = model.compute_score(sentence_pairs)\n\n# method 1: rerank passages\nrerank_results = model.rerank(query, passages)\n```\n\nNOTE:\n\n- In [`RerankerModel.rerank`](./BCEmbedding/models/reranker.py#L137) method, we provide an advanced preproccess that we use in production for making `sentence_pairs`, when \"passages\" are very long.\n\n#### 2. Based on `transformers`\n\nFor `EmbeddingModel`:\n\n```python\nfrom transformers import AutoModel, AutoTokenizer\n\n# list of sentences\nsentences = ['sentence_0', 'sentence_1', ...]\n\n# init model and tokenizer\ntokenizer = AutoTokenizer.from_pretrained('maidalun1020/bce-embedding-base_v1')\nmodel = AutoModel.from_pretrained('maidalun1020/bce-embedding-base_v1')\n\ndevice = 'cuda'  # if no GPU, set \"cpu\"\nmodel.to(device)\n\n# get inputs\ninputs = tokenizer(sentences, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\ninputs_on_device = {k: v.to(self.device) for k, v in inputs.items()}\n\n# get embeddings\noutputs = model(**inputs_on_device, return_dict=True)\nembeddings = outputs.last_hidden_state[:, 0]  # cls pooler\nembeddings = embeddings / embeddings.norm(dim=1, keepdim=True)  # normalize\n```\n\nFor `RerankerModel`:\n\n```python\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\n# init model and tokenizer\ntokenizer = AutoTokenizer.from_pretrained('maidalun1020/bce-reranker-base_v1')\nmodel = AutoModelForSequenceClassification.from_pretrained('maidalun1020/bce-reranker-base_v1')\n\ndevice = 'cuda'  # if no GPU, set \"cpu\"\nmodel.to(device)\n\n# get inputs\ninputs = tokenizer(sentence_pairs, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\ninputs_on_device = {k: v.to(device) for k, v in inputs.items()}\n\n# calculate scores\nscores = model(**inputs_on_device, return_dict=True).logits.view(-1,).float()\nscores = torch.sigmoid(scores)\n```\n\n#### 3. Based on `sentence_transformers`\n\nFor `EmbeddingModel`:\n\n```python\nfrom sentence_transformers import SentenceTransformer\n\n# list of sentences\nsentences = ['sentence_0', 'sentence_1', ...]\n\n# init embedding model\n## New update for sentence-trnasformers. So clean up your \"`SENTENCE_TRANSFORMERS_HOME`/maidalun1020_bce-embedding-base_v1\" or \"～/.cache/torch/sentence_transformers/maidalun1020_bce-embedding-base_v1\" first for downloading new version.\nmodel = SentenceTransformer(\"maidalun1020/bce-embedding-base_v1\")\n\n# extract embeddings\nembeddings = model.encode(sentences, normalize_embeddings=True)\n```\n\nFor `RerankerModel`:\n\n```python\nfrom sentence_transformers import CrossEncoder\n\n# init reranker model\nmodel = CrossEncoder('maidalun1020/bce-reranker-base_v1', max_length=512)\n\n# calculate scores of sentence pairs\nscores = model.predict(sentence_pairs)\n```\n\n### Integrations for RAG Frameworks\n\n#### 1. Used in `langchain`\n\n```python\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_community.vectorstores.utils import DistanceStrategy\n\nquery = 'apples'\npassages = [\n        'I like apples', \n        'I like oranges', \n        'Apples and oranges are fruits'\n    ]\n  \n# init embedding model\nmodel_name = 'maidalun1020/bce-embedding-base_v1'\nmodel_kwargs = {'device': 'cuda'}\nencode_kwargs = {'batch_size': 64, 'normalize_embeddings': True, 'show_progress_bar': False}\n\nembed_model = HuggingFaceEmbeddings(\n    model_name=model_name,\n    model_kwargs=model_kwargs,\n    encode_kwargs=encode_kwargs\n  )\n\n# example #1. extract embeddings\nquery_embedding = embed_model.embed_query(query)\npassages_embeddings = embed_model.embed_documents(passages)\n\n# example #2. langchain retriever example\nfaiss_vectorstore = FAISS.from_texts(passages, embed_model, distance_strategy=DistanceStrategy.MAX_INNER_PRODUCT)\n\nretriever = faiss_vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"score_threshold\": 0.5, \"k\": 3})\n\nrelated_passages = retriever.get_relevant_documents(query)\n```\n\n#### 2. Used in `llama_index`\n\n```python\nfrom llama_index.embeddings import HuggingFaceEmbedding\nfrom llama_index import VectorStoreIndex, ServiceContext, SimpleDirectoryReader\nfrom llama_index.node_parser import SimpleNodeParser\nfrom llama_index.llms import OpenAI\n\nquery = 'apples'\npassages = [\n        'I like apples', \n        'I like oranges', \n        'Apples and oranges are fruits'\n    ]\n\n# init embedding model\nmodel_args = {'model_name': 'maidalun1020/bce-embedding-base_v1', 'max_length': 512, 'embed_batch_size': 64, 'device': 'cuda'}\nembed_model = HuggingFaceEmbedding(**model_args)\n\n# example #1. extract embeddings\nquery_embedding = embed_model.get_query_embedding(query)\npassages_embeddings = embed_model.get_text_embedding_batch(passages)\n\n# example #2. rag example\nllm = OpenAI(model='gpt-3.5-turbo-0613', api_key=os.environ.get('OPENAI_API_KEY'), api_base=os.environ.get('OPENAI_BASE_URL'))\nservice_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model)\n\ndocuments = SimpleDirectoryReader(input_files=[\"BCEmbedding/tools/eval_rag/eval_pdfs/Comp_en_llama2.pdf\"]).load_data()\nnode_parser = SimpleNodeParser.from_defaults(chunk_size=512)\nnodes = node_parser.get_nodes_from_documents(documents[0:36])\nindex = VectorStoreIndex(nodes, service_context=service_context)\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"What is llama?\")\n```\n\n\n## ⚙️ Evaluation\n\n### Evaluate Semantic Representation by MTEB\n\nWe provide evaluateion tools for `embedding` and `reranker` models, based on [MTEB](https://github.com/embeddings-benchmark/mteb) and [C_MTEB](https://github.com/FlagOpen/FlagEmbedding/tree/master/C_MTEB).\n\n  我们基于[MTEB](https://github.com/embeddings-benchmark/mteb)和[C_MTEB](https://github.com/FlagOpen/FlagEmbedding/tree/master/C_MTEB)，提供`embedding`和`reranker`模型的语义表征评测工具。\n\n#### 1. Embedding Models\n\nJust run following cmd to evaluate `your_embedding_model` (e.g. `maidalun1020/bce-embedding-base_v1`) in **bilingual and crosslingual settings** (e.g. `[\"en\", \"zh\", \"en-zh\", \"zh-en\"]`).\n\n  运行下面命令评测`your_embedding_model`（比如，`maidalun1020/bce-embedding-base_v1`）。评测任务将会在**双语和跨语种**（比如，`[\"en\", \"zh\", \"en-zh\", \"zh-en\"]`）模式下评测：\n\n```bash\npython BCEmbedding/tools/eval_mteb/eval_embedding_mteb.py --model_name_or_path maidalun1020/bce-embedding-base_v1 --pooler cls\n```\n\nThe total evaluation tasks contain ***114 datastes*** of **\"Retrieval\", \"STS\", \"PairClassification\", \"Classification\", \"Reranking\" and \"Clustering\"**.\n\n  评测包含 **\"Retrieval\"， \"STS\"， \"PairClassification\"， \"Classification\"， \"Reranking\"和\"Clustering\"** 这六大类任务的 ***114个数据集***。\n\n***NOTE:***\n- **All models are evaluated in their recommended pooling method (`pooler`)**.\n  - `mean` pooler: \"jina-embeddings-v2-base-en\", \"m3e-base\", \"m3e-large\", \"e5-large-v2\", \"multilingual-e5-base\", \"multilingual-e5-large\" and \"gte-large\".\n  - `cls` pooler: Other models.\n- \"jina-embeddings-v2-base-en\" model should be loaded with `trust_remote_code`.\n\n```bash\npython BCEmbedding/tools/eval_mteb/eval_embedding_mteb.py --model_name_or_path {moka-ai/m3e-base | moka-ai/m3e-large} --pooler mean\n\npython BCEmbedding/tools/eval_mteb/eval_embedding_mteb.py --model_name_or_path jinaai/jina-embeddings-v2-base-en --pooler mean --trust_remote_code\n```\n\n  ***注意：***\n  - 所有模型的评测采用各自推荐的`pooler`。\"jina-embeddings-v2-base-en\", \"m3e-base\", \"m3e-large\", \"e5-large-v2\", \"multilingual-e5-base\", \"multilingual-e5-large\"和\"gte-large\"的 `pooler`采用`mean`，其他模型的`pooler`采用`cls`.\n  - \"jina-embeddings-v2-base-en\"模型在载入时需要`trust_remote_code`。\n\n#### 2. Reranker Models\n\nRun following cmd to evaluate `your_reranker_model` (e.g. \"maidalun1020/bce-reranker-base_v1\") in **bilingual and crosslingual settings** (e.g. `[\"en\", \"zh\", \"en-zh\", \"zh-en\"]`).\n\n  运行下面命令评测`your_reranker_model`（比如，`maidalun1020/bce-reranker-base_v1`）。评测任务将会在 **双语种和跨语种**（比如，`[\"en\", \"zh\", \"en-zh\", \"zh-en\"]`）模式下评测：\n\n```bash\npython BCEmbedding/tools/eval_mteb/eval_reranker_mteb.py --model_name_or_path maidalun1020/bce-reranker-base_v1\n```\n\nThe evaluation tasks contain ***12 datastes*** of **\"Reranking\"**.\n\n  评测包含 **\"Reranking\"** 任务的 ***12个数据集***。\n\n#### 3. Metrics Visualization Tool\n\nWe proveide a one-click script to sumarize evaluation results of `embedding` and `reranker` models as [Embedding Models Evaluation Summary](https://github.com/netease-youdao/BCEmbedding/blob/master/Docs/EvaluationSummary/embedding_eval_summary.md) and [Reranker Models Evaluation Summary](https://github.com/netease-youdao/BCEmbedding/blob/master/Docs/EvaluationSummary/reranker_eval_summary.md).\n\n  我们提供了`embedding`和`reranker`模型的指标可视化一键脚本，输出一个markdown文件，详见[Embedding模型指标汇总](https://github.com/netease-youdao/BCEmbedding/blob/master/Docs/EvaluationSummary/embedding_eval_summary.md)和[Reranker模型指标汇总](https://github.com/netease-youdao/BCEmbedding/blob/master/Docs/EvaluationSummary/reranker_eval_summary.md)。\n\n```bash\npython BCEmbedding/evaluation/mteb/summarize_eval_results.py --results_dir {your_embedding_results_dir | your_reranker_results_dir}\n```\n\n### Evaluate RAG by LlamaIndex\n\n[LlamaIndex](https://github.com/run-llama/llama_index) is a famous data framework for LLM-based applications, particularly in RAG. Recently, the [LlamaIndex Blog](https://blog.llamaindex.ai/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83) has evaluated the popular embedding and reranker models in RAG pipeline and attract great attention. Now, we follow its pipeline to evaluate our `BCEmbedding`.\n\n  [LlamaIndex](https://github.com/run-llama/llama_index)是一个著名的大模型应用的开源工具，在RAG中很受欢迎。最近，[LlamaIndex博客](https://blog.llamaindex.ai/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83)对市面上常用的embedding和reranker模型进行RAG流程的评测，吸引广泛关注。下面我们按照该评测流程验证`BCEmbedding`在RAG中的效果。\n\nFirst, install LlamaIndex:\n```bash\npip install llama-index==0.9.22\n```\n\n#### 1. Metrics Definition\n\n- Hit Rate:\n\n  Hit rate calculates the fraction of queries where the correct answer is found within the top-k retrieved documents. In simpler terms, it's about how often our system gets it right within the top few guesses. ***The larger, the better.***\n\n- Mean Reciprocal Rank (MRR):\n  \n  For each query, MRR evaluates the system's accuracy by looking at the rank of the highest-placed relevant document. Specifically, it's the average of the reciprocals of these ranks across all the queries. So, if the first relevant document is the top result, the reciprocal rank is 1; if it's second, the reciprocal rank is 1/2, and so on. ***The larger, the better.***\n\n  - 命中率（Hit Rate）\n  \n    命中率计算的是在检索的前k个文档中找到正确答案的查询所占的比例。简单来说，它反映了我们的系统在前几次猜测中答对的频率。***该指标越大越好。***\n  \n  - 平均倒数排名（Mean Reciprocal Rank，MRR）\n    \n    对于每个查询，MRR通过查看最高排名的相关文档的排名来评估系统的准确性。具体来说，它是在所有查询中这些排名的倒数的平均值。因此，如果第一个相关文档是排名最靠前的结果，倒数排名就是1；如果是第二个，倒数排名就是1/2，依此类推。***该指标越大越好。***\n\n#### 2. Reproduce [LlamaIndex Blog](https://blog.llamaindex.ai/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83)\n\nIn order to compare our `BCEmbedding` with other embedding and reranker models fairly, we provide a one-click script to reproduce results of the LlamaIndex Blog, including our `BCEmbedding`:\n\n  为了公平起见，运行下面脚本，复现LlamaIndex博客的结果，将`BCEmbedding`与其他embedding和reranker模型进行对比分析：\n\n```bash\n# There should be two GPUs available at least.\nCUDA_VISIBLE_DEVICES=0,1 python BCEmbedding/tools/eval_rag/eval_llamaindex_reproduce.py\n```\n\nThen, sumarize the evaluation results by:\n```bash\npython BCEmbedding/tools/eval_rag/summarize_eval_results.py --results_dir results/rag_reproduce_results\n```\n\nResults Reproduced from the LlamaIndex Blog can be checked in ***[Reproduced Summary of RAG Evaluation](https://github.com/netease-youdao/BCEmbedding/blob/master/Docs/EvaluationSummary/rag_eval_reproduced_summary.md)***, with some obvious ***conclusions***:\n- In `WithoutReranker` setting, our `bce-embedding-base_v1` outperforms all the other embedding models.\n- With fixing the embedding model, our `bce-reranker-base_v1` achieves the best performence.\n- ***The combination of `bce-embedding-base_v1` and `bce-reranker-base_v1` is SOTA.***\n\n  输出的指标汇总详见 ***[LlamaIndex RAG评测结果复现](https://github.com/netease-youdao/BCEmbedding/blob/master/Docs/EvaluationSummary/rag_eval_reproduced_summary.md)***。从该复现结果中，可以看出：\n  - 在`WithoutReranker`设置下（**竖排对比**），`bce-embedding-base_v1`比其他embedding模型效果都要好。\n  - 在固定embedding模型设置下，对比不同reranker效果（**横排对比**），`bce-reranker-base_v1`比其他reranker模型效果都要好。\n  - ***`bce-embedding-base_v1`和`bce-reranker-base_v1`组合，表现SOTA。***\n\n#### 3. Broad Domain Adaptability\n\nThe evaluation of [LlamaIndex Blog](https://blog.llamaindex.ai/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83) is **monolingual, small amount of data, and specific domain** (just including \"llama2\" paper). In order to evaluate the **broad domain adaptability, bilingual and crosslingual capability**, we follow the blog to build a multiple domains evaluation dataset (includding \"Computer Science\", \"Physics\", \"Biology\", \"Economics\", \"Math\", and \"Quantitative Finance\"), named [CrosslingualMultiDomainsDataset](https://huggingface.co/datasets/maidalun1020/CrosslingualMultiDomainsDataset), **by OpenAI `gpt-4-1106-preview` for high quality**.\n\n  在上述的[LlamaIndex博客](https://blog.llamaindex.ai/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83)的评测数据只用了“llama2”这一篇文章，该评测是 **单语种，小数据量，特定领域** 的。为了兼容更真实更广的用户使用场景，评测算法模型的 **领域泛化性，双语和跨语种能力**，我们按照该博客的方法构建了一个多领域（计算机科学，物理学，生物学，经济学，数学，量化金融等）的双语种、跨语种评测数据，[CrosslingualMultiDomainsDataset](https://huggingface.co/datasets/maidalun1020/CrosslingualMultiDomainsDataset)。**为了保证构建数据的高质量，我们采用OpenAI的`gpt-4-1106-preview`。**\n\nFirst, run following cmd to evaluate the most popular and powerful embedding and reranker models:\n\n```bash\n# There should be two GPUs available at least.\nCUDA_VISIBLE_DEVICES=0,1 python BCEmbedding/tools/eval_rag/eval_llamaindex_multiple_domains.py\n```\n\nThen, run the following script to sumarize the evaluation results:\n```bash\npython BCEmbedding/tools/eval_rag/summarize_eval_results.py --results_dir results/rag_results\n```\n\nThe summary of multiple domains evaluations can be seen in <a href=#1-multiple-domains-scenarios>Multiple Domains Scenarios</a>.\n\n## 📈 Leaderboard\n\n### Semantic Representation Evaluations in MTEB\n\n#### 1. Embedding Models\n\n| Model | Dimensions | Pooler | Instructions | Retrieval (47) | STS (19) | PairClassification (5) | Classification (21) | Reranking (12) | Clustering (15) | ***AVG*** (119) |  \n|:--------|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|  \n| bge-base-en-v1.5 | 768 | `cls` | Need | 37.14 | 55.06 | 75.45 | 59.73 | 43.00 | 37.74 | 47.19 |  \n| bge-base-zh-v1.5 | 768 | `cls` | Need | 47.63 | 63.72 | 77.40 | 63.38 | 54.95 | 32.56 | 53.62 |  \n| bge-large-en-v1.5 | 1024 | `cls` | Need | 37.18 | 54.09 | 75.00 | 59.24 | 42.47 | 37.32 | 46.80 |  \n| bge-large-zh-v1.5 | 1024 | `cls` | Need | 47.58 | 64.73 | 79.14 | 64.19 | 55.98 | 33.26 | 54.23 |  \n| e5-large-v2 | 1024 | `mean` | Need | 35.98 | 55.23 | 75.28 | 59.53 | 42.12 | 36.51 | 46.52 |  \n| gte-large | 1024 | `mean` | Free | 36.68 | 55.22 | 74.29 | 57.73 | 42.44 | 38.51 | 46.67 |  \n| gte-large-zh | 1024 | `cls` | Free | 41.15 | 64.62 | 77.58 | 62.04 | 55.62 | 33.03 | 51.51 |  \n| jina-embeddings-v2-base-en | 768 | `mean` | Free | 31.58 | 54.28 | 74.84 | 58.42 | 41.16 | 34.67 | 44.29 |  \n| m3e-base | 768 | `mean` | Free | 46.29 | 63.93 | 71.84 | 64.08 | 52.38 | 37.84 | 53.54 |  \n| m3e-large | 1024 | `mean` | Free | 34.85 | 59.74 | 67.69 | 60.07 | 48.99 | 31.62 | 46.78 |  \n| multilingual-e5-base | 768 | `mean` | Need | 54.73 | 65.49 | 76.97 | 69.72 | 55.01 | 38.44 | 58.34 |  \n| multilingual-e5-large | 1024 | `mean` | Need | 56.76 | 66.79 | 78.80 | 71.61 | 56.49 | 43.09 | 60.50 |  \n| ***bce-embedding-base_v1*** | 768 | `cls` | Free | 57.60 | 65.73 | 74.96 | 69.00 | 57.29 | 38.95 | 59.43 |  \n\n***NOTE:***\n- Our ***bce-embedding-base_v1*** outperforms other opensource embedding models with comparable model size.\n- ***114 datastes*** of **\"Retrieval\", \"STS\", \"PairClassification\", \"Classification\", \"Reranking\" and \"Clustering\"** in `[\"en\", \"zh\", \"en-zh\", \"zh-en\"]` setting.\n- The [crosslingual evaluation datasets](https://github.com/netease-youdao/BCEmbedding/blob/master/BCEmbedding/evaluation/c_mteb/Retrieval.py) we released belong to `Retrieval` task.\n- More evaluation details please check [Embedding Models Evaluation Summary](https://github.com/netease-youdao/BCEmbedding/blob/master/Docs/EvaluationSummary/embedding_eval_summary.md).\n\n  ***要点：***\n  - 对比其他开源的相同规模的embedding模型，***bce-embedding-base_v1*** 表现最好，效果比最好的large模型稍差。\n  - 评测包含 **\"Retrieval\"， \"STS\"， \"PairClassification\"， \"Classification\"， \"Reranking\"和\"Clustering\"** 这六大类任务的共 ***114个数据集***。\n  - 我们开源的[跨语种语义表征评测数据](https://github.com/netease-youdao/BCEmbedding/blob/master/BCEmbedding/evaluation/c_mteb/Retrieval.py)属于`Retrieval`任务。\n  - 更详细的评测结果详见[Embedding模型指标汇总](https://github.com/netease-youdao/BCEmbedding/blob/master/Docs/EvaluationSummary/embedding_eval_summary.md)。\n\n#### 2. Reranker Models\n\n| Model                              | Reranking (12) | ***AVG*** (12) |\n| :--------------------------------- | :-------------: | :--------------------: |\n| bge-reranker-base                  |      59.04      |         59.04         |\n| bge-reranker-large                 |      60.86      |         60.86         |\n| ***bce-reranker-base_v1*** | **61.29** |  ***61.29***  |\n\n***NOTE:***\n- Our ***bce-reranker-base_v1*** outperforms other opensource reranker models.\n- ***12 datastes*** of **\"Reranking\"** in `[\"en\", \"zh\", \"en-zh\", \"zh-en\"]` setting.\n- More evaluation details please check [Reranker Models Evaluation Summary](https://github.com/netease-youdao/BCEmbedding/blob/master/Docs/EvaluationSummary/reranker_eval_summary.md).\n\n  ***要点：***\n  - ***bce-reranker-base_v1*** 优于其他开源reranker模型。\n  - 评测包含 **\"Reranking\"** 任务的 ***12个数据集***。\n  - 更详细的评测结果详见[Reranker模型指标汇总](https://github.com/netease-youdao/BCEmbedding/blob/master/Docs/EvaluationSummary/reranker_eval_summary.md)\n\n### RAG Evaluations in LlamaIndex\n\n#### 1. Multiple Domains Scenarios\n\n![image/jpeg](assets/rag_eval_multiple_domains_summary.jpg)\n\n***NOTE:***\n- Evaluated in **[\"en\", \"zh\", \"en-zh\", \"zh-en\"] setting**.\n- In `WithoutReranker` setting, our `bce-embedding-base_v1` outperforms all the other embedding models.\n- With fixing the embedding model, our `bce-reranker-base_v1` achieves the best performence.\n- **The combination of `bce-embedding-base_v1` and `bce-reranker-base_v1` is SOTA**.\n\n  ***要点：***\n  - 评测是在[\"en\", \"zh\", \"en-zh\", \"zh-en\"]设置下。\n  - 在`WithoutReranker`设置下（**竖排对比**），`bce-embedding-base_v1`优于其他Embedding模型，包括开源和闭源。\n  - 在固定Embedding模型设置下，对比不同reranker效果（**横排对比**），`bce-reranker-base_v1`比其他reranker模型效果都要好，包括开源和闭源。\n  - ***`bce-embedding-base_v1`和`bce-reranker-base_v1`组合，表现SOTA。***\n\n## 🛠 Youdao's BCEmbedding API\n\nFor users who prefer a hassle-free experience without the need to download and configure the model on their own systems, `BCEmbedding` is readily accessible through Youdao's API. This option offers a streamlined and efficient way to integrate BCEmbedding into your projects, bypassing the complexities of manual setup and maintenance. Detailed instructions and comprehensive API documentation are available at [Youdao BCEmbedding API](https://ai.youdao.com/DOCSIRMA/html/aigc/api/embedding/index.html). Here, you'll find all the necessary guidance to easily implement `BCEmbedding` across a variety of use cases, ensuring a smooth and effective integration for optimal results.\n\n  对于那些更喜欢直接调用api的用户，有道提供方便的`BCEmbedding`调用api。该方式是一种简化和高效的方式，将`BCEmbedding`集成到您的项目中，避开了手动设置和系统维护的复杂性。更详细的api调用接口说明详见[有道BCEmbedding API](https://ai.youdao.com/DOCSIRMA/html/aigc/api/embedding/index.html)。\n\n## 🧲 WeChat Group\n\nWelcome to scan the QR code below and join the WeChat group.\n\n  欢迎大家扫码加入官方微信交流群。\n\n![image/jpeg](assets/Wechat.jpg)\n\n## ✏️ Citation\n\nIf you use `BCEmbedding` in your research or project, please feel free to cite and star it:\n\n  如果在您的研究或任何项目中使用本工作，烦请按照下方进行引用，并打个小星星～\n\n```\n@misc{youdao_bcembedding_2023,\n    title={BCEmbedding: Bilingual and Crosslingual Embedding for RAG},\n    author={NetEase Youdao, Inc.},\n    year={2023},\n    howpublished={\\url{https://github.com/netease-youdao/BCEmbedding}}\n}\n```\n\n## 🔐 License\n\n`BCEmbedding` is licensed under [Apache 2.0 License](https://github.com/netease-youdao/BCEmbedding/blob/master/LICENSE)\n\n## 🔗 Related Links\n\n[Netease Youdao - QAnything](https://github.com/netease-youdao/qanything)\n\n[FlagEmbedding](https://github.com/FlagOpen/FlagEmbedding)\n\n[MTEB](https://github.com/embeddings-benchmark/mteb)\n\n[C_MTEB](https://github.com/FlagOpen/FlagEmbedding/tree/master/C_MTEB)\n\n[LLama Index](https://github.com/run-llama/llama_index) | [LlamaIndex Blog](https://blog.llamaindex.ai/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83)",
    "card_content": "---\nlicense: apache-2.0\npipeline_tag: text-classification\ntags:\n- transformers\n- sentence-transformers\nlanguage:\n- en\n- zh\n- ja\n- ko\n---\n<!--\n * @Description: \n * @Author: shenlei\n * @Date: 2023-12-19 10:31:41\n * @LastEditTime: 2024-01-10 00:17:02\n * @LastEditors: shenlei\n-->\n<h1 align=\"center\">BCEmbedding: Bilingual and Crosslingual Embedding for RAG</h1>\n\n<p align=\"center\">\n  <a href=\"https://github.com/netease-youdao/BCEmbedding/blob/master/LICENSE\">\n    <img src=\"https://img.shields.io/badge/license-Apache--2.0-yellow\">\n  </a>\n  <a href=\"https://twitter.com/YDopensource\">\n    <img src=\"https://img.shields.io/badge/follow-%40YDOpenSource-1DA1F2?logo=twitter&style={style}\">\n  </a>\n</p>\n\n最新、最详细bce-reranker-base_v1相关信息，请移步（The latest \"Updates\" should be checked in）：\n  \n<p align=\"left\">\n  <a href=\"https://github.com/netease-youdao/BCEmbedding\">GitHub</a>\n</p>\n\n## 主要特点(Key Features)：\n- 中英日韩四个语种，以及中英日韩四个语种的跨语种能力(Multilingual and Crosslingual capability in English, Chinese, Japanese and Korean)；\n- RAG优化，适配更多真实业务场景(RAG adaptation for more domains, including Education, Law, Finance, Medical, Literature, FAQ, Textbook, Wikipedia, etc.)；\n- <a href=\"https://github.com/netease-youdao/BCEmbedding\">BCEmbedding</a>适配长文本做rerank(Handle long passages reranking more than 512 limit in <a href=\"https://github.com/netease-youdao/BCEmbedding\">BCEmbedding</a>)；\n- RerankerModel可以提供 **“绝对”分数**，低质量passage过滤阈值推荐0.35或0.4。（RerankerModel provides **\"meaningful\" (for filtering bad passages with a threshold of 0.35 or 0.4) similarity score**）\n- **最佳实践（Best practice）** ：embedding召回top50-100片段，reranker对这50-100片段精排，最后取top5-10片段。（1. Get top 50-100 passages with [bce-embedding-base_v1](https://huggingface.co/maidalun1020/bce-embedding-base_v1) for \"`recall`\"；    2. Rerank passages with [bce-reranker-base_v1](https://huggingface.co/maidalun1020/bce-reranker-base_v1) and get top 5-10 for \"`precision`\" finally. ）\n\n## News:\n- `BCEmbedding`技术博客（ **Technical Blog** ）: [为RAG而生-BCEmbedding技术报告](https://zhuanlan.zhihu.com/p/681370855)\n- Related link for **EmbeddingModel** : [bce-embedding-base_v1](https://huggingface.co/maidalun1020/bce-embedding-base_v1)\n\n## Third-party Examples:\n- RAG applications: [QAnything](https://github.com/netease-youdao/qanything), [HuixiangDou](https://github.com/InternLM/HuixiangDou), [ChatPDF](https://github.com/shibing624/ChatPDF).\n- Efficient inference framework: [ChatLLM.cpp](https://github.com/foldl/chatllm.cpp), [Xinference](https://github.com/xorbitsai/inference), [mindnlp (Huawei GPU, 华为GPU)](https://github.com/mindspore-lab/mindnlp/tree/master/llm/inference/bce).\n\n![image/jpeg](assets/rag_eval_multiple_domains_summary.jpg)\n\n![image/jpeg](assets/Wechat.jpg)\n\n-----------------------------------------\n<details open=\"open\">\n<summary>Click to Open Contents</summary>\n\n- <a href=\"#-bilingual-and-crosslingual-superiority\" target=\"_Self\">🌐 Bilingual and Crosslingual Superiority</a>\n- <a href=\"#-key-features\" target=\"_Self\">💡 Key Features</a>\n- <a href=\"#-latest-updates\" target=\"_Self\">🚀 Latest Updates</a>\n- <a href=\"#-model-list\" target=\"_Self\">🍎 Model List</a>\n- <a href=\"#-manual\" target=\"_Self\">📖 Manual</a>\n  - <a href=\"#installation\" target=\"_Self\">Installation</a>\n  - <a href=\"#quick-start\" target=\"_Self\">Quick Start (`transformers`, `sentence-transformers`)</a>\n  - <a href=\"#integrations-for-rag-frameworks\" target=\"_Self\">Integrations for RAG Frameworks (`langchain`, `llama_index`)</a>\n- <a href=\"#%EF%B8%8F-evaluation\" target=\"_Self\">⚙️ Evaluation</a>\n  - <a href=\"#evaluate-semantic-representation-by-mteb\" target=\"_Self\">Evaluate Semantic Representation by MTEB</a>\n  - <a href=\"#evaluate-rag-by-llamaindex\" target=\"_Self\">Evaluate RAG by LlamaIndex</a>\n- <a href=\"#-leaderboard\" target=\"_Self\">📈 Leaderboard</a>\n  - <a href=\"#semantic-representation-evaluations-in-mteb\" target=\"_Self\">Semantic Representation Evaluations in MTEB</a>\n  - <a href=\"#rag-evaluations-in-llamaindex\" target=\"_Self\">RAG Evaluations in LlamaIndex</a>\n- <a href=\"#-youdaos-bcembedding-api\" target=\"_Self\">🛠 Youdao's BCEmbedding API</a>\n- <a href=\"#-wechat-group\" target=\"_Self\">🧲 WeChat Group</a>\n- <a href=\"#%EF%B8%8F-citation\" target=\"_Self\">✏️ Citation</a>\n- <a href=\"#-license\" target=\"_Self\">🔐 License</a>\n- <a href=\"#-related-links\" target=\"_Self\">🔗 Related Links</a>\n\n</details>\n<br>\n\n**B**ilingual and **C**rosslingual **Embedding** (`BCEmbedding`), developed by NetEase Youdao, encompasses `EmbeddingModel` and `RerankerModel`. The `EmbeddingModel` specializes in generating semantic vectors, playing a crucial role in semantic search and question-answering, and the `RerankerModel` excels at refining search results and ranking tasks. \n\n`BCEmbedding` serves as the cornerstone of Youdao's Retrieval Augmented Generation (RAG) implmentation, notably [QAnything](http://qanything.ai) [[github](https://github.com/netease-youdao/qanything)], an open-source implementation widely integrated in various Youdao products like [Youdao Speed Reading](https://read.youdao.com/#/home) and [Youdao Translation](https://fanyi.youdao.com/download-Mac?keyfrom=fanyiweb_navigation). \n\nDistinguished for its bilingual and crosslingual proficiency, `BCEmbedding` excels in bridging Chinese and English linguistic gaps, which achieves\n- **A high performence on <a href=\"#semantic-representation-evaluations-in-mteb\">Semantic Representation Evaluations in MTEB</a>**;\n- **A new benchmark in the realm of <a href=\"#rag-evaluations-in-llamaindex\">RAG Evaluations in LlamaIndex</a>**.\n\n  `BCEmbedding`是由网易有道开发的双语和跨语种语义表征算法模型库，其中包含`EmbeddingModel`和`RerankerModel`两类基础模型。`EmbeddingModel`专门用于生成语义向量，在语义搜索和问答中起着关键作用，而`RerankerModel`擅长优化语义搜索结果和语义相关顺序精排。\n  \n  `BCEmbedding`作为有道的检索增强生成式应用（RAG）的基石，特别是在[QAnything](http://qanything.ai) [[github](https://github.com/netease-youdao/qanything)]中发挥着重要作用。QAnything作为一个网易有道开源项目，在有道许多产品中有很好的应用实践，比如[有道速读](https://read.youdao.com/#/home)和[有道翻译](https://fanyi.youdao.com/download-Mac?keyfrom=fanyiweb_navigation)\n  \n  `BCEmbedding`以其出色的双语和跨语种能力而著称，在语义检索中消除中英语言之间的差异，从而实现：\n  - **强大的双语和跨语种语义表征能力【<a href=\"#semantic-representation-evaluations-in-mteb\">基于MTEB的语义表征评测指标</a>】。**\n  - **基于LlamaIndex的RAG评测，表现SOTA【<a href=\"#rag-evaluations-in-llamaindex\">基于LlamaIndex的RAG评测指标</a>】。**\n\n## 🌐 Bilingual and Crosslingual Superiority\n\nExisting embedding models often encounter performance challenges in bilingual and crosslingual scenarios, particularly in Chinese, English and their crosslingual tasks. `BCEmbedding`, leveraging the strength of Youdao's translation engine, excels in delivering superior performance across monolingual, bilingual, and crosslingual settings.\n\n`EmbeddingModel` supports ***Chinese (ch) and English (en)*** (more languages support will come soon), while `RerankerModel` supports ***Chinese (ch), English (en), Japanese (ja) and Korean (ko)***.\n\n  现有的单个语义表征模型在双语和跨语种场景中常常表现不佳，特别是在中文、英文及其跨语种任务中。`BCEmbedding`充分利用有道翻译引擎的优势，实现只需一个模型就可以在单语、双语和跨语种场景中表现出卓越的性能。\n  \n  `EmbeddingModel`支持***中文和英文***（之后会支持更多语种）；`RerankerModel`支持***中文，英文，日文和韩文***。\n\n## 💡 Key Features\n\n- **Bilingual and Crosslingual Proficiency**: Powered by Youdao's translation engine, excelling in Chinese, English and their crosslingual retrieval task, with upcoming support for additional languages.\n\n- **RAG-Optimized**: Tailored for diverse RAG tasks including **translation, summarization, and question answering**, ensuring accurate **query understanding**. See <a href=#rag-evaluations-in-llamaindex>RAG Evaluations in LlamaIndex</a>.\n\n- **Efficient and Precise Retrieval**: Dual-encoder for efficient retrieval of `EmbeddingModel` in first stage, and cross-encoder of `RerankerModel` for enhanced precision and deeper semantic analysis in second stage.\n\n- **Broad Domain Adaptability**: Trained on diverse datasets for superior performance across various fields.\n\n- **User-Friendly Design**: Instruction-free, versatile use for multiple tasks without specifying query instruction for each task.\n\n- **Meaningful Reranking Scores**: `RerankerModel` provides relevant scores to improve result quality and optimize large language model performance.\n\n- **Proven in Production**: Successfully implemented and validated in Youdao's products.\n\n  - **双语和跨语种能力**：基于有道翻译引擎的强大能力，我们的`BCEmbedding`具备强大的中英双语和跨语种语义表征能力。\n  \n  - **RAG适配**：面向RAG做了针对性优化，可以适配大多数相关任务，比如**翻译，摘要，问答**等。此外，针对**问题理解**（query understanding）也做了针对优化，详见 <a href=\"#rag-evaluations-in-llamaindex\">基于LlamaIndex的RAG评测指标</a>。\n  \n  - **高效且精确的语义检索**：`EmbeddingModel`采用双编码器，可以在第一阶段实现高效的语义检索。`RerankerModel`采用交叉编码器，可以在第二阶段实现更高精度的语义顺序精排。\n  \n  - **更好的领域泛化性**：为了在更多场景实现更好的效果，我们收集了多种多样的领域数据。\n  \n  - **用户友好**：语义检索时不需要特殊指令前缀。也就是，你不需要为各种任务绞尽脑汁设计指令前缀。\n  \n  - **有意义的重排序分数**：`RerankerModel`可以提供有意义的语义相关性分数（不仅仅是排序），可以用于过滤无意义文本片段，提高大模型生成效果。\n  \n  - **产品化检验**：`BCEmbedding`已经被有道众多真实产品检验。\n\n## 🚀 Latest Updates\n\n- ***2024-01-03***: **Model Releases** - [bce-embedding-base_v1](https://huggingface.co/maidalun1020/bce-embedding-base_v1) and [bce-reranker-base_v1](https://huggingface.co/maidalun1020/bce-reranker-base_v1) are available.\n- ***2024-01-03***: **Eval Datasets** [[CrosslingualMultiDomainsDataset](https://huggingface.co/datasets/maidalun1020/CrosslingualMultiDomainsDataset)] - Evaluate the performence of RAG, using [LlamaIndex](https://github.com/run-llama/llama_index).\n- ***2024-01-03***: **Eval Datasets** [[Details](https://github.com/netease-youdao/BCEmbedding/blob/master/BCEmbedding/evaluation/c_mteb/Retrieval.py)] - Evaluate the performence of crosslingual semantic representation, using [MTEB](https://github.com/embeddings-benchmark/mteb).\n\n  - ***2024-01-03***: **模型发布** - [bce-embedding-base_v1](https://huggingface.co/maidalun1020/bce-embedding-base_v1)和[bce-reranker-base_v1](https://huggingface.co/maidalun1020/bce-reranker-base_v1)已发布.\n  - ***2024-01-03***: **RAG评测数据** [[CrosslingualMultiDomainsDataset](https://huggingface.co/datasets/maidalun1020/CrosslingualMultiDomainsDataset)] - 基于[LlamaIndex](https://github.com/run-llama/llama_index)的RAG评测数据已发布。\n  - ***2024-01-03***: **跨语种语义表征评测数据** [[详情](https://github.com/netease-youdao/BCEmbedding/blob/master/BCEmbedding/evaluation/c_mteb/Retrieval.py)] - 基于[MTEB](https://github.com/embeddings-benchmark/mteb)的跨语种评测数据已发布.\n\n## 🍎 Model List\n\n| Model Name | Model Type | Languages | Parameters | Weights |  \n|:-------------------------------|:--------:|:--------:|:--------:|:--------:|  \n| bce-embedding-base_v1 | `EmbeddingModel` | ch, en | 279M | [download](https://huggingface.co/maidalun1020/bce-embedding-base_v1) |  \n| bce-reranker-base_v1 | `RerankerModel` | ch, en, ja, ko | 279M | [download](https://huggingface.co/maidalun1020/bce-reranker-base_v1) |  \n\n## 📖 Manual\n\n### Installation\n\nFirst, create a conda environment and activate it.\n\n```bash\nconda create --name bce python=3.10 -y\nconda activate bce\n```\n\nThen install `BCEmbedding` for minimal installation:\n\n```bash\npip install BCEmbedding==0.1.1\n```\n\nOr install from source:\n\n```bash\ngit clone git@github.com:netease-youdao/BCEmbedding.git\ncd BCEmbedding\npip install -v -e .\n```\n\n### Quick Start\n\n#### 1. Based on `BCEmbedding`\n\nUse `EmbeddingModel`, and `cls` [pooler](./BCEmbedding/models/embedding.py#L24) is default.\n\n```python\nfrom BCEmbedding import EmbeddingModel\n\n# list of sentences\nsentences = ['sentence_0', 'sentence_1', ...]\n\n# init embedding model\nmodel = EmbeddingModel(model_name_or_path=\"maidalun1020/bce-embedding-base_v1\")\n\n# extract embeddings\nembeddings = model.encode(sentences)\n```\n\nUse `RerankerModel` to calculate relevant scores and rerank:\n\n```python\nfrom BCEmbedding import RerankerModel\n\n# your query and corresponding passages\nquery = 'input_query'\npassages = ['passage_0', 'passage_1', ...]\n\n# construct sentence pairs\nsentence_pairs = [[query, passage] for passage in passages]\n\n# init reranker model\nmodel = RerankerModel(model_name_or_path=\"maidalun1020/bce-reranker-base_v1\")\n\n# method 0: calculate scores of sentence pairs\nscores = model.compute_score(sentence_pairs)\n\n# method 1: rerank passages\nrerank_results = model.rerank(query, passages)\n```\n\nNOTE:\n\n- In [`RerankerModel.rerank`](./BCEmbedding/models/reranker.py#L137) method, we provide an advanced preproccess that we use in production for making `sentence_pairs`, when \"passages\" are very long.\n\n#### 2. Based on `transformers`\n\nFor `EmbeddingModel`:\n\n```python\nfrom transformers import AutoModel, AutoTokenizer\n\n# list of sentences\nsentences = ['sentence_0', 'sentence_1', ...]\n\n# init model and tokenizer\ntokenizer = AutoTokenizer.from_pretrained('maidalun1020/bce-embedding-base_v1')\nmodel = AutoModel.from_pretrained('maidalun1020/bce-embedding-base_v1')\n\ndevice = 'cuda'  # if no GPU, set \"cpu\"\nmodel.to(device)\n\n# get inputs\ninputs = tokenizer(sentences, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\ninputs_on_device = {k: v.to(self.device) for k, v in inputs.items()}\n\n# get embeddings\noutputs = model(**inputs_on_device, return_dict=True)\nembeddings = outputs.last_hidden_state[:, 0]  # cls pooler\nembeddings = embeddings / embeddings.norm(dim=1, keepdim=True)  # normalize\n```\n\nFor `RerankerModel`:\n\n```python\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\n# init model and tokenizer\ntokenizer = AutoTokenizer.from_pretrained('maidalun1020/bce-reranker-base_v1')\nmodel = AutoModelForSequenceClassification.from_pretrained('maidalun1020/bce-reranker-base_v1')\n\ndevice = 'cuda'  # if no GPU, set \"cpu\"\nmodel.to(device)\n\n# get inputs\ninputs = tokenizer(sentence_pairs, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\ninputs_on_device = {k: v.to(device) for k, v in inputs.items()}\n\n# calculate scores\nscores = model(**inputs_on_device, return_dict=True).logits.view(-1,).float()\nscores = torch.sigmoid(scores)\n```\n\n#### 3. Based on `sentence_transformers`\n\nFor `EmbeddingModel`:\n\n```python\nfrom sentence_transformers import SentenceTransformer\n\n# list of sentences\nsentences = ['sentence_0', 'sentence_1', ...]\n\n# init embedding model\n## New update for sentence-trnasformers. So clean up your \"`SENTENCE_TRANSFORMERS_HOME`/maidalun1020_bce-embedding-base_v1\" or \"～/.cache/torch/sentence_transformers/maidalun1020_bce-embedding-base_v1\" first for downloading new version.\nmodel = SentenceTransformer(\"maidalun1020/bce-embedding-base_v1\")\n\n# extract embeddings\nembeddings = model.encode(sentences, normalize_embeddings=True)\n```\n\nFor `RerankerModel`:\n\n```python\nfrom sentence_transformers import CrossEncoder\n\n# init reranker model\nmodel = CrossEncoder('maidalun1020/bce-reranker-base_v1', max_length=512)\n\n# calculate scores of sentence pairs\nscores = model.predict(sentence_pairs)\n```\n\n### Integrations for RAG Frameworks\n\n#### 1. Used in `langchain`\n\n```python\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_community.vectorstores.utils import DistanceStrategy\n\nquery = 'apples'\npassages = [\n        'I like apples', \n        'I like oranges', \n        'Apples and oranges are fruits'\n    ]\n  \n# init embedding model\nmodel_name = 'maidalun1020/bce-embedding-base_v1'\nmodel_kwargs = {'device': 'cuda'}\nencode_kwargs = {'batch_size': 64, 'normalize_embeddings': True, 'show_progress_bar': False}\n\nembed_model = HuggingFaceEmbeddings(\n    model_name=model_name,\n    model_kwargs=model_kwargs,\n    encode_kwargs=encode_kwargs\n  )\n\n# example #1. extract embeddings\nquery_embedding = embed_model.embed_query(query)\npassages_embeddings = embed_model.embed_documents(passages)\n\n# example #2. langchain retriever example\nfaiss_vectorstore = FAISS.from_texts(passages, embed_model, distance_strategy=DistanceStrategy.MAX_INNER_PRODUCT)\n\nretriever = faiss_vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"score_threshold\": 0.5, \"k\": 3})\n\nrelated_passages = retriever.get_relevant_documents(query)\n```\n\n#### 2. Used in `llama_index`\n\n```python\nfrom llama_index.embeddings import HuggingFaceEmbedding\nfrom llama_index import VectorStoreIndex, ServiceContext, SimpleDirectoryReader\nfrom llama_index.node_parser import SimpleNodeParser\nfrom llama_index.llms import OpenAI\n\nquery = 'apples'\npassages = [\n        'I like apples', \n        'I like oranges', \n        'Apples and oranges are fruits'\n    ]\n\n# init embedding model\nmodel_args = {'model_name': 'maidalun1020/bce-embedding-base_v1', 'max_length': 512, 'embed_batch_size': 64, 'device': 'cuda'}\nembed_model = HuggingFaceEmbedding(**model_args)\n\n# example #1. extract embeddings\nquery_embedding = embed_model.get_query_embedding(query)\npassages_embeddings = embed_model.get_text_embedding_batch(passages)\n\n# example #2. rag example\nllm = OpenAI(model='gpt-3.5-turbo-0613', api_key=os.environ.get('OPENAI_API_KEY'), api_base=os.environ.get('OPENAI_BASE_URL'))\nservice_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model)\n\ndocuments = SimpleDirectoryReader(input_files=[\"BCEmbedding/tools/eval_rag/eval_pdfs/Comp_en_llama2.pdf\"]).load_data()\nnode_parser = SimpleNodeParser.from_defaults(chunk_size=512)\nnodes = node_parser.get_nodes_from_documents(documents[0:36])\nindex = VectorStoreIndex(nodes, service_context=service_context)\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"What is llama?\")\n```\n\n\n## ⚙️ Evaluation\n\n### Evaluate Semantic Representation by MTEB\n\nWe provide evaluateion tools for `embedding` and `reranker` models, based on [MTEB](https://github.com/embeddings-benchmark/mteb) and [C_MTEB](https://github.com/FlagOpen/FlagEmbedding/tree/master/C_MTEB).\n\n  我们基于[MTEB](https://github.com/embeddings-benchmark/mteb)和[C_MTEB](https://github.com/FlagOpen/FlagEmbedding/tree/master/C_MTEB)，提供`embedding`和`reranker`模型的语义表征评测工具。\n\n#### 1. Embedding Models\n\nJust run following cmd to evaluate `your_embedding_model` (e.g. `maidalun1020/bce-embedding-base_v1`) in **bilingual and crosslingual settings** (e.g. `[\"en\", \"zh\", \"en-zh\", \"zh-en\"]`).\n\n  运行下面命令评测`your_embedding_model`（比如，`maidalun1020/bce-embedding-base_v1`）。评测任务将会在**双语和跨语种**（比如，`[\"en\", \"zh\", \"en-zh\", \"zh-en\"]`）模式下评测：\n\n```bash\npython BCEmbedding/tools/eval_mteb/eval_embedding_mteb.py --model_name_or_path maidalun1020/bce-embedding-base_v1 --pooler cls\n```\n\nThe total evaluation tasks contain ***114 datastes*** of **\"Retrieval\", \"STS\", \"PairClassification\", \"Classification\", \"Reranking\" and \"Clustering\"**.\n\n  评测包含 **\"Retrieval\"， \"STS\"， \"PairClassification\"， \"Classification\"， \"Reranking\"和\"Clustering\"** 这六大类任务的 ***114个数据集***。\n\n***NOTE:***\n- **All models are evaluated in their recommended pooling method (`pooler`)**.\n  - `mean` pooler: \"jina-embeddings-v2-base-en\", \"m3e-base\", \"m3e-large\", \"e5-large-v2\", \"multilingual-e5-base\", \"multilingual-e5-large\" and \"gte-large\".\n  - `cls` pooler: Other models.\n- \"jina-embeddings-v2-base-en\" model should be loaded with `trust_remote_code`.\n\n```bash\npython BCEmbedding/tools/eval_mteb/eval_embedding_mteb.py --model_name_or_path {moka-ai/m3e-base | moka-ai/m3e-large} --pooler mean\n\npython BCEmbedding/tools/eval_mteb/eval_embedding_mteb.py --model_name_or_path jinaai/jina-embeddings-v2-base-en --pooler mean --trust_remote_code\n```\n\n  ***注意：***\n  - 所有模型的评测采用各自推荐的`pooler`。\"jina-embeddings-v2-base-en\", \"m3e-base\", \"m3e-large\", \"e5-large-v2\", \"multilingual-e5-base\", \"multilingual-e5-large\"和\"gte-large\"的 `pooler`采用`mean`，其他模型的`pooler`采用`cls`.\n  - \"jina-embeddings-v2-base-en\"模型在载入时需要`trust_remote_code`。\n\n#### 2. Reranker Models\n\nRun following cmd to evaluate `your_reranker_model` (e.g. \"maidalun1020/bce-reranker-base_v1\") in **bilingual and crosslingual settings** (e.g. `[\"en\", \"zh\", \"en-zh\", \"zh-en\"]`).\n\n  运行下面命令评测`your_reranker_model`（比如，`maidalun1020/bce-reranker-base_v1`）。评测任务将会在 **双语种和跨语种**（比如，`[\"en\", \"zh\", \"en-zh\", \"zh-en\"]`）模式下评测：\n\n```bash\npython BCEmbedding/tools/eval_mteb/eval_reranker_mteb.py --model_name_or_path maidalun1020/bce-reranker-base_v1\n```\n\nThe evaluation tasks contain ***12 datastes*** of **\"Reranking\"**.\n\n  评测包含 **\"Reranking\"** 任务的 ***12个数据集***。\n\n#### 3. Metrics Visualization Tool\n\nWe proveide a one-click script to sumarize evaluation results of `embedding` and `reranker` models as [Embedding Models Evaluation Summary](https://github.com/netease-youdao/BCEmbedding/blob/master/Docs/EvaluationSummary/embedding_eval_summary.md) and [Reranker Models Evaluation Summary](https://github.com/netease-youdao/BCEmbedding/blob/master/Docs/EvaluationSummary/reranker_eval_summary.md).\n\n  我们提供了`embedding`和`reranker`模型的指标可视化一键脚本，输出一个markdown文件，详见[Embedding模型指标汇总](https://github.com/netease-youdao/BCEmbedding/blob/master/Docs/EvaluationSummary/embedding_eval_summary.md)和[Reranker模型指标汇总](https://github.com/netease-youdao/BCEmbedding/blob/master/Docs/EvaluationSummary/reranker_eval_summary.md)。\n\n```bash\npython BCEmbedding/evaluation/mteb/summarize_eval_results.py --results_dir {your_embedding_results_dir | your_reranker_results_dir}\n```\n\n### Evaluate RAG by LlamaIndex\n\n[LlamaIndex](https://github.com/run-llama/llama_index) is a famous data framework for LLM-based applications, particularly in RAG. Recently, the [LlamaIndex Blog](https://blog.llamaindex.ai/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83) has evaluated the popular embedding and reranker models in RAG pipeline and attract great attention. Now, we follow its pipeline to evaluate our `BCEmbedding`.\n\n  [LlamaIndex](https://github.com/run-llama/llama_index)是一个著名的大模型应用的开源工具，在RAG中很受欢迎。最近，[LlamaIndex博客](https://blog.llamaindex.ai/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83)对市面上常用的embedding和reranker模型进行RAG流程的评测，吸引广泛关注。下面我们按照该评测流程验证`BCEmbedding`在RAG中的效果。\n\nFirst, install LlamaIndex:\n```bash\npip install llama-index==0.9.22\n```\n\n#### 1. Metrics Definition\n\n- Hit Rate:\n\n  Hit rate calculates the fraction of queries where the correct answer is found within the top-k retrieved documents. In simpler terms, it's about how often our system gets it right within the top few guesses. ***The larger, the better.***\n\n- Mean Reciprocal Rank (MRR):\n  \n  For each query, MRR evaluates the system's accuracy by looking at the rank of the highest-placed relevant document. Specifically, it's the average of the reciprocals of these ranks across all the queries. So, if the first relevant document is the top result, the reciprocal rank is 1; if it's second, the reciprocal rank is 1/2, and so on. ***The larger, the better.***\n\n  - 命中率（Hit Rate）\n  \n    命中率计算的是在检索的前k个文档中找到正确答案的查询所占的比例。简单来说，它反映了我们的系统在前几次猜测中答对的频率。***该指标越大越好。***\n  \n  - 平均倒数排名（Mean Reciprocal Rank，MRR）\n    \n    对于每个查询，MRR通过查看最高排名的相关文档的排名来评估系统的准确性。具体来说，它是在所有查询中这些排名的倒数的平均值。因此，如果第一个相关文档是排名最靠前的结果，倒数排名就是1；如果是第二个，倒数排名就是1/2，依此类推。***该指标越大越好。***\n\n#### 2. Reproduce [LlamaIndex Blog](https://blog.llamaindex.ai/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83)\n\nIn order to compare our `BCEmbedding` with other embedding and reranker models fairly, we provide a one-click script to reproduce results of the LlamaIndex Blog, including our `BCEmbedding`:\n\n  为了公平起见，运行下面脚本，复现LlamaIndex博客的结果，将`BCEmbedding`与其他embedding和reranker模型进行对比分析：\n\n```bash\n# There should be two GPUs available at least.\nCUDA_VISIBLE_DEVICES=0,1 python BCEmbedding/tools/eval_rag/eval_llamaindex_reproduce.py\n```\n\nThen, sumarize the evaluation results by:\n```bash\npython BCEmbedding/tools/eval_rag/summarize_eval_results.py --results_dir results/rag_reproduce_results\n```\n\nResults Reproduced from the LlamaIndex Blog can be checked in ***[Reproduced Summary of RAG Evaluation](https://github.com/netease-youdao/BCEmbedding/blob/master/Docs/EvaluationSummary/rag_eval_reproduced_summary.md)***, with some obvious ***conclusions***:\n- In `WithoutReranker` setting, our `bce-embedding-base_v1` outperforms all the other embedding models.\n- With fixing the embedding model, our `bce-reranker-base_v1` achieves the best performence.\n- ***The combination of `bce-embedding-base_v1` and `bce-reranker-base_v1` is SOTA.***\n\n  输出的指标汇总详见 ***[LlamaIndex RAG评测结果复现](https://github.com/netease-youdao/BCEmbedding/blob/master/Docs/EvaluationSummary/rag_eval_reproduced_summary.md)***。从该复现结果中，可以看出：\n  - 在`WithoutReranker`设置下（**竖排对比**），`bce-embedding-base_v1`比其他embedding模型效果都要好。\n  - 在固定embedding模型设置下，对比不同reranker效果（**横排对比**），`bce-reranker-base_v1`比其他reranker模型效果都要好。\n  - ***`bce-embedding-base_v1`和`bce-reranker-base_v1`组合，表现SOTA。***\n\n#### 3. Broad Domain Adaptability\n\nThe evaluation of [LlamaIndex Blog](https://blog.llamaindex.ai/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83) is **monolingual, small amount of data, and specific domain** (just including \"llama2\" paper). In order to evaluate the **broad domain adaptability, bilingual and crosslingual capability**, we follow the blog to build a multiple domains evaluation dataset (includding \"Computer Science\", \"Physics\", \"Biology\", \"Economics\", \"Math\", and \"Quantitative Finance\"), named [CrosslingualMultiDomainsDataset](https://huggingface.co/datasets/maidalun1020/CrosslingualMultiDomainsDataset), **by OpenAI `gpt-4-1106-preview` for high quality**.\n\n  在上述的[LlamaIndex博客](https://blog.llamaindex.ai/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83)的评测数据只用了“llama2”这一篇文章，该评测是 **单语种，小数据量，特定领域** 的。为了兼容更真实更广的用户使用场景，评测算法模型的 **领域泛化性，双语和跨语种能力**，我们按照该博客的方法构建了一个多领域（计算机科学，物理学，生物学，经济学，数学，量化金融等）的双语种、跨语种评测数据，[CrosslingualMultiDomainsDataset](https://huggingface.co/datasets/maidalun1020/CrosslingualMultiDomainsDataset)。**为了保证构建数据的高质量，我们采用OpenAI的`gpt-4-1106-preview`。**\n\nFirst, run following cmd to evaluate the most popular and powerful embedding and reranker models:\n\n```bash\n# There should be two GPUs available at least.\nCUDA_VISIBLE_DEVICES=0,1 python BCEmbedding/tools/eval_rag/eval_llamaindex_multiple_domains.py\n```\n\nThen, run the following script to sumarize the evaluation results:\n```bash\npython BCEmbedding/tools/eval_rag/summarize_eval_results.py --results_dir results/rag_results\n```\n\nThe summary of multiple domains evaluations can be seen in <a href=#1-multiple-domains-scenarios>Multiple Domains Scenarios</a>.\n\n## 📈 Leaderboard\n\n### Semantic Representation Evaluations in MTEB\n\n#### 1. Embedding Models\n\n| Model | Dimensions | Pooler | Instructions | Retrieval (47) | STS (19) | PairClassification (5) | Classification (21) | Reranking (12) | Clustering (15) | ***AVG*** (119) |  \n|:--------|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|  \n| bge-base-en-v1.5 | 768 | `cls` | Need | 37.14 | 55.06 | 75.45 | 59.73 | 43.00 | 37.74 | 47.19 |  \n| bge-base-zh-v1.5 | 768 | `cls` | Need | 47.63 | 63.72 | 77.40 | 63.38 | 54.95 | 32.56 | 53.62 |  \n| bge-large-en-v1.5 | 1024 | `cls` | Need | 37.18 | 54.09 | 75.00 | 59.24 | 42.47 | 37.32 | 46.80 |  \n| bge-large-zh-v1.5 | 1024 | `cls` | Need | 47.58 | 64.73 | 79.14 | 64.19 | 55.98 | 33.26 | 54.23 |  \n| e5-large-v2 | 1024 | `mean` | Need | 35.98 | 55.23 | 75.28 | 59.53 | 42.12 | 36.51 | 46.52 |  \n| gte-large | 1024 | `mean` | Free | 36.68 | 55.22 | 74.29 | 57.73 | 42.44 | 38.51 | 46.67 |  \n| gte-large-zh | 1024 | `cls` | Free | 41.15 | 64.62 | 77.58 | 62.04 | 55.62 | 33.03 | 51.51 |  \n| jina-embeddings-v2-base-en | 768 | `mean` | Free | 31.58 | 54.28 | 74.84 | 58.42 | 41.16 | 34.67 | 44.29 |  \n| m3e-base | 768 | `mean` | Free | 46.29 | 63.93 | 71.84 | 64.08 | 52.38 | 37.84 | 53.54 |  \n| m3e-large | 1024 | `mean` | Free | 34.85 | 59.74 | 67.69 | 60.07 | 48.99 | 31.62 | 46.78 |  \n| multilingual-e5-base | 768 | `mean` | Need | 54.73 | 65.49 | 76.97 | 69.72 | 55.01 | 38.44 | 58.34 |  \n| multilingual-e5-large | 1024 | `mean` | Need | 56.76 | 66.79 | 78.80 | 71.61 | 56.49 | 43.09 | 60.50 |  \n| ***bce-embedding-base_v1*** | 768 | `cls` | Free | 57.60 | 65.73 | 74.96 | 69.00 | 57.29 | 38.95 | 59.43 |  \n\n***NOTE:***\n- Our ***bce-embedding-base_v1*** outperforms other opensource embedding models with comparable model size.\n- ***114 datastes*** of **\"Retrieval\", \"STS\", \"PairClassification\", \"Classification\", \"Reranking\" and \"Clustering\"** in `[\"en\", \"zh\", \"en-zh\", \"zh-en\"]` setting.\n- The [crosslingual evaluation datasets](https://github.com/netease-youdao/BCEmbedding/blob/master/BCEmbedding/evaluation/c_mteb/Retrieval.py) we released belong to `Retrieval` task.\n- More evaluation details please check [Embedding Models Evaluation Summary](https://github.com/netease-youdao/BCEmbedding/blob/master/Docs/EvaluationSummary/embedding_eval_summary.md).\n\n  ***要点：***\n  - 对比其他开源的相同规模的embedding模型，***bce-embedding-base_v1*** 表现最好，效果比最好的large模型稍差。\n  - 评测包含 **\"Retrieval\"， \"STS\"， \"PairClassification\"， \"Classification\"， \"Reranking\"和\"Clustering\"** 这六大类任务的共 ***114个数据集***。\n  - 我们开源的[跨语种语义表征评测数据](https://github.com/netease-youdao/BCEmbedding/blob/master/BCEmbedding/evaluation/c_mteb/Retrieval.py)属于`Retrieval`任务。\n  - 更详细的评测结果详见[Embedding模型指标汇总](https://github.com/netease-youdao/BCEmbedding/blob/master/Docs/EvaluationSummary/embedding_eval_summary.md)。\n\n#### 2. Reranker Models\n\n| Model                              | Reranking (12) | ***AVG*** (12) |\n| :--------------------------------- | :-------------: | :--------------------: |\n| bge-reranker-base                  |      59.04      |         59.04         |\n| bge-reranker-large                 |      60.86      |         60.86         |\n| ***bce-reranker-base_v1*** | **61.29** |  ***61.29***  |\n\n***NOTE:***\n- Our ***bce-reranker-base_v1*** outperforms other opensource reranker models.\n- ***12 datastes*** of **\"Reranking\"** in `[\"en\", \"zh\", \"en-zh\", \"zh-en\"]` setting.\n- More evaluation details please check [Reranker Models Evaluation Summary](https://github.com/netease-youdao/BCEmbedding/blob/master/Docs/EvaluationSummary/reranker_eval_summary.md).\n\n  ***要点：***\n  - ***bce-reranker-base_v1*** 优于其他开源reranker模型。\n  - 评测包含 **\"Reranking\"** 任务的 ***12个数据集***。\n  - 更详细的评测结果详见[Reranker模型指标汇总](https://github.com/netease-youdao/BCEmbedding/blob/master/Docs/EvaluationSummary/reranker_eval_summary.md)\n\n### RAG Evaluations in LlamaIndex\n\n#### 1. Multiple Domains Scenarios\n\n![image/jpeg](assets/rag_eval_multiple_domains_summary.jpg)\n\n***NOTE:***\n- Evaluated in **[\"en\", \"zh\", \"en-zh\", \"zh-en\"] setting**.\n- In `WithoutReranker` setting, our `bce-embedding-base_v1` outperforms all the other embedding models.\n- With fixing the embedding model, our `bce-reranker-base_v1` achieves the best performence.\n- **The combination of `bce-embedding-base_v1` and `bce-reranker-base_v1` is SOTA**.\n\n  ***要点：***\n  - 评测是在[\"en\", \"zh\", \"en-zh\", \"zh-en\"]设置下。\n  - 在`WithoutReranker`设置下（**竖排对比**），`bce-embedding-base_v1`优于其他Embedding模型，包括开源和闭源。\n  - 在固定Embedding模型设置下，对比不同reranker效果（**横排对比**），`bce-reranker-base_v1`比其他reranker模型效果都要好，包括开源和闭源。\n  - ***`bce-embedding-base_v1`和`bce-reranker-base_v1`组合，表现SOTA。***\n\n## 🛠 Youdao's BCEmbedding API\n\nFor users who prefer a hassle-free experience without the need to download and configure the model on their own systems, `BCEmbedding` is readily accessible through Youdao's API. This option offers a streamlined and efficient way to integrate BCEmbedding into your projects, bypassing the complexities of manual setup and maintenance. Detailed instructions and comprehensive API documentation are available at [Youdao BCEmbedding API](https://ai.youdao.com/DOCSIRMA/html/aigc/api/embedding/index.html). Here, you'll find all the necessary guidance to easily implement `BCEmbedding` across a variety of use cases, ensuring a smooth and effective integration for optimal results.\n\n  对于那些更喜欢直接调用api的用户，有道提供方便的`BCEmbedding`调用api。该方式是一种简化和高效的方式，将`BCEmbedding`集成到您的项目中，避开了手动设置和系统维护的复杂性。更详细的api调用接口说明详见[有道BCEmbedding API](https://ai.youdao.com/DOCSIRMA/html/aigc/api/embedding/index.html)。\n\n## 🧲 WeChat Group\n\nWelcome to scan the QR code below and join the WeChat group.\n\n  欢迎大家扫码加入官方微信交流群。\n\n![image/jpeg](assets/Wechat.jpg)\n\n## ✏️ Citation\n\nIf you use `BCEmbedding` in your research or project, please feel free to cite and star it:\n\n  如果在您的研究或任何项目中使用本工作，烦请按照下方进行引用，并打个小星星～\n\n```\n@misc{youdao_bcembedding_2023,\n    title={BCEmbedding: Bilingual and Crosslingual Embedding for RAG},\n    author={NetEase Youdao, Inc.},\n    year={2023},\n    howpublished={\\url{https://github.com/netease-youdao/BCEmbedding}}\n}\n```\n\n## 🔐 License\n\n`BCEmbedding` is licensed under [Apache 2.0 License](https://github.com/netease-youdao/BCEmbedding/blob/master/LICENSE)\n\n## 🔗 Related Links\n\n[Netease Youdao - QAnything](https://github.com/netease-youdao/qanything)\n\n[FlagEmbedding](https://github.com/FlagOpen/FlagEmbedding)\n\n[MTEB](https://github.com/embeddings-benchmark/mteb)\n\n[C_MTEB](https://github.com/FlagOpen/FlagEmbedding/tree/master/C_MTEB)\n\n[LLama Index](https://github.com/run-llama/llama_index) | [LlamaIndex Blog](https://blog.llamaindex.ai/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83)",
    "library_name": "sentence-transformers",
    "transformers_info": {
      "auto_model": "AutoModelForSequenceClassification",
      "custom_class": null,
      "pipeline_tag": "text-classification",
      "processor": "AutoTokenizer"
    },
    "safetensors": null,
    "model_index": null,
    "trending_score": null
  },
  {
    "model_id": "finiteautomata/bertweet-base-sentiment-analysis",
    "model_name": "finiteautomata/bertweet-base-sentiment-analysis",
    "author": "finiteautomata",
    "downloads": 283038,
    "downloads_all_time": null,
    "likes": 169,
    "tags": [
      "transformers",
      "pytorch",
      "tf",
      "roberta",
      "text-classification",
      "sentiment-analysis",
      "en",
      "arxiv:2106.09462",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/finiteautomata/bertweet-base-sentiment-analysis",
    "dependencies": [
      [
        "pysentimiento",
        null
      ]
    ],
    "last_modified": "2023-02-17T02:17:31+00:00",
    "created_at": "2022-03-02T23:29:05+00:00",
    "analysis_date": "2025-03-22T00:55:51.844509",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "roberta",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": [
        "en"
      ],
      "tags": [
        "sentiment-analysis"
      ]
    },
    "card_text": "# Sentiment Analysis in English\n## bertweet-sentiment-analysis\n\nRepository: [https://github.com/finiteautomata/pysentimiento/](https://github.com/finiteautomata/pysentimiento/)\n\n\nModel trained with SemEval 2017 corpus (around ~40k tweets). Base model is [BERTweet](https://github.com/VinAIResearch/BERTweet), a RoBERTa model trained on English tweets.\n\nUses `POS`, `NEG`, `NEU` labels.\n\n## License\n\n`pysentimiento` is an open-source library for non-commercial use and scientific research purposes only. Please be aware that models are trained with third-party datasets and are subject to their respective licenses. \n\n1. [TASS Dataset license](http://tass.sepln.org/tass_data/download.php)\n2. [SEMEval 2017 Dataset license]()\n\n## Citation\n\nIf you use `pysentimiento` in your work, please cite [this paper](https://arxiv.org/abs/2106.09462)\n\n```\n@misc{perez2021pysentimiento,\n      title={pysentimiento: A Python Toolkit for Sentiment Analysis and SocialNLP tasks},\n      author={Juan Manuel Pérez and Juan Carlos Giudici and Franco Luque},\n      year={2021},\n      eprint={2106.09462},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\nEnjoy! 🤗\n",
    "card_content": "---\nlanguage:\n- en\ntags:\n- sentiment-analysis\n---\n# Sentiment Analysis in English\n## bertweet-sentiment-analysis\n\nRepository: [https://github.com/finiteautomata/pysentimiento/](https://github.com/finiteautomata/pysentimiento/)\n\n\nModel trained with SemEval 2017 corpus (around ~40k tweets). Base model is [BERTweet](https://github.com/VinAIResearch/BERTweet), a RoBERTa model trained on English tweets.\n\nUses `POS`, `NEG`, `NEU` labels.\n\n## License\n\n`pysentimiento` is an open-source library for non-commercial use and scientific research purposes only. Please be aware that models are trained with third-party datasets and are subject to their respective licenses. \n\n1. [TASS Dataset license](http://tass.sepln.org/tass_data/download.php)\n2. [SEMEval 2017 Dataset license]()\n\n## Citation\n\nIf you use `pysentimiento` in your work, please cite [this paper](https://arxiv.org/abs/2106.09462)\n\n```\n@misc{perez2021pysentimiento,\n      title={pysentimiento: A Python Toolkit for Sentiment Analysis and SocialNLP tasks},\n      author={Juan Manuel Pérez and Juan Carlos Giudici and Franco Luque},\n      year={2021},\n      eprint={2106.09462},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\nEnjoy! 🤗\n",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModelForSequenceClassification",
      "custom_class": null,
      "pipeline_tag": "text-classification",
      "processor": "AutoTokenizer"
    },
    "safetensors": null,
    "model_index": null,
    "trending_score": null
  },
  {
    "model_id": "amberoad/bert-multilingual-passage-reranking-msmarco",
    "model_name": "amberoad/bert-multilingual-passage-reranking-msmarco",
    "author": "amberoad",
    "downloads": 279382,
    "downloads_all_time": null,
    "likes": 84,
    "tags": [
      "transformers",
      "pytorch",
      "tf",
      "jax",
      "bert",
      "text-classification",
      "msmarco",
      "multilingual",
      "passage reranking",
      "af",
      "sq",
      "ar",
      "an",
      "hy",
      "ast",
      "az",
      "ba",
      "eu",
      "bar",
      "be",
      "bn",
      "inc",
      "bs",
      "br",
      "bg",
      "my",
      "ca",
      "ceb",
      "ce",
      "zh",
      "cv",
      "hr",
      "cs",
      "da",
      "nl",
      "en",
      "et",
      "fi",
      "fr",
      "gl",
      "ka",
      "de",
      "el",
      "gu",
      "ht",
      "he",
      "hi",
      "hu",
      "is",
      "io",
      "id",
      "ga",
      "it",
      "ja",
      "jv",
      "kn",
      "kk",
      "ky",
      "ko",
      "la",
      "lv",
      "lt",
      "roa",
      "nds",
      "lm",
      "mk",
      "mg",
      "ms",
      "ml",
      "mr",
      "min",
      "ne",
      "new",
      "nb",
      "nn",
      "oc",
      "fa",
      "pms",
      "pl",
      "pt",
      "pa",
      "ro",
      "ru",
      "sco",
      "sr",
      "scn",
      "sk",
      "sl",
      "aze",
      "es",
      "su",
      "sw",
      "sv",
      "tl",
      "tg",
      "ta",
      "tt",
      "te",
      "tr",
      "uk",
      "ud",
      "uz",
      "vi",
      "vo",
      "war",
      "cy",
      "fry",
      "pnb",
      "yo",
      "dataset:msmarco",
      "arxiv:1901.04085",
      "license:apache-2.0",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/amberoad/bert-multilingual-passage-reranking-msmarco",
    "dependencies": [
      [
        "transformers",
        null
      ]
    ],
    "last_modified": "2022-08-26T13:14:54+00:00",
    "created_at": "2022-03-02T23:29:05+00:00",
    "analysis_date": "2025-03-22T00:55:53.297197",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "bert",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": [
        "multilingual",
        "af",
        "sq",
        "ar",
        "an",
        "hy",
        "ast",
        "az",
        "ba",
        "eu",
        "bar",
        "be",
        "bn",
        "inc",
        "bs",
        "br",
        "bg",
        "my",
        "ca",
        "ceb",
        "ce",
        "zh",
        "cv",
        "hr",
        "cs",
        "da",
        "nl",
        "en",
        "et",
        "fi",
        "fr",
        "gl",
        "ka",
        "de",
        "el",
        "gu",
        "ht",
        "he",
        "hi",
        "hu",
        "is",
        "io",
        "id",
        "ga",
        "it",
        "ja",
        "jv",
        "kn",
        "kk",
        "ky",
        "ko",
        "la",
        "lv",
        "lt",
        "roa",
        "nds",
        "lm",
        "mk",
        "mg",
        "ms",
        "ml",
        "mr",
        "min",
        "ne",
        "new",
        "nb",
        "nn",
        "oc",
        "fa",
        "pms",
        "pl",
        "pt",
        "pa",
        "ro",
        "ru",
        "sco",
        "sr",
        "hr",
        "scn",
        "sk",
        "sl",
        "aze",
        "es",
        "su",
        "sw",
        "sv",
        "tl",
        "tg",
        "ta",
        "tt",
        "te",
        "tr",
        "uk",
        "ud",
        "uz",
        "vi",
        "vo",
        "war",
        "cy",
        "fry",
        "pnb",
        "yo"
      ],
      "thumbnail": "https://amberoad.de/images/logo_text.png",
      "tags": [
        "msmarco",
        "multilingual",
        "passage reranking"
      ],
      "license": "apache-2.0",
      "datasets": [
        "msmarco"
      ],
      "metrics": [
        "MRR"
      ],
      "widget": [
        {
          "query": "What is a corporation?",
          "passage": "A company is incorporated in a specific nation, often within the bounds of a smaller subset of that nation, such as a state or province. The corporation is then governed by the laws of incorporation in that state. A corporation may issue stock, either private or public, or may be classified as a non-stock corporation. If stock is issued, the corporation will usually be governed by its shareholders, either directly or indirectly."
        }
      ]
    },
    "card_text": "\n# Passage Reranking Multilingual BERT 🔃 🌍\n\n\n\n## Model description\n**Input:** Supports over 100 Languages. See [List of supported languages](https://github.com/google-research/bert/blob/master/multilingual.md#list-of-languages) for all available.\n\n**Purpose:** This module takes a search query [1] and a passage [2] and calculates if the passage matches the query. \nIt can be used as an improvement for Elasticsearch Results and boosts the relevancy by up to 100%. \n\n**Architecture:** On top of BERT there is a Densly Connected NN which takes the 768 Dimensional [CLS] Token as input and provides the output ([Arxiv](https://arxiv.org/abs/1901.04085)).\n\n**Output:** Just a single value between between -10 and 10. Better matching query,passage pairs tend to have a higher a score.\n\n\n\n## Intended uses & limitations\nBoth query[1] and passage[2] have to fit in 512 Tokens.\nAs you normally want to rerank the first dozens of search results keep in mind the inference time of approximately 300 ms/query.\n\n#### How to use\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\ntokenizer = AutoTokenizer.from_pretrained(\"amberoad/bert-multilingual-passage-reranking-msmarco\")\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\"amberoad/bert-multilingual-passage-reranking-msmarco\")\n```\n\nThis Model can be used as a drop-in replacement in the [Nboost Library](https://github.com/koursaros-ai/nboost)\nThrough this you can directly improve your Elasticsearch Results without any coding. \n\n\n## Training data\n\nThis model is trained using the [**Microsoft MS Marco Dataset**](https://microsoft.github.io/msmarco/ \"Microsoft MS Marco\"). This training dataset contains approximately 400M tuples of a query, relevant and non-relevant passages. All datasets used for training and evaluating are listed in this [table](https://github.com/microsoft/MSMARCO-Passage-Ranking#data-information-and-formating). The used dataset for training is called *Train Triples Large*, while the evaluation was made on *Top 1000 Dev*. There are 6,900 queries in total in the development dataset, where each query is mapped to top 1,000 passage retrieved using BM25 from MS MARCO corpus. \n\n## Training procedure\n\nThe training is performed the same way as stated in this [README](https://github.com/nyu-dl/dl4marco-bert \"NYU Github\"). See their excellent Paper on [Arxiv](https://arxiv.org/abs/1901.04085). \n\nWe changed the BERT Model from an English only to the default BERT Multilingual uncased Model from [Google](https://huggingface.co/bert-base-multilingual-uncased).\n\nTraining was done 400 000 Steps. This equaled 12 hours an a TPU V3-8.\n\n\n## Eval results\n\nWe see nearly similar performance than the English only Model in the English [Bing Queries Dataset](http://www.msmarco.org/). Although the training data is English only internal Tests on private data showed a far higher accurancy in German than all other available models.\n\n\n\nFine-tuned Models                                                                   | Dependency                                                                   | Eval Set                                                           | Search Boost<a href='#benchmarks'> | Speed on GPU\n----------------------------------------------------------------------------------- | ---------------------------------------------------------------------------- | ------------------------------------------------------------------ | ----------------------------------------------------- | ----------------------------------\n**`amberoad/Multilingual-uncased-MSMARCO`**  (This Model)                                       | <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-blue\"/>          |  <a href ='http://www.msmarco.org/'>bing queries</a>               | **+61%** <sub><sup>(0.29 vs 0.18)</sup></sub>         | ~300 ms/query <a href='#footnotes'>\n`nboost/pt-tinybert-msmarco`                                          | <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-red\"/>          |  <a href ='http://www.msmarco.org/'>bing queries</a>               | **+45%** <sub><sup>(0.26 vs 0.18)</sup></sub>         | ~50ms/query <a href='#footnotes'>\n`nboost/pt-bert-base-uncased-msmarco`                                               | <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-red\"/>          | <a href ='http://www.msmarco.org/'>bing queries</a>                | **+62%** <sub><sup>(0.29 vs 0.18)</sup></sub>         | ~300 ms/query<a href='#footnotes'>\n`nboost/pt-bert-large-msmarco`                                                      | <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-red\"/>          | <a href ='http://www.msmarco.org/'>bing queries</a>                | **+77%** <sub><sup>(0.32 vs 0.18)</sup></sub>         | -\n`nboost/pt-biobert-base-msmarco`                                                    | <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-red\"/>          | <a href ='https://github.com/naver/biobert-pretrained'>biomed</a>  | **+66%** <sub><sup>(0.17 vs 0.10)</sup></sub>         | ~300 ms/query<a href='#footnotes'>\n\nThis table is taken from [nboost](https://github.com/koursaros-ai/nboost) and extended by the first line. \n\n\n\n## Contact Infos\n\n![](https://amberoad.de/images/logo_text.png)\n\nAmberoad is a company focussing on Search and Business Intelligence. \nWe provide you: \n* Advanced Internal Company Search Engines thorugh NLP\n* External Search Egnines: Find Competitors, Customers, Suppliers \n\n**Get in Contact now to benefit from our Expertise:**\n\nThe training and evaluation was performed by [**Philipp Reissel**](https://reissel.eu/) and [**Igli Manaj**](https://github.com/iglimanaj) \n\n [![Amberoad](https://i.stack.imgur.com/gVE0j.png) Linkedin](https://de.linkedin.com/company/amberoad) | <svg xmlns=\"http://www.w3.org/2000/svg\" x=\"0px\" y=\"0px\"\nwidth=\"32\" height=\"32\"\nviewBox=\"0 0 172 172\"\nstyle=\" fill:#000000;\"><g fill=\"none\" fill-rule=\"nonzero\" stroke=\"none\" stroke-width=\"1\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\" stroke-miterlimit=\"10\" stroke-dasharray=\"\" stroke-dashoffset=\"0\" font-family=\"none\" font-weight=\"none\" font-size=\"none\" text-anchor=\"none\" style=\"mix-blend-mode: normal\"><path d=\"M0,172v-172h172v172z\" fill=\"none\"></path><g fill=\"#e67e22\"><path d=\"M37.625,21.5v86h96.75v-86h-5.375zM48.375,32.25h10.75v10.75h-10.75zM69.875,32.25h10.75v10.75h-10.75zM91.375,32.25h32.25v10.75h-32.25zM48.375,53.75h75.25v43h-75.25zM80.625,112.875v17.61572c-1.61558,0.93921 -2.94506,2.2687 -3.88428,3.88428h-49.86572v10.75h49.86572c1.8612,3.20153 5.28744,5.375 9.25928,5.375c3.97183,0 7.39808,-2.17347 9.25928,-5.375h49.86572v-10.75h-49.86572c-0.93921,-1.61558 -2.2687,-2.94506 -3.88428,-3.88428v-17.61572z\"></path></g></g></svg>[Homepage](https://de.linkedin.com/company/amberoad) |  [Email](info@amberoad.de)\n\n\n\n\n",
    "card_content": "---\nlanguage:\n- multilingual\n- af\n- sq\n- ar\n- an\n- hy\n- ast\n- az\n- ba\n- eu\n- bar\n- be\n- bn\n- inc\n- bs\n- br\n- bg\n- my\n- ca\n- ceb\n- ce\n- zh\n- cv\n- hr\n- cs\n- da\n- nl\n- en\n- et\n- fi\n- fr\n- gl\n- ka\n- de\n- el\n- gu\n- ht\n- he\n- hi\n- hu\n- is\n- io\n- id\n- ga\n- it\n- ja\n- jv\n- kn\n- kk\n- ky\n- ko\n- la\n- lv\n- lt\n- roa\n- nds\n- lm\n- mk\n- mg\n- ms\n- ml\n- mr\n- min\n- ne\n- new\n- nb\n- nn\n- oc\n- fa\n- pms\n- pl\n- pt\n- pa\n- ro\n- ru\n- sco\n- sr\n- hr\n- scn\n- sk\n- sl\n- aze\n- es\n- su\n- sw\n- sv\n- tl\n- tg\n- ta\n- tt\n- te\n- tr\n- uk\n- ud\n- uz\n- vi\n- vo\n- war\n- cy\n- fry\n- pnb\n- yo\nthumbnail: https://amberoad.de/images/logo_text.png\ntags:\n- msmarco\n- multilingual\n- passage reranking\nlicense: apache-2.0\ndatasets:\n- msmarco\nmetrics:\n- MRR\nwidget:\n- query: What is a corporation?\n  passage: A company is incorporated in a specific nation, often within the bounds\n    of a smaller subset of that nation, such as a state or province. The corporation\n    is then governed by the laws of incorporation in that state. A corporation may\n    issue stock, either private or public, or may be classified as a non-stock corporation.\n    If stock is issued, the corporation will usually be governed by its shareholders,\n    either directly or indirectly.\n---\n\n# Passage Reranking Multilingual BERT 🔃 🌍\n\n\n\n## Model description\n**Input:** Supports over 100 Languages. See [List of supported languages](https://github.com/google-research/bert/blob/master/multilingual.md#list-of-languages) for all available.\n\n**Purpose:** This module takes a search query [1] and a passage [2] and calculates if the passage matches the query. \nIt can be used as an improvement for Elasticsearch Results and boosts the relevancy by up to 100%. \n\n**Architecture:** On top of BERT there is a Densly Connected NN which takes the 768 Dimensional [CLS] Token as input and provides the output ([Arxiv](https://arxiv.org/abs/1901.04085)).\n\n**Output:** Just a single value between between -10 and 10. Better matching query,passage pairs tend to have a higher a score.\n\n\n\n## Intended uses & limitations\nBoth query[1] and passage[2] have to fit in 512 Tokens.\nAs you normally want to rerank the first dozens of search results keep in mind the inference time of approximately 300 ms/query.\n\n#### How to use\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\ntokenizer = AutoTokenizer.from_pretrained(\"amberoad/bert-multilingual-passage-reranking-msmarco\")\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\"amberoad/bert-multilingual-passage-reranking-msmarco\")\n```\n\nThis Model can be used as a drop-in replacement in the [Nboost Library](https://github.com/koursaros-ai/nboost)\nThrough this you can directly improve your Elasticsearch Results without any coding. \n\n\n## Training data\n\nThis model is trained using the [**Microsoft MS Marco Dataset**](https://microsoft.github.io/msmarco/ \"Microsoft MS Marco\"). This training dataset contains approximately 400M tuples of a query, relevant and non-relevant passages. All datasets used for training and evaluating are listed in this [table](https://github.com/microsoft/MSMARCO-Passage-Ranking#data-information-and-formating). The used dataset for training is called *Train Triples Large*, while the evaluation was made on *Top 1000 Dev*. There are 6,900 queries in total in the development dataset, where each query is mapped to top 1,000 passage retrieved using BM25 from MS MARCO corpus. \n\n## Training procedure\n\nThe training is performed the same way as stated in this [README](https://github.com/nyu-dl/dl4marco-bert \"NYU Github\"). See their excellent Paper on [Arxiv](https://arxiv.org/abs/1901.04085). \n\nWe changed the BERT Model from an English only to the default BERT Multilingual uncased Model from [Google](https://huggingface.co/bert-base-multilingual-uncased).\n\nTraining was done 400 000 Steps. This equaled 12 hours an a TPU V3-8.\n\n\n## Eval results\n\nWe see nearly similar performance than the English only Model in the English [Bing Queries Dataset](http://www.msmarco.org/). Although the training data is English only internal Tests on private data showed a far higher accurancy in German than all other available models.\n\n\n\nFine-tuned Models                                                                   | Dependency                                                                   | Eval Set                                                           | Search Boost<a href='#benchmarks'> | Speed on GPU\n----------------------------------------------------------------------------------- | ---------------------------------------------------------------------------- | ------------------------------------------------------------------ | ----------------------------------------------------- | ----------------------------------\n**`amberoad/Multilingual-uncased-MSMARCO`**  (This Model)                                       | <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-blue\"/>          |  <a href ='http://www.msmarco.org/'>bing queries</a>               | **+61%** <sub><sup>(0.29 vs 0.18)</sup></sub>         | ~300 ms/query <a href='#footnotes'>\n`nboost/pt-tinybert-msmarco`                                          | <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-red\"/>          |  <a href ='http://www.msmarco.org/'>bing queries</a>               | **+45%** <sub><sup>(0.26 vs 0.18)</sup></sub>         | ~50ms/query <a href='#footnotes'>\n`nboost/pt-bert-base-uncased-msmarco`                                               | <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-red\"/>          | <a href ='http://www.msmarco.org/'>bing queries</a>                | **+62%** <sub><sup>(0.29 vs 0.18)</sup></sub>         | ~300 ms/query<a href='#footnotes'>\n`nboost/pt-bert-large-msmarco`                                                      | <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-red\"/>          | <a href ='http://www.msmarco.org/'>bing queries</a>                | **+77%** <sub><sup>(0.32 vs 0.18)</sup></sub>         | -\n`nboost/pt-biobert-base-msmarco`                                                    | <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-red\"/>          | <a href ='https://github.com/naver/biobert-pretrained'>biomed</a>  | **+66%** <sub><sup>(0.17 vs 0.10)</sup></sub>         | ~300 ms/query<a href='#footnotes'>\n\nThis table is taken from [nboost](https://github.com/koursaros-ai/nboost) and extended by the first line. \n\n\n\n## Contact Infos\n\n![](https://amberoad.de/images/logo_text.png)\n\nAmberoad is a company focussing on Search and Business Intelligence. \nWe provide you: \n* Advanced Internal Company Search Engines thorugh NLP\n* External Search Egnines: Find Competitors, Customers, Suppliers \n\n**Get in Contact now to benefit from our Expertise:**\n\nThe training and evaluation was performed by [**Philipp Reissel**](https://reissel.eu/) and [**Igli Manaj**](https://github.com/iglimanaj) \n\n [![Amberoad](https://i.stack.imgur.com/gVE0j.png) Linkedin](https://de.linkedin.com/company/amberoad) | <svg xmlns=\"http://www.w3.org/2000/svg\" x=\"0px\" y=\"0px\"\nwidth=\"32\" height=\"32\"\nviewBox=\"0 0 172 172\"\nstyle=\" fill:#000000;\"><g fill=\"none\" fill-rule=\"nonzero\" stroke=\"none\" stroke-width=\"1\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\" stroke-miterlimit=\"10\" stroke-dasharray=\"\" stroke-dashoffset=\"0\" font-family=\"none\" font-weight=\"none\" font-size=\"none\" text-anchor=\"none\" style=\"mix-blend-mode: normal\"><path d=\"M0,172v-172h172v172z\" fill=\"none\"></path><g fill=\"#e67e22\"><path d=\"M37.625,21.5v86h96.75v-86h-5.375zM48.375,32.25h10.75v10.75h-10.75zM69.875,32.25h10.75v10.75h-10.75zM91.375,32.25h32.25v10.75h-32.25zM48.375,53.75h75.25v43h-75.25zM80.625,112.875v17.61572c-1.61558,0.93921 -2.94506,2.2687 -3.88428,3.88428h-49.86572v10.75h49.86572c1.8612,3.20153 5.28744,5.375 9.25928,5.375c3.97183,0 7.39808,-2.17347 9.25928,-5.375h49.86572v-10.75h-49.86572c-0.93921,-1.61558 -2.2687,-2.94506 -3.88428,-3.88428v-17.61572z\"></path></g></g></svg>[Homepage](https://de.linkedin.com/company/amberoad) |  [Email](info@amberoad.de)\n\n\n\n\n",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModelForSequenceClassification",
      "custom_class": null,
      "pipeline_tag": "text-classification",
      "processor": "AutoTokenizer"
    },
    "safetensors": null,
    "model_index": null,
    "trending_score": null
  },
  {
    "model_id": "cross-encoder/mmarco-mMiniLMv2-L12-H384-v1",
    "model_name": "cross-encoder/mmarco-mMiniLMv2-L12-H384-v1",
    "author": "cross-encoder",
    "downloads": 269689,
    "downloads_all_time": null,
    "likes": 49,
    "tags": [
      "transformers",
      "pytorch",
      "safetensors",
      "xlm-roberta",
      "text-classification",
      "en",
      "ar",
      "zh",
      "nl",
      "fr",
      "de",
      "hi",
      "in",
      "it",
      "ja",
      "pt",
      "ru",
      "es",
      "vi",
      "multilingual",
      "dataset:unicamp-dl/mmarco",
      "license:apache-2.0",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/cross-encoder/mmarco-mMiniLMv2-L12-H384-v1",
    "dependencies": [
      [
        "sentence_transformers",
        null
      ],
      [
        "transformers",
        null
      ],
      [
        "torch",
        null
      ]
    ],
    "last_modified": "2024-12-12T11:30:16+00:00",
    "created_at": "2022-06-01T08:27:31+00:00",
    "analysis_date": "2025-03-22T00:55:54.378054",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "xlm-roberta",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "license": "apache-2.0",
      "language": [
        "en",
        "ar",
        "zh",
        "nl",
        "fr",
        "de",
        "hi",
        "in",
        "it",
        "ja",
        "pt",
        "ru",
        "es",
        "vi",
        "multilingual"
      ],
      "datasets": [
        "unicamp-dl/mmarco"
      ]
    },
    "card_text": "# Cross-Encoder for multilingual MS Marco\n\nThis model was trained on the [MMARCO](https://hf.co/unicamp-dl/mmarco) dataset. It is a machine translated version of MS MARCO using Google Translate. It was translated to 14 languages. In our experiments, we observed that it performs also well for other languages.\n\nAs a base model, we used the [multilingual MiniLMv2](https://huggingface.co/nreimers/mMiniLMv2-L12-H384-distilled-from-XLMR-Large) model.\n\nThe model can be used for Information Retrieval: Given a query, encode the query will all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. See [SBERT.net Retrieve & Re-rank](https://www.sbert.net/examples/applications/retrieve_rerank/README.html) for more details. The training code is available here: [SBERT.net Training MS Marco](https://github.com/UKPLab/sentence-transformers/tree/master/examples/training/ms_marco)\n\n## Usage with SentenceTransformers\n\nThe usage becomes easy when you have [SentenceTransformers](https://www.sbert.net/) installed. Then, you can use the pre-trained models like this:\n```python\nfrom sentence_transformers import CrossEncoder\nmodel = CrossEncoder('model_name')\nscores = model.predict([('Query', 'Paragraph1'), ('Query', 'Paragraph2') , ('Query', 'Paragraph3')])\n```\n\n\n\n\n## Usage with Transformers\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\n\nmodel = AutoModelForSequenceClassification.from_pretrained('model_name')\ntokenizer = AutoTokenizer.from_pretrained('model_name')\n\nfeatures = tokenizer(['How many people live in Berlin?', 'How many people live in Berlin?'], ['Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.', 'New York City is famous for the Metropolitan Museum of Art.'],  padding=True, truncation=True, return_tensors=\"pt\")\n\nmodel.eval()\nwith torch.no_grad():\n    scores = model(**features).logits\n    print(scores)\n```\n\n\n\n",
    "card_content": "---\nlicense: apache-2.0\nlanguage:\n- en\n- ar\n- zh\n- nl\n- fr\n- de\n- hi\n- in\n- it\n- ja\n- pt\n- ru\n- es\n- vi\n- multilingual\ndatasets:\n- unicamp-dl/mmarco\n---\n# Cross-Encoder for multilingual MS Marco\n\nThis model was trained on the [MMARCO](https://hf.co/unicamp-dl/mmarco) dataset. It is a machine translated version of MS MARCO using Google Translate. It was translated to 14 languages. In our experiments, we observed that it performs also well for other languages.\n\nAs a base model, we used the [multilingual MiniLMv2](https://huggingface.co/nreimers/mMiniLMv2-L12-H384-distilled-from-XLMR-Large) model.\n\nThe model can be used for Information Retrieval: Given a query, encode the query will all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. See [SBERT.net Retrieve & Re-rank](https://www.sbert.net/examples/applications/retrieve_rerank/README.html) for more details. The training code is available here: [SBERT.net Training MS Marco](https://github.com/UKPLab/sentence-transformers/tree/master/examples/training/ms_marco)\n\n## Usage with SentenceTransformers\n\nThe usage becomes easy when you have [SentenceTransformers](https://www.sbert.net/) installed. Then, you can use the pre-trained models like this:\n```python\nfrom sentence_transformers import CrossEncoder\nmodel = CrossEncoder('model_name')\nscores = model.predict([('Query', 'Paragraph1'), ('Query', 'Paragraph2') , ('Query', 'Paragraph3')])\n```\n\n\n\n\n## Usage with Transformers\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\n\nmodel = AutoModelForSequenceClassification.from_pretrained('model_name')\ntokenizer = AutoTokenizer.from_pretrained('model_name')\n\nfeatures = tokenizer(['How many people live in Berlin?', 'How many people live in Berlin?'], ['Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.', 'New York City is famous for the Metropolitan Museum of Art.'],  padding=True, truncation=True, return_tensors=\"pt\")\n\nmodel.eval()\nwith torch.no_grad():\n    scores = model(**features).logits\n    print(scores)\n```\n\n\n\n",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModelForSequenceClassification",
      "custom_class": null,
      "pipeline_tag": "text-classification",
      "processor": "AutoTokenizer"
    },
    "safetensors": {
      "parameters": {
        "I64": 514,
        "F32": 117641089
      },
      "total": 117641603
    },
    "model_index": null,
    "trending_score": null
  },
  {
    "model_id": "tabularisai/multilingual-sentiment-analysis",
    "model_name": "tabularisai/multilingual-sentiment-analysis",
    "author": "tabularisai",
    "downloads": 253164,
    "downloads_all_time": null,
    "likes": 100,
    "tags": [
      "transformers",
      "safetensors",
      "distilbert",
      "text-classification",
      "sentiment-analysis",
      "sentiment",
      "synthetic data",
      "multi-class",
      "social-media-analysis",
      "customer-feedback",
      "product-reviews",
      "brand-monitoring",
      "multilingual",
      "en",
      "zh",
      "es",
      "hi",
      "ar",
      "bn",
      "pt",
      "ru",
      "ja",
      "de",
      "ms",
      "te",
      "vi",
      "ko",
      "fr",
      "tr",
      "it",
      "pl",
      "uk",
      "tl",
      "nl",
      "gsw",
      "base_model:distilbert/distilbert-base-multilingual-cased",
      "base_model:finetune:distilbert/distilbert-base-multilingual-cased",
      "license:cc-by-nc-4.0",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/tabularisai/multilingual-sentiment-analysis",
    "dependencies": [
      [
        "transformers",
        null
      ],
      [
        "torch",
        null
      ]
    ],
    "last_modified": "2025-01-16T08:15:24+00:00",
    "created_at": "2024-12-07T17:56:18+00:00",
    "analysis_date": "2025-03-22T00:55:55.941158",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "distilbert",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "base_model": "distilbert/distilbert-base-multilingual-cased",
      "language": [
        "en",
        "zh",
        "es",
        "hi",
        "ar",
        "bn",
        "pt",
        "ru",
        "ja",
        "de",
        "ms",
        "te",
        "vi",
        "ko",
        "fr",
        "tr",
        "it",
        "pl",
        "uk",
        "tl",
        "nl",
        "gsw"
      ],
      "library_name": "transformers",
      "license": "cc-by-nc-4.0",
      "pipeline_tag": "text-classification",
      "tags": [
        "text-classification",
        "sentiment-analysis",
        "sentiment",
        "synthetic data",
        "multi-class",
        "social-media-analysis",
        "customer-feedback",
        "product-reviews",
        "brand-monitoring",
        "multilingual"
      ]
    },
    "card_text": "\n\n# 🚀 distilbert-based Multilingual Sentiment Classification Model\n\n<!-- TRY IT HERE: `coming soon`\n -->\n[<img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/Discord%20button.png\" width=\"200\"/>](https://discord.gg/sznxwdqBXj)\n\n\n# NEWS!\n\n- 2024/12: We are excited to introduce a multilingual sentiment model! Now you can analyze sentiment across multiple languages, enhancing your global reach.\n\n## Model Details\n- `Model Name:` tabularisai/multilingual-sentiment-analysis\n- `Base Model:` distilbert/distilbert-base-multilingual-cased\n- `Task:` Text Classification (Sentiment Analysis)\n- `Languages:` Supports English plus Chinese (中文), Spanish (Español), Hindi (हिन्दी), Arabic (العربية), Bengali (বাংলা), Portuguese (Português), Russian (Русский), Japanese (日本語), German (Deutsch), Malay (Bahasa Melayu), Telugu (తెలుగు), Vietnamese (Tiếng Việt), Korean (한국어), French (Français), Turkish (Türkçe), Italian (Italiano), Polish (Polski), Ukrainian (Українська), Tagalog, Dutch (Nederlands), Swiss German (Schweizerdeutsch).\n- `Number of Classes:` 5 (*Very Negative, Negative, Neutral, Positive, Very Positive*)\n- `Usage:`\n  - Social media analysis\n  - Customer feedback analysis\n  - Product reviews classification\n  - Brand monitoring\n  - Market research\n  - Customer service optimization\n  - Competitive intelligence\n\n## Model Description\n\nThis model is a fine-tuned version of `distilbert/distilbert-base-multilingual-cased` for multilingual sentiment analysis. It leverages synthetic data from multiple sources to achieve robust performance across different languages and cultural contexts.\n\n### Training Data\n\nTrained exclusively on synthetic multilingual data generated by advanced LLMs, ensuring wide coverage of sentiment expressions from various languages.\n\n### Training Procedure\n\n- Fine-tuned for 3.5 epochs.\n- Achieved a train_acc_off_by_one of approximately 0.93 on the validation dataset.\n\n## Intended Use\n\nIdeal for:\n- Multilingual social media monitoring\n- International customer feedback analysis\n- Global product review sentiment classification\n- Worldwide brand sentiment tracking\n\n## How to Use\n\nUsing pipelines, it takes only 4 lines:\n\n```python\nfrom transformers import pipeline\n\n# Load the classification pipeline with the specified model\npipe = pipeline(\"text-classification\", model=\"tabularisai/multilingual-sentiment-analysis\")\n\n# Classify a new sentence\nsentence = \"I love this product! It's amazing and works perfectly.\"\nresult = pipe(sentence)\n\n# Print the result\nprint(result)\n```\n\nBelow is a Python example on how to use the multilingual sentiment model without pipelines:\n\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\n\nmodel_name = \"tabularisai/multilingual-sentiment-analysis\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name)\n\ndef predict_sentiment(texts):\n    inputs = tokenizer(texts, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n    with torch.no_grad():\n        outputs = model(**inputs)\n    probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n    sentiment_map = {0: \"Very Negative\", 1: \"Negative\", 2: \"Neutral\", 3: \"Positive\", 4: \"Very Positive\"}\n    return [sentiment_map[p] for p in torch.argmax(probabilities, dim=-1).tolist()]\n\ntexts = [\n    # English\n    \"I absolutely love the new design of this app!\", \"The customer service was disappointing.\", \"The weather is fine, nothing special.\",\n    # Chinese\n    \"这家餐厅的菜味道非常棒！\", \"我对他的回答很失望。\", \"天气今天一般。\",\n    # Spanish\n    \"¡Me encanta cómo quedó la decoración!\", \"El servicio fue terrible y muy lento.\", \"El libro estuvo más o menos.\",\n    # Arabic\n    \"الخدمة في هذا الفندق رائعة جدًا!\", \"لم يعجبني الطعام في هذا المطعم.\", \"كانت الرحلة عادية。\",\n    # Ukrainian\n    \"Мені дуже сподобалася ця вистава!\", \"Обслуговування було жахливим.\", \"Книга була посередньою。\",\n    # Hindi\n    \"यह जगह सच में अद्भुत है!\", \"यह अनुभव बहुत खराब था।\", \"फिल्म ठीक-ठाक थी।\",\n    # Bengali\n    \"এখানকার পরিবেশ অসাধারণ!\", \"সেবার মান একেবারেই খারাপ।\", \"খাবারটা মোটামুটি ছিল।\",\n    # Portuguese\n    \"Este livro é fantástico! Eu aprendi muitas coisas novas e inspiradoras.\", \n    \"Não gostei do produto, veio quebrado.\", \"O filme foi ok, nada de especial.\",\n    # Japanese\n    \"このレストランの料理は本当に美味しいです！\", \"このホテルのサービスはがっかりしました。\", \"天気はまあまあです。\",\n    # Russian\n    \"Я в восторге от этого нового гаджета!\", \"Этот сервис оставил у меня только разочарование.\", \"Встреча была обычной, ничего особенного.\",\n    # French\n    \"J'adore ce restaurant, c'est excellent !\", \"L'attente était trop longue et frustrante.\", \"Le film était moyen, sans plus.\",\n    # Turkish\n    \"Bu otelin manzarasına bayıldım!\", \"Ürün tam bir hayal kırıklığıydı.\", \"Konser fena değildi, ortalamaydı.\",\n    # Italian\n    \"Adoro questo posto, è fantastico!\", \"Il servizio clienti è stato pessimo.\", \"La cena era nella media.\",\n    # Polish\n    \"Uwielbiam tę restaurację, jedzenie jest świetne!\", \"Obsługa klienta była rozczarowująca.\", \"Pogoda jest w porządku, nic szczególnego.\",\n    # Tagalog\n    \"Ang ganda ng lugar na ito, sobrang aliwalas!\", \"Hindi maganda ang serbisyo nila dito.\", \"Maayos lang ang palabas, walang espesyal.\",\n    # Dutch\n    \"Ik ben echt blij met mijn nieuwe aankoop!\", \"De klantenservice was echt slecht.\", \"De presentatie was gewoon oké, niet bijzonder.\",\n    # Malay\n    \"Saya suka makanan di sini, sangat sedap!\", \"Pengalaman ini sangat mengecewakan.\", \"Hari ini cuacanya biasa sahaja.\",\n    # Korean\n    \"이 가게의 케이크는 정말 맛있어요!\", \"서비스가 너무 별로였어요.\", \"날씨가 그저 그렇네요.\",\n    # Swiss German\n    \"Ich find dä Service i de Beiz mega guet!\", \"Däs Esä het mir nöd gfalle.\", \"D Wätter hüt isch so naja.\"\n]\n\nfor text, sentiment in zip(texts, predict_sentiment(texts)):\n    print(f\"Text: {text}\\nSentiment: {sentiment}\\n\")\n```\n\n## Ethical Considerations\n\nSynthetic data reduces bias, but validation in real-world scenarios is advised.\n\n## Citation\n```\nWill be included.\n```\n\n## Contact\n\nFor inquiries, data, private APIs, better models, contact info@tabularis.ai\n\ntabularis.ai",
    "card_content": "---\nbase_model: distilbert/distilbert-base-multilingual-cased\nlanguage:\n- en\n- zh\n- es\n- hi\n- ar\n- bn\n- pt\n- ru\n- ja\n- de\n- ms\n- te\n- vi\n- ko\n- fr\n- tr\n- it\n- pl\n- uk\n- tl\n- nl\n- gsw\nlibrary_name: transformers\nlicense: cc-by-nc-4.0\npipeline_tag: text-classification\ntags:\n- text-classification\n- sentiment-analysis\n- sentiment\n- synthetic data\n- multi-class\n- social-media-analysis\n- customer-feedback\n- product-reviews\n- brand-monitoring\n- multilingual\n---\n\n\n# 🚀 distilbert-based Multilingual Sentiment Classification Model\n\n<!-- TRY IT HERE: `coming soon`\n -->\n[<img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/Discord%20button.png\" width=\"200\"/>](https://discord.gg/sznxwdqBXj)\n\n\n# NEWS!\n\n- 2024/12: We are excited to introduce a multilingual sentiment model! Now you can analyze sentiment across multiple languages, enhancing your global reach.\n\n## Model Details\n- `Model Name:` tabularisai/multilingual-sentiment-analysis\n- `Base Model:` distilbert/distilbert-base-multilingual-cased\n- `Task:` Text Classification (Sentiment Analysis)\n- `Languages:` Supports English plus Chinese (中文), Spanish (Español), Hindi (हिन्दी), Arabic (العربية), Bengali (বাংলা), Portuguese (Português), Russian (Русский), Japanese (日本語), German (Deutsch), Malay (Bahasa Melayu), Telugu (తెలుగు), Vietnamese (Tiếng Việt), Korean (한국어), French (Français), Turkish (Türkçe), Italian (Italiano), Polish (Polski), Ukrainian (Українська), Tagalog, Dutch (Nederlands), Swiss German (Schweizerdeutsch).\n- `Number of Classes:` 5 (*Very Negative, Negative, Neutral, Positive, Very Positive*)\n- `Usage:`\n  - Social media analysis\n  - Customer feedback analysis\n  - Product reviews classification\n  - Brand monitoring\n  - Market research\n  - Customer service optimization\n  - Competitive intelligence\n\n## Model Description\n\nThis model is a fine-tuned version of `distilbert/distilbert-base-multilingual-cased` for multilingual sentiment analysis. It leverages synthetic data from multiple sources to achieve robust performance across different languages and cultural contexts.\n\n### Training Data\n\nTrained exclusively on synthetic multilingual data generated by advanced LLMs, ensuring wide coverage of sentiment expressions from various languages.\n\n### Training Procedure\n\n- Fine-tuned for 3.5 epochs.\n- Achieved a train_acc_off_by_one of approximately 0.93 on the validation dataset.\n\n## Intended Use\n\nIdeal for:\n- Multilingual social media monitoring\n- International customer feedback analysis\n- Global product review sentiment classification\n- Worldwide brand sentiment tracking\n\n## How to Use\n\nUsing pipelines, it takes only 4 lines:\n\n```python\nfrom transformers import pipeline\n\n# Load the classification pipeline with the specified model\npipe = pipeline(\"text-classification\", model=\"tabularisai/multilingual-sentiment-analysis\")\n\n# Classify a new sentence\nsentence = \"I love this product! It's amazing and works perfectly.\"\nresult = pipe(sentence)\n\n# Print the result\nprint(result)\n```\n\nBelow is a Python example on how to use the multilingual sentiment model without pipelines:\n\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\n\nmodel_name = \"tabularisai/multilingual-sentiment-analysis\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name)\n\ndef predict_sentiment(texts):\n    inputs = tokenizer(texts, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n    with torch.no_grad():\n        outputs = model(**inputs)\n    probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n    sentiment_map = {0: \"Very Negative\", 1: \"Negative\", 2: \"Neutral\", 3: \"Positive\", 4: \"Very Positive\"}\n    return [sentiment_map[p] for p in torch.argmax(probabilities, dim=-1).tolist()]\n\ntexts = [\n    # English\n    \"I absolutely love the new design of this app!\", \"The customer service was disappointing.\", \"The weather is fine, nothing special.\",\n    # Chinese\n    \"这家餐厅的菜味道非常棒！\", \"我对他的回答很失望。\", \"天气今天一般。\",\n    # Spanish\n    \"¡Me encanta cómo quedó la decoración!\", \"El servicio fue terrible y muy lento.\", \"El libro estuvo más o menos.\",\n    # Arabic\n    \"الخدمة في هذا الفندق رائعة جدًا!\", \"لم يعجبني الطعام في هذا المطعم.\", \"كانت الرحلة عادية。\",\n    # Ukrainian\n    \"Мені дуже сподобалася ця вистава!\", \"Обслуговування було жахливим.\", \"Книга була посередньою。\",\n    # Hindi\n    \"यह जगह सच में अद्भुत है!\", \"यह अनुभव बहुत खराब था।\", \"फिल्म ठीक-ठाक थी।\",\n    # Bengali\n    \"এখানকার পরিবেশ অসাধারণ!\", \"সেবার মান একেবারেই খারাপ।\", \"খাবারটা মোটামুটি ছিল।\",\n    # Portuguese\n    \"Este livro é fantástico! Eu aprendi muitas coisas novas e inspiradoras.\", \n    \"Não gostei do produto, veio quebrado.\", \"O filme foi ok, nada de especial.\",\n    # Japanese\n    \"このレストランの料理は本当に美味しいです！\", \"このホテルのサービスはがっかりしました。\", \"天気はまあまあです。\",\n    # Russian\n    \"Я в восторге от этого нового гаджета!\", \"Этот сервис оставил у меня только разочарование.\", \"Встреча была обычной, ничего особенного.\",\n    # French\n    \"J'adore ce restaurant, c'est excellent !\", \"L'attente était trop longue et frustrante.\", \"Le film était moyen, sans plus.\",\n    # Turkish\n    \"Bu otelin manzarasına bayıldım!\", \"Ürün tam bir hayal kırıklığıydı.\", \"Konser fena değildi, ortalamaydı.\",\n    # Italian\n    \"Adoro questo posto, è fantastico!\", \"Il servizio clienti è stato pessimo.\", \"La cena era nella media.\",\n    # Polish\n    \"Uwielbiam tę restaurację, jedzenie jest świetne!\", \"Obsługa klienta była rozczarowująca.\", \"Pogoda jest w porządku, nic szczególnego.\",\n    # Tagalog\n    \"Ang ganda ng lugar na ito, sobrang aliwalas!\", \"Hindi maganda ang serbisyo nila dito.\", \"Maayos lang ang palabas, walang espesyal.\",\n    # Dutch\n    \"Ik ben echt blij met mijn nieuwe aankoop!\", \"De klantenservice was echt slecht.\", \"De presentatie was gewoon oké, niet bijzonder.\",\n    # Malay\n    \"Saya suka makanan di sini, sangat sedap!\", \"Pengalaman ini sangat mengecewakan.\", \"Hari ini cuacanya biasa sahaja.\",\n    # Korean\n    \"이 가게의 케이크는 정말 맛있어요!\", \"서비스가 너무 별로였어요.\", \"날씨가 그저 그렇네요.\",\n    # Swiss German\n    \"Ich find dä Service i de Beiz mega guet!\", \"Däs Esä het mir nöd gfalle.\", \"D Wätter hüt isch so naja.\"\n]\n\nfor text, sentiment in zip(texts, predict_sentiment(texts)):\n    print(f\"Text: {text}\\nSentiment: {sentiment}\\n\")\n```\n\n## Ethical Considerations\n\nSynthetic data reduces bias, but validation in real-world scenarios is advised.\n\n## Citation\n```\nWill be included.\n```\n\n## Contact\n\nFor inquiries, data, private APIs, better models, contact info@tabularis.ai\n\ntabularis.ai",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModelForSequenceClassification",
      "custom_class": null,
      "pipeline_tag": "text-classification",
      "processor": "AutoTokenizer"
    },
    "safetensors": {
      "parameters": {
        "F32": 135328517
      },
      "total": 135328517
    },
    "model_index": null,
    "trending_score": null
  },
  {
    "model_id": "cross-encoder/stsb-roberta-base",
    "model_name": "cross-encoder/stsb-roberta-base",
    "author": "cross-encoder",
    "downloads": 245599,
    "downloads_all_time": null,
    "likes": 4,
    "tags": [
      "transformers",
      "pytorch",
      "jax",
      "safetensors",
      "roberta",
      "text-classification",
      "license:apache-2.0",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/cross-encoder/stsb-roberta-base",
    "dependencies": [
      [
        "sentence_transformers",
        null
      ]
    ],
    "last_modified": "2025-03-06T16:30:26+00:00",
    "created_at": "2022-03-02T23:29:05+00:00",
    "analysis_date": "2025-03-22T00:56:04.691065",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "roberta",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "license": "apache-2.0"
    },
    "card_text": "# Cross-Encoder for Semantic Textual Similarity\nThis model was trained using [SentenceTransformers](https://sbert.net) [Cross-Encoder](https://www.sbert.net/examples/applications/cross-encoder/README.html) class.\n\n## Training Data\nThis model was trained on the [STS benchmark dataset](http://ixa2.si.ehu.eus/stswiki/index.php/STSbenchmark). The model will predict a score between 0 and 1 how for the semantic similarity of two sentences. \n\n\n## Usage and Performance\n\nPre-trained models can be used like this:\n```python\nfrom sentence_transformers import CrossEncoder\n\nmodel = CrossEncoder('cross-encoder/stsb-roberta-base')\nscores = model.predict([('Sentence 1', 'Sentence 2'), ('Sentence 3', 'Sentence 4')])\n```\n\nThe model will predict scores for the pairs `('Sentence 1', 'Sentence 2')` and `('Sentence 3', 'Sentence 4')`.\n\nYou can use this model also without sentence_transformers and by just using Transformers ``AutoModel`` class",
    "card_content": "---\nlicense: apache-2.0\n---\n# Cross-Encoder for Semantic Textual Similarity\nThis model was trained using [SentenceTransformers](https://sbert.net) [Cross-Encoder](https://www.sbert.net/examples/applications/cross-encoder/README.html) class.\n\n## Training Data\nThis model was trained on the [STS benchmark dataset](http://ixa2.si.ehu.eus/stswiki/index.php/STSbenchmark). The model will predict a score between 0 and 1 how for the semantic similarity of two sentences. \n\n\n## Usage and Performance\n\nPre-trained models can be used like this:\n```python\nfrom sentence_transformers import CrossEncoder\n\nmodel = CrossEncoder('cross-encoder/stsb-roberta-base')\nscores = model.predict([('Sentence 1', 'Sentence 2'), ('Sentence 3', 'Sentence 4')])\n```\n\nThe model will predict scores for the pairs `('Sentence 1', 'Sentence 2')` and `('Sentence 3', 'Sentence 4')`.\n\nYou can use this model also without sentence_transformers and by just using Transformers ``AutoModel`` class",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModelForSequenceClassification",
      "custom_class": null,
      "pipeline_tag": "text-classification",
      "processor": "AutoTokenizer"
    },
    "safetensors": {
      "parameters": {
        "I64": 514,
        "F32": 124646401
      },
      "total": 124646915
    },
    "model_index": null,
    "trending_score": null
  },
  {
    "model_id": "jitesh/emotion-english",
    "model_name": "jitesh/emotion-english",
    "author": "jitesh",
    "downloads": 214363,
    "downloads_all_time": null,
    "likes": 7,
    "tags": [
      "transformers",
      "pytorch",
      "safetensors",
      "roberta",
      "text-classification",
      "emotion",
      "20 classes",
      "code",
      "emotions",
      "en",
      "license:mit",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/jitesh/emotion-english",
    "dependencies": [
      [
        "transformers",
        null
      ]
    ],
    "last_modified": "2023-05-09T08:30:32+00:00",
    "created_at": "2023-04-12T04:43:49+00:00",
    "analysis_date": "2025-03-22T00:56:07.191899",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "roberta",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "license": "mit",
      "language": [
        "en"
      ],
      "pipeline_tag": "text-classification",
      "tags": [
        "emotion",
        "20 classes",
        "code",
        "emotions"
      ],
      "widget": [
        {
          "text": "I'm so angry right now. I can't believe he did that to me.",
          "example_title": "anger"
        },
        {
          "text": "I'm feeling disgusted by the smell of this food.",
          "example_title": "disgust"
        },
        {
          "text": "I'm feeling very afraid of what might happen next.",
          "example_title": "fear"
        },
        {
          "text": "I'm so joyful right now! This is the best day of my life.",
          "example_title": "joy"
        },
        {
          "text": "I'm feeling neutral about this situation. I don't really care one way or another.",
          "example_title": "neutral"
        },
        {
          "text": "I'm feeling really sad today after my dog passed away.\"",
          "example_title": "sadness"
        },
        {
          "text": "I'm so surprised by what just happened! I never saw that coming.",
          "example_title": "surprise"
        },
        {
          "text": "I'm feeling cheeky today. I'm going to play a little prank on my friend.",
          "example_title": "cheeky"
        },
        {
          "text": "I'm feeling confused about what to do next. I need some guidance.",
          "example_title": "confuse"
        },
        {
          "text": "I'm feeling curious about the world around me. There's so much to learn!",
          "example_title": "curious"
        },
        {
          "text": "I'm feeling empathetic towards my friend who is going through a tough time.",
          "example_title": "empathetic"
        },
        {
          "text": "I'm feeling grumpy today. Everything is annoying me!",
          "example_title": "grumpy"
        },
        {
          "text": "I'm feeling guilty about what I did. I wish I could take it back.",
          "example_title": "guilty"
        },
        {
          "text": "I'm feeling very energetic today. I'm ready to take on the world!",
          "example_title": "energetic"
        },
        {
          "text": "I'm feeling impatient waiting for this movie to start.",
          "example_title": "impatient"
        },
        {
          "text": "I'm feeling so much love for my family right now. They mean everything to me.",
          "example_title": "love"
        },
        {
          "text": "I'm thinking about my future and what I want to achieve.",
          "example_title": "think"
        },
        {
          "text": "I'm feeling serious about this issue. It's important and needs to be addressed.",
          "example_title": "serious"
        },
        {
          "text": "I'm feeling suspicious of what he's telling me. I think he's hiding something.",
          "example_title": "suspicious"
        },
        {
          "text": "I'm feeling whiny today. Everything is bothering me!",
          "example_title": "whiny"
        },
        {
          "text": "I love football so much",
          "example_title": "love 2"
        },
        {
          "text": "I'm reflecting on my experiences to gain insights",
          "example_title": "think 2"
        },
        {
          "text": "I borrowed money from a friend and haven't paid it back yet. Now I feel ashamed.",
          "example_title": "guilty 2"
        },
        {
          "text": "I'm starting to think that he's up to something.",
          "example_title": "suspicious 2"
        },
        {
          "text": "We need to approach this matter with a sense of purpose",
          "example_title": "serious 2"
        }
      ]
    },
    "card_text": "\n# Emotion classification from 20 classes\n\n## 20 Emotion labels\n| id  | label      |\n| --- | ---------- |\n| 0   | anger      |\n| 1   | cheeky     |\n| 2   | confuse    |\n| 3   | curious    |\n| 4   | disgust    |\n| 5   | empathetic |\n| 6   | energetic  |\n| 7   | fear       |\n| 8   | grumpy     |\n| 9   | guilty     |\n| 10  | impatient  |\n| 11  | joy        |\n| 12  | love       |\n| 13  | neutral    |\n| 14  | sadness    |\n| 15  | serious    |\n| 16  | surprise   |\n| 17  | suspicious |\n| 18  | think      |\n| 19  | whiny      |\n\n\n## How to use\nHere is how to use this model to get the emotion label of a given text:\n\n\n```python\nfrom transformers import AutoModelForSequenceClassification, pipeline\n\nmodel_name = 'jitesh/emotion-english'\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name)\nclassifier = pipeline(\"text-classification\", model=model, tokenizer=model_name)\n\ntext = \"I can't wait any longer \"\n\nprediction = classifier(text)\nprint(prediction[0], text)\n```\n\nThe above code outputs the following line.\n```bash\n{'label': 'impatient', 'score': 0.924211859703064} I can't wait any longer \n```",
    "card_content": "---\nlicense: mit\nlanguage:\n- en\npipeline_tag: text-classification\ntags:\n- emotion\n- 20 classes\n- code\n- emotions\nwidget:\n- text: I'm so angry right now. I can't believe he did that to me.\n  example_title: anger\n- text: I'm feeling disgusted by the smell of this food.\n  example_title: disgust\n- text: I'm feeling very afraid of what might happen next.\n  example_title: fear\n- text: I'm so joyful right now! This is the best day of my life.\n  example_title: joy\n- text: I'm feeling neutral about this situation. I don't really care one way or another.\n  example_title: neutral\n- text: I'm feeling really sad today after my dog passed away.\"\n  example_title: sadness\n- text: I'm so surprised by what just happened! I never saw that coming.\n  example_title: surprise\n- text: I'm feeling cheeky today. I'm going to play a little prank on my friend.\n  example_title: cheeky\n- text: I'm feeling confused about what to do next. I need some guidance.\n  example_title: confuse\n- text: I'm feeling curious about the world around me. There's so much to learn!\n  example_title: curious\n- text: I'm feeling empathetic towards my friend who is going through a tough time.\n  example_title: empathetic\n- text: I'm feeling grumpy today. Everything is annoying me!\n  example_title: grumpy\n- text: I'm feeling guilty about what I did. I wish I could take it back.\n  example_title: guilty\n- text: I'm feeling very energetic today. I'm ready to take on the world!\n  example_title: energetic\n- text: I'm feeling impatient waiting for this movie to start.\n  example_title: impatient\n- text: I'm feeling so much love for my family right now. They mean everything to\n    me.\n  example_title: love\n- text: I'm thinking about my future and what I want to achieve.\n  example_title: think\n- text: I'm feeling serious about this issue. It's important and needs to be addressed.\n  example_title: serious\n- text: I'm feeling suspicious of what he's telling me. I think he's hiding something.\n  example_title: suspicious\n- text: I'm feeling whiny today. Everything is bothering me!\n  example_title: whiny\n- text: I love football so much\n  example_title: love 2\n- text: I'm reflecting on my experiences to gain insights\n  example_title: think 2\n- text: I borrowed money from a friend and haven't paid it back yet. Now I feel ashamed.\n  example_title: guilty 2\n- text: I'm starting to think that he's up to something.\n  example_title: suspicious 2\n- text: We need to approach this matter with a sense of purpose\n  example_title: serious 2\n---\n\n# Emotion classification from 20 classes\n\n## 20 Emotion labels\n| id  | label      |\n| --- | ---------- |\n| 0   | anger      |\n| 1   | cheeky     |\n| 2   | confuse    |\n| 3   | curious    |\n| 4   | disgust    |\n| 5   | empathetic |\n| 6   | energetic  |\n| 7   | fear       |\n| 8   | grumpy     |\n| 9   | guilty     |\n| 10  | impatient  |\n| 11  | joy        |\n| 12  | love       |\n| 13  | neutral    |\n| 14  | sadness    |\n| 15  | serious    |\n| 16  | surprise   |\n| 17  | suspicious |\n| 18  | think      |\n| 19  | whiny      |\n\n\n## How to use\nHere is how to use this model to get the emotion label of a given text:\n\n\n```python\nfrom transformers import AutoModelForSequenceClassification, pipeline\n\nmodel_name = 'jitesh/emotion-english'\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name)\nclassifier = pipeline(\"text-classification\", model=model, tokenizer=model_name)\n\ntext = \"I can't wait any longer \"\n\nprediction = classifier(text)\nprint(prediction[0], text)\n```\n\nThe above code outputs the following line.\n```bash\n{'label': 'impatient', 'score': 0.924211859703064} I can't wait any longer \n```",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModelForSequenceClassification",
      "custom_class": null,
      "pipeline_tag": "text-classification",
      "processor": "AutoTokenizer"
    },
    "safetensors": {
      "parameters": {
        "I64": 514,
        "F32": 82133780
      },
      "total": 82134294
    },
    "model_index": null,
    "trending_score": null
  },
  {
    "model_id": "DiTy/cross-encoder-russian-msmarco",
    "model_name": "DiTy/cross-encoder-russian-msmarco",
    "author": "DiTy",
    "downloads": 206360,
    "downloads_all_time": null,
    "likes": 13,
    "tags": [
      "sentence-transformers",
      "safetensors",
      "bert",
      "text-classification",
      "transformers",
      "rubert",
      "cross-encoder",
      "reranker",
      "msmarco",
      "ru",
      "dataset:unicamp-dl/mmarco",
      "base_model:DeepPavlov/rubert-base-cased",
      "base_model:finetune:DeepPavlov/rubert-base-cased",
      "license:mit",
      "region:us"
    ],
    "card_url": "https://huggingface.co/DiTy/cross-encoder-russian-msmarco",
    "dependencies": [
      [
        "sentence-transformers",
        null
      ],
      [
        "torch",
        null
      ],
      [
        "transformers",
        null
      ]
    ],
    "last_modified": "2024-08-02T06:41:12+00:00",
    "created_at": "2024-04-19T15:24:56+00:00",
    "analysis_date": "2025-03-22T00:56:08.539430",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "bert",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "language": [
        "ru"
      ],
      "library_name": "sentence-transformers",
      "tags": [
        "sentence-transformers",
        "text-classification",
        "transformers",
        "rubert",
        "cross-encoder",
        "reranker",
        "msmarco"
      ],
      "datasets": [
        "unicamp-dl/mmarco"
      ],
      "base_model": "DeepPavlov/rubert-base-cased",
      "widget": [
        {
          "text": "как часто нужно ходить к стоматологу? [SEP] Дядя Женя работает врачем стоматологом.",
          "example_title": "Example 1"
        },
        {
          "text": "как часто нужно ходить к стоматологу? [SEP] Минимальный обязательный срок посещения зубного врача – раз в год, но специалисты рекомендуют делать это чаще – раз в полгода, а ещё лучше – раз в квартал. При таком сроке легко отследить любые начинающиеся проблемы и исправить их сразу же.",
          "example_title": "Example 2"
        }
      ],
      "license": "mit"
    },
    "card_text": "\n# DiTy/cross-encoder-russian-msmarco\n\nThis is a [sentence-transformers](https://www.SBERT.net) model based on a pre-trained [DeepPavlov/rubert-base-cased](https://huggingface.co/DeepPavlov/rubert-base-cased) and finetuned with [MS-MARCO Russian passage ranking dataset](https://huggingface.co/datasets/unicamp-dl/mmarco).\nThe model can be used for Information Retrieval in the Russian language: Given a query, encode the query will all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. See [SBERT.net Retrieve & Re-rank](https://www.sbert.net/examples/applications/retrieve_rerank/README.html) for more details.\n\n<!--- Describe your model here -->\n\n\n## Usage (Sentence-Transformers)\n\nUsing this model becomes easy when you have [sentence-transformers](https://www.SBERT.net) installed:\n\n```\npip install -U sentence-transformers\n```\n\nThen you can use the model like this:\n\n```python\nfrom sentence_transformers import CrossEncoder\n\nreranker_model = CrossEncoder('DiTy/cross-encoder-russian-msmarco', max_length=512, device='cuda')\n\nquery = [\"как часто нужно ходить к стоматологу?\"]\ndocuments = [\n    \"Минимальный обязательный срок посещения зубного врача – раз в год, но специалисты рекомендуют делать это чаще – раз в полгода, а ещё лучше – раз в квартал. При таком сроке легко отследить любые начинающиеся проблемы и исправить их сразу же.\",\n    \"Основная причина заключается в истончении поверхностного слоя зуба — эмали, которая защищает зуб от механических, химических и температурных воздействий. Под эмалью расположен дентин, который более мягкий по своей структуре и пронизан множеством канальцев. При повреждении эмали происходит оголение дентинных канальцев. Раздражение с них начинает передаваться на нервные окончания в зубе и возникают болевые ощущения. Чаще всего дентин оголяется в придесневой области зубов, поскольку эмаль там наиболее тонкая и стирается быстрее.\",\n    \"Стоматолог, также известный как стоматолог-хирург, является медицинским работником, который специализируется на стоматологии, отрасли медицины, специализирующейся на зубах, деснах и полости рта.\",\n    \"Дядя Женя работает врачем стоматологом\",\n    \"Плоды малины употребляют как свежими, так и замороженными или используют для приготовления варенья, желе, мармелада, соков, а также ягодного пюре. Малиновые вина, наливки, настойки, ликёры обладают высокими вкусовыми качествами.\",\n]\n\npredict_result = reranker_model.predict([[query[0], documents[0]]])\nprint(predict_result)\n# `array([0.88126713], dtype=float32)`\n\nrank_result = reranker_model.rank(query[0], documents)\nprint(rank_result)\n# `[{'corpus_id': 0, 'score': 0.88126713},\n#  {'corpus_id': 2, 'score': 0.001042091},\n#  {'corpus_id': 3, 'score': 0.0010417715},\n#  {'corpus_id': 1, 'score': 0.0010344835},\n#  {'corpus_id': 4, 'score': 0.0010244923}]`\n```\n\n\n## Usage (HuggingFace Transformers)\nWithout [sentence-transformers](https://www.SBERT.net), you can use the model like this: First, you pass your input through the transformer model, then you need to get the logits from the model.\n\n```python\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\nmodel = AutoModelForSequenceClassification.from_pretrained('DiTy/cross-encoder-russian-msmarco')\ntokenizer = AutoTokenizer.from_pretrained('DiTy/cross-encoder-russian-msmarco')\n\nfeatures = tokenizer([\"как часто нужно ходить к стоматологу?\", \"как часто нужно ходить к стоматологу?\"], [\"Минимальный обязательный срок посещения зубного врача – раз в год, но специалисты рекомендуют делать это чаще – раз в полгода, а ещё лучше – раз в квартал. При таком сроке легко отследить любые начинающиеся проблемы и исправить их сразу же.\", \"Дядя Женя работает врачем стоматологом\"], padding=True, truncation=True, return_tensors='pt')\n \nmodel.eval()\nwith torch.no_grad():\n    scores = model(**features).logits\n    print(scores)\n# `tensor([[ 1.6871],\n#        [-6.8700]])`\n```",
    "card_content": "---\nlanguage:\n- ru\nlibrary_name: sentence-transformers\ntags:\n- sentence-transformers\n- text-classification\n- transformers\n- rubert\n- cross-encoder\n- reranker\n- msmarco\ndatasets:\n- unicamp-dl/mmarco\nbase_model: DeepPavlov/rubert-base-cased\nwidget:\n- text: как часто нужно ходить к стоматологу? [SEP] Дядя Женя работает врачем стоматологом.\n  example_title: Example 1\n- text: как часто нужно ходить к стоматологу? [SEP] Минимальный обязательный срок\n    посещения зубного врача – раз в год, но специалисты рекомендуют делать это чаще\n    – раз в полгода, а ещё лучше – раз в квартал. При таком сроке легко отследить\n    любые начинающиеся проблемы и исправить их сразу же.\n  example_title: Example 2\nlicense: mit\n---\n\n# DiTy/cross-encoder-russian-msmarco\n\nThis is a [sentence-transformers](https://www.SBERT.net) model based on a pre-trained [DeepPavlov/rubert-base-cased](https://huggingface.co/DeepPavlov/rubert-base-cased) and finetuned with [MS-MARCO Russian passage ranking dataset](https://huggingface.co/datasets/unicamp-dl/mmarco).\nThe model can be used for Information Retrieval in the Russian language: Given a query, encode the query will all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. See [SBERT.net Retrieve & Re-rank](https://www.sbert.net/examples/applications/retrieve_rerank/README.html) for more details.\n\n<!--- Describe your model here -->\n\n\n## Usage (Sentence-Transformers)\n\nUsing this model becomes easy when you have [sentence-transformers](https://www.SBERT.net) installed:\n\n```\npip install -U sentence-transformers\n```\n\nThen you can use the model like this:\n\n```python\nfrom sentence_transformers import CrossEncoder\n\nreranker_model = CrossEncoder('DiTy/cross-encoder-russian-msmarco', max_length=512, device='cuda')\n\nquery = [\"как часто нужно ходить к стоматологу?\"]\ndocuments = [\n    \"Минимальный обязательный срок посещения зубного врача – раз в год, но специалисты рекомендуют делать это чаще – раз в полгода, а ещё лучше – раз в квартал. При таком сроке легко отследить любые начинающиеся проблемы и исправить их сразу же.\",\n    \"Основная причина заключается в истончении поверхностного слоя зуба — эмали, которая защищает зуб от механических, химических и температурных воздействий. Под эмалью расположен дентин, который более мягкий по своей структуре и пронизан множеством канальцев. При повреждении эмали происходит оголение дентинных канальцев. Раздражение с них начинает передаваться на нервные окончания в зубе и возникают болевые ощущения. Чаще всего дентин оголяется в придесневой области зубов, поскольку эмаль там наиболее тонкая и стирается быстрее.\",\n    \"Стоматолог, также известный как стоматолог-хирург, является медицинским работником, который специализируется на стоматологии, отрасли медицины, специализирующейся на зубах, деснах и полости рта.\",\n    \"Дядя Женя работает врачем стоматологом\",\n    \"Плоды малины употребляют как свежими, так и замороженными или используют для приготовления варенья, желе, мармелада, соков, а также ягодного пюре. Малиновые вина, наливки, настойки, ликёры обладают высокими вкусовыми качествами.\",\n]\n\npredict_result = reranker_model.predict([[query[0], documents[0]]])\nprint(predict_result)\n# `array([0.88126713], dtype=float32)`\n\nrank_result = reranker_model.rank(query[0], documents)\nprint(rank_result)\n# `[{'corpus_id': 0, 'score': 0.88126713},\n#  {'corpus_id': 2, 'score': 0.001042091},\n#  {'corpus_id': 3, 'score': 0.0010417715},\n#  {'corpus_id': 1, 'score': 0.0010344835},\n#  {'corpus_id': 4, 'score': 0.0010244923}]`\n```\n\n\n## Usage (HuggingFace Transformers)\nWithout [sentence-transformers](https://www.SBERT.net), you can use the model like this: First, you pass your input through the transformer model, then you need to get the logits from the model.\n\n```python\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\nmodel = AutoModelForSequenceClassification.from_pretrained('DiTy/cross-encoder-russian-msmarco')\ntokenizer = AutoTokenizer.from_pretrained('DiTy/cross-encoder-russian-msmarco')\n\nfeatures = tokenizer([\"как часто нужно ходить к стоматологу?\", \"как часто нужно ходить к стоматологу?\"], [\"Минимальный обязательный срок посещения зубного врача – раз в год, но специалисты рекомендуют делать это чаще – раз в полгода, а ещё лучше – раз в квартал. При таком сроке легко отследить любые начинающиеся проблемы и исправить их сразу же.\", \"Дядя Женя работает врачем стоматологом\"], padding=True, truncation=True, return_tensors='pt')\n \nmodel.eval()\nwith torch.no_grad():\n    scores = model(**features).logits\n    print(scores)\n# `tensor([[ 1.6871],\n#        [-6.8700]])`\n```",
    "library_name": "sentence-transformers",
    "transformers_info": {
      "auto_model": "AutoModelForSequenceClassification",
      "custom_class": null,
      "pipeline_tag": "text-classification",
      "processor": "AutoTokenizer"
    },
    "safetensors": {
      "parameters": {
        "F32": 177854209
      },
      "total": 177854209
    },
    "model_index": null,
    "trending_score": null
  },
  {
    "model_id": "cardiffnlp/twitter-xlm-roberta-base-sentiment-multilingual",
    "model_name": "cardiffnlp/twitter-xlm-roberta-base-sentiment-multilingual",
    "author": "cardiffnlp",
    "downloads": 204246,
    "downloads_all_time": null,
    "likes": 26,
    "tags": [
      "transformers",
      "pytorch",
      "xlm-roberta",
      "text-classification",
      "dataset:cardiffnlp/tweet_sentiment_multilingual",
      "model-index",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment-multilingual",
    "dependencies": [
      [
        "tweetnlp",
        null
      ]
    ],
    "last_modified": "2024-03-24T06:10:17+00:00",
    "created_at": "2022-12-01T00:32:11+00:00",
    "analysis_date": "2025-03-22T00:56:14.291996",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "xlm-roberta",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "datasets": [
        "cardiffnlp/tweet_sentiment_multilingual"
      ],
      "metrics": [
        "f1",
        "accuracy"
      ],
      "pipeline_tag": "text-classification",
      "widget": [
        {
          "text": "Get the all-analog Classic Vinyl Edition of \"Takin Off\" Album from {@herbiehancock@} via {@bluenoterecords@} link below {{URL}}",
          "example_title": "topic_classification 1"
        },
        {
          "text": "Yes, including Medicare and social security saving👍",
          "example_title": "sentiment 1"
        },
        {
          "text": "All two of them taste like ass.",
          "example_title": "offensive 1"
        },
        {
          "text": "If you wanna look like a badass, have drama on social media",
          "example_title": "irony 1"
        },
        {
          "text": "Whoever just unfollowed me you a bitch",
          "example_title": "hate 1"
        },
        {
          "text": "I love swimming for the same reason I love meditating...the feeling of weightlessness.",
          "example_title": "emotion 1"
        },
        {
          "text": "Beautiful sunset last night from the pontoon @TupperLakeNY",
          "example_title": "emoji 1"
        }
      ],
      "model-index": [
        {
          "name": "cardiffnlp/twitter-xlm-roberta-base-sentiment-multilingual",
          "results": [
            {
              "task": {
                "type": "text-classification",
                "name": "Text Classification"
              },
              "dataset": {
                "name": "cardiffnlp/tweet_sentiment_multilingual",
                "type": "all",
                "split": "test"
              },
              "metrics": [
                {
                  "type": "micro_f1_cardiffnlp/tweet_sentiment_multilingual/all",
                  "value": 0.6931034482758621,
                  "name": "Micro F1 (cardiffnlp/tweet_sentiment_multilingual/all)"
                },
                {
                  "type": "micro_f1_cardiffnlp/tweet_sentiment_multilingual/all",
                  "value": 0.692628774202147,
                  "name": "Macro F1 (cardiffnlp/tweet_sentiment_multilingual/all)"
                },
                {
                  "type": "accuracy_cardiffnlp/tweet_sentiment_multilingual/all",
                  "value": 0.6931034482758621,
                  "name": "Accuracy (cardiffnlp/tweet_sentiment_multilingual/all)"
                }
              ]
            }
          ]
        }
      ]
    },
    "card_text": "# cardiffnlp/twitter-xlm-roberta-base-sentiment-multilingual \n\nThis model is a fine-tuned version of [cardiffnlp/twitter-xlm-roberta-base](https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base) on the \n[`cardiffnlp/tweet_sentiment_multilingual (all)`](https://huggingface.co/datasets/cardiffnlp/tweet_sentiment_multilingual) \nvia [`tweetnlp`](https://github.com/cardiffnlp/tweetnlp).\nTraining split is `train` and parameters have been tuned on the validation split `validation`.\n\nFollowing metrics are achieved on the test split `test` ([link](https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment-multilingual/raw/main/metric.json)).\n\n- F1 (micro): 0.6931034482758621\n- F1 (macro): 0.692628774202147\n- Accuracy: 0.6931034482758621\n\n### Usage\nInstall tweetnlp via pip.\n```shell\npip install tweetnlp\n```\nLoad the model in python.\n```python\nimport tweetnlp\nmodel = tweetnlp.Classifier(\"cardiffnlp/twitter-xlm-roberta-base-sentiment-multilingual\", max_length=128)\nmodel.predict('Get the all-analog Classic Vinyl Edition of \"Takin Off\" Album from {@herbiehancock@} via {@bluenoterecords@} link below {{URL}}')\n```\n\n### Reference\n```\n@inproceedings{camacho-collados-etal-2022-tweetnlp,\n    title = \"{T}weet{NLP}: Cutting-Edge Natural Language Processing for Social Media\",\n    author = \"Camacho-collados, Jose  and\n      Rezaee, Kiamehr  and\n      Riahi, Talayeh  and\n      Ushio, Asahi  and\n      Loureiro, Daniel  and\n      Antypas, Dimosthenis  and\n      Boisson, Joanne  and\n      Espinosa Anke, Luis  and\n      Liu, Fangyu  and\n      Mart{\\'\\i}nez C{\\'a}mara, Eugenio\" and others,\n    booktitle = \"Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: System Demonstrations\",\n    month = dec,\n    year = \"2022\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.emnlp-demos.5\",\n    pages = \"38--49\"\n}\n\n```\n\n\n",
    "card_content": "---\ndatasets:\n- cardiffnlp/tweet_sentiment_multilingual\nmetrics:\n- f1\n- accuracy\npipeline_tag: text-classification\nwidget:\n- text: Get the all-analog Classic Vinyl Edition of \"Takin Off\" Album from {@herbiehancock@}\n    via {@bluenoterecords@} link below {{URL}}\n  example_title: topic_classification 1\n- text: Yes, including Medicare and social security saving👍\n  example_title: sentiment 1\n- text: All two of them taste like ass.\n  example_title: offensive 1\n- text: If you wanna look like a badass, have drama on social media\n  example_title: irony 1\n- text: Whoever just unfollowed me you a bitch\n  example_title: hate 1\n- text: I love swimming for the same reason I love meditating...the feeling of weightlessness.\n  example_title: emotion 1\n- text: Beautiful sunset last night from the pontoon @TupperLakeNY\n  example_title: emoji 1\nmodel-index:\n- name: cardiffnlp/twitter-xlm-roberta-base-sentiment-multilingual\n  results:\n  - task:\n      type: text-classification\n      name: Text Classification\n    dataset:\n      name: cardiffnlp/tweet_sentiment_multilingual\n      type: all\n      split: test\n    metrics:\n    - type: micro_f1_cardiffnlp/tweet_sentiment_multilingual/all\n      value: 0.6931034482758621\n      name: Micro F1 (cardiffnlp/tweet_sentiment_multilingual/all)\n    - type: micro_f1_cardiffnlp/tweet_sentiment_multilingual/all\n      value: 0.692628774202147\n      name: Macro F1 (cardiffnlp/tweet_sentiment_multilingual/all)\n    - type: accuracy_cardiffnlp/tweet_sentiment_multilingual/all\n      value: 0.6931034482758621\n      name: Accuracy (cardiffnlp/tweet_sentiment_multilingual/all)\n---\n# cardiffnlp/twitter-xlm-roberta-base-sentiment-multilingual \n\nThis model is a fine-tuned version of [cardiffnlp/twitter-xlm-roberta-base](https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base) on the \n[`cardiffnlp/tweet_sentiment_multilingual (all)`](https://huggingface.co/datasets/cardiffnlp/tweet_sentiment_multilingual) \nvia [`tweetnlp`](https://github.com/cardiffnlp/tweetnlp).\nTraining split is `train` and parameters have been tuned on the validation split `validation`.\n\nFollowing metrics are achieved on the test split `test` ([link](https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment-multilingual/raw/main/metric.json)).\n\n- F1 (micro): 0.6931034482758621\n- F1 (macro): 0.692628774202147\n- Accuracy: 0.6931034482758621\n\n### Usage\nInstall tweetnlp via pip.\n```shell\npip install tweetnlp\n```\nLoad the model in python.\n```python\nimport tweetnlp\nmodel = tweetnlp.Classifier(\"cardiffnlp/twitter-xlm-roberta-base-sentiment-multilingual\", max_length=128)\nmodel.predict('Get the all-analog Classic Vinyl Edition of \"Takin Off\" Album from {@herbiehancock@} via {@bluenoterecords@} link below {{URL}}')\n```\n\n### Reference\n```\n@inproceedings{camacho-collados-etal-2022-tweetnlp,\n    title = \"{T}weet{NLP}: Cutting-Edge Natural Language Processing for Social Media\",\n    author = \"Camacho-collados, Jose  and\n      Rezaee, Kiamehr  and\n      Riahi, Talayeh  and\n      Ushio, Asahi  and\n      Loureiro, Daniel  and\n      Antypas, Dimosthenis  and\n      Boisson, Joanne  and\n      Espinosa Anke, Luis  and\n      Liu, Fangyu  and\n      Mart{\\'\\i}nez C{\\'a}mara, Eugenio\" and others,\n    booktitle = \"Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: System Demonstrations\",\n    month = dec,\n    year = \"2022\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.emnlp-demos.5\",\n    pages = \"38--49\"\n}\n\n```\n\n\n",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModelForSequenceClassification",
      "custom_class": null,
      "pipeline_tag": "text-classification",
      "processor": "AutoTokenizer"
    },
    "safetensors": null,
    "model_index": [
      {
        "name": "cardiffnlp/twitter-xlm-roberta-base-sentiment-multilingual",
        "results": [
          {
            "task": {
              "type": "text-classification",
              "name": "Text Classification"
            },
            "dataset": {
              "name": "cardiffnlp/tweet_sentiment_multilingual",
              "type": "all",
              "split": "test"
            },
            "metrics": [
              {
                "name": "Micro F1 (cardiffnlp/tweet_sentiment_multilingual/all)",
                "type": "micro_f1_cardiffnlp/tweet_sentiment_multilingual/all",
                "value": 0.6931034482758621,
                "verified": false
              },
              {
                "name": "Macro F1 (cardiffnlp/tweet_sentiment_multilingual/all)",
                "type": "micro_f1_cardiffnlp/tweet_sentiment_multilingual/all",
                "value": 0.692628774202147,
                "verified": false
              },
              {
                "name": "Accuracy (cardiffnlp/tweet_sentiment_multilingual/all)",
                "type": "accuracy_cardiffnlp/tweet_sentiment_multilingual/all",
                "value": 0.6931034482758621,
                "verified": false
              }
            ]
          }
        ]
      }
    ],
    "trending_score": null
  },
  {
    "model_id": "MoritzLaurer/ModernBERT-large-zeroshot-v2.0",
    "model_name": "MoritzLaurer/ModernBERT-large-zeroshot-v2.0",
    "author": "MoritzLaurer",
    "downloads": 203941,
    "downloads_all_time": null,
    "likes": 43,
    "tags": [
      "transformers",
      "onnx",
      "safetensors",
      "modernbert",
      "text-classification",
      "generated_from_trainer",
      "base_model:answerdotai/ModernBERT-large",
      "base_model:quantized:answerdotai/ModernBERT-large",
      "license:apache-2.0",
      "autotrain_compatible",
      "endpoints_compatible",
      "region:us"
    ],
    "card_url": "https://huggingface.co/MoritzLaurer/ModernBERT-large-zeroshot-v2.0",
    "dependencies": [
      [
        "transformers",
        "4.48.0.dev0"
      ],
      [
        "pytorch",
        "2.5.1+cu124"
      ],
      [
        "datasets",
        "3.2.0"
      ],
      [
        "tokenizers",
        "0.21.0"
      ]
    ],
    "last_modified": "2025-01-16T10:27:34+00:00",
    "created_at": "2024-12-27T23:13:55+00:00",
    "analysis_date": "2025-03-22T00:56:15.869209",
    "repo_size": null,
    "repo_stars": null,
    "repo_visibility": false,
    "model_type": "modernbert",
    "pipeline_tag": "text-classification",
    "task_categories": null,
    "license": null,
    "card_metadata": {
      "library_name": "transformers",
      "license": "apache-2.0",
      "base_model": "answerdotai/ModernBERT-large",
      "tags": [
        "generated_from_trainer"
      ],
      "metrics": [
        "accuracy"
      ],
      "model-index": [
        {
          "name": "ModernBERT-large-zeroshot-v2.0",
          "results": []
        }
      ]
    },
    "card_text": "\n# ModernBERT-base-zeroshot-v2.0\n\n## Model description\n\nThis model is [answerdotai/ModernBERT-large](https://huggingface.co/answerdotai/ModernBERT-large) \nfine-tuned on the same dataset mix as the `zeroshot-v2.0` models in the [Zeroshot Classifiers Collection](https://huggingface.co/collections/MoritzLaurer/zeroshot-classifiers-6548b4ff407bb19ff5c3ad6f).\n\n\n## General takeaways: \n- The model is very fast and memory efficient. It's multiple times faster and consumes multiple times less memory than DeBERTav3.\nThe memory efficiency enables larger batch sizes. I got a ~2x speed increase by enabling bf16 (instead of fp16).\n- It performs slightly worse then DeBERTav3 on average on the tasks tested below.\n- I'm in the process of preparing a newer version trained on better synthetic data to make full use of the 8k context window\nand to update the training mix of the older `zeroshot-v2.0` models.\n\n\n### Training results\n\n|Datasets|Mean|Mean w/o NLI|mnli_m|mnli_mm|fevernli|anli_r1|anli_r2|anli_r3|wanli|lingnli|wellformedquery|rottentomatoes|amazonpolarity|imdb|yelpreviews|hatexplain|massive|banking77|emotiondair|emocontext|empathetic|agnews|yahootopics|biasframes_sex|biasframes_offensive|biasframes_intent|financialphrasebank|appreviews|hateoffensive|trueteacher|spam|wikitoxic_toxicaggregated|wikitoxic_obscene|wikitoxic_identityhate|wikitoxic_threat|wikitoxic_insult|manifesto|capsotu|\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n|Accuracy|0.85|0.851|0.942|0.944|0.894|0.812|0.717|0.716|0.836|0.909|0.815|0.899|0.964|0.951|0.984|0.814|0.8|0.744|0.752|0.802|0.544|0.899|0.735|0.934|0.864|0.877|0.913|0.953|0.921|0.821|0.989|0.901|0.927|0.931|0.959|0.911|0.497|0.73|\n|F1 macro|0.834|0.835|0.935|0.938|0.882|0.795|0.688|0.676|0.823|0.898|0.814|0.899|0.964|0.951|0.984|0.77|0.753|0.763|0.69|0.805|0.533|0.899|0.729|0.925|0.864|0.877|0.901|0.953|0.855|0.821|0.983|0.901|0.927|0.931|0.952|0.911|0.362|0.662|\n|Inference text/sec (A100 40GB GPU, batch=32)|1116.0|1104.0|1039.0|1241.0|1138.0|1102.0|1124.0|1133.0|1251.0|1240.0|1263.0|1231.0|1054.0|559.0|795.0|1238.0|1312.0|1285.0|1273.0|1268.0|992.0|1222.0|894.0|1176.0|1194.0|1197.0|1206.0|1166.0|1227.0|541.0|1199.0|1045.0|1054.0|1020.0|1005.0|1063.0|1214.0|1220.0|\n\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 9e-06\n- train_batch_size: 16\n- eval_batch_size: 32\n- seed: 42\n- gradient_accumulation_steps: 2\n- total_train_batch_size: 32\n- optimizer: Use adamw_torch with betas=(0.9,0.999) and epsilon=1e-08 and optimizer_args=No additional optimizer arguments\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_ratio: 0.06\n- num_epochs: 2\n\n\n### Framework versions\n\n- Transformers 4.48.0.dev0\n- Pytorch 2.5.1+cu124\n- Datasets 3.2.0\n- Tokenizers 0.21.0\n",
    "card_content": "---\nlibrary_name: transformers\nlicense: apache-2.0\nbase_model: answerdotai/ModernBERT-large\ntags:\n- generated_from_trainer\nmetrics:\n- accuracy\nmodel-index:\n- name: ModernBERT-large-zeroshot-v2.0\n  results: []\n---\n\n# ModernBERT-base-zeroshot-v2.0\n\n## Model description\n\nThis model is [answerdotai/ModernBERT-large](https://huggingface.co/answerdotai/ModernBERT-large) \nfine-tuned on the same dataset mix as the `zeroshot-v2.0` models in the [Zeroshot Classifiers Collection](https://huggingface.co/collections/MoritzLaurer/zeroshot-classifiers-6548b4ff407bb19ff5c3ad6f).\n\n\n## General takeaways: \n- The model is very fast and memory efficient. It's multiple times faster and consumes multiple times less memory than DeBERTav3.\nThe memory efficiency enables larger batch sizes. I got a ~2x speed increase by enabling bf16 (instead of fp16).\n- It performs slightly worse then DeBERTav3 on average on the tasks tested below.\n- I'm in the process of preparing a newer version trained on better synthetic data to make full use of the 8k context window\nand to update the training mix of the older `zeroshot-v2.0` models.\n\n\n### Training results\n\n|Datasets|Mean|Mean w/o NLI|mnli_m|mnli_mm|fevernli|anli_r1|anli_r2|anli_r3|wanli|lingnli|wellformedquery|rottentomatoes|amazonpolarity|imdb|yelpreviews|hatexplain|massive|banking77|emotiondair|emocontext|empathetic|agnews|yahootopics|biasframes_sex|biasframes_offensive|biasframes_intent|financialphrasebank|appreviews|hateoffensive|trueteacher|spam|wikitoxic_toxicaggregated|wikitoxic_obscene|wikitoxic_identityhate|wikitoxic_threat|wikitoxic_insult|manifesto|capsotu|\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n|Accuracy|0.85|0.851|0.942|0.944|0.894|0.812|0.717|0.716|0.836|0.909|0.815|0.899|0.964|0.951|0.984|0.814|0.8|0.744|0.752|0.802|0.544|0.899|0.735|0.934|0.864|0.877|0.913|0.953|0.921|0.821|0.989|0.901|0.927|0.931|0.959|0.911|0.497|0.73|\n|F1 macro|0.834|0.835|0.935|0.938|0.882|0.795|0.688|0.676|0.823|0.898|0.814|0.899|0.964|0.951|0.984|0.77|0.753|0.763|0.69|0.805|0.533|0.899|0.729|0.925|0.864|0.877|0.901|0.953|0.855|0.821|0.983|0.901|0.927|0.931|0.952|0.911|0.362|0.662|\n|Inference text/sec (A100 40GB GPU, batch=32)|1116.0|1104.0|1039.0|1241.0|1138.0|1102.0|1124.0|1133.0|1251.0|1240.0|1263.0|1231.0|1054.0|559.0|795.0|1238.0|1312.0|1285.0|1273.0|1268.0|992.0|1222.0|894.0|1176.0|1194.0|1197.0|1206.0|1166.0|1227.0|541.0|1199.0|1045.0|1054.0|1020.0|1005.0|1063.0|1214.0|1220.0|\n\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 9e-06\n- train_batch_size: 16\n- eval_batch_size: 32\n- seed: 42\n- gradient_accumulation_steps: 2\n- total_train_batch_size: 32\n- optimizer: Use adamw_torch with betas=(0.9,0.999) and epsilon=1e-08 and optimizer_args=No additional optimizer arguments\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_ratio: 0.06\n- num_epochs: 2\n\n\n### Framework versions\n\n- Transformers 4.48.0.dev0\n- Pytorch 2.5.1+cu124\n- Datasets 3.2.0\n- Tokenizers 0.21.0\n",
    "library_name": "transformers",
    "transformers_info": {
      "auto_model": "AutoModelForSequenceClassification",
      "custom_class": null,
      "pipeline_tag": "text-classification",
      "processor": "AutoTokenizer"
    },
    "safetensors": {
      "parameters": {
        "BF16": 395833346
      },
      "total": 395833346
    },
    "model_index": [
      {
        "name": "ModernBERT-large-zeroshot-v2.0",
        "results": []
      }
    ],
    "trending_score": null
  }
]